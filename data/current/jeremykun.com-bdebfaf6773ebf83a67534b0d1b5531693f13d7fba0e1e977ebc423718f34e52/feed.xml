<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>Math ∩ Programming</title>
	<atom:link href="https://jeremykun.com/feed/" rel="self" type="application/rss+xml"/>
	<link>https://jeremykun.com</link>
	<description/>
	<lastBuildDate>Wed, 09 Jun 2021 03:19:24 +0000</lastBuildDate>
	<language>en</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
<cloud domain="jeremykun.com" port="80" path="/?rsscloud=notify" registerProcedure="" protocol="http-post"/>
<image>
		<url>https://secure.gravatar.com/blavatar/ffc08531463d8605aef9e0b51a9ac71f?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</url>
		<title>Math ∩ Programming</title>
		<link>https://jeremykun.com</link>
	</image>
	<atom:link rel="search" type="application/opensearchdescription+xml" href="https://jeremykun.com/osd.xml" title="Math ∩ Programming"/>
	<atom:link rel="hub" href="https://jeremykun.com/?pushpress=hub"/>
	<item>
		<title>Searching for RH Counterexamples — Exploring Data</title>
		<link>https://jeremykun.com/2021/06/14/searching-for-rh-counterexamples-exploring-data/</link>
					<comments>https://jeremykun.com/2021/06/14/searching-for-rh-counterexamples-exploring-data/#respond</comments>
		
		<dc:creator><![CDATA[j2kun]]></dc:creator>
		<pubDate>Mon, 14 Jun 2021 15:00:00 +0000</pubDate>
				<category><![CDATA[General]]></category>
		<category><![CDATA[data visualization]]></category>
		<category><![CDATA[databases]]></category>
		<category><![CDATA[machine learning]]></category>
		<category><![CDATA[mathematics]]></category>
		<category><![CDATA[postgres]]></category>
		<category><![CDATA[programming]]></category>
		<category><![CDATA[python]]></category>
		<category><![CDATA[regression]]></category>
		<category><![CDATA[riemann hypothesis]]></category>
		<category><![CDATA[superabundant numbers]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">http://jeremykun.com/?p=118097</guid>

					<description><![CDATA[We summarize the results of the RH counterexample search, and plot some pictures.]]></description>
										<content:encoded><![CDATA[
<p>We’re ironically searching for counterexamples to the Riemann Hypothesis.</p>



<ol><li><a href="https://jeremykun.com/2020/09/11/searching-for-rh-counterexamples-setting-up-pytest/">Setting up Pytest</a></li><li><a href="https://jeremykun.com/2020/09/11/searching-for-rh-counterexamples-adding-a-database/">Adding a Database</a></li><li><a href="https://jeremykun.com/2020/09/28/searching-for-rh-counterexamples-search-strategies/">Search Strategies</a></li><li><a href="https://jeremykun.com/2020/10/13/searching-for-rh-counterexamples-unbounded-integers/">Unbounded integers</a></li><li><a href="https://jeremykun.com/2021/01/04/searching-for-rh-counterexamples-deploying-with-docker/">Deploying with Docker</a></li><li><a href="https://jeremykun.com/2021/02/02/searching-for-rh-counterexamples-performance-profiling/">Performance Profiling</a></li><li><a href="https://jeremykun.com/2021/02/16/searching-for-rh-counterexamples-scaling-up/">Scaling up</a></li><li><a href="https://jeremykun.com/2021/03/06/searching-for-rh-counterexamples-productionizing/">Productionizing</a></li></ol>



<p>In the last article we added a menagerie of &#8220;production readiness&#8221; features like continuous integration tooling (automating test running and static analysis), alerting, and a simple deployment automation. Then I let it loose on AWS, got <em>extremely</em> busy with buying a house, forgot about this program for a few weeks (no alerts means it worked flawlessly!), and then saw my AWS bill.</p>



<p>So I copied the database off AWS using <a href="https://www.postgresql.org/docs/9.3/app-pgdump.html">pg_dump</a> (piped to gzip), terminated the instances, and inspected the results. A copy of the database is <a href="https://github.com/j2kun/riemann-divisor-sum/blob/main/divisordb-2021-04-28.gz">here</a>. You may need <a href="https://git-lfs.github.com/">git-lfs</a> to clone it. If I wanted to start it back up again, I could spin them back up, and use <code>gunzip | psql</code> to restore the database, and it would start back up from where it left off. A nice benefit of all the software engineering work done thus far.</p>



<p>This article will summarize some of the data, show plots, and try out some exploratory data analysis techniques.</p>



<h2 class="has-text-align-center">Summary</h2>



<p>We stopped the search mid-way through the set of numbers with 136 prime divisors.</p>



<p>The largest number processed was </p>



<p>1255923956750926940807079376257388805204<br>00410625719434151527143279285143764977392<br>49474111379646103102793414829651500824447<br>17178682617437033476033026987651806835743<br>3694669721205424205654368862231754214894<br>07691711699791787732382878164959602478352<br>11435434547040000</p>



<p>Which in factored form is the product of these terms</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
  2^8   3^7   5^4   7^4  11^3  13^3  17^2  19^2  23^2  29^2
 31^2  37^2  41^2  43^1  47^1  53^1  59^1  61^1  67^1  71^1
 73^1  79^1  83^1  89^1  97^1 101^1 103^1 107^1 109^1 113^1
127^1 131^1 137^1 139^1 149^1 151^1 157^1 163^1 167^1 173^1
179^1 181^1 191^1 193^1 197^1 199^1 211^1 223^1 227^1 229^1
233^1 239^1 241^1 251^1 257^1 263^1 269^1 271^1 277^1 281^1
283^1 293^1 307^1 311^1 313^1 317^1 331^1 337^1 347^1 349^1
353^1 359^1 367^1 373^1 379^1 383^1 389^1 397^1 401^1 409^1
419^1 421^1 431^1 433^1 439^1 443^1 449^1 457^1 461^1 463^1
467^1 479^1 487^1 491^1 499^1 503^1 509^1 521^1 523^1 541^1
547^1 557^1 563^1 569^1 571^1 577^1
</pre></div>


<p>The best witness—the number with the largest witness value—was</p>



<p>38824169178385494306912668979787078930475<br>9208283469279319659854547822438432284497<br>11994812030251439907246255647505123032869<br>03750131483244222351596015366602420554736<br>87070007801035106854341150889235475446938<br>52188272225341139870856016797627204990720000</p>



<p>which has witness value 1.7707954880001586, which is still significantly smaller than the needed 1.782 to disprove RH.</p>



<p>The factored form of the best witness is</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
 2^11   3^7   5^4   7^3  11^3  13^2  17^2  19^2  23^2  29^2 
 31^2  37^1  41^1  43^1  47^1  53^1  59^1  61^1  67^1  71^1 
 73^1  79^1  83^1  89^1  97^1 101^1 103^1 107^1 109^1 113^1 
127^1 131^1 137^1 139^1 149^1 151^1 157^1 163^1 167^1 173^1 
179^1 181^1 191^1 193^1 197^1 199^1 211^1 223^1 227^1 229^1 
233^1 239^1 241^1 251^1 257^1 263^1 269^1 271^1 277^1 281^1 
283^1 293^1 307^1 311^1 313^1 317^1 331^1 337^1 347^1 349^1 
353^1 359^1 367^1 373^1 379^1 383^1 389^1 397^1 401^1 409^1 
419^1 421^1 431^1 433^1 439^1 443^1 449^1 457^1 461^1 463^1 
467^1 479^1 487^1 491^1 499^1 503^1 509^1 521^1 523^1 541^1 
547^1 557^1 563^1 
</pre></div>


<p>The average search block took 4m15s to compute, while the max took 7m7s and the min took 36s.</p>



<p>The search ran for about 55 days (hiccups included), starting at 2021-03-05 05:47:53 and stopping at 2021-04-28 15:06:25. The total AWS bill—including development, and periods where the application was broken but the instances still running, and including instances I wasn&#8217;t using but forgot to turn off—was <strong>$380.25</strong>. When the application was running at its peak, the bill worked out to about <strong>$100/month</strong>, though I think I could get it much lower by deploying fewer instances, after we made the performance optimizations that reduced the need for resource-heavy instances. There is also the possibility of using something that integrates more tightly with AWS, such as <a href="https://aws.amazon.com/serverless/">serverless</a> jobs for the cleanup, generate, and process worker jobs.</p>



<h2 class="has-text-align-center">Plots</h2>



<p>When in doubt, plot it out. I started by writing an <a href="https://github.com/j2kun/riemann-divisor-sum/pull/33">export function</a> to get the data into a simpler CSV, which for each <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="n" class="latex" /> only stored <img src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;log(n)" class="latex" /> and the witness value. </p>



<p>I did this once for the final computation results. I&#8217;ll call this the &#8220;small&#8221; database because it only contains the largest witness value in each block. I did it again for an <a href="https://jeremykun.com/2021/02/02/searching-for-rh-counterexamples-performance-profiling/">earlier version</a> of the database before we introduced optimizations (I&#8217;ll call this the &#8220;large&#8221; database), which had all witness values for all superabundant numbers processed up to 80 prime factors.. The small database was only a few dozen megabytes in size, and the large database was ~40 GiB, so I had to use <a href="https://www.postgresql.org/docs/9.2/plpgsql-cursors.html">postgres cursors</a> to avoid loading the large database into memory. Moreover, then generated CSV was about 8 GiB in size, and so it required a few extra steps to sort it, and get it into a format that could be plotted in a reasonable amount of time.</p>



<p>First, using GNU <code>sort</code> to sort the file by the first column, <img src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;log(n)" class="latex" /> </p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
sort -t , -n -k 1 divisor_sums.csv -o divisor_sums_sorted.csv
</pre></div>


<p>Then, I needed to do some simple operations on massive CSV files, including computing a cumulative max, and filtering down to a subset of rows that are sufficient for plotting. After trying to use pandas and vaex, I realized that the old <a href="https://en.wikipedia.org/wiki/AWK">awk</a> command line tool would be great at this job. So I wrote a simple awk script to process the data, and compute data used for the cumulative max witness value plots below.</p>



<p>Then finally we can use <code><a href="https://vaex.io/docs/index.html">vaex</a></code> to create two plots. The first is a heatmap of witness value counts. The second is a plot of the cumulative max witness value. For the large database:</p>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/05/witness_value_less_squinty.png"><img data-attachment-id="118111" data-permalink="https://jeremykun.com/witness_value_less_squinty/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_less_squinty.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="witness_value_less_squinty" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_less_squinty.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_less_squinty.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/witness_value_less_squinty.png?w=1024" alt="" class="wp-image-118111" srcset="https://jeremykun.files.wordpress.com/2021/05/witness_value_less_squinty.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/witness_value_less_squinty.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/witness_value_less_squinty.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/witness_value_less_squinty.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/05/witness_value_less_squinty.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>Witness value heatmap for the large database</figcaption></figure>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge_2021-01-27.png"><img data-attachment-id="118122" data-permalink="https://jeremykun.com/witness_value_ridge_2021-01-27/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge_2021-01-27.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="witness_value_ridge_2021-01-27" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge_2021-01-27.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge_2021-01-27.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge_2021-01-27.png?w=1024" alt="" class="wp-image-118122" srcset="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge_2021-01-27.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge_2021-01-27.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge_2021-01-27.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge_2021-01-27.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge_2021-01-27.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>The cumulative maximum witness value for the large database.</figcaption></figure>



<p>And for the small database</p>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/05/witness_value_scatter.png"><img data-attachment-id="118113" data-permalink="https://jeremykun.com/witness_value_scatter/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_scatter.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="witness_value_scatter" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_scatter.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_scatter.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/witness_value_scatter.png?w=1024" alt="" class="wp-image-118113" srcset="https://jeremykun.files.wordpress.com/2021/05/witness_value_scatter.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/witness_value_scatter.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/witness_value_scatter.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/witness_value_scatter.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/05/witness_value_scatter.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>A heatmap for the witness values for the small database</figcaption></figure>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge.png"><img data-attachment-id="118115" data-permalink="https://jeremykun.com/witness_value_ridge/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="witness_value_ridge" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge.png?w=1024" alt="" class="wp-image-118115" srcset="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>The cumulative maximum witness value for the small database.</figcaption></figure>



<p>Note, the two ridges disagree slightly (the large database shows a longer flat line than the small database for the same range), because of the way that the superabundant enumeration doesn&#8217;t go in increasing order of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="n" class="latex" />. So larger witness values in the range 400-500 are found later.</p>



<h2 class="has-text-align-center">Estimating the max witness value growth rate</h2>



<p>The next obvious question is whether we can fit the curves above to provide an estimate of how far we might have to look to find the first witness value that exceeds the desired 1.782 threshold. Of course, this will obviously depend on the appropriateness of the underlying model.</p>



<p>A simple first guess would be split between two options: the real data is asymptotic like <img src="https://s0.wp.com/latex.php?latex=a+%2B+b+%2F+x&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="a + b / x" class="latex" /> approaching some number less than 1.782 (and hence this approach cannot disprove RH), or the real data grows slowly (perhaps doubly-logarithmic) like <img src="https://s0.wp.com/latex.php?latex=a+%2B+b+%5Clog+%5Clog+x&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="a + b &#92;log &#92;log x" class="latex" />, but eventually surpasses 1.782 (and RH is false). For both cases, we can use <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html">scipy&#8217;s curve fitting routine</a> as in this <a href="https://github.com/j2kun/riemann-divisor-sum/pull/35">pull request</a>.</p>



<p>For the large database (roughly using log n &lt; 400 since that&#8217;s when the curve flatlines due to the enumeration order), we get a reciprocal fit of</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f%28x%29+%5Capprox+1.77579122+-+2.72527824+%2F+x&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle f(x) &#92;approx 1.77579122 - 2.72527824 / x" class="latex" /></p>



<p>and a logarithmic fit of </p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f%28x%29+%5Capprox+1.65074314+%2B+0.06642373+%5Clog%28%5Clog%28x%29%29&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle f(x) &#92;approx 1.65074314 + 0.06642373 &#92;log(&#92;log(x))" class="latex" /> </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><a href="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png"><img loading="lazy" data-attachment-id="118139" data-permalink="https://jeremykun.com/divisor-sums-2021-01-27-reciprocal/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="divisor-sums-2021-01-27-reciprocal" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png?w=1024" alt="" class="wp-image-118139" width="611" height="458" srcset="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png?w=611 611w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png?w=1222 1222w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png?w=768 768w" sizes="(max-width: 611px) 100vw, 611px" /></a><figcaption>The fit of the large database to a + b/x. Note the asymptote of 1.7757 suggests this will not disprove RH.</figcaption></figure></div>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><a href="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png"><img loading="lazy" data-attachment-id="118147" data-permalink="https://jeremykun.com/divisor-sums-2021-01-27-log-1/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="divisor-sums-2021-01-27-log-1" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png?w=1024" alt="" class="wp-image-118147" width="602" height="452" srcset="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png?w=602 602w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png?w=1204 1204w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png?w=768 768w" sizes="(max-width: 602px) 100vw, 602px" /></a><figcaption>The fit of the large database to a + b log log x. If this is accurate, we would find the counterexample around log(n) = 1359.</figcaption></figure></div>



<p>The estimated asymptote is around 1.7757 in the first case, and the second case estimates we&#8217;d find an RH counterexample at around <img src="https://s0.wp.com/latex.php?latex=log%28n%29+%3D+1359&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="log(n) = 1359" class="latex" />.</p>



<p>For the small database of only sufficiently large witness values, this time going up to about <img src="https://s0.wp.com/latex.php?latex=log%28n%29+%5Capprox+575&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="log(n) &#92;approx 575" class="latex" />, the asymptotic approximation is</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+1.77481154+-2.31226382+%2F+x&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle 1.77481154 -2.31226382 / x" class="latex" /></p>



<p>And the logarithmic approximation is</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+1.70825262+%2B+0.03390312+%5Clog%28%5Clog%28x%29%29&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle 1.70825262 + 0.03390312 &#92;log(&#92;log(x))" class="latex" /></p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><a href="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png"><img loading="lazy" data-attachment-id="118143" data-permalink="https://jeremykun.com/divisor-sums-2021-04-28-reciprocal/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="divisor-sums-2021-04-28-reciprocal" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png?w=1024" alt="" class="wp-image-118143" width="647" height="486" srcset="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png?w=647 647w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png 1280w" sizes="(max-width: 647px) 100vw, 647px" /></a><figcaption>The reciprocal approximation of the small database with asymptote 1.77481154</figcaption></figure></div>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><a href="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png"><img loading="lazy" data-attachment-id="118148" data-permalink="https://jeremykun.com/divisor-sums-2021-04-28-log-1/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="divisor-sums-2021-04-28-log-1" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png?w=1024" alt="" class="wp-image-118148" width="645" height="484" srcset="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png?w=645 645w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png 1280w" sizes="(max-width: 645px) 100vw, 645px" /></a><figcaption>The logarithmic approximation of the small database with RH counterexample estimate at log(n) = 6663</figcaption></figure></div>



<p>Now the asymptote is slightly lower, at 1.7748, and the logarithmic model approximates the counterexample can be found at approximately <img src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29+%3D+6663&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;log(n) = 6663" class="latex" />.</p>



<p>Both of the logarithmic approximations suggest that if we want to find an RH counterexample, we would need to look at numbers with thousands of prime factors. The first estimate puts a counterexample at about <img src="https://s0.wp.com/latex.php?latex=2%5E%7B1960%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="2^{1960}" class="latex" />, the second at <img src="https://s0.wp.com/latex.php?latex=2%5E%7B9612%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="2^{9612}" class="latex" />, so let&#8217;s say between 1k and 10k prime factors.</p>



<p>Luckily, we can actually jump forward in the superabundant enumeration to exactly the set of candidates with <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="m" class="latex" /> prime factors. So it might make sense to jump ahead to, say, 5k prime factors and search in that region. However, the size of a level set of the superabundant enumeration still grows exponentially in <img src="https://s0.wp.com/latex.php?latex=m&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="m" class="latex" />. Perhaps we should (heuristically) narrow down the search space by looking for patterns in the distribution of prime factors for the best witness values we&#8217;ve found thus far. Perhaps the values of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="n" class="latex" /> with the best witness values tend to have a certain concentration of prime factors.</p>



<h2 class="has-text-align-center">Exploring prime factorizations</h2>



<p>At first, my thought was to take the largest witness values, look at their prime factorizations, and try to see a pattern when compared to smaller witness values. However, other than the obvious fact that the larger witness values correspond to larger numbers (more and larger prime factors), I didn&#8217;t see an obvious pattern from squinting at plots.</p>



<p>To go in a completely different direction, I wanted to try out the <a href="https://umap-learn.readthedocs.io/">UMAP</a> software package, a very nice and mathematically sophisticated for high dimensional data visualization. It&#8217;s properly termed a <em>dimensionality reduction</em> technique, meaning it takes as input a high-dimensional set of data, and produces as output a low-dimensional embedding of that data that tries to maintain the same shape as the input, where &#8220;shape&#8221; is in the sense of a certain Riemannian metric inferred from the high dimensional data. If there is structure among the prime factorizations, then UMAP should plot a pretty picture, and perhaps that will suggest some clearer approach.</p>



<p>To apply this to the RH witness value dataset, we can take each pair <img src="https://s0.wp.com/latex.php?latex=%28n%2C+%5Csigma%28n%29%2F%28n+%5Clog+%5Clog+n%29%29&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="(n, &#92;sigma(n)/(n &#92;log &#92;log n))" class="latex" />, and associate that with a new (high dimensional) data point corresponding to the witness value paired with the number&#8217;s prime factorization</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%5Csigma%28n%29%2F%28n+%5Clog+%5Clog+n%29%2C+k_1%2C+k_2%2C+%5Cdots%2C+k_d%29&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle (&#92;sigma(n)/(n &#92;log &#92;log n), k_1, k_2, &#92;dots, k_d)" class="latex" />,</p>



<p>where <img src="https://s0.wp.com/latex.php?latex=n+%3D+2%5E%7Bk_1%7D+3%5E%7Bk_2%7D+5%5E%7Bk_3%7D+%5Cdots+p_d%5E%7Bk_d%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="n = 2^{k_1} 3^{k_2} 5^{k_3} &#92;dots p_d^{k_d}" class="latex" />, with zero-exponents included so that all points have the same dimension. This <a href="https://github.com/j2kun/riemann-divisor-sum/pull/34">pull request</a> adds the ability to factorize and export the witness values to a CSV file as specified, and <a href="https://github.com/j2kun/riemann-divisor-sum/pull/36">this pull request</a> adds the CSV data (using <a href="https://git-lfs.github.com/">git-lfs</a>), along with the script to run UMAP, the resulting plots shown below, and the saved embeddings as <code>.npy</code> files (numpy arrays).</p>



<p>When we do nothing special to the data and run it through UMAP we see this plot.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://jeremykun.files.wordpress.com/2021/05/umap_1.png"><img data-attachment-id="118151" data-permalink="https://jeremykun.com/umap_1/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/umap_1.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="umap_1" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/umap_1.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/umap_1.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/umap_1.png?w=1024" alt="" class="wp-image-118151" srcset="https://jeremykun.files.wordpress.com/2021/05/umap_1.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/umap_1.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/umap_1.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/umap_1.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/05/umap_1.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>UMAP plotted on the raw prime factorization and witness value dataset.</figcaption></figure></div>



<p>It looks cool, but if you stare at it for long enough (and if you zoom in when you generate the plot yourself in matplotlib) you can convince yourself that it&#8217;s not finding much useful structure. The red dots dominate (lower witness values) and the blue dots are kind of spread haphazardly throughout the red regions. The &#8220;ridges&#8221; along the chart are probably due to how the superabundant enumeration skips lots of numbers, and that&#8217;s why it thins out on one end: the thinning out corresponds to fewer numbers processed that are that large since the enumeration is not uniform.</p>



<p>It also seemed like there is too much data. The plot above has some 80k points on it. After filtering down to just those points whose witness values are bigger than 1.769, we get a more manageable plot.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://jeremykun.files.wordpress.com/2021/05/umap_2.png"><img data-attachment-id="118153" data-permalink="https://jeremykun.com/umap_2/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/umap_2.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="umap_2" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/umap_2.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/umap_2.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/umap_2.png?w=1024" alt="" class="wp-image-118153" srcset="https://jeremykun.files.wordpress.com/2021/05/umap_2.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/umap_2.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/umap_2.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/umap_2.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/05/umap_2.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>Witness values and prime factors processed with UMAP, where the witness value is at least 1.769.</figcaption></figure></div>



<p>This is a bit more reasonable. You can see a stripe of blue dots going through the middle of the plot.</p>



<p>Before figuring out how that blue ridge might relate to the prime factor patterns, let&#8217;s take this a few steps further. Typically in machine learning contexts, it helps to normalize your data, i.e., to transform each input dimension into a standard Z-score with respect to the set of values seen in that dimension, subtracting the mean and dividing by the standard deviation. Since the witness values are so close to each other, they&#8217;re a good candidate for such normalization. Here&#8217;s what UMAP plots when we normalize the witness value column only.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://jeremykun.files.wordpress.com/2021/05/umap_2_normalized.png"><img data-attachment-id="118156" data-permalink="https://jeremykun.com/umap_2_normalized/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/umap_2_normalized.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="umap_2_normalized" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/umap_2_normalized.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/umap_2_normalized.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/umap_2_normalized.png?w=1024" alt="" class="wp-image-118156" srcset="https://jeremykun.files.wordpress.com/2021/05/umap_2_normalized.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/umap_2_normalized.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/umap_2_normalized.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/umap_2_normalized.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/05/umap_2_normalized.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>UMAP applied to the (normalized) witness values and prime factorizations. Applied to all witness values.</figcaption></figure></div>



<p>Now this is a bit more interesting! Here the colormap on the right is in units of standard deviation of witness values. You can see a definite bluest region, and it appears that the data is organized into long brushstrokes, where the witness values increase as you move from one end of the stroke to the other. At worst, this suggests that the dataset has structure that a learning algorithm could discover.</p>



<p>Going even one step further, what if we normalize <em>all</em> the columns? Well, it&#8217;s not as interesting.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns-1.png"><img data-attachment-id="118170" data-permalink="https://jeremykun.com/umap_3_normalized_all_columns-1/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns-1.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="umap_3_normalized_all_columns-1" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns-1.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns-1.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns-1.png?w=1024" alt="" class="wp-image-118170" srcset="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns-1.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns-1.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns-1.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns-1.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns-1.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>UMAP when normalizing all columns, not just the witness value.</figcaption></figure></div>



<p>If you zoom in, you can see that the same sort of &#8220;brushstroke&#8221; idea is occurring here too, with blue on one end and red on the other. It&#8217;s just harder to see.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns_zoomed-1.png"><img data-attachment-id="118171" data-permalink="https://jeremykun.com/umap_3_normalized_all_columns_zoomed-1/" data-orig-file="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns_zoomed-1.png" data-orig-size="1280,960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="umap_3_normalized_all_columns_zoomed-1" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns_zoomed-1.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns_zoomed-1.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns_zoomed-1.png?w=1024" alt="" class="wp-image-118171" srcset="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns_zoomed-1.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns_zoomed-1.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns_zoomed-1.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns_zoomed-1.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns_zoomed-1.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>The previous image, zoomed in around a cluster of data</figcaption></figure></div>



<p>We would like to study the prettiest picture and see if we can determine what pattern of prime numbers the blue region has, if any. The embedding files are stored on github, and I put up (one version of) the <a href="http://j2kun.github.io/riemann-divisor-sum/umap_viz.html">UMAP visualization as an interactive plot</a> via this <a href="https://github.com/j2kun/riemann-divisor-sum/pull/37">pull request</a>.</p>



<p>I&#8217;ve been sitting on this draft for a while, and while this article didn&#8217;t make a ton of headway, the pictures will have to do while I&#8217;m still dealing with my new home purchase.</p>



<p>Some ideas for next steps:</p>



<ul><li>Squint harder at the distributions of primes for the largest witness values in comparison to the rest.</li><li>See if a machine learning algorithm can regress witness values based on their prime factorizations (and any other useful features I can derive). Study the resulting hypothesis to determine which features are the most important. Use that to refine the search strategy.</li><li>Try searching randomly in the superabundant enumeration around 1k and 10k prime factors, and see if the best witness values found there match the log-log regression.</li><li>Since witness values above a given threshold seem to be quite common, and because the UMAP visualization shows some possible &#8220;locality&#8221; structure for larger witness values, it suggests if there is a counterexample to RH then there are probably many. So a local search method (e.g., local neighborhood search/discrete gradient ascent with random restarts) might allow us to get a better sense for whether we are on the right track.</li></ul>



<p>Until next time!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://jeremykun.com/2021/06/14/searching-for-rh-counterexamples-exploring-data/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/90b179348780a6e7fe8e502968dc534a?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">jeremykun</media:title>
		</media:content>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/witness_value_less_squinty.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge_2021-01-27.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/witness_value_scatter.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/witness_value_ridge.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-reciprocal.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-01-27-log-1.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-reciprocal.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/divisor-sums-2021-04-28-log-1.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/umap_1.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/umap_2.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/umap_2_normalized.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns-1.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/05/umap_3_normalized_all_columns_zoomed-1.png?w=1024" medium="image"/>
	</item>
		<item>
		<title>Regression and Linear Combinations</title>
		<link>https://jeremykun.com/2021/03/29/regression-and-linear-combinations/</link>
					<comments>https://jeremykun.com/2021/03/29/regression-and-linear-combinations/#comments</comments>
		
		<dc:creator><![CDATA[j2kun]]></dc:creator>
		<pubDate>Mon, 29 Mar 2021 16:00:00 +0000</pubDate>
				<category><![CDATA[General]]></category>
		<category><![CDATA[gradient descent]]></category>
		<category><![CDATA[kernelization]]></category>
		<category><![CDATA[linear algebra]]></category>
		<category><![CDATA[linear combination]]></category>
		<category><![CDATA[linear regression]]></category>
		<category><![CDATA[mathematics]]></category>
		<category><![CDATA[programming]]></category>
		<guid isPermaLink="false">http://jeremykun.com/?p=118056</guid>

					<description><![CDATA[Recently I&#8217;ve been helping out with a linear algebra course organized by Tai-Danae Bradley and Jack Hidary, and one of the questions that came up a few times was, &#8220;why should programmers care about the concept of a linear combination?&#8221; For those who don&#8217;t know, given vectors , a linear combination of the vectors is [&#8230;]]]></description>
										<content:encoded><![CDATA[
<p>Recently I&#8217;ve been helping out with a linear algebra course organized by <a href="https://www.math3ma.com/about">Tai-Danae Bradley</a> and <a href="https://hidaryfoundation.org/about/">Jack Hidary</a>, and one of the questions that came up a few times was, &#8220;why should programmers care about the concept of a linear combination?&#8221;</p>



<p>For those who don&#8217;t know, given vectors <img src="https://s0.wp.com/latex.php?latex=v_1%2C+%5Cdots%2C+v_n&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="v_1, &#92;dots, v_n" class="latex" />, a <em>linear combination</em> of the vectors is a choice of some coefficients <img src="https://s0.wp.com/latex.php?latex=a_i&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="a_i" class="latex" /> with which to weight the vectors in a sum <img src="https://s0.wp.com/latex.php?latex=v+%3D+%5Csum_%7Bi%3D1%7D%5En+a_i+v_i&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="v = &#92;sum_{i=1}^n a_i v_i" class="latex" />.</p>



<p>I must admit, math books do a poor job of painting the concept as more than theoretical—perhaps linear combinations are only needed for proofs, while the real meat is in matrix multiplication and cross products. But no, linear combinations truly lie at the heart of many practical applications.</p>



<p>In some cases, the <em>entire goal</em> of an algorithm is to find a &#8220;useful&#8221; linear combination of a set of vectors. The vectors are the building blocks (often a vector space or subspace basis), and the set of linear combinations are the legal ways to combine the blocks. Simpler blocks admit easier and more efficient algorithms, but their linear combinations are less expressive. Hence, a tradeoff. </p>



<p>A concrete example is regression. Most people think of regression in terms of <a href="https://jeremykun.com/2013/08/18/linear-regression/">linear regression</a>. You&#8217;re looking for a linear function like <img src="https://s0.wp.com/latex.php?latex=y+%3D+mx%2Bb&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="y = mx+b" class="latex" /> that approximates some data well. For multiple variables, you have, e.g., <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D+%3D+%28x_1%2C+x_2%2C+x_3%29&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;mathbf{x} = (x_1, x_2, x_3)" class="latex" /> as a vector of input variables, and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D+%3D+%28w_1%2C+w_2%2C+w_3%29&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;mathbf{w} = (w_1, w_2, w_3)" class="latex" /> as a vector of weights, and the function is <img src="https://s0.wp.com/latex.php?latex=y+%3D+%5Cmathbf%7Bw%7D%5ET+%5Cmathbf%7Bx%7D+%2B+b&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="y = &#92;mathbf{w}^T &#92;mathbf{x} + b" class="latex" />. </p>



<p>To avoid the shift by <img src="https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="b" class="latex" /> (which makes the function affine instead of purely linear; formulas of purely linear functions are easier to work with because the shift is like a pesky special case you have to constantly account for), authors often add a fake input variable <img src="https://s0.wp.com/latex.php?latex=x_0&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="x_0" class="latex" /> which is always fixed to 1, and relabel <img src="https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="b" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=w_0&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="w_0" class="latex" /> to get <img src="https://s0.wp.com/latex.php?latex=y+%3D+%5Cmathbf%7Bw%7D%5ET+%5Cmathbf%7Bx%7D+%3D+%5Csum_i+w_i+x_i&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="y = &#92;mathbf{w}^T &#92;mathbf{x} = &#92;sum_i w_i x_i" class="latex" /> as the final form. The optimization problem to solve becomes the following, where your data set to approximate is <img src="https://s0.wp.com/latex.php?latex=%5C%7B+%5Cmathbf%7Bx%7D_1%2C+%5Cdots%2C+%5Cmathbf%7Bx%7D_k+%5C%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;{ &#92;mathbf{x}_1, &#92;dots, &#92;mathbf{x}_k &#92;}" class="latex" />. </p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin_w+%5Csum_%7Bi%3D1%7D%5Ek+%28y_i+-+%5Cmathbf%7Bw%7D%5ET+%5Cmathbf%7Bx%7D_i%29%5E2&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle &#92;min_w &#92;sum_{i=1}^k (y_i - &#92;mathbf{w}^T &#92;mathbf{x}_i)^2" class="latex" /></p>



<p>In this case, the function being learned—the output of the regression—doesn&#8217;t look like a linear combination. Technically it is, just not in an interesting way.</p>



<p>It becomes more obviously related to linear combinations when you try to model non-linearity. The idea is to define a class of functions called <em>basis functions</em> <img src="https://s0.wp.com/latex.php?latex=B+%3D+%5C%7B+f_1%2C+%5Cdots%2C+f_m+%5Cmid+f_i%3A+%5Cmathbb%7BR%7D%5En+%5Cto+%5Cmathbb%7BR%7D+%5C%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="B = &#92;{ f_1, &#92;dots, f_m &#92;mid f_i: &#92;mathbb{R}^n &#92;to &#92;mathbb{R} &#92;}" class="latex" />, and allow your approximation to be any linear combination of functions in <img src="https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="B" class="latex" />, i.e., any function in the span of B.</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%28%5Cmathbf%7Bx%7D%29+%3D+%5Csum_%7Bi%3D1%7D%5Em+w_i+f_i%28%5Cmathbf%7Bx%7D%29&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle &#92;hat{f}(&#92;mathbf{x}) = &#92;sum_{i=1}^m w_i f_i(&#92;mathbf{x})" class="latex" /></p>



<p>Again, instead of weighting each coordinate of the input vector with a <img src="https://s0.wp.com/latex.php?latex=w_i&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="w_i" class="latex" />, we&#8217;re weighting each basis function&#8217;s contribution (when given the whole input vector) to the output. If the basis functions were to output a single coordinate (<img src="https://s0.wp.com/latex.php?latex=f_i%28%5Cmathbf%7Bx%7D%29+%3D+x_i&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="f_i(&#92;mathbf{x}) = x_i" class="latex" />), we would be back to linear regression.</p>



<p>Then the optimization problem is to choose the weights to minimize the error of the approximation.</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin_w+%5Csum_%7Bj%3D1%7D%5Ek+%28y_j+-+%5Chat%7Bf%7D%28%5Cmathbf%7Bx%7D_j%29%29%5E2&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle &#92;min_w &#92;sum_{j=1}^k (y_j - &#92;hat{f}(&#92;mathbf{x}_j))^2" class="latex" /></p>



<p>As an example, let&#8217;s say that we wanted to do regression with a basis of quadratic polynomials. Our basis for three input variables might look like</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7B+1%2C+x_1%2C+x_2%2C+x_3%2C+x_1x_2%2C+x_1x_3%2C+x_2x_3%2C+x_1%5E2%2C+x_2%5E2%2C+x_3%5E2+%5C%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle &#92;{ 1, x_1, x_2, x_3, x_1x_2, x_1x_3, x_2x_3, x_1^2, x_2^2, x_3^2 &#92;}" class="latex" /></p>



<p>Any quadratic polynomial in three variables can be written as a linear combination of these basis functions. Also note that if we treat this as the basis of a vector space, then a vector is a tuple of 10 numbers—the ten coefficients in the polynomial. It&#8217;s the same as <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B10%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;mathbb{R}^{10}" class="latex" />, just with a different interpretation of what the vector&#8217;s entries mean. With that, we can see how we would compute dot products, projections, and other nice things, though they may not have quite the same geometric sensibility.</p>



<p>These are not the usual basis functions used for polynomial regression in practice (see the note at the end of this article), but we can already do some damage in writing regression algorithms.</p>



<h2 class="has-text-align-center">A simple stochastic gradient descent</h2>



<p>Although there is a <a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote14.html">closed form solution</a> to many regression problems (including the quadratic regression problem, though with a slight twist), gradient descent is a simple enough solution to showcase how an optimization solver can find a useful linear combination. This code will be written in Python 3.9. <a href="https://github.com/j2kun/regression-linear-combinations">It&#8217;s on Github.</a></p>



<p>First we start with some helpful type aliases</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate">
from typing import Callable, Tuple, List

Input = Tuple&#91;float, float, float]
Coefficients = List&#91;float]
Gradient = List&#91;float]
Hypothesis = Callable&#91;&#91;Input], float]
Dataset = List&#91;Tuple&#91;Input, float]]
</pre></div>


<p>Then define a simple wrapper class for our basis functions</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate">
class QuadraticBasisPolynomials:
    def __init__(self):
        self.basis_functions = &#91;
            lambda x: 1,
            lambda x: x&#91;0],
            lambda x: x&#91;1],
            lambda x: x&#91;2],
            lambda x: x&#91;0] * x&#91;1],
            lambda x: x&#91;0] * x&#91;2],
            lambda x: x&#91;1] * x&#91;2],
            lambda x: x&#91;0] * x&#91;0],
            lambda x: x&#91;1] * x&#91;1],
            lambda x: x&#91;2] * x&#91;2],
        ]

    def __getitem__(self, index):
        return self.basis_functions&#91;index]

    def __len__(self):
        return len(self.basis_functions)

    def linear_combination(self, weights: Coefficients) -&gt; Hypothesis:
        def combined_function(x: Input) -&gt; float:
            return sum(
                w * f(x)
                for (w, f) in zip(weights, self.basis_functions)
            )

        return combined_function

basis = QuadraticBasisPolynomials()
</pre></div>


<p>The <code>linear_combination</code> function returns a function that computes the weighted sum of the basis functions. Now we can define the error on a dataset, as well as for a single point</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate">
def total_error(weights: Coefficients, data: Dataset) -&gt; float:
    hypothesis = basis.linear_combination(weights)
    return sum(
        (actual_output - hypothesis(example)) ** 2
        for (example, actual_output) in data
    )


def single_point_error(
        weights: Coefficients, point: Tuple&#91;Input, float]) -&gt; float:
    return point&#91;1] - basis.linear_combination(weights)(point&#91;0])
</pre></div>


<p>We can then define the gradient of the error function with respect to the weights and a single data point. Recall, the error function is defined as</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+E%28%5Cmathbf%7Bw%7D%29+%3D+%5Csum_%7Bj%3D1%7D%5Ek+%28y_j+-+%5Chat%7Bf%7D%28%5Cmathbf%7Bx%7D_j%29%29%5E2&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle E(&#92;mathbf{w}) = &#92;sum_{j=1}^k (y_j - &#92;hat{f}(&#92;mathbf{x}_j))^2" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;hat{f}" class="latex" /> is a linear combination of basis functions </p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Cmathbf%7Bx%7D_j%29+%3D+%5Csum_%7Bs%3D1%7D%5En+w_s+f_s%28%5Cmathbf%7Bx%7D_j%29&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;hat{f}(&#92;mathbf{x}_j) = &#92;sum_{s=1}^n w_s f_s(&#92;mathbf{x}_j)" class="latex" /></p>



<p>Since we&#8217;ll do stochastic gradient descent, the error formula is a bit simpler. We compute it not for the whole data set but only a single random point at a time. So the error is</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+E%28%5Cmathbf%7Bw%7D%29+%3D+%28y_j+-+%5Chat%7Bf%7D%28%5Cmathbf%7Bx%7D_j%29%29%5E2&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle E(&#92;mathbf{w}) = (y_j - &#92;hat{f}(&#92;mathbf{x}_j))^2" class="latex" /></p>



<p>Then we compute the gradient with respect to the individual entries of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;mathbf{w}" class="latex" />, using the chain rule and noting that the only term of the linear combination that has a nonzero contribution to the gradient for <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+E%7D%7B%5Cpartial+w_i%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;frac{&#92;partial E}{&#92;partial w_i}" class="latex" /> is the term containing <img src="https://s0.wp.com/latex.php?latex=w_i&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="w_i" class="latex" />. This is one of the major benefits of using linear combinations: the gradient computation is easy.</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac%7B%5Cpartial+E%7D%7B%5Cpartial+w_i%7D+%3D+-2+%28y_j+-+%5Chat%7Bf%7D%28%5Cmathbf%7Bx%7D_j%29%29+%5Cfrac%7B%5Cpartial+%5Chat%7Bf%7D%7D%7B%5Cpartial+w_i%7D%28%5Cmathbf%7Bx%7D_j%29+%3D+-2+%28y_j+-+%5Chat%7Bf%7D%28%5Cmathbf%7Bx%7D_j%29%29+f_i%28%5Cmathbf%7Bx%7D_j%29&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;displaystyle &#92;frac{&#92;partial E}{&#92;partial w_i} = -2 (y_j - &#92;hat{f}(&#92;mathbf{x}_j)) &#92;frac{&#92;partial &#92;hat{f}}{&#92;partial w_i}(&#92;mathbf{x}_j) = -2 (y_j - &#92;hat{f}(&#92;mathbf{x}_j)) f_i(&#92;mathbf{x}_j)" class="latex" /></p>



<p>Another advantage to being linear is that this formula is agnostic to the content of the underlying basis functions. This will hold so long as the weights don&#8217;t show up in the formula for the basis functions. As an exercise: try changing the implementation to use <a href="https://en.wikipedia.org/wiki/Radial_basis_function">radial basis functions</a> around each data point. (see the note at the end for why this would be problematic in real life)</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate">
def gradient(weights: Coefficients, data_point: Tuple&#91;Input, float]) -&gt; Gradient:
    error = single_point_error(weights, data_point)
    dE_dw = &#91;0] * len(weights)

    for i, w in enumerate(weights):
        dE_dw&#91;i] = -2 * error * basis&#91;i](data_point&#91;0])

    return dE_dw
</pre></div>


<p>Finally, the gradient descent core with a debugging helper.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate">
import random

def print_debug_info(step, grad_norm, error, progress):
    print(f&quot;{step}, {progress:.4f}, {error:.4f}, {grad_norm:.4f}&quot;)


def gradient_descent(
        data: Dataset,
        learning_rate: float,
        tolerance: float,
        training_callback = None,
) -&gt; Hypothesis:
    weights = &#91;random.random() * 2 - 1 for i in range(len(basis))]
    last_error = total_error(weights, data)
    step = 0
    progress = tolerance * 2
    grad_norm = 1.0

    if training_callback:
        training_callback(step, 0.0, last_error, 0.0)

    while abs(progress) &gt; tolerance or grad_norm &gt; tolerance:
        grad = gradient(weights, random.choice(data))
        grad_norm = sum(x**2 for x in grad)
        
        for i in range(len(weights)):
            weights&#91;i] -= learning_rate * grad&#91;i]

        error = total_error(weights, data)
        progress = error - last_error
        last_error = error
        step += 1

        if training_callback:
            training_callback(step, grad_norm, error, progress)

    return basis.linear_combination(weights)
</pre></div>


<p>Next create some sample data and run the optimization</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate">
def example_quadratic_data(num_points: int):
    def fn(x, y, z):
        return 2 - 4*x*y + z + z**2

    data = &#91;]
    for i in range(num_points):
        x, y, z = random.random(), random.random(), random.random()
        data.append(((x, y, z), fn(x, y, z)))

    return data


if __name__ == "__main__":
    data = example_quadratic_data(30)
    gradient_descent(
        data, 
        learning_rate=0.01, 
        tolerance=1e-06, 
        training_callback=print_debug_info
    )
</pre></div>


<p>Depending on the randomness, it may take a few thousand steps, but it typically converges to an error of &lt; 1. Here&#8217;s the plot of error against gradient descent steps.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://jeremykun.files.wordpress.com/2021/03/graph.png"><img data-attachment-id="118075" data-permalink="https://jeremykun.com/graph/" data-orig-file="https://jeremykun.files.wordpress.com/2021/03/graph.png" data-orig-size="640,480" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="graph" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/03/graph.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/03/graph.png?w=640" src="https://jeremykun.files.wordpress.com/2021/03/graph.png?w=640" alt="" class="wp-image-118075" srcset="https://jeremykun.files.wordpress.com/2021/03/graph.png 640w, https://jeremykun.files.wordpress.com/2021/03/graph.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/03/graph.png?w=300 300w" sizes="(max-width: 640px) 100vw, 640px" /></a><figcaption>Gradient descent showing log(total error) vs number of steps, for the quadratic regression problem.</figcaption></figure></div>



<h2 class="has-text-align-center">Kernels and Regularization</h2>



<p>I&#8217;ll finish with explanations of the parentheticals above.</p>



<p><strong>The real polynomial kernel</strong>. We chose a simple set of polynomial functions. This is closely related to the concept of a &#8220;kernel&#8221;, but the <a href="https://en.wikipedia.org/wiki/Polynomial_kernel">&#8220;real&#8221; polynomial kernel</a> uses slightly different basis functions. It scales some of the basis functions by <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B2%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;sqrt{2}" class="latex" />. This is OK because a linear combination can compensate by using coefficients that are appropriately divided by <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B2%7D&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="&#92;sqrt{2}" class="latex" />. But why would one want to do this? The answer boils down to a computational efficiency technique called the &#8220;<a href="https://en.wikipedia.org/wiki/Kernel_method">Kernel trick</a>.&#8221; In short, it allows you to compute the dot product between two linear combinations of vectors in this vector space without explicitly representing the vectors in the space to begin with. If your regression algorithm uses only dot products in its code (as is true of <a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote14.html">the closed form solution</a> for regression), you get the benefits of nonlinear feature modeling without the cost of computing the features directly. There&#8217;s a lot more mathematical theory to discuss here (cf. <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">Reproducing Kernel Hilbert Space</a>) but I&#8217;ll have to leave it there for now.</p>



<p><strong>What&#8217;s wrong with the radial basis function exercise?</strong> This exercise asked you to create a family of basis functions, one for each data point. The problem here is that having so many basis functions makes the linear combination space too expressive. The optimization will <a href="https://en.wikipedia.org/wiki/Overfitting">overfit</a> the data. It&#8217;s like a lookup table: there&#8217;s one entry dedicated to each data point. New data points not in the training would be rarely handled well, since they aren&#8217;t in the &#8220;lookup table&#8221; the optimization algorithm found. To get around this, in practice one would add an extra term to the error corresponding to the L1 or L2 norm of the weight vector. This allows one to ensure that the total size of the weights is small, and in the L1 case that usually corresponds to most weights being zero, and only a few weights (the most important) being nonzero. The process of penalizing the &#8220;magnitude&#8221; of the linear combination is called <em>regularization</em>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://jeremykun.com/2021/03/29/regression-and-linear-combinations/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/90b179348780a6e7fe8e502968dc534a?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">jeremykun</media:title>
		</media:content>

		<media:content url="https://jeremykun.files.wordpress.com/2021/03/graph.png?w=640" medium="image"/>
	</item>
		<item>
		<title>Searching for RH Counterexamples — Productionizing</title>
		<link>https://jeremykun.com/2021/03/06/searching-for-rh-counterexamples-productionizing/</link>
					<comments>https://jeremykun.com/2021/03/06/searching-for-rh-counterexamples-productionizing/#respond</comments>
		
		<dc:creator><![CDATA[j2kun]]></dc:creator>
		<pubDate>Sat, 06 Mar 2021 18:08:59 +0000</pubDate>
				<category><![CDATA[General]]></category>
		<guid isPermaLink="false">http://jeremykun.com/?p=117979</guid>

					<description><![CDATA[We’re ironically searching for counterexamples to the Riemann Hypothesis. Setting up Pytest Adding a Database Search Strategies Unbounded integers Deploying with Docker Performance Profiling Scaling up In the last article we rearchitected the application so that we could run as many search instances as we want in parallel, and speed up the application by throwing [&#8230;]]]></description>
										<content:encoded><![CDATA[
<p>We’re ironically searching for counterexamples to the Riemann Hypothesis.</p>



<ol><li><a href="https://jeremykun.com/2020/09/11/searching-for-rh-counterexamples-setting-up-pytest/">Setting up Pytest</a></li><li><a href="https://jeremykun.com/2020/09/11/searching-for-rh-counterexamples-adding-a-database/">Adding a Database</a></li><li><a href="https://jeremykun.com/2020/09/28/searching-for-rh-counterexamples-search-strategies/">Search Strategies</a></li><li><a href="https://jeremykun.com/2020/10/13/searching-for-rh-counterexamples-unbounded-integers/">Unbounded integers</a></li><li><a href="https://jeremykun.com/2021/01/04/searching-for-rh-counterexamples-deploying-with-docker/">Deploying with Docker</a></li><li><a href="https://jeremykun.com/2021/02/02/searching-for-rh-counterexamples-performance-profiling/">Performance Profiling</a></li><li><a href="https://jeremykun.com/2021/02/16/searching-for-rh-counterexamples-scaling-up/">Scaling up</a></li></ol>



<p>In the last article we rearchitected the application so that we could run as many search instances as we want in parallel, and speed up the application by throwing more compute resources at the problem.</p>



<p>This is good, but comes with a cost. The complexity of this new architecture requires us to manage many different containers and AWS instances. Let&#8217;s take a step back. In this article, we&#8217;ll focus on improving the &#8220;production worthiness&#8221; of the application. In particular, we will:</p>



<ul><li>Automate running tests on every pull request</li><li>Add some extra error handling code</li><li>Clean up stale blocks when worker jobs fail</li><li>Automate checking Python type hints and test coverage</li><li>Add static analysis checks</li><li>Add alerting to tell us when jobs fail</li><li>Automate the process of updating the application with new code</li></ul>



<h2 class="has-text-align-center">Automating test running</h2>



<p>The main benefit of writing tests is that you can run them. It&#8217;s even better when the tests are run automatically on a pull request. It guards buggy changes from breaking the main code branch.</p>



<p>There are many systems that work with GitHub to automate running tests. For this project I used <a href="https://circleci.com/">CircleCI</a>, which has a nice free tier. &#8220;CI&#8221; stands for <a href="https://en.wikipedia.org/wiki/Continuous_integration">continuous integration</a>, which is an idea that if you guard your main branch well enough, you can ensure that the main branch is always in a state that can be released or deployed to production servers. And if this is the case, then you might as well have the computer manage regular releases for you (provided all tests pass).</p>



<p>Since we don&#8217;t yet have a way to automate releases, we&#8217;ll start by running tests on every pull request. This <a href="https://github.com/j2kun/riemann-divisor-sum/pull/16">pull request</a> was my first failed attempt and configuring CircleCI based on their tutorial, and <a href="https://github.com/j2kun/riemann-divisor-sum/commit/a13d06ef5b37982a4421a9347ebcdc3169a36a8c">this commit</a> fixed it (which was really 15 attempts squashed into one commit post hoc). Originally I thought I could use CircleCI&#8217;s built in python testing jobs, but since this project requires a compiled component—the gmp library for unbounded integer arithmetic, and the associated Postgres extension—we need to manually install some other packages. Thankfully, CircleCI uses containers, so the configuration looks a lot like a Dockerfile. Unfortunately, CircleCI&#8217;s containers use a different Ubuntu operating system for their base image, which means the <a href="https://github.com/j2kun/riemann-divisor-sum/commit/a13d06ef5b37982a4421a9347ebcdc3169a36a8c#diff-78a8a19706dbd2a4425dd72bdab0502ed7a2cef16365ab7030a5a0588927bf47R16">packages I had to install</a> have different names than I expected.</p>



<p>So after an hour or two of trial and error, now it works. When you open a new pull request you see the tests run.</p>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/02/tests-running.png"><img data-attachment-id="117985" data-permalink="https://jeremykun.com/tests-running/" data-orig-file="https://jeremykun.files.wordpress.com/2021/02/tests-running.png" data-orig-size="1818,270" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="tests-running" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/02/tests-running.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/02/tests-running.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/02/tests-running.png?w=1024" alt="" class="wp-image-117985" srcset="https://jeremykun.files.wordpress.com/2021/02/tests-running.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/02/tests-running.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/02/tests-running.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/02/tests-running.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/02/tests-running.png 1818w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure>



<p>And when the tests are finished and passing you see green (otherwise red).</p>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/02/tests-finished.png"><img data-attachment-id="117986" data-permalink="https://jeremykun.com/tests-finished/" data-orig-file="https://jeremykun.files.wordpress.com/2021/02/tests-finished.png" data-orig-size="1832,258" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="tests-finished" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/02/tests-finished.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/02/tests-finished.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/02/tests-finished.png?w=1024" alt="" class="wp-image-117986" srcset="https://jeremykun.files.wordpress.com/2021/02/tests-finished.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/02/tests-finished.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/02/tests-finished.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/02/tests-finished.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/02/tests-finished.png 1832w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure>



<p>I must admit, most of these tools (see Coveralls below) are primarily designed for Python projects without compiled components. The defaults all tend to fail and you fall back to running arbitrary shell scripts. That&#8217;s one good reason to get familiar with standard linux package managers.</p>



<h2 class="has-text-align-center">Better error handling</h2>



<p>A few times the block processing job ran into some errors, and the container died. I don&#8217;t have a means to alert when the jobs fail, so I manually checked on them every once in a while. One error was an overflow error as described in <a href="https://github.com/j2kun/riemann-divisor-sum/issues/14">this issue</a>. While I might want to dig in a bit more to solve that problem, the effect was that the job died and the block was left in the <code>IN_PROGRESS</code> state. I want errors to result in marking the block as failed, engaging the retry loop, to exercise the possibility that the issue is somehow transient or specific to one broken worker job.</p>



<p>It&#8217;s relatively simple, and <a href="https://github.com/j2kun/riemann-divisor-sum/pull/17">this pull request</a> does the trick. <a href="https://github.com/j2kun/riemann-divisor-sum/pull/17/commits/fb065b4d04af8b7ae98f48640fc5ad6b74c35533">This commit</a> is the main part. </p>



<p>I added a nice test that demonstrates (again) the benefit of having interfaces. To test this behavior, we need to inject a fake error into the <code>search_strategy.process_block</code> function. One way to do that is to use mocks, and &#8220;<a href="https://stackoverflow.com/questions/5626193/what-is-monkey-patching">monkey patch</a>&#8221; the <code>process_block</code> method to operate differently during that test. In my opinion, this is bad design, because those tests can quickly become stale, and monkey patching results in odd behavior when you don&#8217;t set up the patch just right. Instead, writing a test-specific implementation of the search strategy interface is a safer approach. <a href="https://github.com/j2kun/riemann-divisor-sum/pull/17/commits/729dd8f2ae4d7741c1f31c0b824750f603caca8a">This commit</a> does that.</p>



<p>Once that was working, I turned to the overflow error, which I fixed in this <a href="https://github.com/j2kun/riemann-divisor-sum/pull/22">pull request</a>. Turns out, the integers involved in the computation are so large that their logarithms don&#8217;t fit in a single floating point number. So I had to switch to using the log function from the gmp library, which outputs a multi-precision floating point number.</p>



<h2 class="has-text-align-center">Handle stale blocks</h2>



<p>Due to the jobs failing in the previous section, and as an extra safety measure against further failures—including when a job is killed in order to relaunch it with new code—I decided to add an extra job devoted to looking for stale blocks (which have been <code>in_progress</code> for more than an hour) and marking them as failed. <a href="https://github.com/j2kun/riemann-divisor-sum/issues/12">This issue</a> describes the idea. Once deployed, the current production system will automatically patch up its past failures. If only it were so easy for we humans. This <a href="https://github.com/j2kun/riemann-divisor-sum/pull/18">pull request</a> adds the new job, a new Dockerfile, and an end-to-end test. I deployed the job on the same server as the generator job, since both jobs spend most of their time sleeping.</p>



<h2 class="has-text-align-center">Type checker tests</h2>



<p>So far in the project, I&#8217;ve been adding type hints to most of the functions. Type hints are optional in Python, but if you use them, you can add an automated test to ensure that your code matches what the type hints say. This helps you have more confidence that your code is correct. There&#8217;s a lot more to say on this subject (software engineers often argue about the value of type checking), but I&#8217;ll refrain from providing my opinion for now, assume it&#8217;s worthwhile, and show what it&#8217;s like to use it.</p>



<p>For this project I&#8217;ll use <a href="https://mypy.readthedocs.io/">mypy</a>. Since I haven&#8217;t been running a type checker on this project yet, I expect the first run will find a lot of errors. When I run <code>mypy riemann/*.py</code> it <a href="https://github.com/j2kun/riemann-divisor-sum/issues/19#issuecomment-781075870">reports 29 errors</a>. Notably, these two errors suggest problems with our dependencies.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
riemann/superabundant.py:6: error: Cannot find implementation or library stub for module named 'gmpy2'
riemann/superabundant.py:7: error: Cannot find implementation or library stub for module named 'numba'
</pre></div>


<p>The problem here is that if a dependency doesn&#8217;t use type hints, then the type checker won&#8217;t know what to do with functions from the dependent module. The options are to create a &#8220;stub&#8221; for those types, or to tell the type checker to ignore them. Stubbing is supposed to be easy, but for project that have compiled dependencies it can be considerably harder. For now I went with ignoring missing stubs. <a href="https://github.com/j2kun/riemann-divisor-sum/pull/24">This pull request</a> does the extra work to run the type checker in the automated test suite, and fix some minor type errors (one of which spotted a <a href="https://github.com/j2kun/riemann-divisor-sum/issues/11">real bug</a>).</p>



<h2 class="has-text-align-center">Test Coverage</h2>



<p>Test coverage (or &#8220;code coverage&#8221;) tools report which lines of your program are exercised by some test. Like type hints, requiring test coverage has advantages and disadvantages. Briefly, I feel code coverage is a good baseline understanding of what&#8217;s tested, but coverage does not imply adequate testing, nor does lack of coverage imply poor testing. Rather, it&#8217;s a rough guide that can show you glaring holes  in what you thought to write tests for. There&#8217;s more to say, but not today.</p>



<p>Pytest has <a href="https://pypi.org/project/pytest-cov/">plugins for coverage measuring</a>, and when you install them you can just run <code>pytest --cov --cov-report html:cov_html</code> to generate html pages that show line-by-line and file-by-file coverage stats. Mine looked like this.</p>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.09.32-pm.png"><img data-attachment-id="118012" data-permalink="https://jeremykun.com/screen-shot-2021-02-20-at-8-09-32-pm/" data-orig-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.09.32-pm.png" data-orig-size="1390,1094" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2021-02-20-at-8.09.32-pm" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.09.32-pm.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.09.32-pm.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.09.32-pm.png?w=1024" alt="" class="wp-image-118012" srcset="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.09.32-pm.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.09.32-pm.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.09.32-pm.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.09.32-pm.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.09.32-pm.png 1390w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>This project&#8217;s coverage report (after fixing up issues described below)</figcaption></figure>



<p>A few obstacles, again coming from the fact that the project has compiled parts. In particular, <a href="https://github.com/numba/numba/issues/4268">Numba does not provide coverage support yet</a>. So to get coverage statistics, you have to disable the jit compiler during the test execution. This simply requires <a href="https://github.com/j2kun/riemann-divisor-sum/pull/25/commits/297e6fc57d627e0dac9853e7d7ddb4471e66cf98">setting</a> the environment variable <code>NUMBA_DISABLE_JIT=1</code>. Disabling this made some of the tests run extremely slowly, but <a href="https://github.com/j2kun/riemann-divisor-sum/pull/25/commits/763f744fcb8ff22e060e4114cfdaed98cd2716a1">some minor changes</a> restored a reasonable runtime (2 minutes, mostly end-to-end tests).</p>



<p>The other problem was the end to end tests. You need to add <a href="https://github.com/j2kun/riemann-divisor-sum/pull/25/commits/b1e58dc878a217742bab50a03b61169dfbbe8d35">a special hook</a> to allow the Python coverage tool to see the lines of code executed by subprocesses, which is used heavily by the end to end tests. <a href="https://github.com/j2kun/riemann-divisor-sum/pull/25">This pull request</a> shows the steps to overcoming both of these problems.</p>



<p>Also in that pull request is the configuration of <a href="https://coveralls.io/">Coveralls</a>. This allows code coverage to be computed during the continuous integration test run, and shown on the pull request before merging, with the option to &#8220;fail&#8221; the code coverage test if the changes in the pull request reduce code coverage too much.</p>



<p>The <a href="https://circleci.com/developer/orbs/orb/coveralls/coveralls">configuration together</a> between CircleCI and Coveralls required setting the Coveralls project token as an environment variable during CircleCI workflows, and running <code>coveralls</code> after the <code>pytest</code> command finished. Now pull requests look like this.</p>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-6.57.20-pm.png"><img data-attachment-id="118008" data-permalink="https://jeremykun.com/screen-shot-2021-02-20-at-6-57-20-pm/" data-orig-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-6.57.20-pm.png" data-orig-size="1844,614" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2021-02-20-at-6.57.20-pm" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-6.57.20-pm.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-6.57.20-pm.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-6.57.20-pm.png?w=1024" alt="" class="wp-image-118008" srcset="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-6.57.20-pm.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-6.57.20-pm.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-6.57.20-pm.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-6.57.20-pm.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-6.57.20-pm.png 1844w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>Now Pull Requests show a coverage report and warn if the coverage percentage is too low.</figcaption></figure>



<h2 class="has-text-align-center">Static Analysis</h2>



<p>Static analysis refers to tools that look at your code and point out problems or suggest improvements. In Python there are scant few good static analysis tools, in part because Python is designed to be very dynamic. It&#8217;s not possible for an automated system to be 100% sure what a piece of code does, because even importing a module can do weird things like <a href="https://www.python.org/dev/peps/pep-0238/#the-future-division-statement">change the behavior of arithmetic operations</a>. Static analysis plummets in value as false positives increase, but being conservative in making recommendations also limits the value.</p>



<p>Static analysis also refers to the ability to do autocomplete, or reformatting source code to match a particular style, as well as type checking. But I just want to focus on semantic code improvements, not &#8220;prettifying&#8221; code. A simple example is <a href="https://github.com/jendrikseipp/vulture">dead code</a>, which is unused by the application and can be safely deleted. Another is <a href="https://github.com/uber/py-find-injection">SQL injection vulnerabilities</a>.</p>



<p>For this project I will configure <a href="http://lgtm.com">lgtm.com</a>, which is a nice general-purpose static analysis framework that hooks into any GitHub repository without configuration and supports a few important languages. Here is an <a href="https://lgtm.com/logs/9506f1e88b0c2d21aeac8052f0442217c972f856/lang:python">example run</a>, which resulted in 11 alerts like this one</p>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.46.42-pm.png"><img data-attachment-id="118016" data-permalink="https://jeremykun.com/screen-shot-2021-02-20-at-8-46-42-pm/" data-orig-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.46.42-pm.png" data-orig-size="1376,660" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2021-02-20-at-8.46.42-pm" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.46.42-pm.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.46.42-pm.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.46.42-pm.png?w=1024" alt="" class="wp-image-118016" srcset="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.46.42-pm.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.46.42-pm.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.46.42-pm.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.46.42-pm.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.46.42-pm.png 1376w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>An example alert found by LGTM</figcaption></figure>



<p>And it&#8217;s right! I didn&#8217;t need that entire statement because the variable it defined was unused. This one was low-stakes, but another alert <a href="https://github.com/j2kun/riemann-divisor-sum/commit/5da4bbe828bb2c86df44100a1d06397874548221">pointed to a more serious and subtle bug</a>, related to how the <code>SuperabundantSearchStrategy</code> class mutates its internal state. I can clean that up later, but for now let&#8217;s marvel at how the static analysis tool caught the bug with no work on my part. Fantastic!</p>



<p>This <a href="https://github.com/j2kun/riemann-divisor-sum/pull/27">pull request</a> addresses the 11 alerts generated by LGTM, most of which are unused imports. LGTM is even nice enough to post comments showing the improvements by PR.</p>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.27.10-pm.png"><img data-attachment-id="118020" data-permalink="https://jeremykun.com/screen-shot-2021-02-20-at-9-27-10-pm/" data-orig-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.27.10-pm.png" data-orig-size="1414,414" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2021-02-20-at-9.27.10-pm" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.27.10-pm.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.27.10-pm.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.27.10-pm.png?w=1024" alt="" class="wp-image-118020" srcset="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.27.10-pm.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.27.10-pm.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.27.10-pm.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.27.10-pm.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.27.10-pm.png 1414w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>The nice comment left by LGTM when merging a PR that fixes alerts.</figcaption></figure>



<h2 class="has-text-align-center">Badges</h2>



<p>A little bit of flair: I added badges to the project README, so I could show off the fact that tests are passing and I have decent code coverage and code that passes static analysis checks.</p>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.13.11-pm.png"><img data-attachment-id="118018" data-permalink="https://jeremykun.com/screen-shot-2021-02-20-at-9-13-11-pm/" data-orig-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.13.11-pm.png" data-orig-size="1660,294" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2021-02-20-at-9.13.11-pm" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.13.11-pm.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.13.11-pm.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.13.11-pm.png?w=1024" alt="" class="wp-image-118018" srcset="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.13.11-pm.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.13.11-pm.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.13.11-pm.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.13.11-pm.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.13.11-pm.png 1660w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure>



<h2 class="has-text-align-center">Alerting on job failure</h2>



<p>Next, I wanted to set up some rudimentary alerting so that I can know when my docker containers fail. If I don&#8217;t have this, I have to manually check up on them, and if they fail and I forget, I end up paying money to run the servers for nothing.</p>



<p>There are many approaches to do monitoring and alerting, many software packages, and many opinions. This series is supposed to show you the ideas and problems that software engineers care about, but not go overboard so that we still have time to do math. So I&#8217;ll implement what I consider the simplest solution, describe what it lacks, and briefly mention some other approaches.</p>



<p>We need two components: the ability to tell if a container has failed, and the ability to send an email. The former is easy, because the docker runtime provides a pleasant CLI.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
docker ps -a --format="{{.Image}}" --filter="status=exited"
</pre></div>


<p><code>docker ps -a</code> shows the status of all running containers, the <code>--filter</code> flag makes the command show only the containers that have exited, and the <code>--format</code> flag makes the command print a restricted subset of the information, in this case just the image name. This makes it suitable for providing as input to a program, since there is no special text added to the output just for human-readability.</p>



<p>The second part, sending an email, is more complicated because popular email services like Gmail have stringent rules. In short, Gmail blocks email from any unknown sender by default (because spam), and so in order to send an email successfully from a program, you need to jump through a bunch of hoops to authenticate the system to send on behalf of an account with a popular provider. You cant just use <code>sendmail</code>. Since I use Gmail, we&#8217;ll use it for authentication.</p>



<p>To authenticate a program to send from a Gmail account, you have to generate an &#8220;<a href="https://support.google.com/accounts/answer/185833?hl=en">app password</a>&#8221; for your Google account, and then configure the mail sender program to use it. The program I chose is the CLI <a href="https://linux.die.net/man/8/ssmtp">ssmtp</a>, and you can configure it to use your Google account and password via a configuration file typically stored at <code>/etc/ssmtp/ssmtp.conf</code>. The configuration file looks like this (with sensitive data censored with x&#8217;s)</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
root=xxxx@gmail.com
mailhub=smtp.gmail.com:465
FromLineOverride=YES
AuthUser=xxxx@gmail.com
AuthPass=xxxxxxxxxxxxx
TLS_CA_FILE=/etc/ssl/certs/ca-certificates.crt
UseTLS=Yes
rewriteDomain=gmail.com
hostname=xx.xx.xx.xx.xx
</pre></div>


<p>Once you have this, you can send email using a command-line invocation like</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
echo "Email message body goes here" | ssmtp recipient@gmail.com
</pre></div>


<p>However, this poses another small problem. While we could manually configure each of these config files on each of our servers, we would like to automate it eventually (next section), and automating it would seem to require us to store the sensitive information in a script that goes into our version-controlled (public!) code repository.</p>



<p>To deal with this, we need a simple way to manage secrets. The typical way to do this is via unix <a href="https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/">environment variables</a>. In short, environment variables are variables and values you can store in the execution context of any script or terminal session. The most common example is <code>$PATH</code>, which stores a list of directories to search for programs in when you run a program. E.g., if you run <code>docker ps</code>,  it will look to see if there&#8217;s a program called <code>docker</code> in <code>/user/bin/</code>, and if not try <code>/usr/local/bin/</code>, and so on until exhausting all entries in <code>$PATH</code>. We used environment variables to pass the postgres host ip to our docker containers, but it can also be used for passing secrets to programs to avoid storing passwords in text files that might become public accidentally.</p>



<p>You can set an environment variable using <code>export VARIABLE="any value here"</code>.  All values are strings. In our case, we will define two environment variables <code>GMAIL_APP_USER</code>—for the email address of the Google account to authenticate as—and <code>GMAIL_APP_PASS</code> for the app password. Then <a href="https://github.com/j2kun/riemann-divisor-sum/commit/36e80fea43511348c2c569d48682023e300d7ca4">this bash script</a> (run with <code>sudo -E</code> to ensure environment variables are passed from the calling context) will set up the configuration in one go, and <a href="https://github.com/j2kun/riemann-divisor-sum/commit/3e0aac25f78e3ad9b9898494b8b16035a2b85e01">this python script</a> will orchestrate the loop of checking <code>docker ps -a</code> and sending an email if it sees any failed containers, <a href="https://github.com/j2kun/riemann-divisor-sum/commit/2c28988fcad88eae1102cc6337bfb4bc3f1ae131">passing the password</a> via an environment variable as well. An improvement <a href="https://github.com/j2kun/riemann-divisor-sum/commit/2c28988fcad88eae1102cc6337bfb4bc3f1ae131">I discovered later</a> allows us to also avoid storing the password in the ssmtp configuration file, and instead pass it directly from the environment to the python script.</p>



<p>Note this uses the <code>python-dotenv</code> library to easily import environment variables stored in a <code>.env</code> file. We must be careful not to check the <code>.env</code> file into version control, so we <a href="https://github.com/j2kun/riemann-divisor-sum/commit/eef42bf05ae7895da4ad8236b3a5ebaa19d5309a">update the .gitignore</a> and add an <code>.env.template</code> as a hint to show what environment variables are needed in <code>.env</code>.</p>



<p>Finally, though the deploy script is not quite useful as a standalone script (the docker containers live on different machines), it&#8217;s still useful to remind us how to manually deploy. So after exporting the right environment variables we have <a href="https://github.com/j2kun/riemann-divisor-sum/commit/36e80fea43511348c2c569d48682023e300d7ca4#diff-30883d1df2fc62f59d66255a70dc05f60d3d4ef4d3957e6e3e38e62f6e471bd0R24">a few simple new steps</a> to follow to run the monitoring script.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
sudo apt install -y python3-pip ssmtp
pip3 install -r alerts/requirements.txt
sudo -E alerts/configure_ssmtp.sh
nohup python3 -m alerts.monitor_docker &
</pre></div>


<p>The last line may be new. <code>nohup</code> tells the operating system to &#8220;disconnect&#8221; the program from the shell that launched it (it&#8217;s actually <a href="https://en.wikipedia.org/wiki/Nohup">a bit more specific</a>, but has this effect), and the <code>&amp;</code> has the program run in the background. This effectively allows us to launch the monitoring job in an <code>ssh</code> session, and then close the <code>ssh</code> session without terminating the program.</p>



<p>That raises an interesting point: how can we be sure that the monitoring program doesn&#8217;t fail? Who watches the watcher? For us, nobody. There is always more engineering you can do if you want a higher level of safety and security, but for us the returns diminish quickly. Eventually we will check the program manually and notice if it broke.</p>



<h2 class="has-text-align-center">Automating deployment</h2>



<p>The final step of our &#8220;productionization&#8221; process will be to set up some mechanism to deploy our application when we make updates. Like monitoring and alerting, there are quite a few frameworks out there. The main one for us to consider is called a <em>container orchestration system</em>, and the biggest one for Docker is called <a href="https://kubernetes.io/">Kubernetes</a>. If you read through the Kubernetes homepage, you&#8217;ll see all kinds of benefits that align with what we want: automatic deployment and container management, including monitoring, restarting jobs, and changing compute resource limits like RAM limit and CPU. These are all things which have taken up our time while building this project! AWS <a href="https://aws.amazon.com/containers/services/">has support for Kubernetes</a>, as well as their own homegrown solutions.</p>



<p>The difficulty around these topics is that you can spend as much (or more) of your time learning the details of the framework than you would making a functional-but-suboptimal version yourself. Such frameworks can also extra layers of frustration because of the additional jargon and learning curve, the mess of configuration, and the tendency for corporate products to change and force you to change in turn. Docker was already a decent investment, it&#8217;s asking a lot to invest more!</p>



<p>If I had all the time in the world, I&#8217;d go with Kubernetes. But in the next article I want to get back to doing some math and visualization with the data we&#8217;ve collected. So let&#8217;s pretend we can&#8217;t use a framework, and write a basic script that will automate our deployment for us. That is, the script will ssh into the EC2 instances that run each piece of the application, run <code>git pull</code> on each, run <code>docker stop</code> on the containers and remove them, build new container images from the new code, launch those images, and do it all in the right dependency order. The approach won&#8217;t be super resilient or have all the engineering ribbons and bows, but in the worst case we can manually fix any problems.</p>



<p><a href="https://github.com/j2kun/riemann-divisor-sum/blob/f191947e894e1209f703849857600a9c9dadd654/deploy.py">This script does just that</a>. It&#8217;s simple enough for starters, and didn&#8217;t take me more than an hour to get working. We&#8217;ll leave Kubernetes for future work.</p>



<p>That said, in the process of building and testing the deployment script I hit a disappointing problem: docker was deleting the database when I removed the container. Or rather, it started over and I couldn&#8217;t find where the data was stored on the host machine. This appears to be because I was letting the Postgres base Dockerfile define a &#8220;volume,&#8221; which wasn&#8217;t saved when I removed the built Docker image (<code>docker prune</code>). So I read up about <a href="https://docs.docker.com/storage/volumes/">docker volumes</a> and created a named volume and <a href="https://github.com/j2kun/riemann-divisor-sum/commit/f191947e894e1209f703849857600a9c9dadd654">attached it</a> to the docker container at startup time. The volume won&#8217;t disappear when the container stops now, and I can automatically back up the volume (<a href="https://github.com/j2kun/riemann-divisor-sum/issues/30">coming soon</a>), so that further blunders don&#8217;t result in losing all the data again.</p>



<p>Next time we&#8217;ll get back to the mathematical aspects and look for patterns in the data we&#8217;ve found so far. I, for one, am optimistic we&#8217;ll find the secrets we need to disprove RH.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://jeremykun.com/2021/03/06/searching-for-rh-counterexamples-productionizing/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/90b179348780a6e7fe8e502968dc534a?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">jeremykun</media:title>
		</media:content>

		<media:content url="https://jeremykun.files.wordpress.com/2021/02/tests-running.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/02/tests-finished.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.09.32-pm.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-6.57.20-pm.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-8.46.42-pm.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.27.10-pm.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/02/screen-shot-2021-02-20-at-9.13.11-pm.png?w=1024" medium="image"/>
	</item>
		<item>
		<title>Searching for RH Counterexamples — Scaling Up</title>
		<link>https://jeremykun.com/2021/02/16/searching-for-rh-counterexamples-scaling-up/</link>
					<comments>https://jeremykun.com/2021/02/16/searching-for-rh-counterexamples-scaling-up/#respond</comments>
		
		<dc:creator><![CDATA[j2kun]]></dc:creator>
		<pubDate>Tue, 16 Feb 2021 17:00:00 +0000</pubDate>
				<category><![CDATA[General]]></category>
		<category><![CDATA[hashing]]></category>
		<category><![CDATA[mathematics]]></category>
		<category><![CDATA[postgres]]></category>
		<category><![CDATA[programming]]></category>
		<category><![CDATA[riemann hypothesis]]></category>
		<category><![CDATA[software]]></category>
		<guid isPermaLink="false">http://jeremykun.com/?p=117910</guid>

					<description><![CDATA[We&#8217;re ironically searching for counterexamples to the Riemann Hypothesis. Setting up Pytest Adding a Database Search Strategies Unbounded integers Deploying with Docker Performance Profiling Last time we made the audacious choice to remove primary keys from the RiemannDivisorSums table for performance reasons. To help with that, we will do two things in this post Reduce [&#8230;]]]></description>
										<content:encoded><![CDATA[
<p>We&#8217;re ironically searching for counterexamples to the Riemann Hypothesis.</p>



<ol><li><a href="https://jeremykun.com/2020/09/11/searching-for-rh-counterexamples-setting-up-pytest/">Setting up Pytest</a></li><li><a href="https://jeremykun.com/2020/09/11/searching-for-rh-counterexamples-adding-a-database/">Adding a Database</a></li><li><a href="https://jeremykun.com/2020/09/28/searching-for-rh-counterexamples-search-strategies/">Search Strategies</a></li><li><a href="https://jeremykun.com/2020/10/13/searching-for-rh-counterexamples-unbounded-integers/">Unbounded integers</a></li><li><a href="https://jeremykun.com/2021/01/04/searching-for-rh-counterexamples-deploying-with-docker/">Deploying with Docker</a></li><li><a href="https://jeremykun.com/2021/02/02/searching-for-rh-counterexamples-performance-profiling/">Performance Profiling</a></li></ol>



<p>Last time we made the audacious choice to remove primary keys from the <code>RiemannDivisorSums</code> table for performance reasons. To help with that, we will do two things in this post</p>



<ul><li>Reduce the storage footprint of the whole application (it was 60 GiB when it crashed, and we got up to 84 prime factors).</li><li>Refactor the application into a worker architecture.</li></ul>



<p>The first one is straightforward, but uses a possibly new idea to the mathematician, and the second is a relatively major rearchitecture.</p>



<h2 class="has-text-align-center">Hashing a search block</h2>



<p>Instead of storing every single Riemann divisor sum, we can just store the ones that are interesting, that is, the ones that are larger than some threshold—in our case, 1.767, which had just under a thousand examples in the last post. </p>



<p>That raises a problem. We want to be able to say that we checked all examples up to some limit. We&#8217;ve been saying &#8220;up to all numbers having at most K prime factors&#8221; because that&#8217;s how the superabundant search strategy enumerates the candidates. If we don&#8217;t store all the witness values, how can we prove to a skeptical observer that we checked what we said we checked?</p>



<p>One approach is to instead store a &#8220;summary string&#8221; derived from the full set of witness values. This &#8220;summary string&#8221; should be deterministically reproducible, so that if someone has the metadata about the block of numbers, they can recompute the summary and compare it with our summary.</p>



<p>A simple example summary might be the string-concatenation of all the witness values one after another, like <code>1.4323,1.5678,0.8792</code>. But that still takes up a lot of space (each block had 250,000 different witness values). It would be better if we could find a summary that had a fixed size, regardless of how big the input is. Taking a cue from computer science, a <em><a href="https://en.wikipedia.org/wiki/Hash_function">hash function</a></em> does exactly that. Hash functions aren&#8217;t perfect, they can have collisions, but the collisions are unlikely enough that, if you tested a hundred random blocks and all your hashes agreed with all of my hashes, you would be confident I checked everything the same as you. That gives you confidence that the rest of my blocks (say, a million of them) were computed correctly.</p>



<p>The hash function we&#8217;ll use is called sha256, and we add it in <a href="https://github.com/j2kun/riemann-divisor-sum/pull/10">this pull request</a>. Now the database application shouldn&#8217;t need a ton of disk space to run. (We&#8217;ll confirm this later in the article)</p>



<p>For more background and related topics around hash functions, see <a href="https://jeremykun.com/2015/12/28/load-balancing-and-the-power-of-hashing/">Load Balancing and the Power of Hashing</a> and <a href="https://jeremykun.com/2016/01/04/hashing-to-estimate-the-size-of-a-stream/">Hashing to Estimate the Size of a Stream</a>.</p>



<h2 class="has-text-align-center">Worker architecture</h2>



<p>Next we want to rearchitect the application to enable two things: better protection against the possibility of duplicating/skipping divisor sum computations, and allowing us to scale up the rate of computation by adding additional computers to the mix. <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8">This pull request</a> achieves the goal, but note that it&#8217;s a large change, so let&#8217;s cover the broad ideas, then zoom in on a few details, and finally discuss the PR&#8217;s structure as a whole from the perspective of software engineering.</p>



<p>We start by <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/baa87ba27d4ebbcbbf78febf74a82b0fe28eb16c">adding a &#8220;state&#8221; field</a> to a search block. The state can be one of <code>NOT_STARTED, IN_PROGRESS, FINISHED, FAILED</code>. Then one worker job (read, container) will be responsible for computing new blocks in the <code>NOT_STARTED</code> state, and a group of workers jobs will continually &#8220;claim&#8221; an eligible block, compute the divisor sums, and then mark it as finished and insert the interesting witness values it found.</p>



<p>This suggests a drastic change to the database interface, as shown in <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/9f4390497a5f6c71c002aaf1f24f40d32780a1bb">this commit</a>, to align with the new &#8220;claim/finish&#8221; workflow. We replace the existing functions with <code>insert_search_blocks</code>, <code>claim_next_search_block</code>, and <code>finish_search_block</code>. The first two operate only on the <code>SearchMetadata</code> table, while the third couples together the changes to both tables, by marking a block as finished and inserting the relevant witness values. We&#8217;ll see how we ensure these two updates occur atomically in a moment. Also, because of the new coupling of the two tables, I merged the two database interfaces into one.</p>



<p>We need two safety measures to make this work. First, we need to ensure that we don&#8217;t accidentally generate multiple duplicate search blocks. We do this <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/a20ff79fcc74e2c7f1d771c33a007819177b2228#diff-eecc974eb4090364fb1e938751e15a08bfcc8a8461560fdfd4312d2e1bc3d508R57">by adding a uniqueness constraint</a> on the bounds of a search block at the database level. If a client calls <code>insert_search_blocks</code> with something that&#8217;s already in the database, the database itself will reject it, and the client will be forced to catch the failure and recover.</p>



<p>Second, we need to ensure that two workers don&#8217;t claim the same search block. This involves a bit of database wizardry. I had to do a bit of research to make it work, and it took a few attempts. The gist of the problem is that if you want to claim a search block, you need to do two queries. First, you need to look up the oldest eligible search block that you want to claim. Then, you need to mark it as claimed. But if you have multiple workers accessing the database simultaneously, you can get this order of operations</p>



<ol><li>Worker 1 runs lookup query, gets search block 7</li><li>Worker 2 runs lookup query, also gets search block 7</li><li>Worker 1 runs update query, marks search block 7 as claimed</li><li>Worker 2 runs update query, also marks search block 7 as claimed.</li></ol>



<p>This can even happen if there&#8217;s only &#8220;one&#8221; query, and the lookup step is a subquery. After reading a bit about PostgreSQL&#8217;s transaction isolation guarantees, I learned that you can &#8220;lock&#8221; the rows read by the subquery until the update step is complete by adding <code>FOR UPDATE</code> to the subquery. <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/21f6f76cee46434435e843da35803c5cdea52ebb">This commit</a> has the full query, but it looks something like this</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
UPDATE SearchMetadata
SET state = 'IN_PROGRESS'
FROM (
  SELECT starting_search_index
  FROM SearchMetadata
  WHERE state = 'NOT_STARTED'
  ORDER BY creation_time ASC
  LIMIT 1
  FOR UPDATE         &lt;---- key clause!
) as m
WHERE
  starting_search_index = m.starting_search_index;
</pre></div>


<p>I also added some <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/5b1af3f9d641ba822113bd08c78741c226e0cf17#diff-411dacf03e6c6b818162c382db4a017a2e75f7911b49160c0356626ed5ca4e08">special tests</a> (that use the finished worker jobs) that will specifically fail when this <code>FOR UPDATE</code> clause is removed, to confirm it fixes the behavior. This is particularly important because it&#8217;s hard to test database queries that protect against race conditions. Plus, I tried other approaches and got it wrong, which proved the value of having such tests.</p>



<p>Finally, the <code>finish_search_block</code> function needs to ensure that the update of the <code>SearchMetadata</code> and <code>RiemannDivisorSums</code> tables happen <a href="https://en.wikipedia.org/wiki/Atomicity_(database_systems)">atomically</a>. This is done <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/18ec01806925d4221cb69bf538c17096e20485ed">in this commit</a> by leveraging the concept of a database <em>transaction</em>. In a transaction, all of the queries between <code>BEGIN</code> and <code>COMMIT</code> happen at once or not at all, e.g., in the case that a later query fails. The &#8220;not at all&#8221; case is called a <em>rollback.</em></p>



<p>Thankfully, the <code>psycopg2</code> library we&#8217;re using uses transactions by default, and requires us to call <code>connection.commit</code> to make any query persist. So we are already using transactions and get the guarantee without extra work.</p>



<p>Next, we needed to update the <code>SearchStrategy</code> interface to separate the task of generating new search blocks and processing blocks, since these will be done by separate workers. <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/f85ecfd3f1e326f540fb1b0714ba240a978f7d1a">This commit</a> updates the interface, <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/b470f2dc4d3d2697def6bfe6110ba26bd02d63f1">this commit the tests</a>, and <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/c93ef55f2b410b2cd03c353506cf050ac80028a9">this commit the implementation</a>.</p>



<p>Finally, once all the rearchitecting and re-testing is done, we ditched our old <code>populate_database</code> entry script, and replaced it with a worker script for <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/4c04addf53f9f5b03ea814e625b28f035e701d64">generating new search blocks</a>, and one for <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/23015cda8bb6d9df86cdc62e1d63a4a306aea85f">claiming and processing blocks</a>. These have the same general structure: infinitely loop, use the database and search strategy interfaces as expected, and then <a href="https://en.wikipedia.org/wiki/Exponential_backoff">exponentially back off</a> when failures occur, eventually quitting if there are too many repeated failures. In the case of the generator worker, we also have configuration around how many new blocks to create, what minimum threshold to trigger doing work, and a delay between checks (since the workers will be much slower than the generator).</p>



<h2 class="has-text-align-center">Ship it!</h2>



<p>Now we&#8217;re ready to launch it, and this time since we have improved our memory issues, we can use an AWS instance with smaller memory requirements, but launch four of them to allow one for each container. After a few of the following steps, this worked out just fine:</p>



<ul><li>Configure an AWS security group (which limits which IP addresses EC2 instances can communicate with) so that Postgres communication was allowed between all the launched instances. This was made easy by the fact that you can configure a security group once so that it allows communication to and from any other instances with the same security group (in addition to my laptop&#8217;s IP).</li><li>Keep track of the ip addresses of each of the instances I launched, and make sure that the instance with a 60 GiB disk is the instance I put the database on.</li><li>Manually install docker and launch the right container on each one, and point their <code>PGHOST</code> variables to the address of the database instance.</li></ul>



<p>I will improve this workflow in the next article, but for now I am letting it run for a few days and observing that it works. The generator job adds 100 new search blocks any time there are less than 100 eligible blocks, and this query shows they are getting worked on properly.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
divisor=# select state, count(*) from searchmetadata group by state;
    state    | count 
-------------+-------
 NOT_STARTED |   129
 FINISHED    |   367
 IN_PROGRESS |     4
</pre></div>


<p>As I was stopping and restarting jobs, two search blocks got stuck in the &#8220;in_progress&#8221; state, as you can see from the lagging-behind start time.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
divisor=# select start_time, starting_search_index, ending_search_index from searchmetadata where state='IN_PROGRESS' order by start_time asc;
         start_time         | starting_search_index | ending_search_index 
----------------------------+-----------------------+---------------------
 2021-02-09 04:46:13.616085 | 64,433492             | 64,683491    &lt;-- stuck!
 2021-02-09 04:48:08.554847 | 65,191862             | 65,441861    &lt;-- stuck!
 2021-02-09 14:22:08.331754 | 78,9803652            | 78,10053651
 2021-02-09 14:22:44.36657  | 78,10053652           | 78,10303651
(4 rows)
</pre></div>


<p>This suggests I should create a new job for cleaning up &#8220;stale&#8221; <code>IN_PROGRESS</code> blocks and marking them as failed. I&#8217;ll do this in the next article, and it won&#8217;t hurt us to leave some blocks in the transient state, because once the cleanup job is running it will make those blocks eligible to be worked on again, and the worker&#8217;s claim the earliest block first.</p>



<p>I also noticed an issue where I forgot a <a href="https://github.com/j2kun/riemann-divisor-sum/commit/939749be5b45442b18cd7655fb95ff344d7f90fc">commit statement</a>, which didn&#8217;t show up in testing, and I can&#8217;t quite figure out how to set up a test that requires this commit statement to be present. Nevertheless, after I noticed the problem, I patched it, stopped the containers, ran <code>git pull</code> on each instance, rebuilt the containers, and started them again, and it continued along as if nothing was wrong.</p>



<p>Since the search application is running well now for long periods of time, I&#8217;ll let it run for a week or two (or until something breaks), and return to it in the next article to see what bigger witness values we find. In the mean time, I&#8217;d like to discuss how I organized all of the changes that went into that massive pull request.</p>



<h2 class="has-text-align-center">How to organize a big pull request</h2>



<p>The <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8">pull request</a> to change the application architecture is quite substantial, and I had a lot of cleanup I wanted to do along the way.</p>



<p>The basic pattern of the PR, which was repeated a few times throughout, was to split each unit of change into three parts: update the interface, update the tests that use the interface, and update the implementation. As an example, one commit <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/f85ecfd3f1e326f540fb1b0714ba240a978f7d1a">updates the SearchStrategy interface</a>, one <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/b470f2dc4d3d2697def6bfe6110ba26bd02d63f1">the tests</a>, and one <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/c93ef55f2b410b2cd03c353506cf050ac80028a9">the implementation</a>.</p>



<p>Doing it this way helps in a few ways. First, I think a lot about how the interface should work, and I prove that it feels right by writing tests first. That way, I don&#8217;t get bogged down in the implementation details when thinking about how it will be used. Likewise, when doing this for the database change, after updating the interface and tests, I first updated the <code>InMemoryDatabase</code> implementation, because it was simpler, and having this made me confident the interface could be implemented decently, and allowed me to tweak the interface early, before turning to the harder problem of figuring out PostgreSQL <code>FOR UPDATE</code> stuff.</p>



<p>Much like the proof of a mathematical theorem, the final commits hide the work that went into this, and commit messages, code comments, and PR description provide a deeper explanation. Typically what I do is make changes, and then use git&#8217;s staging area concept to pull out different sub-changes within a file into a single commit. This blog post about &#8220;<a href="https://jeremykun.com/2020/01/14/the-communicative-value-of-using-git-well/">using git well</a>&#8221; covers this in more detail. But this, and git&#8217;s rebase and amend features, allowed me to edit older commits so that the final pull request lays out the commits in a logical order. </p>



<p>It also allows me to split off the cleanup parts as best as possible, such as <a href="https://github.com/j2kun/riemann-divisor-sum/pull/8/commits/9af07d1b4ee491637812d495f11062fead75bd24">this commit</a> to rename SearchState to SearchIndex (after I added the new &#8220;state&#8221; field). I made the commit, had some places I missed and fixed later, extracted them into their own commit, and rebased and squashed them together to unify the change. Keeping each commit as close to an atomic thought as possible (&#8220;add this test&#8221; or &#8220;implement this method&#8221;) enables easy squashing and reordering.</p>



<p>Though nobody is reviewing my code now (except you, dear reader, after the fact), a primary benefit of all this cleanup is that the code is much easier to review. The pull request as a whole is a feature, each commit is a smaller, but comprehensible piece of that bigger feature, and the commits are laid out in such a way that if you browse the commits in order, you get a clear picture of how the whole feature was assembled.</p>



<p>If you find yourself doing mathematical work in the context of software, you should expect to spend a lot of time reviewing code and having your code reviewed. The extra effort you put into making that process easier pays off by producing better feedback, uncovering more bugs, and reducing total review time. Though it might be considered a soft skill, smooth code reviews are a critical part of a well-functioning software organization.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://jeremykun.com/2021/02/16/searching-for-rh-counterexamples-scaling-up/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/90b179348780a6e7fe8e502968dc534a?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">jeremykun</media:title>
		</media:content>
	</item>
		<item>
		<title>Searching for RH Counterexamples — Performance Profiling</title>
		<link>https://jeremykun.com/2021/02/02/searching-for-rh-counterexamples-performance-profiling/</link>
					<comments>https://jeremykun.com/2021/02/02/searching-for-rh-counterexamples-performance-profiling/#respond</comments>
		
		<dc:creator><![CDATA[j2kun]]></dc:creator>
		<pubDate>Tue, 02 Feb 2021 16:00:00 +0000</pubDate>
				<category><![CDATA[General]]></category>
		<category><![CDATA[databases]]></category>
		<category><![CDATA[debugging]]></category>
		<category><![CDATA[docker]]></category>
		<category><![CDATA[mathematics]]></category>
		<category><![CDATA[postgres]]></category>
		<category><![CDATA[profiling]]></category>
		<category><![CDATA[programming]]></category>
		<category><![CDATA[riemann hypothesis]]></category>
		<category><![CDATA[software]]></category>
		<guid isPermaLink="false">http://jeremykun.com/?p=117854</guid>

					<description><![CDATA[We&#8217;re ironically searching for counterexamples to the Riemann Hypothesis. Setting up Pytest Adding a Database Search Strategies Unbounded integers Deploying with Docker In the last article we ran into some performance issues with our deployed docker application. In this article we&#8217;ll dig in to see what happened, fix the problem, run into another problem, fix [&#8230;]]]></description>
										<content:encoded><![CDATA[
<p>We&#8217;re ironically searching for counterexamples to the Riemann Hypothesis.</p>



<ol><li><a href="https://jeremykun.com/2020/09/11/searching-for-rh-counterexamples-setting-up-pytest/">Setting up Pytest</a></li><li><a href="https://jeremykun.com/2020/09/11/searching-for-rh-counterexamples-adding-a-database/">Adding a Database</a></li><li><a href="https://jeremykun.com/2020/09/28/searching-for-rh-counterexamples-search-strategies/">Search Strategies</a></li><li><a href="https://jeremykun.com/2020/10/13/searching-for-rh-counterexamples-unbounded-integers/">Unbounded integers</a></li><li><a href="https://jeremykun.com/2021/01/04/searching-for-rh-counterexamples-deploying-with-docker/">Deploying with Docker</a></li></ol>



<p>In the last article we ran into some performance issues with our deployed docker application. In this article we&#8217;ll dig in to see what happened, fix the problem, run into another problem, fix it, and run the search until we rule out RH witness-value-based counterexamples among all numbers with fewer than 85 prime factors.</p>



<p>When debugging any issue, there are some straightforward steps to follow.</p>



<ol><li>Gather information to determine where the problem is.</li><li>Narrow down the source of the problem.</li><li>Reproduce the problem in an isolated environment (on your local machine or a separate EC2 instance).</li><li>Fix the problem.</li></ol>



<p>So far what I know is that the search ran for days on EC2 with docker, and didn&#8217;t make significantly more progress than when I ran it on my local machine for a few hours.</p>



<h2 class="has-text-align-center">Gathering information</h2>



<p>Gathering information first is an important step that&#8217;s easy to skip when you think you know the problem. In my experience, spending an extra 5-10 minutes just looking around can save hours of work you might spend fixing the wrong problem.</p>



<p>So we know the application is slow. But there are many different kinds of slow. We could be using up all the CPU, running low on free RAM, throttled by network speeds, and more. To narrow down which of these might be the problem, we can look at the server&#8217;s resource usage.</p>



<p>The CPU utilization EC2 dashboard below shows that after just a few hours of running the application, the average CPU utilization drops to 5-10% and stays at that level. Running <code>docker stats</code> shows that the search container has over 5 GiB of unused RAM. <code>df -h</code> shows that the server has more than 60% of its disk space free.</p>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-01-at-9.00.15-am.png"><img data-attachment-id="117882" data-permalink="https://jeremykun.com/screen-shot-2021-01-01-at-9-00-15-am/" data-orig-file="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-01-at-9.00.15-am.png" data-orig-size="2058,518" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2021-01-01-at-9.00.15-am" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-01-at-9.00.15-am.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-01-at-9.00.15-am.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-01-at-9.00.15-am.png?w=1024" alt="" class="wp-image-117882" srcset="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-01-at-9.00.15-am.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-01-at-9.00.15-am.png?w=2048 2048w, https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-01-at-9.00.15-am.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-01-at-9.00.15-am.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-01-at-9.00.15-am.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure>



<p>The fact that the CPU spikes like this suggests that the program is doing its work, just with gaps in between the real work, and some sort of waiting consuming the rest of the time.</p>



<p>Another angle we can look at is the timestamps recorded in the SearchMetadata table.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
divisor=# select * from searchmetadata order by start_time desc limit 9;
         start_time         |          end_time          |       search_state_type       | starting_search_state | ending_search_state 
----------------------------+----------------------------+-------------------------------+-----------------------+---------------------
 2021-01-01 15:40:42.374536 | 2021-01-01 16:15:34.838774 | SuperabundantEnumerationIndex | 71,1696047            | 71,1946047
 2021-01-01 15:06:13.947216 | 2021-01-01 15:40:42.313078 | SuperabundantEnumerationIndex | 71,1446047            | 71,1696047
 2021-01-01 14:32:14.692185 | 2021-01-01 15:06:13.880209 | SuperabundantEnumerationIndex | 71,1196047            | 71,1446047
 2021-01-01 13:57:39.725843 | 2021-01-01 14:32:14.635433 | SuperabundantEnumerationIndex | 71,946047             | 71,1196047
 2021-01-01 13:25:53.376243 | 2021-01-01 13:57:39.615891 | SuperabundantEnumerationIndex | 71,696047             | 71,946047
 2021-01-01 12:59:14.58666  | 2021-01-01 13:25:53.331857 | SuperabundantEnumerationIndex | 71,446047             | 71,696047
 2021-01-01 12:43:08.503441 | 2021-01-01 12:59:14.541995 | SuperabundantEnumerationIndex | 71,196047             | 71,446047
 2021-01-01 12:27:49.698012 | 2021-01-01 12:43:08.450301 | SuperabundantEnumerationIndex | 70,4034015            | 71,196047
 2021-01-01 12:14:44.970486 | 2021-01-01 12:27:49.625687 | SuperabundantEnumerationIndex | 70,3784015            | 70,4034015
</pre></div>


<p>As you can see, computing a single block of divisor sums takes over a half hour in many cases! This is nice, because we can isolate the computation of a single block on our local machine and time it.</p>



<h2 class="has-text-align-center">Narrowing Down</h2>



<p>Generally, since we ran this application on a local machine just fine for longer than we ran it in docker on EC2, I would think the culprit is related to the new thing that was introduced: docker and/or EC2. My local machine is a Mac, and the EC2 machine was Ubuntu linux, so there could be a difference there. It&#8217;s also strange that the system only started to slow down after about two hours, instead of being slow the whole time. That suggests there is something wrong with scaling, i.e., what happens as the application starts to grow in its resource usage.</p>



<p>Just to rule out the possibility that it&#8217;s a problem with computing and storing large blocks, let&#8217;s re-test the block with starting index <code>71,196047</code> and ending index <code>71,446047</code>, which took approximated 16 minutes on EC2. These three lines are between the start/end timestamps in the table. Only the second line does substantive work.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
start_state = search_strategy.search_state()
db.upsert(search_strategy.next_batch(batch_size))
end_state = search_strategy.search_state()
</pre></div>


<p>First we&#8217;ll just run the <code>next_batch</code> method, to remove the database upsert from consideration. We&#8217;ll also do it in a vanilla Python script, to remove the possibility that docker is introducing inefficiency, which is the most likely culprit, since docker is the new part from the last article. <a href="https://github.com/j2kun/riemann-divisor-sum/commit/4a9ec20ae1b4e7582706a5647202813899334874">This commit</a> has the timing test and the result shows that on average the block takes two minutes on my local machine.</p>



<p>Running the same code in a docker container on EC2 has the same result, which I verified by copying the <code>divisorsearch.Dockerfile</code> to <a href="https://github.com/j2kun/riemann-divisor-sum/commit/a775ee1e3301831f46f9a791c61bee931962e56e">a new dockerfile</a> and replacing the entrypoint command with</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
ENTRYPOINT &#91;"python3", "-u", "-m", "timing.ec2_timing_test"]
</pre></div>


<p>Then running (on a fresh EC2 instance with docker installed and the repo cloned)</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
docker build -t timing -f timing.Dockerfile .
docker run -dit --name timing --memory="15G" --env PGHOST="$PGHOST" timing:latest
docker logs -f timing
</pre></div>


<p>Once it finishes, I see it takes on average 40 seconds to compute the block. So any prior belief that the divisor computation might be the bottleneck are now gone.</p>



<p>Now let&#8217;s try the call to <code>db.upsert</code>, which builds the query and sends it over the network to the database container. Testing that in isolation—by computing the block first and then timing the upsert with <a href="https://github.com/j2kun/riemann-divisor-sum/commit/d0e40ecc6c48f592d09312bf91934e2c5f70e897">this commit</a> —shows it takes about 8 seconds to update an empty database with this block.</p>



<p>Both of these experiments were run on EC2 in docker, so we&#8217;ve narrowed down the problem enough to say that it only happens when the system has been running for that two hour window. My best guess is something to do with having a large database, or something to do with container-to-container networking transfer rates (the docker overhead). The next step in narrowing down the problem is to set up the scenario and run performance profiling tools on the system in situ.</p>



<p>I spin back up an r5.large. Then I run the deploy script and wait for two hours. It started slowing down after about 1h45m of runtime. At this point I stopped the <code>divisorsearch</code> container and ran the timing test container. And indeed, the upsert step has a strangely slow pattern (the numbers are in <em>seconds</em>):</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
Running sample 0
55.63926291465759
Running sample 1
61.28182792663574
Running sample 2
245.36470413208008  # 4 minutes
Running sample 3
1683.663686990738   # 28 minutes
</pre></div>


<p>At this point I killed the timing test. The initial few writes, while not great, are not as concerning as the massive degradation in performance over four writes. At this point I&#8217;m convinced that the database is the problem. So I read a lot about Postgres performance tuning (e.g. <a href="https://www.postgresql.org/docs/current/runtime-config-wal.html">these docs</a> and <a href="https://www.percona.com/blog/2018/08/31/tuning-postgresql-database-parameters-to-optimize-performance/">this blog post</a>), which I had never done before.</p>



<p>It appears that the way Postgres inserts work is by writing to a &#8220;write cache,&#8221; and then later a background process copies the content of the write cache to disk. This makes sense because disk writes are slow. But if the write cache is full, then Postgres will wait until the write cache gets emptied before it can continue. This seems likely here: the first few writes are fast, and then when the cache fills up later writes take eons.</p>



<p>I also learned that the <code>PRIMARY KEY</code> <a href="https://github.com/j2kun/riemann-divisor-sum/blob/825ba40f773d1091aaff525e9c24eb410fec2597/riemann/postgres_database.py#L36">part of our Postgres schema</a> incurs a penalty to write performance, because in order to enforce the uniqueness Postgres needs to maintain a data structure and update it on every write. I can imagine that, because an <code>mpz</code> (unbounded integer) is our primary key, that data structure might be unprepared to handle such large keys. This might explain why these writes are taking so long in the first place, given that each write itself is relatively small (~2 MiB). In our case we can probably get by without a primary key on this table, and instead put any uniqueness constraints needed on the search metadata table.</p>



<p>So let&#8217;s change one of these and see if it fixes the problem. First we can drop the primary key constraint, which we can do in situ by running the query</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
alter table riemanndivisorsums drop constraint divisor_sum_pk;
</pre></div>


<p>This query takes a long time, which seems like a good sign. Perhaps it&#8217;s deleting a lot of data. Once that&#8217;s done, I removed the <a href="https://github.com/j2kun/riemann-divisor-sum/blob/825ba40f773d1091aaff525e9c24eb410fec2597/riemann/postgres_database.py#L72"><code>ON CONFLICT</code> clause</a> from the upsert command (it becomes just an insert command), and then rebuild and re-run the timing container. This shows that each insert takes only 3-4 seconds, which is much more reasonable.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
Running sample 0
3.8325071334838867
Running sample 1
3.7897982597351074
Running sample 2
3.7978150844573975
Running sample 3
3.810023784637451
Running sample 4
3.8057897090911865
</pre></div>


<p>I am still curious about the possible fix by increasing the cache size, but my guess is that without the primary key change, increasing the cache size would just delay the application from slowing instead of fixing the problem permanently, so for now I will not touch it. Perhaps a reader with more experience with Postgres tuning would know better.</p>



<h2 class="has-text-align-center">Updating the application</h2>



<p>So with this new understanding, how should we update the application? Will anything go wrong if we delete the primary key on the <code>RiemannDivisorSums</code> table?</p>



<p>As suggested by our need to remove the <code>ON CONFLICT</code> clause, we might end up with duplicate rows. That is low risk, but scarier is if we stop the search mid-block and restart it, then if we&#8217;re really unlucky, we might get into a state where we skip a block and don&#8217;t notice, and that block contains the unique counterexample to the Riemann Hypothesis! We simply can&#8217;t let that happen. This is far too important. To be fair, this is a risk even before we remove the primary key. I&#8217;m just realizing it now.</p>



<p>We&#8217;ll rearchitect the system to mitigate that in a future post, and it will double as providing a means to scale horizontally. For now, this <a href="https://github.com/j2kun/riemann-divisor-sum/pull/7">pull request</a> removes the primary key constraint and does a bit of superficial cleanup. Now let&#8217;s re-run it!</p>



<h2 class="has-text-align-center">Out of RAM again</h2>



<p>This time, it ran for three hours before the divisorsearch container hit the 15 GiB RAM limit and crashed. Restarting the container and watching <code>docker stats</code> showed that it plain ran out of RAM. This is a much easier problem to solve because it involves only one container, and the virtual machine doesn&#8217;t slow to a crawl when it occurs, the container just stops running. </p>



<p>A quick <a href="https://github.com/j2kun/riemann-divisor-sum/commit/fef630598668330b8f2e4a40ea861ac35d118a76">python memory profile reveals</a> the top 10 memory usages by line, the top being <code>search_strategy.py:119</code> clocking in at about 2.2 GiB.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
&#91; Top 10 ]
riemann/search_strategy.py:119: size=2245 MiB, count=9394253, average=251 B
venv/lib/python3.7/site-packages/llvmlite/ir/values.py:224: size=29.6 MiB, count=9224, average=3367 B
&lt;string&gt;:2: size=26.7 MiB, count=499846, average=56 B
riemann/superabundant.py:67: size=19.1 MiB, count=500003, average=40 B
riemann/superabundant.py:69: size=15.3 MiB, count=250000, average=64 B
riemann/superabundant.py:70: size=13.4 MiB, count=250000, average=56 B
venv/lib/python3.7/site-packages/llvmlite/ir/_utils.py:48: size=3491 KiB, count=6411, average=558 B
venv/lib/python3.7/site-packages/llvmlite/ir/values.py:215: size=2150 KiB, count=19267, average=114 B
riemann/search_strategy.py:109: size=2066 KiB, count=1, average=2066 KiB
venv/lib/python3.7/site-packages/llvmlite/ir/_utils.py:58: size=1135 KiB, count=2018, average=576 B
</pre></div>


<p>This line, not surprisingly, is the computation of the full list of partitions of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="n" class="latex" />. The length of this list <a href="https://en.wikipedia.org/wiki/Partition_(number_theory)#Partition_function">grows superpolynomially</a> in <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="n" class="latex" />, which explains why it takes up all the RAM. </p>



<p>Since we only need to look at at most <code>batch_size</code> different partitions of <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="n" class="latex" /> at a given time, we can probably fix this by adding a layer of indirection so that new sections of the partition list are generated on the fly and old sections are forgotten once the search strategy is done with them.</p>



<p>This <a href="https://github.com/j2kun/riemann-divisor-sum/pull/9">pull request</a> does that, and running the memory test again shows the 2.5 GiB line above reduced to 0.5 GiB. The <a href="https://github.com/j2kun/riemann-divisor-sum/pull/9">pull request</a> description also has some thoughts about the compute/memory tradeoff being made by this choice, and thoughts about how to improve the compute side.</p>



<p>So let&#8217;s run it again!! This time it runs for about 19 hours before crashing, as shown by the EC2 CPU usage metrics.</p>



<figure class="wp-block-image size-large"><a href="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-26-at-6.33.26-pm.png"><img data-attachment-id="117925" data-permalink="https://jeremykun.com/screen-shot-2021-01-26-at-6-33-26-pm/" data-orig-file="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-26-at-6.33.26-pm.png" data-orig-size="1972,398" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2021-01-26-at-6.33.26-pm" data-image-description="" data-image-caption="" data-medium-file="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-26-at-6.33.26-pm.png?w=300" data-large-file="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-26-at-6.33.26-pm.png?w=1024" src="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-26-at-6.33.26-pm.png?w=1024" alt="" class="wp-image-117925" srcset="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-26-at-6.33.26-pm.png?w=1024 1024w, https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-26-at-6.33.26-pm.png?w=150 150w, https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-26-at-6.33.26-pm.png?w=300 300w, https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-26-at-6.33.26-pm.png?w=768 768w, https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-26-at-6.33.26-pm.png 1972w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>The application ran for about 19 hours before crashing.</figcaption></figure>



<p>According to the <code>SearchMetadata</code> table, we got up to part way through <img src="https://s0.wp.com/latex.php?latex=n%3D87&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="n=87" class="latex" /></p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
divisor=# select * from searchmetadata order by start_time desc limit 3;
         start_time         |          end_time          |       search_state_type       | starting_search_state | ending_search_state 
----------------------------+----------------------------+-------------------------------+-----------------------+---------------------
 2021-01-27 00:00:11.59719  | 2021-01-27 00:01:14.709715 | SuperabundantEnumerationIndex | 87,15203332           | 87,15453332
 2021-01-26 23:59:01.285508 | 2021-01-27 00:00:11.596075 | SuperabundantEnumerationIndex | 87,14953332           | 87,15203332
 2021-01-26 23:57:59.809282 | 2021-01-26 23:59:01.284257 | SuperabundantEnumerationIndex | 87,14703332           | 87,14953332
</pre></div>


<p>Logging into the server and running <code>df -h</code> we can see the reason for the crash is that the disk is full. We filled up a 60 GiB SSD full of Riemann divisor sums. It&#8217;s quite an achievement. I&#8217;d like to thank my producers, the support staff, my mom, and the Academy.</p>



<p>But seriously, this is the best case scenario. Our limiting factor now is just how much we want to pay for disk space (and/or, any tricks we come up with to reduce disk space).</p>



<p>Analyzing the results, it seems we currently have 949 examples of numbers with witness values bigger than 1.767. Here are the top few:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
                                                                                       n                                                                                        |                                                                                   divisor_sum                                                                                   |   witness_value    
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------
 533187564151227457465199401229454876347036513892234205802944360099435118364718466037392872608220305945979716166395732328054742493039981726997486787797703088097204529280000    | 5634790045188254963919691913741193557789227457256740288527114259589826494137632464425981545755572664137227875215269764405574488353138596297944567958732800000000000000000000    | 1.7689901658397056
 1392134632248635659178066321747923959130643639405311242305337754828812319490126543178571468950966856255821713228187290673772173611070448373361584302343872292681996160000      | 14673932409344413968540864358701024890076113169939427834706026717681839828483417876109326942071803812857364258373098344806183563419631761192563979059200000000000000000000      | 1.7688977455109272
 3673178449204843427910465228886342900080853929829317262019360830682882109472629401526573796704398037614305311947723722094385682351109362462695473093255599716839040000         | 38615611603537931496160169365002697079147666236682704828173754520215367969693204937129807742294220560150958574666048275805746219525346739980431523840000000000000000000         | 1.7688303488154073
 2784269264497271318356132643495847918261287278810622484610675509657624638980253086357142937901933712511643426456374581347544347222140896746723168604687744585363992320000      | 29355033324995298095346770663840105972086801871471400577978104254473441181064775868425839681379597759477726740614478613571725301516068423098949435392000000000000000000000      |  1.768798862584946
 9847663402693950208875241900499578820592101688550448423644399009873678577674609655567221975078815114247467324256631962719532660458738237165403413118647720420480000            | 103250298405181635016471041082894911976330658386852151946988648449773711148912312666122480594369573690243204745096385764186487217982210534707036160000000000000000000           | 1.7687597437473672
</pre></div>


<p>The best so far achieves 1.76899. Recall out last best number achieved a witness value of 1.7679, a modest improvement if we hope to get to 1.82 (or even the supposedly infinitely many examples with witness value bigger than 1.81!).</p>



<h2 class="has-text-align-center">What&#8217;s next?</h2>



<p>We&#8217;re not stopping until we disprove the Riemann hypothesis. So strap in, this is going to be a long series.</p>



<p>Since the limiting factor in our search is currently storage space, I&#8217;d like to spend some time coming up with a means to avoid storing the witness value and divisor sum for every single number in our search strategy, but still maintain the ability to claim our result (no counterexamples below N) while providing the means for a skeptical person to verify our claims as much as they desire without also storing the entire search space on disk.</p>



<p>Once we free ourselves from disk space constraints, the second limiting factor is that our search uses only a single CPU at a time. We should rearchitect the system so that we can <em>scale horizontally</em>, meaning we can increase the search rate by using more computers. We can do this by turning our system into a &#8220;worker&#8221; model, in which there is a continually growing queue of search blocks to process, and each worker machine asks for the next block to compute, computes it, and then stores the result in the database. There are some tricks required to ensure the workers don&#8217;t step on each other&#8217;s toes, but it&#8217;s a pretty standard computational model.</p>



<p>There&#8217;s also the possibility of analyzing our existing database to try to identify what properties of the top <img src="https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=36312d&#038;s=0&#038;c=20201002" alt="n" class="latex" /> values make their witness values so large. We could do this by re-computing the prime factorization of each of the numbers and squinting at it very hard. The benefit of doing this would ultimately be to design a new <code>SearchStrategy</code> that might find larger witness values faster than the superabundant enumeration. That approach also has the risk that, without a <em>proof</em> that the new search strategy is exhaustive, we might accidentally skip over <em>the unique counterexample</em> to the Riemann hypothesis!</p>



<p>And then, of course, there&#8217;s the business of a front-end and plotting. But I&#8217;m having a bit too much fun with the back end stuff to worry about that for now. Protest in the comments if you prefer I work on visualization instead.</p>



<h2 class="has-text-align-center">Postscript: a false start</h2>



<p>Here&#8217;s one thing I tried during debugging that turned out to be a dead end: the <a href="https://perf.wiki.kernel.org/index.php/Main_Page">linux <code>perf</code> tool</a>. It is way more detailed than I can understand, but it provides a means to let me know if the program is stuck in the OS kernel or whether it&#8217;s the applications that are slow. I ran the perf tool when the deployed application was in the &#8220;good performance&#8221; regime (early in its run). I saw something like this:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
Samples: 59K of event 'cpu-clock:pppH', Event count (approx.): 119984000000
Overhead  Command          Shared Object                                Symbol
  49.13%  swapper          &#91;kernel.kallsyms]                            &#91;k] native_safe_halt
   8.04%  python3          libpython3.7m.so.1.0                         &#91;.] _PyEval_EvalFrameDefault
   1.35%  python3          libpython3.7m.so.1.0                         &#91;.] _PyDict_LoadGlobal
   0.86%  python3          libc-2.28.so                                 &#91;.] realloc
   0.86%  python3          libpython3.7m.so.1.0                         &#91;.] 0x0000000000139736
   0.77%  python3          libpython3.7m.so.1.0                         &#91;.] PyTuple_New
   0.74%  python3          libpython3.7m.so.1.0                         &#91;.] PyType_IsSubtype

</pre></div>


<p>It says half the time is spent by something called &#8220;swapper&#8221; doing something called <code>native_safe_halt</code>, and this in kernel mode (<code>[k]</code>), i.e., run by the OS. The rest of the list is dominated by python3 and postgres doing stuff like <code>_PyDict_LoadGlobal</code> which I assume is useful Python work. When I <a href="https://lists.opensuse.org/opensuse-bugs/2015-07/msg03106.html">look up</a> <code>native_safe_halt</code> (finding an explanation is surprisingly hard), I learn that this indicates the system is doing nothing. So 49% of the time the CPU is idle. This fits with good behavior in our case, because each docker container gets one of the two CPUs on the host, and the postgres container is most often doing nothing while waiting for the search container to send it data. This also matches the CPU utilization on the EC2 dashboard. So everything appears in order.</p>



<p>I ran perf again after the application started slowing down, and I saw that <code>native_safe_halt</code> is at 95%. This tells me nothing new, sadly. I also tried running it during the timing test and saw about 40% usage of some symbols like <code>__softirqentry_text_start</code> and <code>__lock_text_start</code>. Google failed to help me understand what that was, so it was a dead end.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://jeremykun.com/2021/02/02/searching-for-rh-counterexamples-performance-profiling/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/90b179348780a6e7fe8e502968dc534a?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">jeremykun</media:title>
		</media:content>

		<media:content url="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-01-at-9.00.15-am.png?w=1024" medium="image"/>

		<media:content url="https://jeremykun.files.wordpress.com/2021/01/screen-shot-2021-01-26-at-6.33.26-pm.png?w=1024" medium="image"/>
	</item>
	</channel>
</rss>
