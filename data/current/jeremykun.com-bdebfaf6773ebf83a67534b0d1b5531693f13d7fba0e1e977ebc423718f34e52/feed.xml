<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#">

<channel>
	<title>Math ∩ Programming</title>
	<atom:link href="https://jeremykun.com/feed/" rel="self" type="application/rss+xml"/>
	<link>https://jeremykun.com</link>
	<description/>
	<lastBuildDate>Wed, 16 Nov 2022 19:22:43 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	

<image>
	<url>https://s0.wp.com/i/webclip.png</url>
	<title>Math ∩ Programming</title>
	<link>https://jeremykun.com</link>
	<width>32</width>
	<height>32</height>
</image> 
<site xmlns="com-wordpress:feed-additions:1">23934684</site>	<item>
		<title>Polynomial Multiplication Using the FFT</title>
		<link>https://jeremykun.com/2022/11/16/polynomial-multiplication-using-the-fft/</link>
					<comments>https://jeremykun.com/2022/11/16/polynomial-multiplication-using-the-fft/#respond</comments>
		
		<dc:creator><![CDATA[j2kun]]></dc:creator>
		<pubDate>Wed, 16 Nov 2022 16:00:00 +0000</pubDate>
				<category><![CDATA[Algorithms]]></category>
		<category><![CDATA[Number Theory]]></category>
		<category><![CDATA[Program Gallery]]></category>
		<category><![CDATA[fft]]></category>
		<category><![CDATA[fourier transform]]></category>
		<category><![CDATA[mathematics]]></category>
		<category><![CDATA[polynomial interpolation]]></category>
		<category><![CDATA[polynomials]]></category>
		<category><![CDATA[programming]]></category>
		<category><![CDATA[python]]></category>
		<guid isPermaLink="false">https://jeremykun.com/?p=118895</guid>

					<description><![CDATA[Problem: Compute the product of two polynomials efficiently. Solution: Discussion: The Fourier Transform has a lot of applications to science, and I&#8217;ve covered it on this blog before, see the Signal Processing section of Main Content. But it also has applications to fast computational mathematics. The naive algorithm for multiplying two polynomials is the &#8220;grade-school&#8221; [&#8230;]]]></description>
										<content:encoded><![CDATA[
<p><strong>Problem:</strong> Compute the product of two polynomials efficiently.</p>



<p><strong>Solution:</strong></p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate">
import numpy
from numpy.fft import fft, ifft


def poly_mul(p1, p2):
    &quot;&quot;&quot;Multiply two polynomials.

    p1 and p2 are arrays of coefficients in degree-increasing order.
    &quot;&quot;&quot;
    deg1 = p1.shape&#91;0] - 1
    deg2 = p1.shape&#91;0] - 1
    # Would be 2*(deg1 + deg2) + 1, but the next-power-of-2 handles the +1
    total_num_pts = 2 * (deg1 + deg2)
    next_power_of_2 = 1 &lt;&lt; (total_num_pts - 1).bit_length()

    ff_p1 = fft(numpy.pad(p1, (0, next_power_of_2 - p1.shape&#91;0])))
    ff_p2 = fft(numpy.pad(p2, (0, next_power_of_2 - p2.shape&#91;0])))
    product = ff_p1 * ff_p2
    inverted = ifft(product)
    rounded = numpy.round(numpy.real(inverted)).astype(numpy.int32)
    return numpy.trim_zeros(rounded, trim='b')
</pre></div>


<p><strong>Discussion:</strong> The <a href="https://jeremykun.com/2012/05/27/the-fourier-transform-a-primer/">Fourier Transform</a> has a lot of applications to science, and I&#8217;ve covered it on this blog before, see the Signal Processing section of <a href="http://Signal Processing">Main Content</a>. But it also has applications to fast computational mathematics.</p>



<p>The naive algorithm for multiplying two polynomials is the &#8220;grade-school&#8221; algorithm most readers will already be familiar with (see e.g., <a href="https://brilliant.org/wiki/polynomial-multiplication/">this page</a>), but for large polynomials that algorithm is slow. It requires $O(n^2)$ arithmetic operations to multiply two polynomials of degree $n$.</p>



<p>This short tip shows a different approach, which is based on the idea of <em>polynomial interpolation</em>. As a side note, I show the basic theory of polynomial interpolation in chapter 2 of my book, <a href="https://pimbook.org">A Programmer&#8217;s Introduction to Mathematics</a>, along with an application to cryptography called &#8220;<a href="https://jeremykun.com/2014/06/23/the-mathematics-of-secret-sharing/">Secret Sharing</a>.&#8221;</p>



<p>The core idea is that given $n+1$ distinct evaluations of a polynomial $p(x)$ (i.e., points $(x, p(x))$ with different $x$ inputs), you can reconstruct the coefficients of $p(x)$ exactly. And if you have two such point sets for two different polynomials $p(x), q(x)$, a valid point set of the product $(pq)(x)$ is the product of the points that have the same $x$ inputs.</p>



<p class="has-text-align-center">\[ \begin{aligned} p(x) &amp;= \{ (x_0, p(x_0)), (x_1, p(x_1)), \dots, (x_n, p(x_n))  \} \\ q(x) &amp;= \{ (x_0, q(x_0)), (x_1, q(x_1)), \dots, (x_n, q(x_n))  \} \\ (pq)(x) &amp;= \{ (x_0, p(x_0)q(x_0)), (x_1, p(x_1)q(x_1)), \dots, (x_n, p(x_n)q(x_n))  \} \end{aligned} \]</p>



<p>The above uses $=$ loosely to represent that the polynomial $p$ can be represented by the point set on the right hand side.</p>



<p>So given two polynomials $p(x), q(x)$ in their coefficient forms, one can first convert them to their point forms, multiply the points, and then reconstruct the resulting product.</p>



<p>The problem is that the two conversions, both to and from the coefficient form, are inefficient for arbitrary choices of points $x_0, \dots, x_n$. The trick comes from choosing special points, in such a way that the intermediate values computed in the conversion steps can be reused. This is where the Fourier Transform comes in: choose $x_0 = \omega_{N}$, the complex-N-th root of unity, and $x_k = \omega_N^k$ as its exponents. $N$ is required to be large enough so that $\omega_N$&#8217;s exponents have at least $2n+1$ distinct values required for interpolating a degree-at-most-$2n$ polynomial, and because we&#8217;re doing the Fourier Transform, it will naturally be &#8220;the next largest power of 2&#8221; bigger than the degree of the product polynomial.</p>



<p>Then one has to observe that, by its very formula, the Fourier Transform <em>is exactly</em> the evaluation of a polynomial at the powers of the $N$-th root of unity! In formulas: if $a = (a_0, \dots, a_{n-1})$ is a list of real numbers define $p_a(x) = a_0 + a_1x + \dots + a_{n-1}x^{n-1}$. Then $\mathscr{F}(a)(k)$, the Fourier Transform of $a$ at index $k$, is equal to $p_a(\omega_n^k)$. <a href="http://www.cs.toronto.edu/~denisp/csc373/docs/tutorial3-adv-writeup.pdf">These notes</a> by Denis Pankratov have more details showing that the Fourier Transform formula is a polynomial evaluation (see Section 3), and <a href="https://www.youtube.com/watch?v=h7apO7q16V0">this YouTube video by Reducible</a> also has a nice exposition. This interpretation of the FT as polynomial evaluation seems to inspire quite a few additional techniques for computing the Fourier Transform that I plan to write about in the future.</p>



<p>The last step is to reconstruct the product polynomial from the product of the two point sets, but because the Fourier Transform is an invertible function (and linear, too), the inverse Fourier Transform does exactly that: given a list of the $n$ evaluations of a polynomial at $\omega_n^k, k=0, \dots, n-1$, return the coefficients of the polynomial.</p>



<p>This all fits together into the code above:</p>



<ol>
<li>Pad the input coefficient lists with zeros, so that the lists are a power of 2 and at least 1 more than the degree of the output product polynomial.</li>



<li>Compute the FFT of the two padded polynomials.</li>



<li>Multiply the result pointwise.</li>



<li>Compute the IFFT of the product.</li>



<li>Round the resulting (complex) values back to integers.</li>
</ol>



<p>Hey, wait a minute! What about precision issues?</p>



<p>They are a problem when you have large numbers or large polynomials, because the intermediate values in steps 2-4 can lose precision due to the floating point math involved. <a href="https://people.eecs.berkeley.edu/~fateman/papers/shortfft">This short note</a> of Richard Fateman discusses some of those issues, and two paths forward include: deal with it somehow, or use an integer-exact analogue called the <a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform_over_a_ring#Number-theoretic_transform">Number Theoretic Transform</a> (which itself has issues I&#8217;ll discuss in a future, longer article).</p>



<p>Postscript: I&#8217;m not sure how widely this technique is used. It appears the <a href="https://libntl.org/">NTL library</a> uses the polynomial version of <a href="https://en.wikipedia.org/wiki/Karatsuba_algorithm">Karatsuba multiplication</a> instead (though it implements FFT elsewhere). However, I know for sure that much software involved in doing <a href="https://en.wikipedia.org/wiki/Homomorphic_encryption">fully homomorphic encryption</a> rely on the FFT for performance reasons, and the ones that don&#8217;t instead use the NTT.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://jeremykun.com/2022/11/16/polynomial-multiplication-using-the-fft/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">118895</post-id>	</item>
		<item>
		<title>Carnival of Mathematics #209</title>
		<link>https://jeremykun.com/2022/10/02/carnival-of-mathematics-209/</link>
					<comments>https://jeremykun.com/2022/10/02/carnival-of-mathematics-209/#respond</comments>
		
		<dc:creator><![CDATA[j2kun]]></dc:creator>
		<pubDate>Sun, 02 Oct 2022 15:00:00 +0000</pubDate>
				<category><![CDATA[General]]></category>
		<category><![CDATA[carnival]]></category>
		<category><![CDATA[knot theory]]></category>
		<category><![CDATA[mathematics]]></category>
		<category><![CDATA[sieve]]></category>
		<category><![CDATA[umap]]></category>
		<guid isPermaLink="false">https://jeremykun.com/?p=118672</guid>

					<description><![CDATA[Welcome to the 209th Carnival of Mathematics! 209 has a few distinctions, including being the smallest number with 6 representations as a sum of 3 positive squares: $$\begin{aligned}209 &#38;= 1^2 + 8^2 + 12^2 \\ &#38;= 2^2 + 3^2 + 14^2 \\ &#38;= 2^2 + 6^2 + 13^2 \\ &#38;= 3^2 + 10^2 + 10^2 [&#8230;]]]></description>
										<content:encoded><![CDATA[
<p>Welcome to the 209th <a href="https://aperiodical.com/carnival-of-mathematics/">Carnival of Mathematics</a>!</p>



<p>209 has a few distinctions, including being the smallest number with 6 representations as a sum of 3 positive squares: </p>



<p>$$\begin{aligned}209 &amp;= 1^2 + 8^2 + 12^2 \\ &amp;= 2^2 + 3^2 + 14^2 \\ &amp;= 2^2 + 6^2 + 13^2 \\ &amp;= 3^2 + 10^2 + 10^2 \\ &amp;= 4^2 + 7^2 + 12^2 \\ &amp;= 8^2 + 8^2 + 9^2 \end{aligned}$$</p>



<p>As well as being the <a href="https://oeis.org/A002858/list">43rd Ulam number</a>, the <a href="https://oeis.org/A000837/list">number of partitions of 16 into relatively prime parts</a> and the <a href="https://oeis.org/A001156/list">number of partitions of 63 into squares</a>.</p>



<p>Be sure to submit fun math you find in October to <a href="https://aperiodical.com/carnival-of-mathematics/">the next carvinal</a> host!</p>



<h2>Math YouTubers</h2>



<p>The Heidelberg Laureate forum took place, which featured lectures from renowned mathematicians and computer scientists, like <a href="https://www.youtube.com/watch?v=7vpTTH1_OCo&amp;list=PLS0TjJA_GZXf0GaDoa991OcSv6_blmm-X&amp;index=3">Rob Tarjan</a> and <a href="https://www.youtube.com/watch?v=oGUDQ0pg9wI&amp;list=PLS0TjJA_GZXf0GaDoa991OcSv6_blmm-X&amp;index=9">Avi Wigderson</a> on the CS theory side, as well as a <a href="https://www.youtube.com/watch?v=y-xNxhBmnc8&amp;list=PLS0TjJA_GZXf0GaDoa991OcSv6_blmm-X&amp;index=14">panel discussion on post-quantum cryptography</a> with none other than Vint Cerf, Whitfield Diffie, and Adi Shamir. All the videos are <a href="https://www.youtube.com/playlist?list=PLS0TjJA_GZXf0GaDoa991OcSv6_blmm-X">on YouTube</a>.</p>



<p><a href="https://community.plu.edu/~edgartj/">Tom Edgar</a>, who is behind the <a data-type="URL" data-id="https://www.youtube.com/channel/UCT9Fyqn0izh-wX-wDzKBwAA" href="https://www.youtube.com/channel/UCT9Fyqn0izh-wX-wDzKBwAA">Mathematical Visual Proofs</a> YouTube channel, <a href="https://www.youtube.com/watch?v=KhfZK5IIK9E">published a video</a> (using <a href="https://github.com/3b1b/manim">manim</a>) exploring for which $n$ it is possible to divide a disk into $n$ equal pieces using a straightedge and compass. It was based on a proof from Roger Nelsen&#8217;s and Claudi Alsina&#8217;s book, <a data-type="URL" data-id="https://amzn.to/3RxmUwm" href="https://amzn.to/3RxmUwm">&#8220;Icons of Mathematics&#8221;</a>.</p>



<p>The folks at <a href="https://ganitcharcha.com/">Ganit Charcha</a> also published a talk &#8220;<a href="https://www.youtube.com/watch?v=9piXoLVvgnc">Fascinating Facts About Pi</a>&#8221; from a Pi Day 2022 celebration. The video includes a question that was new to me about interpreting subsequences of pi digits as indexes and doing reverse lookups until you find a loop.</p>



<p>Henry Segerman published two nice videos, including one on <a href="https://www.youtube.com/shorts/KYMYshbhKcw">an illusion of a square and circle in the same shape</a>, and a preview of a <a href="https://www.youtube.com/watch?v=CW0RJ-ejuvI">genus-2 holonomy maze</a> (Augh, my wallet! I have both of his original holonomy mazes and my houseguests love playing with them!)</p>



<p>Steve Mould published <a href="https://www.youtube.com/watch?v=rjueHI002Fg">a nice video</a> about the <a href="https://publicdomainreview.org/collection/chladni-figures-1787">Chladni figures</a> used (or adapted) in the new Lord of the Rings TV series&#8217; title sequence.</p>



<p>The Simons institute has been <a href="https://www.youtube.com/playlist?list=PLgKuh-lKre13jJUlIAxed0F0awWJa2aXK">doing a workshop on graph limits</a>, which aims to cover some of the theory about things like low-rank matrix completion, random graphs, and various models of networks. Their lectures are posted on their YouTube page.</p>



<h2>Math Twitter</h2>



<p>Peter Rowlett shared <a href="https://twitter.com/peterrowlett/status/1571413441507921925">a nice activity</a> with his son about distinct colorings of a square divided into four triangular regions.</p>



<p>Krystal Guo <a href="https://twitter.com/guo_krystal/status/1573358029449601030">showed off her approach</a> to LiveTeX&#8217;ing lectures.</p>



<p>Tamás Görbe gave <a href="https://twitter.com/TamasGorbe">a nice thread</a> about a function that enumerates all rational numbers exactly once.</p>



<p>Every math club leader should be called <a href="https://twitter.com/bobloch/status/1571921252348207104">the Prime Minister</a>.</p>



<p>In doing research for <a href="https://jeremykun.com/2022/03/16/my-next-book-will-be-practical-math-for-programmers/">my book</a>, I was writing a chapter on balanced incomplete block designs, and I found a few nice tidbits in threads (<a href="https://twitter.com/jeremyjkun/status/1568631206220304384">thread 1</a>, <a href="https://twitter.com/jeremyjkun/status/1571170955967475712">thread 2</a>). A few here: Latin squares were on <a href="https://twitter.com/jeremyjkun/status/1571172888035856385">Islamic amulets</a> from the 1200&#8217;s. The entire back catalog of &#8220;The Mathematical Scientist&#8221; journal <a href="https://twitter.com/VinceVatter/status/1571246664660262913">is available on Google Drive</a>, and through it I found an old article describing the very first use of Latin squares for experimental design, in which a man ran an experiment on what crop was best to feed his sheep during the winter months in France in the 1800&#8217;s. Finally, I determined that NFL season scheduling is done <a href="https://www.gurobi.com/case_study/nfl-english/">via integer linear programming</a>.</p>



<h2>Math Bloggers</h2>



<p><a href="https://twitter.com/cronokirby">Lúcás Meier</a> published <a href="https://cronokirby.com/posts/2022/08/the-paper-that-keeps-showing-up/">a nice article</a> at the end of August (which I only discovered in September, it counts!) going over the details of his favorite cryptography paper “Unifying Zero-Knowledge Proofs of Knowledge”, by Ueli Maurer, which gives a single zero-knowledge protocol that generalizes Schnorr, Fiat-Shamir, and a few others for proving knowledge of logarithms and roots.</p>



<p>Ralph Levien <a href="https://raphlinus.github.io/curves/2022/09/09/parallel-beziers.html">published a blog post</a> about how to efficiently draw a decent approximation to the curve parallel to a given cubic Bezier curve. He has a previous blog post about <a href="https://raphlinus.github.io/curves/2021/03/11/bezier-fitting.html">fitting cubic Beziers to data</a>, and a variety of other interesting graphics-inspired <a href="https://raphlinus.github.io/">math articles</a> in between articles about Rust and GPUs.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://jeremykun.com/2022/10/02/carnival-of-mathematics-209/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">118672</post-id>	</item>
		<item>
		<title>Key Switching in LWE</title>
		<link>https://jeremykun.com/2022/08/29/key-switching-in-lwe/</link>
					<comments>https://jeremykun.com/2022/08/29/key-switching-in-lwe/#respond</comments>
		
		<dc:creator><![CDATA[j2kun]]></dc:creator>
		<pubDate>Mon, 29 Aug 2022 15:00:00 +0000</pubDate>
				<category><![CDATA[Analysis]]></category>
		<category><![CDATA[Cryptography]]></category>
		<category><![CDATA[cryptography]]></category>
		<category><![CDATA[fully homomorphic encryption]]></category>
		<category><![CDATA[gadget decomposition]]></category>
		<category><![CDATA[learning with errors]]></category>
		<category><![CDATA[mathematics]]></category>
		<category><![CDATA[real analysis]]></category>
		<guid isPermaLink="false">https://jeremykun.com/?p=118460</guid>

					<description><![CDATA[An overview of key switching, a technique used to change a ciphertext from being encrypted with one secret key to another.]]></description>
										<content:encoded><![CDATA[
<p><a href="https://jeremykun.com/2022/07/16/modulus-switching-in-lwe/" data-type="post" data-id="118420">Last time</a> we covered an operation in the LWE encryption scheme called modulus switching, which allows one to switch from one modulus to another, at the cost of introducing a small amount of extra noise, roughly $\sqrt{n}$, where $n$ is the dimension of the LWE ciphertext.</p>



<p>This time we&#8217;ll cover a more sophisticated operation called <em>key switching</em>, which allows one to switch an LWE ciphertext from being encrypted under one secret key to another, without ever knowing either secret key.</p>



<h2 class="has-text-align-center">Reminder of LWE</h2>



<p>A literal repetition of the last article. The LWE encryption scheme I&#8217;ll use has the following parameters:</p>



<ul>
<li>A plaintext space $\mathbb{Z}/q\mathbb{Z}$, where $q \geq 2$ is a positive integer. This is the space that the underlying message comes from.</li>



<li>An <em>LWE dimension</em> $n \in \mathbb{N}$.</li>



<li>A discrete Gaussian <em>error distribution</em> $D$ with a mean of zero and a fixed standard deviation.</li>
</ul>



<p>An LWE secret key is defined as a vector in $\{0, 1\}^n$ (uniformly sampled). An LWE ciphertext is defined as a vector $a = (a_1, \dots, a_n)$, sampled uniformly over $(\mathbb{Z} / q\mathbb{Z})^n$, and a scalar $b = \langle a, s \rangle + m + e$, where $e$ is drawn from $D$ and all arithmetic is done modulo $q$. Note that $e$ must be small for the encryption to be valid.</p>



<p>Sometimes I will denote by $\textup{LWE}_s(x)$ the LWE encryption of plaintext $x$ under the secret key $s$, and it should be understood that this is a fixed (but arbitrary) draw from the distribution of LWE ciphertexts described above.</p>



<h2 class="has-text-align-center">Main idea: homomorphically almost-decrypt</h2>



<p>The main idea is to encrypt each entry of the original secret key using the new secret key (this collection of encryptions is jointly called a key-switching key), and then use this to homomorphically evaluate the first step of the decryption function (i.e., compute $b &#8211; \langle a, s \rangle$). The result is an encryption of the (noisy) message under the new key.</p>



<p>First we&#8217;ll show how this works in a naïve sense. In particular, doing what I said in the last paragraph verbatim won&#8217;t work because the error will grow too large. But we&#8217;ll do it anyway, measure the error, and the remainder of the article will show how the <a href="https://jeremykun.com/2021/12/11/the-gadget-decomposition-in-fhe/" data-type="post" data-id="118248">gadget decomposition</a> can be used to reduce the error.</p>



<h2 class="has-text-align-center">Key switching, without gadget decompositions</h2>



<p>Start with an LWE ciphertext for the plaintext $m$. Call it </p>



<p class="has-text-align-center">$\displaystyle c = (a_1, \dots, a_n, b) \in (\mathbb{Z}/q\mathbb{Z})^{n+1}$</p>



<p>where </p>



<p class="has-text-align-center">$\displaystyle b = \left ( \sum_{i=1}^n a_i s_i \right ) + m + e_{\textup{original}}$</p>



<p>and $s = (s_1, \dots, s_n) \in \{ 0,1\}^n$ is the secret key. Now say we have another secret key, possibly of a different dimension $t = (t_1, \dots, t_m) \in \{ 0, 1\}^m$, and we would like to switch the ciphertext $c$ to a ciphertext $c&#8217;$ which encrypts the same underlying message $m$, but under the new secret key $t$. That is, we would like to write</p>



<p class="has-text-align-center">$\displaystyle c&#8217; = (a&#8217;_1, \dots, a&#8217;_m, b&#8217;) \in (\mathbb{Z}/q\mathbb{Z})^{m+1}$</p>



<p>where </p>



<p class="has-text-align-center">$\displaystyle b&#8217; = \left ( \sum_{i=1}^n a&#8217;_i t_i \right ) + m + e_{\textup{original}} + e_{\textup{new}}$</p>



<p>implying that there is possibly some additional error introduced as a result. As usual, so long as the total error in the ciphertext remains small enough (and $m$ is stored in the significant bits of the underlying integer space), the result will still be a valid LWE ciphertext.</p>



<p>Define the <em>key switching key</em> $\textup{KSK}(s, t)$ as follows (I will omit the $s, t$ and just call it KSK from now on):</p>



<p class="has-text-align-center">$\displaystyle \textup{KSK} = \{ \textup{KSK}_i = \textup{LWE}_t(s_i) = (x_{i, 1}, \dots, x_{i, m}, y_i) \mid i=1, \dots, n\}$</p>



<p>In other words, $\textup{KSK}_i$ encrypts bit $s_i$, and $y_i = \langle x_i, t \rangle + s_i + e_i$ makes it a valid LWE encryption.</p>



<p>Now the algorithm to switch keys is merely as follows (where the first vector has $m$ leading zeros to ensure the dimensions align):</p>



<p class="has-text-align-center">$\displaystyle c&#8217; = (0, \dots, 0, b) &#8211; \sum_{i=1}^n a_i \textup{KSK}_i$</p>



<p>This is computing a linear combination of the $\textup{KSK}_i$. The specific linear combination is the first step of LWE decryption ($b &#8211; \langle a, s \rangle$), but performed on ciphertexts of $b$ and the $s_i$. Note, $(0, \dots, 0, b)$ is a valid (but insecure) LWE ciphertext of $b$ under any secret key, in part because we&#8217;re pretending the LWE samples and error were all sampled as zero; an unlikely but coherent outcome used to jumpstart a homomorphic computation in more places than key switching. So if you wanted to, you could write $c&#8217;$ as follows, to highlight how we&#8217;re computing additions and linear scalings of LWE ciphertexts.</p>



<p class="has-text-align-center">$\displaystyle c&#8217; = \textup{LWE}_{\textup{t}}(b) &#8211; \sum_{i=1}^n a_i \textup{LWE}_t(s_i)$</p>



<p>This should be enough to show that $c&#8217;$ is a valid LWE encryption (if we accept that adding and scaling preserves LWE validity). But to warm up for the rest of the article we&#8217;ll reprove it with a slightly different technique. This will also help us understand the error growth. Because LWE naturally admits sums and scalar products with corresponding added error, we expect the error to grow proportionally to the number of additions and the magnitudes of the $a_i$&#8217;s. And you may already be able to tell that because the $a_i$&#8217;s are uniform $\mathbb{Z}/q\mathbb{Z}$ elements, this part will be far too large to be useful. Let&#8217;s make this explicit now.</p>



<p>To show it&#8217;s a valid LWE encryption, we define the function $\varphi_s$, defined on any LWE ciphertext $c = (a_1, \dots, a_n, b)$ as $\varphi_s(c) = b &#8211; \langle a, s \rangle$. Some authors call $\varphi_s$ the &#8220;phase&#8221; function, but I think of it as a close friend: the first step of the decryption function for LWE (the second step would be rounding off the error). Critically, an LWE encryption is valid if and only if $\varphi_s(c) = m + e$ (provided $e$ is sufficiently small).</p>



<p>Because $\varphi_s$ is a linear function, it factors through the definition of $c&#8217;$ nicely, and we get</p>



<p class="has-text-align-center">$\displaystyle \begin{aligned} \varphi_t(c&#8217;) &amp;= \varphi_t((0, \dots, 0, b)) &#8211; \sum_{i=1}^n a_i \varphi_t(\textup{KSK}_i) \\ &amp;= b &#8211; \sum_{i=1}^n a_i (y_i &#8211; \langle x_i, t \rangle) \\ &amp;= b &#8211; \sum_{i=1}^n a_i (s_i + e_i) \end{aligned}$</p>



<p>where (reminder) $e_i$ is the error sample from $\textup{KSK}_i$&#8217;s definition. Distributing $a_i$ across the $(s_i + e_i)$ simplifies everything nicely</p>



<p class="has-text-align-center">$\displaystyle \begin{aligned} &amp;= b &#8211; \sum_{i=1}^n a_i s_i &#8211; \sum_{i=1}^n a_i e_i \\ &amp;= m + e_{\textup{original}} &#8211; \sum_{i=1}^n a_i e_i \end{aligned}$</p>



<p>Now as we foreshadowed, $e_{\textup{new}} = -\sum_{i=1}^n a_i e_i$ is simply too large. A typical LWE ciphertext will have error at least 1 (or it would be useless), and if $q = 2^{32}$, the $a_i$&#8217;s would also be of magnitude roughly $2^{31}$, so summing even two of those would corrupt even a 1-bit message stored in the most significant bit of the plaintext.</p>



<p>The way to deal with this is to use a bit decomposition.</p>



<h2 class="has-text-align-center">Key switching, with gadget decompositions</h2>



<p>Recall from the <a href="https://jeremykun.com/2021/12/11/the-gadget-decomposition-in-fhe/" data-type="post" data-id="118248">gadget decomposition</a> article that the core function of a gadget decomposition is to preserve the ultimate value of a dot product while making the vectors multiplicands larger (spending space/time) but also making the size of the coefficients of one of the vectors smaller (reducing the accumulation of error due to that dot product).</p>



<p>This is exactly the approach we&#8217;ll take here. The &#8220;dot product&#8221; in question is $(a_1, \dots, a_n) \cdot \textup{KSK}$ (where KSK is viewed as a matrix), and we&#8217;ll expand the values $a_i$ into a vector of its digits in a base-$B$ number system, while modifying the key switching key so that those missing powers of $B$ are part of the LWE encryption. This will result in replacing the error term that looked like $\sum_{i=1}^n a_i e_i$ with an error term like $\sum_{i=1}^n c B e_i$ for some small constant $c$ (expect it to be even less than $B$).</p>



<p>More specifically, define <em>decomposition parameters</em> as a triple of numbers $(B, k, L)$. The number $B$ is a power of 2 no bigger than $q/2$, and $L$, or the <em>number of levels </em>of the decomposition, is the positive integer such that $B^L = q$ (this is forced by the choice of $B$). Then finally, $k$ is a number between $0$ and $L-1$ describing the &#8220;lowest level&#8221; (or least-significant digit) included in the decomposition.</p>



<p>An <em>error-free </em>decomposition sets the parameter $k=0$, and this is defined simply as a base-$B$ representation of a number. For example, suppose $q = 2^{32}$, and $(B, k, L) = (256, 0, 4)$, and we&#8217;re decomposing $x=2^{32} &#8211; 2$. Then $\textup{Decomp}_{256, 0, 4}(x) = (254, 255, 255, 255)$. I subtracted 2 to emphasize that the digits are little-Endian (the right-most entry is the most significant, representing the $256^3$ place).</p>



<p>An <em>approximate</em> decomposition is one with $k &gt; 0$. For example, suppose $(B, k, L) = (256, 2, 4)$ and again $x=2^{32} &#8211; 2$. Setting $k=2$ means that we represent this number as if it were $(0, 0, 255, 255)$, wiping out the two least significant digits. The error of this approximation is $65534 = 254 + 255 \cdot 256^1$. As we will see, an approximate decomposition may help reduce overall error by splitting the newly introduced error into a sum of two terms, where $k$ scales the error differently in each term.</p>



<p>Let&#8217;s go through the key-switching key derivation again, using an error-free decomposition $(B, 0, L)$. First, re-define the key switching key as follows.</p>



<p class="has-text-align-center">$\displaystyle \textup{KSK} = \{ \textup{KSK}_{i, j} = \textup{LWE}_t(s_i B^j) \mid i=1, \dots, n ; j = 0, \dots, L-1\}$</p>



<p>Note that this increases the dimension of the key-switching key by 1. Previously the key-switching key was a list of LWE ciphertexts (2-dimensional array of numbers), and now it&#8217;s a 3-dimensional array, with the new dimension corresponding to the decomposition digit $j$.</p>



<p>Because the powers of $B$ are attached to the message, they will factor out and allow us to reconstruct the original $a_i$&#8217;s, but they will not be included in the error part because error is added to the message during encryption.</p>



<p>Next, to perform the key switch, define $\textup{Decomp}(a_i) = (a_{i,0}, \dots, a_{i,L-1})$ and compute</p>



<p class="has-text-align-center">$\displaystyle c&#8217; = (0, \dots, 0, b) &#8211; \sum_{i=1}^n \sum_{j=0}^{L-1} a_{i,j} \textup{KSK}_{i,j}$</p>



<p>This is the same as the original key switch, but the extra summation accounts for the extra dimension introduced by the gadget decomposition. Then we can repeat the same $\varphi_t$ trick and see how the original $a_i$&#8217;s are reconstructed.</p>



<p class="has-text-align-center">$\displaystyle \begin{aligned} \varphi_t(c&#8217;) &amp;= b &#8211; \sum_{i=1}^n \sum_{j=0}^{L-1} a_{i,j} \varphi_t(\textup{KSK}_{i,j}) \\ &amp;= b -\sum_{i=1}^n \sum_{j=0}^{L-1} a_{i,j} (s_i B^j + e_i) \\ &amp;= b -\sum_{i=1}^n \sum_{j=0}^{L-1} a_{i,j} s_i B^j &#8211;  \sum_{i=1}^n \sum_{j=0}^{L-1} a_{i,j} e_i \\ &amp;= b -\sum_{i=1}^n a_i s_i &#8211;  \sum_{i=1}^n \sum_{j=0}^{L-1} a_{i,j} e_i \\ &amp;= m + e_{\textup{original}} &#8211;  \sum_{i=1}^n \sum_{j=0}^{L-1} a_{i,j} e_i \end{aligned}$</p>



<p>One key ingredient above is noticing that in $\sum_{i=1}^n \sum_{j=0}^{L-1} a_{i,j} s_i B^j$, the $s_i$ factors out of the innermost sum, and what you have left is $\sum_{j=0}^{L-1} a_{i,j} B^j$, which is exactly how to reconstruct $a_i$ from its base-$B$ digits.</p>



<p>The second key ingredient is that the innermost term on the second line is $a_{i,j} (s_i B^j + e_i)$, which means that only the digits $a_{i,j}$ are multiplied by the error terms, not including the powers of $B$, and so the final error can be bounded by the largest allowable value of a single digit $B-1$, resulting in the new error being $L (B-1) \sum_{i=1}^n e_i$. For a Gaussian centered at zero, the expectation of these errors is zero, and using <a href="https://jeremykun.com/2013/04/15/probabilistic-bounds-a-primer/" data-type="post" data-id="3262">standard bounding arguments like Chernoff bounds</a>, you can prove that with high probability this new error is at most $L(B-1) \sigma \sqrt{2n \log n}$, where $\sigma$ is the standard deviation of the error distribution.</p>



<p>Now, finally, we can run through this argument <em>one more time</em>, but using an approximate decomposition. This merely changes the sum&#8217;s lower bound from $j=0$ to $j=k$. Start by calling $\tilde{a}_i = \sum_{j=k}^{L-1} a_{i,j} B^j$, the approximation of $a_i$ from its most significant bits. Then the error of this approximation is $a_i &#8211; \tilde{a}_i = \sum_{j=0}^{k-1} a_{i,j} B^j$, a relatively small quantity at most $(B^k &#8211; 1) / (B-1)$ (if each $a_{i,j} = B-1$ is as large as possible).</p>



<p class="has-text-align-center">$\displaystyle \begin{aligned} \varphi_t(c&#8217;) &amp;= b &#8211; \sum_{i=1}^n \sum_{j=k}^{L-1} a_{i,j} \varphi_t(\textup{KSK}_{i,j}) \\ &amp;= b -\sum_{i=1}^n \sum_{j=k}^{L-1} a_{i,j} (s_i B^j + e_i) \\ &amp;= b -\sum_{i=1}^n s_i \sum_{j=k}^{L-1} a_{i,j} B^j &#8211;  \sum_{i=1}^n \sum_{j=k}^{L-1} a_{i,j} e_i \\ &amp;= b -\sum_{i=1}^n s_i \tilde{a}_i &#8211;  \sum_{i=1}^n \sum_{j=k}^{L-1} a_{i,j} e_i \end{aligned}$</p>



<p>Mentally zoom in on the first sum $\sum_{i=1}^n s_i \tilde{a}_i$. Use the trick of adding zero to get</p>



<p class="has-text-align-center">$\displaystyle  \sum_{i=1}^n s_i \tilde{a}_i = \sum_{i=1}^n s_i (a_i + \tilde{a}_i &#8211; a_i) =  \sum_{i=1}^n s_i a_i &#8211; \sum_{i=1}^n s_i(a_i &#8211; \tilde{a}_i)$</p>



<p>The term $\sum_{i=1}^n s_i(a_i &#8211; \tilde{a}_i)$ is part of our new error term, and recalling that the secret key bits are binary, you should think of this in expectation as roughly $\frac{n}{2} B^{k-1}$ (more precisely, $\frac{n}{2} (B^{k}-1)/(B-1)$).</p>



<p>Continuing, we arrive at</p>



<p class="has-text-align-center">$\displaystyle \begin{aligned} \varphi_t(c&#8217;) &amp;= b -\sum_{i=1}^n a_i s_i  &#8211; \sum_{i=1}^n s_i(a_i &#8211; \tilde{a}_i) &#8211; \sum_{i=1}^n \sum_{j=k}^{L-1} a_{i,j} e_i \\ &amp;= m + e_{\textup{original}} &#8211; \sum_{i=1}^n s_i(a_i &#8211; \tilde{a}_i) &#8211; \sum_{i=1}^n \sum_{j=k}^{L-1} a_{i,j} e_i \end{aligned}$</p>



<h2 class="has-text-align-center">Rough error analysis</h2>



<p>Now the choice of $k$ admits a tradeoff that one can optimize for to minimize the total newly introduced error. I&#8217;m going to switch to a sloppy mode of math to heuristically navigate this tradeoff.</p>



<p>The triangle inequality lets us bound the magnitude of the error by the sum of the magnitudes of the parts, i.e., the error is bounded from above by </p>



<p class="has-text-align-center">$\displaystyle  \left | \sum_{i=1}^n s_i(a_i &#8211; \tilde{a}_i) \right | + \left | \sum_{i=1}^n \sum_{j=k}^{L-1} a_{i,j} e_i \right |$</p>



<p>The left term is like $\frac{n}{2} B^{k-1}$ as we stated earlier, and with high probability it&#8217;s at most $(n/2 + \sqrt{n \log n}) B^{k-1}$. The right term is at most $(L-k)B \sum_{i=1}^n e_i$, (worst case size of $a_{i,j}$, increasing $B-1$ to $B$ because why not), and with high probability the sum of the $e_i$ is like $\sigma \sqrt{2n \log n}$, making the whole term bounded by $(L-k)B \sigma \sqrt{2n \log n}$. So we want to minimize the sum</p>



<p class="has-text-align-center">$\displaystyle (n/2 + \sqrt{n \log n}) B^{k-1} + (L-k)B \sigma \sqrt{2n \log n}$</p>



<p>We could try to explicitly optimize this for $k$, treating the other terms as constant, but it won&#8217;t be nice because $k$ is present in both a linear term and an exponent. We could also just stare at it and think. The approximation error (the term on the left) is going to get exponentially larger as $k$ grows, so we want to keep $k$ relatively small. But on the other hand, the standard deviation $\sigma$ should be much larger than $n$ to keep LWE secure. This is effectively what we&#8217;re trying to suppress: error that grows like $O(n)$ is small enough to deal with, but error that grows like $\omega(n)$ is problematic. Increasing $k$ gives us a meager (but nontrivial) means to reduce the constant coefficient on that part of the error in exchange for $\Theta(n)$ growth with in the other term.</p>



<p>I admit, as of the time of this writing I still don&#8217;t understand how to set production security parameters for LWE. Is it still linear in $n$? Super-linear? Not sure. I&#8217;m betting future Jeremy will clarify this to me in another article. Even if it were linear in $n$, the right term multiplies $\sigma$ by $\sqrt{n \log n}$ which makes the whole thing super-linear, whereas the left term adds a square root factor. So the tradeoff in $k$ should still help.</p>



<p>Until I understand LWE security, I won&#8217;t have the asymptotics I need to analyze this further. Moreover, the allowed values of $B, k$ are so small that we can brute force evaluate all options. For example, if $B = 16$ then $k$ can be between 0 and 7. And realistically, if $n \approx 2^{10}$, then letting $k = 4$ makes the first term roughly $2^{26}$, which leaves only 6 bits left for the message (further reduced by any error introduced by the second term).</p>



<p>Thanks to <a href="https://cathieyun.github.io/">Cathie Yun</a> and <a href="https://mobile.twitter.com/asraentr0py">Asra Ali</a> for providing feedback on an early draft of this article.</p>



<p>Until next time!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://jeremykun.com/2022/08/29/key-switching-in-lwe/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">118460</post-id>	</item>
		<item>
		<title>Modulus Switching in LWE</title>
		<link>https://jeremykun.com/2022/07/16/modulus-switching-in-lwe/</link>
					<comments>https://jeremykun.com/2022/07/16/modulus-switching-in-lwe/#respond</comments>
		
		<dc:creator><![CDATA[j2kun]]></dc:creator>
		<pubDate>Sat, 16 Jul 2022 21:39:56 +0000</pubDate>
				<category><![CDATA[Cryptography]]></category>
		<category><![CDATA[Group Theory]]></category>
		<category><![CDATA[cryptography]]></category>
		<category><![CDATA[fhe]]></category>
		<category><![CDATA[learning with errors]]></category>
		<category><![CDATA[mathematics]]></category>
		<category><![CDATA[modulus switching]]></category>
		<category><![CDATA[programming]]></category>
		<guid isPermaLink="false">https://jeremykun.com/?p=118420</guid>

					<description><![CDATA[The Learning With Errors problem is the basis of a few cryptosystems, and a foundation for many fully homomorphic encryption (FHE) schemes. In this article I&#8217;ll describe a technique used in some of these schemes called modulus switching. In brief, an LWE sample is a vector of values in $\mathbb{Z}/q\mathbb{Z}$ for some $q$, and in [&#8230;]]]></description>
										<content:encoded><![CDATA[
<p>The <a href="https://en.wikipedia.org/wiki/Learning_with_errors">Learning With Errors</a> problem is the basis of a few cryptosystems, and a foundation for many <a href="https://en.wikipedia.org/wiki/Homomorphic_encryption">fully homomorphic encryption</a> (FHE) schemes. In this article I&#8217;ll describe a technique used in some of these schemes called <em>modulus switching</em>.</p>



<p>In brief, an LWE sample is a vector of values in $\mathbb{Z}/q\mathbb{Z}$ for some $q$, and in LWE cryptosystems an LWE sample can be modified so that it hides a secret message $m$. Modulus switching allows one to convert an LWE encryption from having entries in $\mathbb{Z}/q{Z}$ to entries in some other $\mathbb{Z}/q'{Z}$, i.e., change the modulus from $q$ to $q&#8217; &lt; q$.</p>



<p>The reason you&#8217;d want to do this are a bit involved, so I won&#8217;t get into them here and instead back-reference this article in the future.</p>



<h2 class="has-text-align-center">LWE encryption</h2>



<p>Briefly, the LWE encryption scheme I&#8217;ll use has the following parameters:</p>



<ul><li>A plaintext space $ \mathbb{Z}/q\mathbb{Z}$, where $ q \geq 2$ is a positive integer. This is the space that the underlying message comes from.</li><li>An <em>LWE dimension</em> $ n \in \mathbb{N}$.</li><li>A discrete Gaussian <em>error distribution</em> $ D$ with a mean of zero and a fixed standard deviation.</li></ul>



<p>An LWE secret key is defined as a vector in $ \{0, 1\}^n$ (uniformly sampled). An LWE ciphertext is defined as a vector $ a = (a_1, \dots, a_n)$, sampled uniformly over $ (\mathbb{Z} / q\mathbb{Z})^n$, and a scalar $ b = \langle a, s \rangle + m + e$, where $ e$ is drawn from $ D$ and all arithmetic is done modulo $ q$.</p>



<p>Without the error term, an attacker could determine the secret key from a polynomial-sized collection of LWE ciphertexts with something like <a href="https://en.wikipedia.org/wiki/Gaussian_elimination">Gaussian elimination</a>. The set of samples looks like a linear (or affine) system, where the secret key entries are the unknown variables. With an error term, the problem of solving the system is believed to be hard, and only exponential time/space algorithms are known.</p>



<p>However, the error term in an LWE encryption encompasses all of the obstacles to FHE. For starters, if your message is $ m=1$ and the error distribution is wide (say, a standard deviation of 10), then the error will completely obscure the message from the start. You can&#8217;t decrypt the LWE ciphertext because you can&#8217;t tell if the error generated in a particular instance was 9 or 10. So one thing people do is have a much smaller <em>cleartext</em> space (actual messages) and encode cleartexts as plaintexts by putting the messages in the higher-order bits of the plaintext space. E.g., you can encode 10-bit messages in the top 10 bits of a 32-bit integer, and leave the remaining 22 bits of the plaintext for the error distribution.</p>



<p>Moreover, for FHE you need to be able to add and multiply ciphertexts to get the corresponding sum/product of the underlying plaintexts. One can easily see that adding two LWE ciphertexts produces an LWE ciphertext of the sum of the plaintexts (multiplication is harder and beyond the scope of this article). Summing ciphertexts also sums the error terms together. So the error grows with each homomorphic operation, and eventually the error may overtake the message, at which point decryption fails. How to deal with this error accumulation is 99% of the difficulty of FHE.</p>



<p>Finally, because the error can be negative, even if you store a message in the high-order bits of the plaintext, you can&#8217;t decrypt by simply clearing the low order error bits. In that case an error of -1 would result in a corrupted message. Instead, to decrypt, we <em>round </em>the value $ b &#8211; \langle a, s \rangle = m + e$ to the nearest multiple of $ 2^k$, where $k$ is the number of bits &#8220;reserved&#8221; for error, as described above. In particular, decryption will only succeed if the error is small enough in absolute value. So to make this work in practice, one must coordinate the encoding scheme (how many bits to reserve for error), the dimension of the vector $ a$, and the standard deviation of the error distribution.</p>



<h2 class="has-text-align-center">Modulus switching</h2>



<p>With a basic outline of an LWE ciphertext, we can talk about modulus switching.</p>



<p>Start with an LWE ciphertext for the plaintext $ m$. Call it $ (a_1, \dots, a_n, b) \in (\mathbb{Z}/q\mathbb{Z})^{n+1}$, where </p>



<p class="has-text-align-center">$ \displaystyle b = \left ( \sum_{i=1}^n a_i s_i \right ) + m + e_{\textup{original}}$</p>



<p>Given $ q&#8217; &lt; q$, we would like to produce a vector $ (a&#8217;_1, \dots, a&#8217;_n, b&#8217;) \in (\mathbb{Z}/q&#8217;\mathbb{Z})^{n+1}$ (all that has changed is I&#8217;ve put a prime on all the terms to indicate which are changing, most notably the new modulus $ q&#8217;$) that also encrypts $ m$, without knowing $ m$ or $ e_{\textup{original}}$, i.e., without access to the secret key.</p>



<p><strong>Failed attempt: </strong>why not simply reduce each entry in the ciphertext vector modulo $ q&#8217;$? That would set $ a&#8217;_i = a_i \mod q&#8217;$ and $ b&#8217; = b \mod q&#8217;$. Despite the fact that this operation produces a perfectly valid equation, it won&#8217;t work. The problem is that taking $ m \mod q&#8217;$ destroys part or all of the underlying message. For example, say $ x$ is a 12-bit number stored in the top 12 bits of the plaintext, i.e., $ m = x \cdot 2^{20}$. If $ q&#8217; = 2^{15}$, then the message is a multiple of $ q&#8217;$ already, so the proposed modulus produces zero.</p>



<p>For this reason, we can&#8217;t hope to perfectly encrypt $ m$, as the output ciphertext entries may not have a modulus large enough to represent $ m$ at all. Rather, we can only hope to encrypt <em>something like</em> &#8220;the message $ x$ that&#8217;s encoded in $ m$, but instead with $ x$ stored in lower order bits than $ m$ originally used.&#8221; In more succinct terms, we can hope to encrypt $ m&#8217; = m q&#8217; / q$. Indeed, the operation of $ m \mapsto m q&#8217; / q$ shifts up by $ \log_2(q&#8217;)$ many bits (temporarily exceeding the maximum allowable bit length), and then shifting down by $ \log_2(q)$ many bits.</p>



<p>For example, say the number $ x=7$ is stored in the top 3 bits of a 32-bit unsigned integer ($ q = 2^{32}$), i.e., $ m = 7 \cdot 2^{29}$ and $ q&#8217; = 2^{10}$. Then $ m q&#8217; / q = 7 \cdot 2^{29} \cdot 2^{10} / 2^{32} = 7 \cdot 2^{29+10 &#8211; 32} = 7 \cdot 2^7$, which stores the same underlying number $ x=7$, but in the top three bits of a 10-bit message. In particular, $ x$ is in the same &#8220;position&#8221; in the plaintext space, while the plaintext space has shrunk around it.</p>



<p>Side note: because of this change to the cleartext-to-plaintext encoding, the decryption/decoding steps before and after a modulus switch are slightly different. In decryption you use different moduli, and in decoding you round to different powers of 2. </p>



<p>So the trick is instead to apply $ z \mapsto z q&#8217; / q$ to all the entries of the LWE ciphertext vector. However, because the entries like $ a_i$ use the entire space of bits in the plaintext, this transformation will not necessarily result in an integer. So we can round the result to an integer and analyze that. The final proposal for a modulus switch is</p>



<p class="has-text-align-center">$ \displaystyle a&#8217;_i = \textup{round}(a_i q&#8217; / q)$</p>



<p class="has-text-align-center">$ \displaystyle b&#8217; = \textup{round}(b q&#8217; / q)$</p>



<p>Because the error growth of LWE ciphertexts permeates everything, in addition to proving this transformation produces a valid ciphertext, we also have to understand how it impacts the error term.</p>



<h2 class="has-text-align-center">Analyzing the modulus switch</h2>



<p>The statement summarizing the last section:</p>



<p><strong>Theorem:</strong> Let $ \mathbf{c} = (a_1, \dots, a_n, b) \in (\mathbb{Z}/q\mathbb{Z})^{n+1}$ be an LWE ciphertext encrypting plaintext $ m$ with error term $ e_\textup{original}$. Let $ q&#8217; &lt; q$. Then $ c&#8217; = \textup{round}(\mathbf{c} q&#8217; / q)$ (where rounding is performed entrywise) is an LWE encryption of $ m&#8217; = m q&#8217; / q$, provided $ m&#8217;$ is an integer.</p>



<p><em>Proof.</em> The only substantial idea is that $ \textup{round}(x) = x + \varepsilon$, where $ |\varepsilon| \leq 0.5$. This is true by the definition of rounding, but that particular way to express it allows us to group the error terms across a sum-of-rounded-things in isolation, and then everything else has a factor of $ q&#8217;/q$ that can be factored out. Let&#8217;s proceed.</p>



<p>Let $ c&#8217; = (a&#8217;_1, \dots, a&#8217;_n, b&#8217;)$, where $ a&#8217;_i = \textup{round}(a_i q&#8217; / q)$ and likewise for $ b&#8217;$. need to show that $ b&#8217; = \left ( \sum_{i=1}^n a&#8217;_i s_i \right ) + m q&#8217; / q + e_{\textup{new}}$, where $ e_{\textup{new}}$ is a soon-to-be-derived error term.</p>



<p>Expanding $ b&#8217;$ and using the &#8220;only substantial idea&#8221; above, we get</p>



<p class="has-text-align-center">$ \displaystyle b&#8217; = \textup{round}(b q&#8217; / q) = bq&#8217;/q + \varepsilon_b$</p>



<p>For some $ \varepsilon_b$ with magnitude at most $ 1/2$. Continuing to expand, and noting that $ b$ is related to the $ a_i$ only modulo $ q$, we have</p>



<p class="has-text-align-center">$ \displaystyle \begin{aligned} b&#8217; &amp;= bq&#8217;/q + \varepsilon_b \\ b&#8217; &amp;= \left ( \left ( \sum_{i=1}^n a_i s_i \right ) + m + e_{\textup{original}} \right ) \frac{q&#8217;}{q} + \varepsilon_b \mod q \end{aligned}$</p>



<p>Because we&#8217;re switching moduli, it makes sense to rewrite this over the integers, which means we add a term $ Mq$ for some integer $ M$ and continue to expand</p>



<p class="has-text-align-center">$ \displaystyle \begin{aligned} b&#8217; &amp;= \left ( \left ( \sum_{i=1}^n a_i s_i \right ) + m + e_{\textup{original}} + Mq \right ) \frac{q&#8217;}{q} + \varepsilon_b \\ &amp;= \left ( \sum_{i=1}^n \left ( a_i \frac{q&#8217;}{q} \right) s_i \right ) + m \frac{q&#8217;}{q} + e_{\textup{original}}\frac{q&#8217;}{q} + Mq \frac{q&#8217;}{q} + \varepsilon_b \\ &amp;= \left ( \sum_{i=1}^n \left ( a_i \frac{q&#8217;}{q} \right) s_i \right ) + m&#8217; + e_{\textup{original}}\frac{q&#8217;}{q} + Mq&#8217; + \varepsilon_b \end{aligned}$</p>



<p>The terms with $ a_i$ are still missing their rounding, so, just like $ b&#8217;$, rewrite $ a&#8217;_i = a_i q&#8217;/q + \varepsilon_i$ as $ a_i q&#8217;/q = a&#8217;_i &#8211; \varepsilon_i$, expanding, simplifying, and finally reducing modulo $ q&#8217;$ to get</p>



<p class="has-text-align-center">$ \displaystyle \begin{aligned} b&#8217; &amp;= \left ( \sum_{i=1}^n \left ( a&#8217;_i &#8211; \varepsilon_i \right) s_i \right ) + m&#8217; + e_{\textup{original}}\frac{q&#8217;}{q} + Mq&#8217; + \varepsilon_b \\ &amp;= \left ( \sum_{i=1}^n a&#8217;_i s_i \right ) &#8211; \left ( \sum_{i=1}^n \varepsilon_i s_i \right) + m&#8217; + e_{\textup{original}}\frac{q&#8217;}{q} + Mq&#8217; + \varepsilon_b \\   &amp;= \left ( \sum_{i=1}^n a&#8217;_i s_i \right ) + m&#8217; + Mq&#8217; +  \left [ e_{\textup{original}}\frac{q&#8217;}{q} &#8211; \left ( \sum_{i=1}^n \varepsilon_i s_i \right)  + \varepsilon_b \right ] \\   &amp;=  \left ( \sum_{i=1}^n a&#8217;_i s_i \right ) + m&#8217; + \left [ e_{\textup{original}}\frac{q&#8217;}{q} &#8211; \left ( \sum_{i=1}^n \varepsilon_i s_i \right)  + \varepsilon_b \right ]  \mod q&#8217; \end{aligned}$</p>



<p>Define the square bracketed term as $ e_{\textup{new}}$, and we have proved the theorem.</p>



<p class="has-text-align-right">$ \square$</p>



<p>The error after modulus switching is laid out. It&#8217;s the original error scaled, plus at most $ n+1$ terms, each of which is at most $ 1/2$. However, note that this is larger than it appears. If the new modulus is, say, $ q&#8217;=1024$, and the dimension is $ n = 512$, then in the worst case the error right after modulus switching will leave us only $ 1$ bit left for the message. This is not altogether unrealistic, as production (128-bit) security parameters for LWE put $ n$ around 600. But it is compensated for by the fact that the secret $ s$ is chosen uniformly at random, and the errors are symmetric around zero. So in expectation only half the bits will be set, and half of the set bits will have a positive error, and half a negative error. Using these facts, you can bound the probability that the error exceeds, say, $ \sqrt{n \log n}$ using a <a href="https://jeremykun.com/2013/04/15/probabilistic-bounds-a-primer/">standard Hoeffding bound argument</a>. I further believe that the error is bounded by $ \sqrt{n}$. I have verified it empirically, but haven&#8217;t been able to quite nail down a proof.</p>



<p>Until next time!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://jeremykun.com/2022/07/16/modulus-switching-in-lwe/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">118420</post-id>	</item>
		<item>
		<title>“Practical Math” Preview: Collect Sensitive Survey Responses Privately</title>
		<link>https://jeremykun.com/2022/05/14/practical-math-preview-collect-sensitive-survey-responses-privately/</link>
					<comments>https://jeremykun.com/2022/05/14/practical-math-preview-collect-sensitive-survey-responses-privately/#respond</comments>
		
		<dc:creator><![CDATA[j2kun]]></dc:creator>
		<pubDate>Sat, 14 May 2022 16:40:49 +0000</pubDate>
				<category><![CDATA[Algorithms]]></category>
		<category><![CDATA[Books]]></category>
		<category><![CDATA[Fairness]]></category>
		<category><![CDATA[Probability]]></category>
		<category><![CDATA[Program Gallery]]></category>
		<category><![CDATA[Statistics]]></category>
		<category><![CDATA[differential privacy]]></category>
		<category><![CDATA[mathematics]]></category>
		<category><![CDATA[politics]]></category>
		<category><![CDATA[practical math]]></category>
		<category><![CDATA[programming]]></category>
		<category><![CDATA[python]]></category>
		<category><![CDATA[randomized algorithm]]></category>
		<guid isPermaLink="false">https://jeremykun.com/?p=118403</guid>

					<description><![CDATA[This is a draft of a chapter from my in-progress book, Practical Math for Programmers: A Tour of Mathematics in Production Software, about the randomized response mechanism for surveys.]]></description>
										<content:encoded><![CDATA[
<p>This is a draft of a chapter from my in-progress book, <em><a href="https://jeremykun.com/2022/03/16/my-next-book-will-be-practical-math-for-programmers/">Practical Math for Programmers: A Tour of Mathematics in Production Software</a></em>.</p>



<p><strong>Tip: </strong>Determine an aggregate statistic about a sensitive question, when survey respondents do not trust that their responses will be kept secret.</p>



<p><strong>Solution:</strong></p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate">
import random

def respond_privately(true_answer: bool) -&gt; bool:
    '''Respond to a survey with plausible deniability about your answer.'''
    be_honest = random.random() &lt; 0.5
    random_answer = random.random() &lt; 0.5
    return true_answer if be_honest else random_answer

def aggregate_responses(responses: List&#91;bool]) -&gt; Tuple&#91;float, float]:
    '''Return the estimated fraction of survey respondents that have a truthful
    Yes answer to the survey question.
    '''
    yes_response_count = sum(responses)
    n = len(responses)
    mean = 2 * yes_response_count / n - 0.5
    # Use n-1 when estimating variance, as per Bessel's correction.
    variance = 3 / (4 * (n - 1))
    return (mean, variance)
</pre></div>


<p>In the late 1960&#8217;s, most abortions were illegal in the United States. Daniel G. Horvitz, a statistician at The Research Triangle Institute in North Carolina and a leader in survey design for social sciences, was tasked with estimating how many women in North Carolina were receiving illegal abortions. The goal was to inform state and federal policymakers about the statistics around abortions, many of which were unreported, even when done legally.</p>



<p>The obstacles were obvious. As Horvitz put it, &#8220;a prudent woman would not divulge to a stranger the fact that she was party to a crime for which she could be prosecuted.&#8221; [Abernathy70] This resulted in a strong bias in survey responses. Similar issues had plagued surveys of illegal activity of all kinds, including drug abuse and violent crime. Lack of awareness into basic statistics about illegal behavior led to a variety of misconceptions, such as that abortions were not frequently sought out.</p>



<p>Horvitz worked with biostatisticians James Abernathy and Bernard Greenberg to test out a new method to overcome this obstacle, without violating the respondent&#8217;s privacy or ability to plausibly deny illegal behavior. The method, called <em>randomized response</em>, was invented by Stanley Warner in 1965, just a few years earlier. [Warner65] Warner&#8217;s method was a bit different from what we present in this Tip, but both Warner&#8217;s method and the code sample above use the same strategy of adding randomization to the survey.</p>



<p>The mechanism, as presented in the code above, requires respondents to start by flipping a coin. If heads, they answer the sensitive question truthfully. If tails, they flip a second coin to determine how to answer the question&#8212;heads resulting in a &#8220;yes&#8221; answer, tails in a &#8220;no&#8221; answer. Naturally, the coin flips are private and controlled by the respondent. And so if a respondent answers &#8220;Yes&#8221; to the question, they may plausibly claim the &#8220;Yes&#8221; was determined by the coin, preserving their privacy. The figure below describes this process as a diagram.</p>



<figure class="wp-block-image size-large"><a href="https://i0.wp.com/jeremykun.com/wp-content/uploads/2022/05/randomized-response-diagram.png?ssl=1"><img data-attachment-id="118408" data-permalink="https://jeremykun.com/2022/05/14/practical-math-preview-collect-sensitive-survey-responses-privately/randomized-response-diagram/#main" data-orig-file="https://i0.wp.com/jeremykun.com/wp-content/uploads/2022/05/randomized-response-diagram.png?fit=1184%2C779&amp;ssl=1" data-orig-size="1184,779" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="randomized-response-diagram" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/jeremykun.com/wp-content/uploads/2022/05/randomized-response-diagram.png?fit=300%2C197&amp;ssl=1" data-large-file="https://i0.wp.com/jeremykun.com/wp-content/uploads/2022/05/randomized-response-diagram.png?fit=1024%2C674&amp;ssl=1" decoding="async" width="1184" height="779" src="https://i0.wp.com/jeremykun.com/wp-content/uploads/2022/05/randomized-response-diagram.png?resize=1184%2C779&#038;ssl=1" alt="" class="wp-image-118408" data-recalc-dims="1" /></a><figcaption>A branching diagram showing the process a survey respondent takes to record their response.</figcaption></figure>



<p>Another way to describe the outcome is to say that each respondent&#8217;s answer is a single bit of information that is flipped with probability 1/4. This is half way between two extremes on the privacy/accuracy tradeoff curve. The first extreme is a &#8220;perfectly honest&#8221; response, where the bit is never flipped and all information is preserved. The second extreme has the bit flipped with probability 1/2, which is equivalent to ignoring the question and choosing your answer completely at random, losing all information in the aggregate responses. In this perspective, the aggregate survey responses can be thought of as a digital signal, and the privacy mechanism adds noise to that signal.</p>



<p>It remains to determine how to recover the aggregate signal from these noisy responses. In other words, the surveyor cannot know any individual&#8217;s true answer, but they can, with some extra work, estimate statistics about the underlying population by correcting for the statistical bias. This is possible because the randomization is well understood. The expected fraction of &#8220;Yes&#8221; answers can be written as a function of the true fraction of &#8220;Yes&#8221; answers, and hence the true fraction can be solved for. In this case, where the random coin is fair, that formula is as follows (where $ \mathbf{P}$ stands for &#8220;the probability of&#8221;).</p>



<p class="has-text-align-center">$ \displaystyle \mathbf{P}(\textup{Yes answer}) = \frac{1}{2} \mathbf{P}(\textup{Truthful yes answer}) + \frac{1}{4}$</p>



<p>And so we solve for $ \mathbf{P}(\textup{Truthful yes answer})$</p>



<p class="has-text-align-center">$ \displaystyle \mathbf{P}(\textup{Truthful yes answer}) = 2 \mathbf{P}(\textup{Yes answer}) &#8211; \frac{1}{2}$</p>



<p>We can replace the true probability $ \mathbf{P}(\textup{Yes answer})$ above with our fraction of &#8220;Yes&#8221; responses from the survey, and the result is an estimate $ \hat{p}$ of $ \mathbf{P}(\textup{Truthful yes answer})$. This estimate is unbiased, but has additional variance—beyond the usual variance caused by picking a finite random sample from the population of interest—introduced by the randomization mechanism.</p>



<p>With a bit of effort, one can calculate that the variance of the estimate is</p>



<p class="has-text-align-center">$ \displaystyle \textup{Var}(\hat{p}) = \frac{3}{4n}$</p>



<p>And via Chebyshev&#8217;s inequality, which bounds the likelihood that an estimator is far away from its expectation, we can craft a confidence interval and determine the needed sample sizes. Specifically, the estimate $ \hat{p}$ has additive error at most $ q$ with probability at most $ \textup{Var}(\hat{p}) / q^2$. This implies that for a confidence of $ 1-c$, one requires at least $ n \geq 3 / (4 c q^2)$ samples. For example, to achieve error 0.01 with 90 percent confidence ($ c=0.1$), one requires 7,500 responses.</p>



<p>Horvitz&#8217;s randomization mechanism didn&#8217;t use coin flips. Instead they used an opaque box with red or blue colored balls which the respondent, who was in the same room as the surveyor, would shake and privately reveal a random color through a small window facing away from the surveyor. The statistical principle is the same. Horvitz and his associates surveyed the women about their opinions of the privacy protections of this mechanism. When asked whether their friends would answer a direct question about abortion honestly, over 80% either believed their friends would lie, or were unsure. <em>[footnote: A common trick in survey methodology when asking someone if they would be dishonest is to instead ask if their <strong>friends</strong> would be dishonest. This tends to elicit more honesty, because people are less likely to uphold a false perception of the moral integrity of others, and people also don&#8217;t realize that their opinion of their friends correlates with their own personal behavior and attitudes. In other words, liars don&#8217;t admit to lying, but they think lying is much more common than it really is.]</em> But 60% were convinced there was no trick involved in the randomization, while 20% were unsure and 20% thought there was a trick. This suggests many people were convinced that Horvitz&#8217;s randomization mechanism provided the needed safety guarantees to answer honestly.</p>



<p>Horvitz&#8217;s survey was a resounding success, both for randomized response as a method and for measuring abortion prevalence. [Abernathy70] They estimated the abortion rate at about 22 per 100 conceptions, with a distinct racial bias—minorities were twice as likely as whites to receive an abortion. Comparing their findings to a prior nationwide study from 1955—the so-called Arden House estimate—which gave a range of between 200,000 and 1.2 million abortions per year, Horvitz&#8217;s team estimated more precisely that there were 699,000 abortions in 1955 in the United States, with a reported standard deviation of about 6,000, less than one percent. For 1967, the year of their study, they estimated 829,000.</p>



<p>Their estimate was referenced widely in the flurry of abortion law and court cases that followed due to a surging public interest in the topic. For example, it is cited in the 1970 California Supreme Court opinion for the case <em>Ballard v. Anderson</em>, which concerned whether a minor needs parental consent to receive an otherwise legal abortion. [Ballard71, Roemer71] It was also cited in <em>amici curiae</em> briefs submitted to the United States Supreme Court in 1971 for <em>Roe v. Wade</em>, the famous case that invalidated most U.S. laws making abortion illegal. One such brief was filed jointly by the country&#8217;s leading women&#8217;s rights organizations like the National Organization for Women. Citing Horvitz for this paragraph, it wrote, [Womens71]</p>



<blockquote class="wp-block-quote"><p>While the realities of law enforcement, social and public health problems posed by abortion laws have been openly discussed [&#8230;] only within a period of not more than the last ten years, one fact appears undeniable, although unverifiable statistically. There are at least one million illegal abortions in the United States each year. Indeed, studies indicate that, if the local law still has qualifying requirements, the relaxation in the law has not diminished to any substantial extent the numbers in which women procure illegal abortions.</p></blockquote>



<p>It&#8217;s unclear how the authors got this one million number (Horvitz&#8217;s estimate was 20% less for 1967), nor what they meant by &#8220;unverifiable statistically.&#8221; It may have been a misinterpretation of the randomized response technique. In any event, randomized response played a crucial role in providing a foundation for political debate.</p>



<p>Despite Horvitz&#8217;s success, and decades of additional research on crime, drug use, and other sensitive topics, randomized response mechanisms have been applied poorly. In some cases, the desired randomization is inextricably complex, such as when requiring a continuous random number. In these cases, a manual randomization mechanism is too complex for a respondent to use accurately. Trying to use software-assisted devices can help, but can also produce mistrust in the interviewee. See [Rueda16] for additional discussion of these pitfalls and what software packages exist for assisting in using randomized response. See [Fox16] for an analysis of the statistical differences between the variety of methods used between 1970 and 2010.</p>



<p>In other contexts, analogues to randomized response may not elicit the intended effect. In the 1950&#8217;s, Utah used death by firing squad as capital punishment. To avoid a guilty conscience of the shooters, one of five marksmen was randomly given a blank, providing him some plausible deniability that he knew he had delivered the killing shot. However, this approach failed on two counts. First, once a shot was fired the marksman could tell whether the bullet was real based on the recoil. Second, a 20% chance of a blank was not enough to dissuade a guilty marksman from purposely missing. In the 1951 execution of Elisio Mares, all four real bullets missed the condemned man&#8217;s heart, hitting his chest, stomach, and hip. He died, but it was neither painless nor instant.</p>



<p>Of many lessons one might draw from the botched execution, one is that randomization mechanisms must take into account both the psychology of the participants as well as the severity of a failed outcome.</p>



<h3>References</h3>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
@book{Fox16,
  title = {{Randomized Response and Related Methods: Surveying Sensitive Data}},
  author = {James Alan Fox},
  edition = {2nd},
  year = {2016},
  doi = {10.4135/9781506300122},
}

@article{Abernathy70,
  author = {Abernathy, James R. and Greenberg, Bernard G. and Horvitz, Daniel G.
            },
  title = {{Estimates of induced abortion in urban North Carolina}},
  journal = {Demography},
  volume = {7},
  number = {1},
  pages = {19-29},
  year = {1970},
  month = {02},
  issn = {0070-3370},
  doi = {10.2307/2060019},
  url = {https://doi.org/10.2307/2060019},
}

@article{Warner65,
  author = {Stanley L. Warner},
  journal = {Journal of the American Statistical Association},
  number = {309},
  pages = {63--69},
  publisher = {{American Statistical Association, Taylor \&amp; Francis, Ltd.}},
  title = {Randomized Response: A Survey Technique for Eliminating Evasive
           Answer Bias},
  volume = {60},
  year = {1965},
}

@article{Ballard71,
  title = {{Ballard v. Anderson}},
  journal = {California Supreme Court L.A. 29834},
  year = {1971},
  url = {https://caselaw.findlaw.com/ca-supreme-court/1826726.html},
}

@misc{Womens71,
  title = {{Motion for Leave to File Brief Amici Curiae on Behalf of Women’s
           Organizations and Named Women in Support of Appellants in Each Case,
           and Brief Amici Curiae.}},
  booktitle = {{Appellate Briefs for the case of Roe v. Wade}},
  number = {WL 128048},
  year = {1971},
  publisher = {Supreme Court of the United States},
}

@article{Roemer71,
  author = {R. Roemer},
  journal = {Am J Public Health},
  pages = {500--509},
  title = {Abortion law reform and repeal: legislative and judicial developments
           },
  volume = {61},
  number = {3},
  year = {1971},
}

@incollection{Rueda16,
  title = {Chapter 10 - Software for Randomized Response Techniques},
  editor = {Arijit Chaudhuri and Tasos C. Christofides and C.R. Rao},
  series = {Handbook of Statistics},
  publisher = {Elsevier},
  volume = {34},
  pages = {155-167},
  year = {2016},
  booktitle = {Data Gathering, Analysis and Protection of Privacy Through
               Randomized Response Techniques: Qualitative and Quantitative Human
               Traits},
  doi = {https://doi.org/10.1016/bs.host.2016.01.009},
  author = {M. Rueda and B. Cobo and A. Arcos and R. Arnab},
}
</pre></div>]]></content:encoded>
					
					<wfw:commentRss>https://jeremykun.com/2022/05/14/practical-math-preview-collect-sensitive-survey-responses-privately/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">118403</post-id>	</item>
	</channel>
</rss>