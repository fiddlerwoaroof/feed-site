<?xml version="1.0" encoding="UTF-8" standalone="no"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US">
  <id>tag:www.akitaonrails.com,2008:/posts</id>
  <link href="https://www.akitaonrails.com/" rel="alternate" type="text/html">
  <link href="https://www.akitaonrails.com/posts.atom" rel="self" type="application/atom+xml">
  <title>AkitaOnRails.com</title>
  <updated>2022-11-08T10:21:17-03:00</updated>
  <generator uri="http://www.akitaonrails.com">AkitaOnRails</generator>
  <author>
    <name>Fabio Akita</name>
    <email>boss@akitaonrails.com</email>
  </author>
  <xhtml:meta content="noindex" name="robots" xmlns:xhtml="http://www.w3.org/1999/xhtml"/><entry>
    <id>tag:www.akitaonrails.com,2008:Post/6001</id>
    <published>2022-11-08T10:20:00-03:00</published>
    <updated>2022-11-08T10:21:17-03:00</updated>
    <link href="/2022/11/08/akitando-131-sua-seguranca-e-uma-droga-gerenciadores-de-senhas-2fa-encriptacao" rel="alternate" type="text/html">
    <title>[Akitando] #131 - Sua Segurança é uma DROGA | Gerenciadores de Senhas, 2FA, Encriptação</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/s7ldn31OEFc&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Você já deve ter consciência que não deveria estar usando a mesma senha pra tudo, nem senhas curtas, nem senhas fáceis, e não deveria estar recebendo código 2FA via SMS, e deveria ter habilitado encriptação nos seus HDs.&lt;/p&gt;

&lt;p&gt;Mas por quê? Hoje quero demonstrar porque todas essas &quot;boas práticas&quot; que todo mundo tá careca de saber, e até hoje tem preguiça de colocar em prática, realmente significa. Vamos entender.&lt;/p&gt;

&lt;p&gt;== Errata&lt;/p&gt;

&lt;p&gt;3:57 - eu falo que o Mitnick não mostra o programa mas eu que não prestei atenção, é o bom e velho HASHCAT. Procurem no Google pra saber a respeito.&lt;/p&gt;

&lt;p&gt;== Capítulos&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;01:14 - Cap 01 - Pra que Senhas Fortes? Quebrando senhas&lt;/li&gt;
&lt;li&gt;06:11 - Cap 02 - Como se Guarda Senhas no Banco de Dados? bcrypt vs sha512&lt;/li&gt;
&lt;li&gt;09:41 - Cap 03 - Pra que Servem Gerenciadores de Senhas? Bitwarden&lt;/li&gt;
&lt;li&gt;17:17 - Cap 04 - Como funciona Two-Factor Authentication? TOTP&lt;/li&gt;
&lt;li&gt;22:32 - Cap 05 - Como se pega um Ransomware? Pirataria&lt;/li&gt;
&lt;li&gt;31:49 - Cap 06 - Encripte seus Drives&lt;/li&gt;
&lt;li&gt;33:59 - Cap 07 - Veracrypt: Segredo do Segredo&lt;/li&gt;
&lt;li&gt;40:44 - Bloopers&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;== Links&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How Easy It Is To Crack Your Password, With Kevin Mitnick (https://www.youtube.com/watch?v=K-96JmC2AkE)&lt;/li&gt;
&lt;li&gt;Hashcat (https://hashcat.net/hashcat/)&lt;/li&gt;
&lt;li&gt;Bitwarden (https://bitwarden.com/)&lt;/li&gt;
&lt;li&gt;Host Your Own Open Source Password Manager (https://bitwarden.com/blog/host-your-own-open-source-password-manager/)&lt;/li&gt;
&lt;li&gt;Authy (https://authy.com/)&lt;/li&gt;
&lt;li&gt;Veracrypt (https://www.veracrypt.fr/en/Home.html)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Você é iniciante em programação ou já trabalha com tecnologia? Usa a mesma senha pra tudo? Anota senha num arquivo texto no seu dropbox? Baixa instaladores piratas de coisas como Photoshop ou cheats pros seus jogos? Recebe o código de autenticação two factor via SMS? Pois bem, hoje é o dia de saber porque você tá fazendo tudo errado e também porque.&lt;/p&gt;

&lt;p&gt;Mais do que isso, qual a diferença de usar um SHA256 ou bcrypt pra gerar hash de senha pra gravar no banco? Como funciona o algoritmo de two factor? Que aplicativos devo usar de fato pra estar minimamente protegido? Finalmente vamos explorar um pouco mais sobre segurança pessoal e um pouco de como essas coisas funcionam por baixo dos panos. Então o episódio deve servir tanto pra programadores como pra quem só está preocupado em se proteger. E não importa o que faça, você nunca vai estar 100% protegido.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Meses atrás eu já tinha feito um video falando um pouco sobre hacking, consultorias de segurança e como esse mercado idiota de selo de segurança é uma grande porcaria. Depois assistam aquele video também. Mas hoje é sobre segurança profissional e não infosec.&lt;/p&gt;

&lt;p&gt;A primeira coisa mais básica que todo mundo erra é a famosa recomendação de usar uma senha forte diferente pra cada site que tem conta. Senhas fortes é usar o máximo de caracteres diferentes possíveis, incluindo letras minúsculas e maiúsculas, números, caracteres especiais como cerquilha, asterisco, e ter um comprimento de no mínimo uns 12 caracteres. Ideal é preencher o campo de senha até o máximo que permite. Sites que até hoje limitam a senha a coisas ridículas como 8 ou 10 caracteres foram feitos por imbecis morônicos amadores que não entendem absolutamente nada de segurança.&lt;/p&gt;

&lt;p&gt;Vamos entender de verdade porque isso é importante. Tem um video de 2018 de ninguém menos que Kevin Mitnick, um dos hackers mais famosos do mundo. Não porque é o melhor, só que virou celebridade na época em que foi preso. Nesse video ele começa demonstrando como uma pessoa normal poderia pensar em fazer uma senha que acha forte. Pra começar, vai num gerador de palavras aleatórias e escolhe uma que acha comprido, como &quot;quadrilateral&quot;, que tem nada menos que 13 letras. Olha só, deve ser forte né?&lt;/p&gt;

&lt;p&gt;Mas só isso não parece seguro. Vamos adicionar uns números e caracteres especiais no fim como &quot;1&quot;, &quot;2&quot;, asterisco e cifrão. Mas mesmo assim ainda não parece seguro. Uma coisa que todo mundo faz é converter palavras em leet codes. Leet code é trocar &quot;A&quot; pelo número 4 ou arroba, o &quot;i&quot; pelo número &quot;1&quot;, &quot;e&quot; por &quot;3&quot; e assim por diante. E olha só, agora assim, parece uma senha forte, não parece? Uma senha de 17 caracteres misturando letras, números, caracteres especiais.&lt;/p&gt;

&lt;p&gt;A história hipotécia que ele conta é que conseguiu copiar o banco de dados de senhas do Windows de alguém. O Windows antigo não grava sua senha aberta, passa por um algoritmo de hash. Lembra o que é hashing? Eu explico isso no 1o video de criptografia. Assiste depois. É como uma impressão digital, não é encriptação. Ele passa essa senha por uma função que cospe uma string de tamanho fixo. Se fosse uma senha de 2 caracteres ou 100 caracteres, no final essa função cospe uma string com o mesmo tamanho fixo de 512 bits. Ou seja, pra saber se adivinhamos a senha, precisamos passar por essa mesma função e comparar o hash gerado com o hash que estava no banco de dados do Windows.&lt;/p&gt;

&lt;p&gt;Todo desenvolvedor web já fez isso alguma vez na vida. Quando o usuário se cadastra no seu site e digita a senha no formulário, você passa por uma função de hashing como SHA512 da vida e grava o hash no banco, na esperança que se alguém um dia conseguir roubar seu banco de dados, pelo menos as senhas não vão estar facilmente visíveis. E agora quero mostrar porque só isso não ajuda em nada e tanto esse seu site quanto a autenticação antiga do Windows com NTLM são ruins.&lt;/p&gt;

&lt;p&gt;O Mitnick usa um programa que não mostra qual é pra quebrar essa senha. Provavelmente usa uma mistura de rainbow tables, ataque de dicionário e processamento paralelo em GPUs pra gerar hashes e tentar quebrar na força bruta. Ele já tem o hash que quer achar, e a GPU vai gerando hashes a partir do dicionário e comparando pra ver se acha. Já vou explicar o que é cada uma dessas coisas, mas olhem no video. A máquina dele de 2018 tem nada menos que 8 GPUs GTX 1080 em paralelo. Lembrando que essas GPUs são 3 gerações defasadas, depois veio a RTX 2080, depois saiu a RTX 3080 que por acaso é a que eu tenho, e já foi lançada a nova RTX 4090.&lt;/p&gt;

&lt;p&gt;E é isso, em menos de 1 minuto rodando, aquela nossa senha que parecia forte, foi quebrada. Nem 1 minuto. 17 caracteres, com números e letras especiais misturado. Só 1 minuto pra quebrar em 2018. Em 2022, com meia dúzia de RTX 3090 Ti rodando, essa senha pode ser quebrada em segundos. E agora? Tá lembrando a senha que você usa pra proteger seu email? Suas contas pessoais? Olha só, é a mesma senha pra tudo? E sua senha é mais curta e mais fraca que essa que eu mostrei? Nem sei pra que você se incomoda de colocar senha, deixa tudo aberto de uma vez, é a mesma coisa.&lt;/p&gt;

&lt;p&gt;&quot;Ahhh, mas ninguém nem sabe que eu existo.&quot; É verdade. Todo mundo pensa que ninguém vai se importar com você. Afinal você é insignificante, ninguém te segue nas redes sociais, ninguém vai tentar quebrar justamente suas contas. E é aí que você se engana. Todo ano dezenas de sites ao redor do mundo tem sua infraestrutura comprometida por diversos motivos. Por exemplo, em Janeiro de 2022, via uma conta de funcionário interno, vazou uma quantidade gigante de emails, números de telefone e muito mais. Você tem conta no Twitter? Provavelmente seu email agora está no banco de dados de gente que vai tentar quebrar contas.&lt;/p&gt;

&lt;p&gt;Mas não é só isso. Existe um site chamado &quot;Have I been Pwned&quot; que documenta incidentes como esse do Twitter. São eventos grandes e de muita repercussão e no banco de dados deles, no momento que estou escrevendo este episódio, tem documentado a invasão de mais de 600 sites, mais de 11 bilhões de contas que vazaram. Sim, seu email e possivelmente o hash da sua senha, já vazaram e tem robôs nesse exato momento tentando entrar em uma de suas contas.&lt;/p&gt;

&lt;p&gt;Antes de falar de como dificultar isso. Vamos entender um pouco mais por baixo dos panos pra você que é programador. Todo site bem feito nunca grava sua senha aberta. Sempre vai passar por um algoritmo de hashing. Mas não é qualquer algoritmo como MD5 ou SHA512. Não, ele vai usar um algoritmo de derivação de chaves, como bcrypt ou PBKDF2 ou o mais novo Argon2. Se você já usou um framework web como Rails ou Django, já deve ter visto esses nomes.&lt;/p&gt;

&lt;p&gt;Qual a diferença de um algoritmo de hash como SHA512 e um algoritmo de derivação de chaves como bcrypt? É o tempo de execução. Um SHA512 foi desenhado pra ser eficiente e rápido. Ele é usado pra rapidamente conseguirmos gerar uma impressão digital de um PDF ou algo assim pra assinarmos digitalmente. É bom pra passarmos um arquivo que fizemos download como a ISO de uma distro Linux e checar com o SHA no site deles pra ver se o download veio perfeito. Mas justamente porque é eficiente, que ele não presta pra fazer hash de senhas. Bizarro isso né? Pensa: por que ser eficiente é ruim nesse caso?&lt;/p&gt;

&lt;p&gt;Quando um hacker como um Mitnick roda o tal programa dele, precisa ir tentando diversas combinações de letras até achar a sua senha. E a cada tentativa ele precisa rodar pelo mesmo algoritmo de hash pra comparar com o hash que está no banco de dados que foi vazado. Se o algoritmo de hash for eficiente, é rápido gerar milhões ou até bilhões de hashes por segundo. Por isso usamos um bcrypt ou PBKDF2, porque não tem jeito de fazer esses algoritmos serem mais rápidos, faz parte do design deles serem lentos de rodar. Isso dificulta o trabalho de quebrar senhas. As GPUs dele vão executar bem mais lento, de bilhões de hashes por segundo com um SHA512 vai cair pra centenas de hashes de bcrypt. Estamos dificultando a vida do hacker, e isso sempre é bom.&lt;/p&gt;

&lt;p&gt;Por isso, se você ver um SHA512 sendo usado pra hashear senhas, tá errado, tem que ser bcrypt ou PBKDF2. Eles são uma categoria diferente. Não são algoritmos de hash e sim de derivação de senhas. Como o nome diz, foram feitos pra pegar uma senha curta ou fraca e tentar derivar uma senha mais longa e mais forte. Além disso, esse é o motivo de porque todo sistema de autenticação minimamente seguro também faz uso de salts.&lt;/p&gt;

&lt;p&gt;Salt é um número aleatório. Ele vai gravado aberto na tabela e o hash gerado não é só da senha, e sim da senha concatenado com esse salt. Digamos que duas pessoas no site usem a mesma senha &quot;teste1234&quot;, que todo mundo usa. O hash dos dois acabaria sendo o mesmo. Portanto quando o hacker quebrar a senha de um, automaticamente tem acesso à conta dos outros usuários que tem a mesma senha. Mas se o salt for aleatório pra cada usuário, o hash dessa senha, concatenado com o salt, vai ser diferente. Agora dificultamos mais ainda pro hacker. Se ele conseguir quebrar a senha de um deles, não tem como saber que o outro também usa a mesma senha. Ele vai precisar repetir a força bruta pros dois, pra chegar na mesma senha.&lt;/p&gt;

&lt;p&gt;Mas os hackers não são burros. Obviamente todo mundo sabe que um site bem feito vai usar bcrypt da vida. E sabe que ficar gerando hash de bcrypt é lento. Por isso eles colaboram pra gerar tabelas pré-computadas. Começa pegando um dicionário grande de palavras em inglês. E gera o bcrypt de todas e grava num banco de dados. Agora pega cada palavra e concatena com milhões de combinações diferentes de números e letras especiais, antes e depois. Daí também gera o bcrypt de todas as palavras convertidas em leet code. E assim por diante.&lt;/p&gt;

&lt;p&gt;Nas deep web da vida você não acha só o banco de dados vazado dos sites com Twitter ou Adobe. Também tem pra comprar esses bancos de dados de hashes pré-computados. É isso que chamamos de Rainbow Tables. Em vez de gastar tempo computando o bcrypt de tudo na mão, é mais fácil gastar gigabytes ou terabytes de espaço num banco de dados e cruzar com o hash do banco vazado. Comparar milhões de linhas de um banco de dados é super rápido hoje em dia. Em questão de minutos, a poucas horas, dá pra achar todo mundo que usou senhas fracas. Esses vão se foder primeiro. Portanto o hacker troca o trabalho de computar hashes pesadas pelo trabalho mais rápido de procura num banco de dados gigante de hashes pré-computadas.&lt;/p&gt;

&lt;p&gt;A única forma de ficar minimamente seguro é usar senhas que não são facilmente deriváveis de palavras em dicionários. Por isso que toda vez falamos que você precisa usar senhas geradas aleatoriamente. Porque não vai ter em nenhum dicionário, o hacker vai ser obrigado a tentar achar via força bruta testando todas as combinações de todos os caracteres possíveis. O alfabeto tem 26 letras. Dobre isso porque tem maiúsculo e minúsculo. Temos 10 números. E temos pelo menos 30 caracteres especiais como asterisco ou parênteses.&lt;/p&gt;

&lt;p&gt;Isso dá um total de 26 mais 26 mais 10 mais 30 caracteres que são 92 possíveis letras. Se fizermos uma senha de 10 caracteres aleatórios, o hacker na pior das hipóteses vai precisar calcular hash pra 92 vezes 92 vezes 92 vezes 92 ... 10 vezes. Isso dá nada menos que 43 quintilhões de possibilidades. Se aumentar só um caractere pra 11, aumentamos as possibilidades mais 92 vezes e o número sobe pra 3 sextilhões. Cada caracter a mais dificulta 92 vezes. Quanto mais longa for a senha, mais fode a vida do hacker.&lt;/p&gt;

&lt;p&gt;Então vou repetir: nunca use senhas derivadas de palavras que existem. Todas as combinações que você imaginar de embaralhar um pouquinho, tipo colocar arroba no lugar do &quot;a&quot; ou exclamação no lugar do &quot;i&quot; ou &quot;l&quot;, tudo isso já foi pré-computado e existe numa rainbow table. Sempre use senhas totalmente aleatórias. E sempre use senhas o mais longo quanto for possível. 50 caracteres, 80 caracteres. O máximo que o formulário de cadastro do site deixar. E sim, você não deve ser capaz de conseguir decorar essa senha, e esse é o objetivo. Se você consegue decorar, significa que a senha é fraca e facilmente quebrável.&lt;/p&gt;

&lt;p&gt;Na pior da pior das hipóteses, todo navegador moderno como Firefox, Safari, Edge ou Chrome, tem já embutido um gerenciador de senhas. Não recomendo usar deles porque tem alternativas melhores, mas eu diria que se você for nesse nível de preguiçoso, pelo menos use do navegador. Único inconveniente é que vai precisar usar o mesmo navegador no seu PC de casa, no notebook e no smartphone, pra ter acesso às suas senhas, porque esses navegadores conseguem sincronizar o banco de dados via sua conta da Apple ou Google ou Microsoft da vida. Significa que pelo menos a versão encriptada dessas senhas pode estar armazenada nos servidores do Google ou Apple. Mas é um risco que vale a pena correr, muito menos risco do que usar a mesma senha fácil pra tudo.&lt;/p&gt;

&lt;p&gt;O ideal é não ter esse banco de senhas, mesmo encriptado, no servidor de ninguém e por isso eu gosto do aplicativo Bitwarden. Ele é código-aberto, então você mesmo pode auditar o código se quiser. E tem a opção de você mesmo configurar um servidor pra manter seu próprio banco encriptado de senhas. Daí a extensão no seu navegador vai puxar tudo de um servidor que só você controla. Eu só recomendo fazer isso se você já for um devops avançado e experiente. Pra 99% das pessoas o melhor é confiar mesmo na hospedagem de algum serviço que vai ter segurança melhor do que a sua. Seja o próprio Bitwarden, 1password, Lastpass ou similares.&lt;/p&gt;

&lt;p&gt;Olhem um exemplo de senha gerado pelo Bitwarden. É esse tipo de senha que você deve usar nos sites por aí. E como vai estar gravado e encriptado, não precisa se preocupar em decorar nada. Gere uma senha gigante e aleatória pra cada site. Entre em cada um dos sites que tem conta e altere sua senha hoje mesmo. Lembra que eu falei que vira e mexe algum site é hackeado e o banco de dados acaba na mão de algum hacker? Mesmo se ele conseguir quebrar sua senha desse site, ter senhas diferentes significa que mesmo que ele mande os robôs dele ficar tentando em dezenas de outros sites, essa senha não vai abrir nada pra ele, porque só é usado nesse um site. Quando você usa a mesma senha pra tudo, basta um site que você usa se ferrar que ele ganha acesso a todos os outros sites que você usa. Esse é o risco real e imediato.&lt;/p&gt;

&lt;p&gt;A única senha que vai precisar decorar é a senha mestra que abre o banco encriptado desses aplicativos. Nesse caso, infelizmente, vai precisar decorar então não dá pra ser uma senha totalmente aleatória, então no mínimo use uma passphrase e não um password. Literalmente pense numa frase longa, de mais de 20 caracteres, no mínimo. Alguma coisa tipo &quot;acomidaqueeumaisgostoémilho&quot; ou &quot;aviagemqueeumaisgosteifoiprojapao&quot;. Todos os bons aplicativos vão pedir essa senha da primeira vez pra sincronizar o banco encriptado de senhas e eles costumam ter opção pra habilitar autenticação via impressão digital, no caso de smartphones. Daí fica mais prático de usar.&lt;/p&gt;

&lt;p&gt;&quot;Akita, qual desses aplicativos é melhor?&quot; Foda-se. Use qualquer um deles. Tem gente que morre se não for 1password. Tem gente que acha o LastPass feio. Tem gente que acha um Bitwarden mais ou menos. Tem gente que prefere mesmo usar o do próprio Google ou Microsoft no navegador. Tanto faz. O mais importante é a passphrase mestra e você hoje mesmo sair alterando a senha de todos, todos os sites que acessa, e cadastrar senhas super longas e aleatórias diferentes pra cada uma.&lt;/p&gt;

&lt;p&gt;A próxima recomendação é habilitar two factor authentication, o famoso 2FA ou autenticação de dois fatores. Que é aquele código que muda a cada 30 segundos e que recebe via SMS ou usa um aplicativo como Microsoft Authenticator ou Authy. Já de cara vou falar pra evitar o Google Authenticator. Sempre dá problema essa bosta, especialmente se você troca de celular e precisa instalar de novo. Use qualquer um, menos do Google. Aplicativos que gerenciam senhas como 1password ou o Bitwarden, já tem suporte a 2FA também, então pode usar tudo no mesmo aplicativo.&lt;/p&gt;

&lt;p&gt;Outra coisa: eu não recomendo de jeito nenhum receber esse código por SMS. Celulares podem ser roubados. E não importa se seu celular tem senha. Basta tirar o SIM card e colocar em outro celular. Ou posso usar engenharia social. Ligar na Vivo ou Claro e enganar o atendente. Um hacker bom de engenharia social já tem seus dados, seu nome, RG, CPF, email, nome da mãe e tudo mais. Não são informações difíceis de achar dado que diversos sites do governo já foram hackeados e bancos de dados com todas essas informações já vazaram. Todo hacker já sabe seus dados.&lt;/p&gt;

&lt;p&gt;Conseguir roubar seu número de celular é questão de tempo. Fora o inconveniente se você realmente perder o celular. E agora? Como você faz login nos sites que vão pedir código de dois fatores? Vai te mandar por SMS pro celular que você perdeu ou foi roubado. Até conseguir outro SIM card, vai ficar uns dias sem conseguir acessar suas coisas. Um puta pé no saco. E como Murphy sempre é eficiente nessas horas, vai ser justo num domingo de feriado prolongado e bem num dia que você precisava acessar um site importante no meio de uma viagem.&lt;/p&gt;

&lt;p&gt;Por que, lógico, durante essa viagem você encheu a cara na balada e só percebeu que tava sem celular quando voltou pro hotel. É assim que você se ferra à toa. E nem precisa de muito. Basta estar num lugar com sinal fraco de celular ou sem sinal. Ou estar viajando sem roaming. Como vai receber SMS sem sinal?&lt;/p&gt;

&lt;p&gt;O que é esse tal de two-factor? É uma função que recebe pelo menos dois parâmetros, um segredo compartilhado e uma hora do sistema e gera um código. O padrão mais conhecido é o TOTP ou Time-based One-Time Password, ou literalmente senha descartável baseada em tempo, que é definida na RFC 6238. O que acontece é o seguinte, digamos que você entre num site novo e vai cadastrar sua conta. Preenche tudo, coloca sua senha e manda habilitar two factor.&lt;/p&gt;

&lt;p&gt;O site te dá um QR Code, que tem um código que chamamos de seed ou semente, esse é o segredo compartilhado entre o servidor e nosso app. Se estiver num app como o Bitwarden, vai ter um campo chamado Authenticator Key (TOTP) que é onde se copia e cola o seed, que costuma estar embaixo do QR Code. Ou se estiver num app separado como o Authy, pode abrir a câmera dele e ler o QR Code. Além do seed, nesse QR Code vai ter informações extras pra facilitar o cadastro como o nome do site, URL e tals.&lt;/p&gt;

&lt;p&gt;E pronto. O site vai pedir pra confirmar se tudo correu bem, e pra isso é só digitar o código que o app mostra. Ele vai checar e confirmar. Agora, toda vez que fizer um novo login, depois da checagem de senha,  vai pedir o código de two factor. Só abrir o app, e digitar o código dentro da janela de tempo que ele indica.&lt;/p&gt;

&lt;p&gt;O que acontece é o seguinte, depois do cadastro via QR Code, o servidor tem um segredo gravado e seu app tem esse segredo também gravado no app, ambos encriptados. Quando você abre o app, ele pega o horário atual, arredondado em períodos de 30 ou 60 segundos, por isso tem o timerzinho toda vez. O código que aparece é derivado de passar o segredo que é o seed mais o horário atual e gerar um hash baseado num algoritmo como HMAC-SHA1. Ele trunca o resultado em 6 dígitos e é isso que você vê no app.&lt;/p&gt;

&lt;p&gt;Quando digita o código e manda pro servidor, lá vai fazer a mesma coisa, pegar o segredo, o horário no servidor, e gerar o mesmo código via HMAC-SHA1. Se o código bater com o que você digitou, significa que é você mesmo. Se não bater, pode ser que justo passou o período de 30 ou 60 segundos e vai pedir pra digitar de novo. A vantagem de uma senha secundária dessas é que mesmo que alguém veja esse código, ele só tem validade dentro de uma janela de menos de 1 minuto. Parece pouco tempo, mas na realidade é bastante, já falo sobre isso. De qualquer forma, como é baseado no horário, por isso é importante seu celular ter horário sincronizado com um servidor de NTP. Quando você tem problemas, primeira coisa é checar isso.&lt;/p&gt;

&lt;p&gt;Na prática parece um troço complicado. Já tem que ter senha complicada, agora depois de digitar a senha ainda tem esse troço que dá gatilho na minha ansiedade com esse timer maldito mandando digitar rápido. Não é a toa que muita gente mais sensível ou mais preguiçosa realmente não quer ter que lidar com isso. Mas esse é o preço que se paga pela internet ter ficado popular.&lt;/p&gt;

&lt;p&gt;Vamos resumir o que falamos até agora: instale um aplicativo de gerenciador de senhas que, de preferência, também tenha suporte a two factor authentication. Minhas escolha pessoal é o Bitwarden, mas é mais por costume. Garanta que a senha mestra é uma frase longa e que realmente só você saiba. Não anote num post-it embaixo do seu teclado. E a partir daí crie senhas hiper longas e aleatórias e habilite two factor em todas as suas contas. Com isso, mesmo se outro site que você tenha conta for invadido e alguém conseguir fazer força bruta pra quebrar sua senha, na pior das hipóteses não vai conseguir usar essa senha em nenhum outro site que você tenha conta, porque são todas com senhas diferentes.&lt;/p&gt;

&lt;p&gt;A gente fala two factor ou autenticação de dois fatores, porque em segurança se fala em ter no mínimo 3 fatores de segurança. O mais simples é o alguma coisa que você sabe, que é sua senha. O segundo fator é alguma coisa que você tem, que pode ser seu aplicativo de two factor como o Authy ou o equivalente em hardware disso que seria algo com um Yubikey. Eu nem vou falar de Yubikey hoje, mas teoricamente ele é melhor do que só um app de two factor. Problema que nem tudo suporta hardware keys e a usabilidade não é exatamente a mais simples. Mas vale a pena dar uma pesquisada se tiver interesse.&lt;/p&gt;

&lt;p&gt;O terceiro fator é alguma coisa que você é. No caso costumamos nos referir à biometria, como sua face num iPhone ou com Windows Hello, ou o mais comum que é impressão digital. Hoje em dia isso é mais comum em smartphones e muitos notebooks. Antigamente eram muito mau feitos, mas acho que hoje em dia a maioria das implementações é segura o suficiente. Em smartphones como iPhone, sua impressão digital não fica armazenada no mesmo lugar que seus dados e aplicativos. Ele fica numa memória separada que o sistema operacional não tem acesso e controlado por um chip independente da sua CPU.&lt;/p&gt;

&lt;p&gt;Isso é o que chamamos de Secure Enclave, que num iPhone é o chip T2, num PC é o chip de TPM ou Trusted Platform Module, que o Windows 11 quis tentar ser obrigatório e deu treta e acho que voltaram atrás. Mas o correto é mesmo ter um TPM no seu sistema, um chip e memória encriptada que mesmo que seu computador seja comprometido por malwares, esse chip continuaria intacto.&lt;/p&gt;

&lt;p&gt;E falando em malwares, agora vem a parte mais problemática: evite ao máximo pegar virus e malwares. Parece meio óbvio, mas todo mundo a essa altura já pegou um malware. Se você baixou pirataria recentemente, como photoshop, com certeza já tem malware instalado. Filmes e música tem baixa probabilidade de ter. Só se alguém descobriu um bug nas bibliotecas que fazem decoding de filmes e consegue injetar malware via algum tipo de buffer overflow da vida. Antigamente tinha em imagens, não lembro se era PNG. Mas esses componentes costumam ser bem maduros e não mudam mais, então as chances são baixas.&lt;/p&gt;

&lt;p&gt;O problema são executáveis mesmo. Instaladores de Photoshop por exemplo, esses são campeões de carregar coisas como Ransomwares. Se você tem Photoshop instalado, eu faria backup de todos os seus dados importantes num HD externo hoje, logo depois de terminar de assistir, porque provavelmente tem um ransomware encriptando seus arquivos aos poucos em background e você nem tá sabendo.&lt;/p&gt;

&lt;p&gt;Povo que é gamer e baixa cheats e trainers pra games, muito provavelmente vocês estão infectados também. Você acha que seu computador tá ficando lento porque tem muitos games. Mas na realidade é porque tem virus e malware rodando aí já. E não adianta baixar um antivirus e tentar rodar. Muitos virus conseguem se esconder de antivirus. Uma vez infectado já é meio tarde.&lt;/p&gt;

&lt;p&gt;Se quiser pelo menos checar, tem vários antivirus que tem opção de queimar num pendrive e bootar direto dele. Assim o antivirus roda antes do seu Windows bootar, aí o virus não tem chance de se esconder. Acho que marcas como Kaspersky, Comodo, ESET, Avira e outros tem essa opção. Vá num computador que tem mais certeza que tá limpo, baixe a ISO. Num Windows, use um programa como o Rufus e grave a imagem num pendrive. E agora boote pelo pendrive e mande escanear seu computador. Mas lembre-se, só porque o antivirus não achou nada, não quer dizer que você tá limpo.&lt;/p&gt;

&lt;p&gt;A regra é simples: não baixe pirataria. Eu sei, você quer jogar de graça. Mas não rola. Pelo menos tenha um PC numa rede isolada que usa só pra jogar e nunca pra acessar coisas como seu banco, no mesmo computador. Se você instala games piratas baixado em bittorrent ou de sites como mega da vida, as chances de já estar contaminado são enormes. Nem tô brincando, serião. Em particular se você roda Windows.&lt;/p&gt;

&lt;p&gt;Quem joga em Linux, usando um ambiente isolado de emulação como Wine com Proton, tem menos chances de ter problemas, porque um virus feito pra Windows não vai infectar um Linux ou um Mac. Virus é específico pro sistema operacional que quer contaminar. Problema de jogar em Linux é que muitos jogos não permitem jogar online em ambiente emulado porque eles não conseguem carregar os programas de anti-cheat. Depende do jogo, acho que um Overwatch da vida já dá pra jogar, mas Fortnite parece que ainda não. Se for como eu que só joga single player, aí Proton já é bem mais útil.&lt;/p&gt;

&lt;p&gt;O tipo de malware que ficou mais famoso são os ransomware. Eles ficam trabalhando escondidos no seu PC sem você perceber. Vão encriptando seus arquivos um a um. E quando você percebe que tem algo de errado, a maioria dos seus arquivos tão lá ainda mas encriptados. Daí aparece a famosa janela de resgate, onde falam que tem a chave pra desencriptar, mas só vai te dar se mandar uma certa quantia de bitcoins pra eles. Tem garantia que eles vão te dar a chave? Não. Tem como recuperar os dados depois que foram encriptados. Não. Você se fodeu. Só aceita que dói menos, e você foi avisado, se arriscou porque quis.&lt;/p&gt;

&lt;p&gt;A única forma de garantia que você tem caso seus arquivos sejam encriptados por um ransomware é ter backup. E tomar cuidado pra ter várias semanas de backups, porque quando o ransomware começou a criptografar seus arquivos, você pode estar fazendo backup dos arquivos já criptografados. Então precisa garantir que tem versões antigas dos arquivos, de antes do ransomware ter começado a te foder. Por isso eu vim falando do meu NAS e como sistemas de arquivos com suporte a snapshot como btrfs ou ZFS fazem a diferença nesses dados. Eu tenho snapshots de várias semanas e meses no meu backup, então mesmo que um ransomware apareça no meu PC, consigo recuperar praticamente tudo sem pagar nada.&lt;/p&gt;

&lt;p&gt;Se você usa Linux, recomendo usar o btrfs e o programa Timeshift pra configurar tudo. Não é tema do video de hoje, mas snapshots são leves, ocupam pouco espaço, e você pode recuperar arquivos que foram apagados meses atrás se configurar direito, e é nativo do sistema operacional. Se usa Windows, precisa ter um backup que suporte um sistema de snapshots, mesmo que seja online. Serviços como o BlackBlaze. Dêem uma olhada. Backup em cloud é melhor que backup nenhum. A desvantagem é que depende da sua conexão de internet. Se perder tudo e precisar recuperar, vai demorar horas ou dias pra baixar tudo de volta via internet. Por isso o melhor é backup físico local, tipo HDs externos ou um NAS de verdade.&lt;/p&gt;

&lt;p&gt;Na prática, não instale jogos nem programas piratas. Você vai pegar malwares. Se precisar muito rodar um programa pirata, abra uma máquina virtual, seja o VirtualBox, ou se for pra rodar só uma vez, use o Windows Sandbox, que vem no próprio Windows. Se não sabia disso, basta abrir o programa de ligar funcionalidades opcionais, ticar aqui em Windows Sandbox e deixar ele se instalar e reiniciar. E pronto, olha só, eu abro o Sandbox e é um Windows vazio numa máquina virtual. Quando fechar essa janela, tudo que tem dentro é apagado. Perfeito pra rodar programas estranhos sem riscos pro seu Windows principal.&lt;/p&gt;

&lt;p&gt;Pra rodar máquina virtual seu PC precisa ser mais forte que a média. Nada abaixo de 16 giga de RAM. E quanto mais cores tiver sua CPU, melhor, de preferência uma CPU com 8 cores ou mais. Meu PC é um Ryzen 9 5950X com 64 giga de RAM. Isso já é demais pra maioria das pessoas, você não precisa de tudo isso. Eu gosto de brincar com máquinas virtuais porque posso fazer experimentos destrutivos e se eu quebrar a máquina, basta voltar o último snapshot dele que funcionava. Pra testar coisas, estudar sobre redes, estudar sobre vulnerabilidades e segurança, é a melhor coisa.&lt;/p&gt;

&lt;p&gt;Falando em Windows, muita gente ignora isso mas depois dos tempos sombrios de Windows XP, que era totalmente furado, o Windows 10 e 11 até que são bonzinhos em segurança. Se seu PC não for muito antigo, configure sua BIOS pra habilitar duas coisas: o TPM que falei antes e o Secure Boot. Secure Boot em particular é bem importante. Se algum malware tentar se injetar em arquivos importantes pro boot do Windows, a própria BIOS vai detectar e impedir o boot. Uma das formas de um virus se esconder de um antivirus é justamente se injetar na kernel ou em drivers e se carregar no boot, daí ele consegue se esconder dos antivirus.&lt;/p&gt;

&lt;p&gt;Vale a pena fuçar o aplicativo de Security do Windows. Mas não desligue nada nele. Tem muito tutorial bosta que fala pra desligar coisas como o Defender. Sim, o Defender custa recursos e em máquinas fraquinhas pode deixar um pouco mais lento, mas não, não compensa o risco de desligar segurança. Se tem máquina fraca, o melhor é desistir de Windows e migrar pra Linux se puder. Ou fazer upgrade de hardware, como instalar um SSD caso não tenha.&lt;/p&gt;

&lt;p&gt;Um dos grandes problemas do Windows são drivers. Hoje em dia muitos drivers já vem pelo Windows Update e esses teoricamente são checados e são seguros. Mas muito hardware de fabricante bosta ainda manda você baixar driver do site deles, isso quando tem. E quando procura no Google tem sites falsos com esses drivers modificados. Aí você baixa achando que é legítimo e instala um malware. Aliás, nunca baixe nenhum desses programas que prometem atualizar seus drivers automaticamente pra você e também otimizar a performance do seu PC. Muitos são malwares, a maioria é só fearware, software que fica dizendo &quot;olha só, achei problemas na sua máquina, me paga que eu conserto&quot;. É golpe. Não existe isso.&lt;/p&gt;

&lt;p&gt;Não tem performance de graça, mesmo otimizando manualmente, você não vai notar tanta diferença assim de verdade. É tudo fraude. A performance que você tem logo que instala o Windows do zero é tudo que vai ter. Se quiser um pouco mais, de novo, instale Linux e mesmo assim não vai ser nada revolucionário. Trabalhe, economize e compre SSD, mais RAM ou um PC novo.&lt;/p&gt;

&lt;p&gt;Por último, se tiver notebook, pense em investir num programa de VPN. Seja ExpressVPN ou NordVPN ou SurfShark ou qualquer um desses. Especialmente se usa muito redes públicas como Wifi em cafés, hotel, ou qualquer lugar fora da sua casa. Eu já expliquei nos episódios de rede como é possível ficar escutando seu tráfego. Além disso também é possível fazer &quot;DNS poisoning&quot; que é literalmente envenenar o DNS. Lembra como seu computador acha o endereço IP das coisas perguntando pro DNS? E se o DNS ou DHCP que o Wifi público que você se conectou estiver contaminado?&lt;/p&gt;

&lt;p&gt;Você pode tentar baixar um programa qualquer e na realidade o DNS envenenado te redirecionou pra um site falso, tipo pra baixar a porcaria do programa de banco de um Itaú da vida. Aí você baixa um instalador falso que vai te contaminar com um malware. Todo cuidado é pouco. Não baixe nada a partir de redes que você não confia, como redes públicas. Sempre use uma VPN pelo menos pra garantir. Em alguns países do mundo como uma China ou no Oriente Médio, seu tráfego pode realmente estar sendo monitorado por agências do governo.&lt;/p&gt;

&lt;p&gt;E falando em viagens, a última coisa que você precisa fazer se tem notebook é habilitar encriptação do seu HD. No Windows isso se chama BitLocker, em Macs se chama FileVault e em Linux se chama LUKS. Não importa qual sistema operacional usa, habilite encriptação. Eu não sei se não é óbvio, mas vou dizer o óbvio: aquela senha que você usa pra se logar na sua conta, não protege nada.&lt;/p&gt;

&lt;p&gt;Se eu roubar seu notebook, posso tirar o SSD, NVME e plugar em outro computador e vou ter acesso a 100% dos seus dados. Simples assim. Ah sim, você é aquele animal que guarda senha num arquivo de texto no desktop do seu computador? Se eu tiver acesso ao seu computador, vou ler tudo. Macs são mais convenientes ainda. Você pode bootar ele em Target Disk Mode e ele literalmente vira um HD externo USB gigante. Basta eu conectar via USB em outro computador e vou enxergar como se fosse um pendrive gigante. Se não sabia disso, reinicie e deixe apertado a tecla T antes do boot.&lt;/p&gt;

&lt;p&gt;Eu já tive um macbook roubado. Bobeei uma vez a noite, numa rua escura, com as janelas abertas e minha mochila no banco de passageiro. Motoqueiro parou do meu lado, apontou uma arma e mandou eu dar a mochila. E lá se foi meu Macbook. Minha única consolação é que tava tudo encriptado, então ninguém ia poder ver nenhum arquivo. E por sorte eu tava com Find my Mac ativado. Isso é uma grande vantagem de Macs e iPhones, que tem isso por padrão. Semanas depois alguém ligou esse Macbook e eu consegui achar ele online e mandei comando de apagar tudo remotamente. Se você tem produtos Apple, ative o Find my Mac e Find my iPhone.&lt;/p&gt;

&lt;p&gt;Eu sou tão paranóico com essas coisas que mesmo HDs velhos, eu nunca jogo fora. Mesmo a maioria deles estando encriptado. Até pra destruir HDs você precisa fazer direito, porque um bom investigador forense tem técnicas pra recuperar dados de HDs em condições que você não acredita. Se for jogar um HD fora, pegue pregos, martelo e fure todos os discos. E mesmo usando partição encriptada eu ainda crio discos virtuais encriptados dentro usando um programa que uso faz anos, o Veracrypt.&lt;/p&gt;

&lt;p&gt;Pra quem não conhece, o Veracrypt é o antigo TrueCrypt. Ele serve pra criar os discos virtuais mais seguros do mundo. Mesmo partição encriptada, com algum trabalho e força bruta, dá pra quebrar. Isso porque eles usam o mesmo algoritmo toda vez e é conhecido. Já no Veracrypt você pode escolher dentre uma gama de algoritmos, e criar uma partição escondida separada dentro do disco virtual encriptado, então ele teria uma partição falsa que você pode abrir caso esteja com alguém com uma arma na sua cabeça, e uma segunda partição no mesmo arquivo escondido que ninguém vai saber. Sério, dá uma estudada em Veracrypt, é animal e funciona em todo sistema operacional. Arquivos mais confidenciais, eu sempre deixo em discos criptografados por Veracrypt, porque a chance de ser quebrado é muito perto de zero.&lt;/p&gt;

&lt;p&gt;So de curiosidade, deixa eu criar um disco virtual com Veracrypt. Aqui na opção de Volume Type posso escolher Hidden volume, ou volume escondido. Seguindo, agora eu posso escolher o algoritmo de encriptação. Pra ser veloz, o certo é escolher AES porque hoje em dia toda CPU acelera encriptação AES em hardware. Mas pra ser seguro, performance não é importante. Eu posso escolher qualquer outro, como esse kuznichik ou sei lá como pronuncia. E o algoritmo de hash vai ser o streebog. Vou criar um volume de 50 mega só pra demonstração aqui.&lt;/p&gt;

&lt;p&gt;Agora eu coloco, lógico, uma senha super longa e super complicada como já expliquei antes, um passphrase gigante. Como é um disco virtual, posso escolher que sistema de arquivos quero que seja formatado dentro. Se quiser compatibilide com múltiplos sistemas operacionais, posso formatar em FAT já que o volume é pequeno ou exFAT. Só pra Windows posso escolher NTFS. Se for pra Linux posso escolher ext4. Vou escolher FAT mesmo.&lt;/p&gt;

&lt;p&gt;Nesse outro passo ele captura o movimento do meu mouse pra gerar um número aleatório seguro. Em vez de usar um gerador pseudo aleatório como o dispositivo /dev/random que tem no Linux, eu como ser humano me mexendo, vou gerar posições aleatórias de verdade, e ele vai usar isso internamente pra criptografia dele.&lt;/p&gt;

&lt;p&gt;Agora vem o truque. Ele terminou de criar o volume, formatar como ext4 e montar como drive. Lembram quando expliquei sobre imagens, drives virtuais e pontos de montagem no video de Slackware? Revejam se não lembram. Olha só, ele montou como &quot;/mnt/veracrypt1&quot;. É como se fosse um pendrive virtual. Mas essa é a partição falsa. Aqui eu vou gravar arquivos quaisquer que não são secretos de verdade. Por exemplo, vou arrastar uma imagem aqui dentro.&lt;/p&gt;

&lt;p&gt;Pronto, ele fechou aquela partição e agora vai abrir outra, a de verdade que é a escondida. De novo, posso escolher outros algoritmos de encriptação pra essa partição, que vai ser protegida com outra senha. Pra simplificar, vou manter as mesmas escolhas de antes. Ele pede pra escolher o tamanho, eu tinha escolhido 50 antes, gravei uma imagem dentro, e agora posso escolher o resto do tamanho pra essa nova partição. Vou escolher tipo 20 mega só. Pra ainda ter espaço sobrando na partição falsa de antes e não ficar muito óbvio que tem uma partição escondida. Pronto, mesma coisa, ele vai formatar e no final eu posso montar.&lt;/p&gt;

&lt;p&gt;Vamos montar a partição escondida e criar um arquivo secreto onde eu escreveria segredos altamente confidenciais. Pronto, montou. Navego até o ponto de montagem e olha só, tá vazio. Vamos criar o arquivo &quot;segredo.txt&quot;, abrimos e vou escrever um texto super secreto. Pronto. Tá salvo. Volto pro Veracrypt e posso desmontar.&lt;/p&gt;

&lt;p&gt;Por que eu fiz tudo isso. Digamos que algum malfeitor me pegou de refém e ele sabe que eu tenho segredos nesse arquivo. E ele vai cortar meus dedos se eu ficar falando que esqueci a senha. Então não tem jeito. Vamos montar a imagem, mas vou usar a primeira senha e não a segunda. Olha só o que vai acontecer. Montou, vamos navegar lá. Lembra que na primeira partição eu tinha só gravado uma imagem? É isso que abriu. O malfeitor não sabe que existe uma segunda partição. Mas como eu abri essa, ele não tem porque duvidar de mim. Eu dei a senha pra ele, só não a que ele queria. Nosso segredo está salvo na partição escondida. Sacaram o poder do Veracrypt?&lt;/p&gt;

&lt;p&gt;Pra finalizar, quando estava terminando este script alguém me mandou uma DM no Insta com um site no mínimo curioso. Olha só isso. Eu esqueci de gravar a imagem original mas postei esta story no insta no mesmo segundo. Admirem a beleza desse site só por um segundo. É o símbolo de que tudo que eu disse até agora não serve pra absolutamente nada. Porque no final do dia, um monte de gente realmente vai digitar os dados do próprio cartão num site estranho, por livre e espontânea vontade. Os hackers não precisam fazer nada muito avançado. As pessoas deixam as portas de casa abertas elas mesmas. Esse tipo de golpe não só funciona, como funciona muito bem. Não seja um otário.&lt;/p&gt;

&lt;p&gt;Não importa as proteções que você tem se você é um otário que sai clicando em tudo que te mandam. Eu fiz um teste outro dia com um tweet de gemidão. Centenas de pessoas caíram e saíram xingando. Foi um dos tweets com mais acesso que eu tive. Todo mundo clica nas coisas. É impressionante. Eu nunca clico em nada. Na verdade, se vem algum email ou mensagem com link, eu imediatamente bloqueio. Nem quero saber quem é. Não tem botão que eu uso mais em rede social que de bloqueio e botão de spam no Gmail.&lt;/p&gt;

&lt;p&gt;Olha alguns exemplos de emails que tá no meu spam. Eu não clico em nenhum link. Se eu não sei quem é a pessoa, ela nem devia estar me mandando mensagem. &quot;Ah, mas pode ser importante&quot;. Eu sempre parto do princípio que se for importante mesmo, essa mesma pessoa vai achar outras formas de entrar em contato comigo. Vai me ligar. Mas se ela acha que eu vou clicar num link, tá muito enganada. O tipo de golpe que de longe é o mais aplicado e de maior sucesso se chama phishing, que é ficar bombardeando as pessoas com mensagens aleatórias assim até que algum idiota clica no link. Vem via email, whatsapp, sms e muito mais. Regra de ouro: JAMAIS CLIQUE EM NENHUM LINK, JAMAIS, ZERO.&lt;/p&gt;

&lt;p&gt;E é isso aí. Hoje eu dei uma pincelada rápida por vários conceitos e várias ferramentas simples. São coisas que eu realmente uso no meu dia a dia e venho usando faz anos em Windows, Mac e Linux. Recomendo que todo mundo, seja programador ou não, se interesse por esse assunto, porque no final do dia você só pode contar com você mesmo. Ninguém vai te dar segurança, você precisa criar sua própria segurança, e isso começa com as coisas mais básicas, seu próprio celular, seu próprio computador. Não se preocupar com essas coisas é que nem fazer sexo sem proteção. Uma hora vai dar merda, e quando der não tem mais como voltar atrás. Se ficaram com dúvidas, mandem nos comentários abaixo. Se curtiram o video deixem um joinha, assinem o canal e compartilhem o video com seus amigos. A gente se vê, até mais!&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5999</id>
    <published>2022-10-25T10:30:00-03:00</published>
    <updated>2022-10-25T10:31:18-03:00</updated>
    <link href="/2022/10/25/akitando-130-rant-projetos-testes-e-estimativa-rated-r" rel="alternate" type="text/html">
    <title>[Akitando] #130 - Rant: Projetos, TESTES e Estimativa??? | Rated-R</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/H_-7o_pLn1s&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Finalmente vou discutir um pouco sobre Programação Orientada a Testes e o que isso tem a ver com gerenciamento de projetos, equipes, e um pouco sobre como todo mundo ainda encara estimativas da maneira errada.&lt;/p&gt;

&lt;p&gt;Este video é pra você programador, gerente, cliente, empreendedor, que tem dificuldades de entregar projetos de qualidade. Vamos entender tudo que vocês estão fazendo de errado e porque dá errado.&lt;/p&gt;

&lt;p&gt;== Conteúdo&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;01:13 - CAP 01 - Suas Premissas estão Erradas - Esqueça Bullshit&lt;/li&gt;
&lt;li&gt;05:33 - Programadores não são Pedreiros!&lt;/li&gt;
&lt;li&gt;08:31 - CAP 02 - Provas de Conceito - Comecem a Fazer!&lt;/li&gt;
&lt;li&gt;13:01 - CAP 03 - Programação Orientada a Testes - Comecem a Fazer!&lt;/li&gt;
&lt;li&gt;17:53 - Demonstração de testes de Firefox&lt;/li&gt;
&lt;li&gt;19:02 - Demonstração de testes de React&lt;/li&gt;
&lt;li&gt;19:27 - Demonstração de testes de Tailwind&lt;/li&gt;
&lt;li&gt;19:57 - Demonstração de testes de Django&lt;/li&gt;
&lt;li&gt;20:28 - Demonstração de testes de Laravel&lt;/li&gt;
&lt;li&gt;21:06 - Demonstração de testes de VSCode&lt;/li&gt;
&lt;li&gt;22:14 - Demonstração de testes de Rust&lt;/li&gt;
&lt;li&gt;24:02 - Demonstração de testes de Spring Boot&lt;/li&gt;
&lt;li&gt;27:25 - Vocês não SABEM fazer testes!&lt;/li&gt;
&lt;li&gt;28:45 - CAP 04 - Como a falta de Testes afeta Produtividade? A Raíz do Ágil&lt;/li&gt;
&lt;li&gt;32:27 - Como é um fluxo de trabalho Ágil?&lt;/li&gt;
&lt;li&gt;37:01 - CAP 05 - O que significa Estimar? Estimar não é Prever&lt;/li&gt;
&lt;li&gt;47:27 - CAP 06 - Removendo Maus Elementos. Não acumule problemas.&lt;/li&gt;
&lt;li&gt;50:07 - CAP 07 - Resumindo os Princípios: O Mínimo pra Começar&lt;/li&gt;
&lt;li&gt;52:27 - Bloopers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Vamos dar uma pausa na série de videos técnicos. Se não assistiu ainda, fiz uma minissérie falando sobre redes e logo em seguida começando a falar sobre Linux. Como foi muita coisa técnica, eu me entediei, então resolvi mudar a marcha hoje pra falar sobre gerenciamento de projetos, em particular finalmente quero discutir sobre o famigerado desenvolvimento orientado a testes. É um puta tema cabeludo que daria pra fazer um canal inteiro sobre, mas a idéia é discutir só alguns pontos básicos, pra iniciantes no assunto mesmo. Então vamos lá.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Se você já tem experiência, trabalha em grandes empresas e grandes projetos, certamente vai discordar de algumas coisas que vou falar hoje e não está errado. Processos de gerenciamento de projetos de software variam bastante e devem ser customizados especificamente pro seu tipo de empresa, pro seu estilo de trabalho em equipe. Não existe nenhuma metodologia que vai se encaixar magicamente em todos os lugares. Simplesmente não existe, é inútil tentar fazer isso. Eu falei sobre isso no video de Esqueça Metodologias Ágeis. Recomendo assistir esse também depois.&lt;/p&gt;

&lt;p&gt;A expectativa é que existe algum conjunto de procedimentos que toda equipe consegue seguir pra manter os projetos estáveis e eficientes. Certamente deve existir algum tipo de PMI, Agile, CMMi, bla bla bla Alguma fórmula que o Spotify, o Netflix, a Microsoft, alguém inventou, que resolve todos os problemas. E não tem. Quanto mais cedo aceitarem isso, menos dor de cabeça vão ter. Se você se ver dizendo &quot;a gente segue o modelo Spotify&quot; ou segue qualquer modelo, certamente está fazendo errado. Veja no meu video do Guia Definitivo de Organizações.&lt;/p&gt;

&lt;p&gt;Além disso, se você é um empreendedor não-técnico, certamente assistiu um monte de palestras motivacionais, bullshit de startups e tá fazendo um monte de coisas erradas. Eu fiz 3 videos pra vocês também, o de Empreendendo Errado com Software e o video em duas partes dos 10 Mitos sobre Tech Startups. Vou tentar não repetir o que já disse lá, então assistam depois também.&lt;/p&gt;

&lt;p&gt;Vamos lá, no final do dia, você, como membro de uma equipe de desenvolvimento de software, precisa entregar alguma coisa que seu chefe ou seu cliente precisam no produto deles. Ou mesmo no seu próprio produto. Se for iniciante, é muito mais fácil integrar uma equipe que já existe, que tem membros experientes e processos já em andamento. Um iniciante sempre vai fazer errado da primeira vez, especialmente sozinho. Não tem como acertar de cara sem nunca ter feito, e isso não é um defeito, é natural. São muitas peças móveis que você nunca viu funcionando. É que nem eu dar um carro desmontado e mandar montar tudo e ainda funcionar direito no final. Impossível.&lt;/p&gt;

&lt;p&gt;Vai parecer que é simples, só encaixar as peças e apertar os parafusos. Mas não é só encaixar. Precisa monitorar cada pedaço, testar, ver se não tem vazamentos, se tá tudo alinhado, balanceado, etc. Porque se sair só montando tudo sem testar nada, quando terminar e tentar ligar o carro, vai explodir. Ou, o mais provável, nem vai ligar, e ninguém vai saber porque. E agora que tá tudo montado, vai precisar desmontar tudo e ir testando. No mínimo vai ser uma grande perda de tempo.&lt;/p&gt;

&lt;p&gt;Antes de mais nada, você precisa de pelo menos um membro da equipe um pouco mais experiente. Nem vou dizer sênior porque não quero ficar tentando definir hoje o que é junior ou sênior, mas alguém que já entregou no mínimo um projeto de sucesso. Porque essa pessoa precisa se responsabilizar pelas decisões técnicas. Decidir qual framework, bibliotecas, módulos, como as coisas vão ficar organizadas, como vai ser feito deployment. E lógico, todo mundo da equipe pode e deve contribuir com idéias, sugestões, mas alguém precisa bater o martelo e todo mundo seguir em frente.&lt;/p&gt;

&lt;p&gt;Senão toda hora se perde tempo com discussão inútil. Um grupo cheio de opiniões que leva horas ou dias pra decidir as coisas é ineficiente e o resultado sempre vai ser uma droga. Aliás, se isso acontece, é um bom sinal que o tal sênior responsável é ruim. Nenhuma equipe respeita alguém que só fala mas não faz. Um bom sênior fala menos e mostra mais, ensina como faz na prática. E o melhor é errar cedo, ter evidências de porque a primeira decisão deu errado, e isso servir de input pra uma segunda decisão melhor. Do que ficar horas discutindo o melhor caminho, pra no final dar errado de qualquer jeito.&lt;/p&gt;

&lt;p&gt;Isso talvez seja a coisa mais fundamental numa equipe. Errar antes e consertar rápido é infinitamente melhor do que horas e horas de discussão tentando chegar em consenso e bikeshedding. Ninguém tem certeza? Foda-se. Cara ou coroa. Faz um teste. Vê se funciona. Se falhar, agora veja porque falhou, conserta, e segue em frente. Essa é a essência do processo de lidar com o desconhecido. Todo mundo pensa que quando algo é desconhecido o certo é tentar planejar infinitamente até ter todos os passos minuciosamente detalhados. Mas tá errado. É desconhecido, qualquer planejamento que tentar fazer, quanto mais detalhado for, mais errado vai estar.&lt;/p&gt;

&lt;p&gt;Acho que esse é o ponto principal da discussão de hoje. Quando você e sua equipe são inexperientes, das duas uma, ou vão andar super lento porque vão tentar planejar em excesso, discutir em excesso, ou vão ser kamikazes retardados que vão começar a fazer e só ir fazendo sem parar pra ver se estão indo na direção certa. Ambos estão errados. Não consigo entender porque o desespero não leva ao mais óbvio, que é errar cedo, consertar rápido e o mais importante: garantir que o que já foi feito e está certo, não quebre à toa, porque isso seria o equivalente a andar pra trás.&lt;/p&gt;

&lt;p&gt;Não tem nenhum problema rascunhar o que precisa ser feito. Faça um checklist de funcionalidades. Faça post-its com cada tarefa. Faça uma planilha. Não importa. Todo mundo da equipe precisa ter uma noção geral do que precisa ser feito. Mas não tente detalhar em excesso. Só coloque detalhes no que estiver muito incerto, no momento em que precisa ser feito e não com meses de antecedência. Portanto, qualquer um que tenha estudado engenharia de software ou gerenciamento de projetos tradicional, e viu coisas como requisitos, casos de uso, ou até mesmo stories, pára. Você só precisa de uma lista, curta, com poucas palavras. Não é pra levar dias escrevendo, é pra levar horas, no máximo. Vou voltar nesse ponto mais pro fim.&lt;/p&gt;

&lt;p&gt;Não importa quantos diagramas, fluxogramas, photoshops ou words vocês produzam, nada disso é o software. Pra um amador, dá impressão que o ideal é ter o máximo de páginas de requerimentos detalhados quanto possível. Tá errado. Essa noção errada vem pelo seguinte: qualquer projeto de engenharia civil, pra fazer uma casa ou um prédio, precisa de meses de planejamento, várias plantas baixas, diagramas de encanamento, diagrama elétrico e tudo mais, pra garantir que o prédio seja construído perfeitamente dentro das especificações.&lt;/p&gt;

&lt;p&gt;Portanto, software deveria ser a mesma coisa. Vamos fazer quinhentas páginas de diagramas UML, que é o equivalente a planta baixa, porque daí, quando for escrever o software propriamente dito, vai sair tudo perfeitamente dentro das especificações. Essa premissa está errada. Todo mundo acha que um programador é um pedreiro. Não querendo desvalorizar o trabalho de pedreiros, mas um programador, na realidade, é o arquiteto. O trabalho de pedreiro, é 100% automático desde sempre. O pedreiro de software é o compilador ou interpretador. A planta baixa É o código fonte do software. Portanto todo programador, quer ele queira ou não, faz trabalho de um arquiteto.&lt;/p&gt;

&lt;p&gt;Lógico, se sua linha de trabalho é só instalar Wordpress pra diversos clientes. Não se está produzindo software novo, só copiando e colando o que já existe e esse trabalho, sim, parece mais uma linha de fábrica. Mas estamos falando de fazer software que não existe, quase do zero. Reusando vários componentes como frameworks ou bibliotecas, sim, mas o software final não seria possível fazer numa linha de fábrica. Cada passo no desenvolvimento desse software requer tomadas de decisões de cada programador envolvido. Não existe receita.&lt;/p&gt;

&lt;p&gt;O famoso exemplo de fazer um botão com cor na tela. Dá pra fazer de dezenas de jeitos diferentes. Posso fazer todo o CSS e HTML do zero. Posso escolher usar um Tailwind da vida. Mas se escolher isso, não pode ser só pra um botão, o certo é usar em todos os botões e elementos gráficos de todas as telas. Eu quero isso? Posso escolher fazer todo o sistema de eventos do zero ou usar React ou Vue. Mas de novo, isso vai afetar todo o sistema. Qual minha equipe tem mais facilidade pra usar? Já tinha começado como um sistema em Angular? Se eu decidir que quero usar Vue no lugar, quanto custa reescrever tudo? Vale a pena?&lt;/p&gt;

&lt;p&gt;Tudo são decisões. Muitas sem grandes consequências, mas algumas podem quebrar seu projeto inteiro se feito de forma irresponsável. Como se toma esse tipo de decisão? Não tem receita pra isso. Na maioria das vezes, seu prazo tá apertado, e o ideal é não escolher decisões que envolvam jogar tudo fora e fazer tudo de novo, então isso já elimina várias escolhas. Mas algumas são incertas, se der certo talvez vocês ganhem produtividade pra frente. Pra isso se fazem provas de conceito.&lt;/p&gt;

&lt;p&gt;Todo mundo ignora o poder das provas de conceito. Prova de conceito ou protótipos são pedaços de código, uma pequena parte da aplicação, que deve ser feito com o objetivo de potencialmente jogar fora depois. Portanto não precisa ser integrado ao código principal. Pode ser feito como uma branch do repositório, o que num GitHub chamamos de pull requests ou num GitLab chamamos de merge requests. Tecnicamente é um desvio no repositório de código. Enquanto todo mundo continua trabalhando no tronco principal, a branch &lt;code&gt;main&lt;/code&gt;, alguém trabalha nesse desvio, num galho, por alguns poucos dias, pra ver na prática se a mudança sugerida realmente trás benefícios ou não.&lt;/p&gt;

&lt;p&gt;Provas de conceito tem que ser um empreendimento curto, um micro-projeto de prazo fixo. Coisa de 2 dias, uma semana no máximo. Mesmo estando incompleto, no final desse período deve dar pra equipe ver na prática como a idéia se materializa em código, os impactos que causa, e se todo mundo está confortável em seguir em frente. Daí pode até jogar fora esse galho e todo mundo voltar pro tronco principal aplicando seja lá qual seja essa mudança técnica. Ou então se prova que a idéia era mais complexa do que se imaginava, e todo mundo descarta essa idéia. Mas isso não foi perda de tempo.&lt;/p&gt;

&lt;p&gt;Você tem 10 grandes dúvidas técnicas que podem custar muito tempo, coisas no nível: devemos usar React ou Vue? Devemos ficar no Postgres ou migrar pra Firebase? Devemos ficar só em APIs REST ou já começar tudo em GraphQL? Vamos dar deploy das coisas em Vercel ou criar Terraform pra AWS? Só que ninguém da equipe nunca teve muita experiência em nenhuma dessas coisas. Cada uma pode potencialmente custar 2 a 4 meses. Sua equipe só tem 3 pessoas. Então fácil, se for fazer tudo, não vai sair por menos que 10 meses pra equipe, e com a falta de experiência, fácil vai virar o ano.&lt;/p&gt;

&lt;p&gt;Em vez disso, pára tudo, cada um da equipe vai fazer provas de conceito de cada coisa, num prazo máximo de 2 semanas. E esse prazo é fixo, depois de 2 semanas vamos ver o que saiu. No final, mais da metade dessas coisas se provam realmente mais complicadas ou o custo claramente não vai compensar. Em vez de ir em frente com 10 coisas incertas que vai custar pelo menos 1 ano pra fazer, vocês concordam agora em desistir de 7 delas, porque a prova de conceito deu evidências que não vai dar certo. Por outro lado, 3 dessas coisas se provaram boas, e vocês vão seguir em frente.&lt;/p&gt;

&lt;p&gt;Ao custo de 2 semanas da equipe, vocês economizaram mais de meio ano de trabalho que ia dar errado, e vão prosseguir com 3 coisas que vão custar um mês da equipe e que todo mundo tem mais certeza que vai dar certo. Esse é o grande truque: não se comprometa com coisas de longo prazo que ninguém tem certeza. Pague um tempo curto pra descobrir se dá pra cobrir essa incerteza. Não é jogar tempo fora, é pagar um pouco pra não jogar muito mais fora. Essa noção deveria ser simples, mas na minha experiência, pouca gente parece entender que mais vale a pena jogar fora 2 semanas do que se comprometer cegamente com um projeto incerto que vai levar um ano.&lt;/p&gt;

&lt;p&gt;Na cabeça de muita gente, como 1 ano tá longe, fica aquela sensação de &quot;vamos indo, lá na frente a gente vê como faz&quot;. Especialmente em tech startups que vivem de dinheiro de investimento, esse tipo de decisão errada acontece muito, porque afinal é muito mais fácil gastar o dinheiro dos outros. E mesmo quem tem que gastar do próprio bolso, fica apreensivo com essa noção de &quot;jogar 2 semanas fora&quot;. Só que não é &quot;jogar fora&quot;, é o mesmo que pagar um seguro.&lt;/p&gt;

&lt;p&gt;Ninguém gosta de pagar mil reais, dois mil reais, todo ano, que ao final do ano, parece que não serviu pra nada. Só que se durante esse ano tiver um acidente, e você atropelar e machucar alguém, agora não é dois mil reais que vai custar, vai ser 100 mil, 200 mil reais. Quem faz economia porca e fica de bravata achando que &quot;não precisa, não vai acontecer&quot;, é o primeiro que se envolve em acidentes e depois fica falido. Em projetos é a mesma coisa. Se for coisa que você já faz várias vezes antes, o risco é baixo, já sabe o que tem que fazer, tem noção de quanto vai custar e mais ou menos sabe o que pode ou não dar errado, então consegue ir em frente. Mas se nunca fez, é melhor fazer um teste antes de decidir.&lt;/p&gt;

&lt;p&gt;E caindo no dia a dia de programação, é esse o mesmo argumento de porque falamos que todo programador deveria fazer, no mínimo, testes unitários de tudo que está trabalhando. Toda nova funcionalidade, deveria ser coberta de testes. Toda correção de bugs, deveria começar com um teste que simula o bug, pra que já comece falhando, e você sabe que corrigiu quando esse teste passa. Não só isso, esse bug em particular não vai acontecer de novo. Todo teste que você começa escrevendo antes de fazer o código, é como uma mini prova de conceito antes de tentar fazer uma coisa que não tem muita certeza. Se não tem certeza, comece escrevendo um teste.&lt;/p&gt;

&lt;p&gt;Muitas pessoas já me pediram pra fazer um video sobre testes, mas eu realmente não tenho vontade, porque testes é um assunto que tem já toneladas de documentação, livros, tutoriais, cursos, blog posts, PDFs gratuitos online, pra cada linguagem diferente, pra cada framework. Testes não é um troço que você aprende em 5 minutos e faz igual pra sempre. É um conjunto de técnicas. O ideal é um junior parear com alguém mais experiente e aprender na prática: quando precisa fazer, e com que nível de detalhes. É muito importante que sua equipe tenha pelo menos uma pessoa mais experiente que entende a importância de ter testes e oriente os demais a fazer.&lt;/p&gt;

&lt;p&gt;Deixa eu fazer o sales pitch de 2 minutos de porque todo programador deve fazer testes, de porque nenhum pull request pode ser mergeado sem testes, e porque uma pessoa que se diz sênior e é contra fazer testes, por definição, não deveria se considerar sênior. Toda regressão é tempo e dinheiro jogado fora. E o que é uma regressão?&lt;/p&gt;

&lt;p&gt;Regressão é exatamente o que o nome diz. Se até este momento, digamos que alguém de QA testou tudo e tinha zero bugs. Alguém me faz um pull request que ficou trabalhando sem integrar por duas semanas. Sem testar. E dá merge na master. Faz deploy, e o que acontece? Surgem um monte de bugs. Não só na funcionalidade que ele tava trabalhando, mas em outros lugares que antes funcionava. Esse amador regrediu o trabalho de todo mundo. Tudo deu passos pra trás.&lt;/p&gt;

&lt;p&gt;Agora o certo é dar revert nesse pull request, reverter mesmo. Só que esse pull request não tem mais como voltar atrás, porque dar deploy, ele alterou o schema do banco de dados e não tem como voltar mais ao schema anterior sem um grande downtime. Um desastre. E, claro, tenha certeza que esse indivíduo fez esse deploy na tarde de sexta-feira. Lógico. Cancela o happy hour porque a noite de sexta vai ser longa, fim de semana adentro, pra consertar essa cagada.&lt;/p&gt;

&lt;p&gt;O fluxo é simples: todo programador, seja fazendo testes-antes ou testes-depois, quando diz que concluiu seu pull request, também adicionou um mínimo de testes, fez rebase com a master, e indicou pra equipe que tá tudo pronto. Alguém da equipe sempre deve avaliar esse pull request, normalmente alguém mais experiente. Pelo menos pra bater o olho e ver que não tem nada que parece muito mau feito ou muito fora do lugar e, principalmente, se tem testes. Se não tiver, já rejeita.&lt;/p&gt;

&lt;p&gt;Se for uma equipe minimamente bem equipada, está usando repositórios privados de Git seja num GitHub ou GitLab. Ambos tem suporte a rodar testes automatizados num serviço separado. Nesse momento ele já puxou o que está nesse pull request, subiu um container de docker, e executou os testes automatizados do projeto. E quando o avaliador for checar o pull request, já vai saber inclusive se os testes passaram ou não. O GitLab tem a própria ferramenta que é o GitLab CI, de Continuous Integration. Tem diversos serviços externos que integram com GitHub como CircleCI, Travis CI, Semaphore e outros. É super simples configurar no seu projeto e não vejo nenhum motivo pra não usar.&lt;/p&gt;

&lt;p&gt;Se o pull request parece que está ok, e o CI diz que os testes passaram, só agora é permitido fazer merge desse código novo na master. No mínimo, sabemos que as funcionalidades que funcionavam antes, cobertas com testes, continuam funcionando depois, mesmo com a inclusão desse código novo. É um bom indicativo pra toda a equipe que as coisas estão andando pra frente, e não ficando com débito técnico acumulado pra trás. Eu vi um tweet alguns dias atrás, de alguém que tava relatando como um idiota da equipe resolveu atualizar a versão do Java, acho que do 16 pro 18 e fez deploy pra produção. Óbvio que deu pau. Óbvio que o pau apareceu na sexta-feira.&lt;/p&gt;

&lt;p&gt;Tem idiota que acha que porque usa uma linguagem compilada, com tipagem estática como Java, não tem problema não ter testes, porque se tudo compila, obviamente é porque funciona. Não tem nada que é mais certificado de júnior, do que pensar assim. Ahhnnnn como assim? Voltando ao exemplo de software livre. Vamos ver o projeto do navegador Firefox. Sabe Firefox? Sim, aquele Firefox. Eu baixei o código aqui na minha máquina. Firefox é feito na maior parte em C++, um pouco de Rust, um pouco de Javascript. É um navegador bem estável e quem usa acho que raramente tem algum problema dele renderizar errado, crashear ou coisas estranhas assim.&lt;/p&gt;

&lt;p&gt;Se você é desenvolvedor de front-end, certamente já usou o Developer Tools certo? Pra debugar o que está programando. E esse Developer Tools tem uma suite de testes, pra garantir que toda nova versão, antes de lançar pra você baixar na sua máquina, passa todos os testes e não introduz nenhuma regressão. Ou seja, tudo que funcionava numa versão continua funcionando na mais nova. No repositório de código fonte, só rodar &lt;code&gt;mach test devtools/*&lt;/code&gt; e olha só, abre a versão que acabei de compilar direto do código fonte e roda uma suite de testes de integração, simulando cada funcionalidade do Developer Tools. É assim que a Mozilla garante que código novo não introduz regressões óbvias.&lt;/p&gt;

&lt;p&gt;Se der uma fuçada no código, olha só, cada diretório tem um sub-diretório de testes. E muitos deles tem testes específicos pra bugs que foram reportados. Assim eles garantem que bugs corrigidos também não vão aparecer do nada de novo. Bugs que foram corrigidos e aparecem de novo porque não tinha um teste pra cobrir, se chama &quot;jogar dinheiro fora&quot;. Porque tem que corrigir mais de uma vez. Ao longo do tempo, você está perdendo tempo, dinheiro, eficiência, e mercado pra outro projeto que vai fazer melhor que você com menos recursos.&lt;/p&gt;

&lt;p&gt;Tem gente que acha que porque é programador front-end, não precisa se preocupar com testes. Acho que todo mundo gosta do projeto React do Facebook, quer dizer &quot;meta&quot;, certo? Eu baixei aqui o código fonte da biblioteca React. E olha só, se rodar &lt;code&gt;yarn test&lt;/code&gt;, o React tem uma suite de testes feita em Jest. Pelo mesmo objetivo. Mais de 7 mil testes unitários garantem que quando você atualiza o React, não vai quebrar tudo.&lt;/p&gt;

&lt;p&gt;Sabe aquela biblioteca de estilos, o TailwindCSS? Um amador poderia imaginar que é só uma coleção de CSS, obviamente não precisa de testes, certo? Errado. Eu baixei o código fonte do Tailwind e adivinha, se rodar &lt;code&gt;yarn test&lt;/code&gt;, mais testes unitários em Jest. Quase 900 testes unitários, fora checagens com eslint que por acaso deu alguns erros. Eu baixei do branch principal, ainda estão mexendo e na próxima versão esses bugs serão corrigidos. Mas está claro o que funciona e o que não funciona pra quem está contribuindo.&lt;/p&gt;

&lt;p&gt;Bah. Esse povo de front tá muito fresco. Ficar fazendo testes é coisa de hipster. Certamente povo de Python, mais calejado, mais raíz, não se importa com essas besteiras. Por isso eu baixei aqui o código fonte do framework Django. E olha só, tem um diretório chamado &lt;code&gt;tests&lt;/code&gt; e dentro eu posso rodar esse script chamado &lt;code&gt;runtests.py&lt;/code&gt;. E lá vai ele rodar centenas de testes pra garantir também que versões novas do Django não tá quebrando nada que já funcionava antes. Mais de 16 mil testes. Passando bonitinho.&lt;/p&gt;

&lt;p&gt;Foda-se povo de Javascript e Python. Vamos mudar pra PHP. PHP sempre foi conhecido por ser Quick and Dirty, rápido e sujo. Código raíz. Programador Cowboy que programa orientado a gambiarra. POG-zão na veia. Eu baixei aqui o código fonte do framework Laravel que é um dos mais usados hoje. Só que não. Eu posso rodar &lt;code&gt;composer exec phpunit&lt;/code&gt; e olha só, quase 8 mil testes, de novo, pra garantir que a versão mais nova não introduz nenhuma regressão. Por acaso falhou, povo ainda tá mexendo, próxima versão estável certamente já vai ter corrigido esse bug. Mas nós sabemos que tem o bug, porque tem suite de testes.&lt;/p&gt;

&lt;p&gt;Aliás, sabe o editor de textos mais usados por programadores hoje? O tal do Visual Studio Code? Provavelmente até você assistindo que não gosta de testes aí usa pra fazer seus códigos porcaria. Eu baixei o código fonte também e tem testes automatizados rodando &lt;code&gt;yarn smoketest&lt;/code&gt;, que segundo a documentação deles, é rodado antes de gerar o binário final que você baixa pra instalar. Igual o Firefox, tem testes de integração onde abre o aplicativo e vai testando cada funcionalidade que a gente aceita que simplesmente funciona em toda nova versão. E tudo só funciona, porque tem testes pra garantir isso.&lt;/p&gt;

&lt;p&gt;E voltando ao Firefox. Parte dele é feito em Rust. E pra você, seu infeliz, que ainda acha que só porque o código compila, tá tudo bem? Não era o Rust que tem o tal compilador mais inteligente de todos os tempos? Que muita gente acha que é mágica e garante não só binário super performático como se tivesse sido feito em C e sem bugs de gerenciamento de memória e problemas de segurança? Muita gente só falta dizer que o compilador escreve o código pra você, de tão bom que ele é. Certamente, se o código em Rust compila, não precisa de testes, certo?&lt;/p&gt;

&lt;p&gt;Bom, eu baixei o código fonte da biblioteca Tokio, que é o framework pra desenvolvimento assíncrono em aplicativos feitos em Rust. Todo projeto Rust acompanha um arquivo &lt;code&gt;Cargo.toml&lt;/code&gt;, assim como todo projeto Javascript acompanha um &lt;code&gt;package.json&lt;/code&gt;. E assim como existe npm ou yarn, em Rust temos o comando &lt;code&gt;cargo&lt;/code&gt;. &lt;code&gt;cargo install&lt;/code&gt; é equivalente a um &lt;code&gt;npm install&lt;/code&gt;. E adivinha, o próprio cargo, ferramenta padrão que já vem com o Rust, tem essa opção &lt;code&gt;test&lt;/code&gt;. E o que isso faz? Vamos ver. Só executar &lt;code&gt;cargo test&lt;/code&gt;. Olha só ele rodando a suite de testes da biblioteca Tokio.&lt;/p&gt;

&lt;p&gt;Ou seja, nem mesmo um projeto em Rust, que é armado com o compilador que é considerado a obra prima dos compiladores, evita os autores do Tokio de não fazer testes. Aliás, pode ver, ele sai rodando centenas de testes, não é um ou dois. Ahhh mas só de compilar já devia ser suficiente ... não, sua múmia paralítica, os criadores do Rust armaram o Cargo pra rodar testes. Você acha que você, logo você, entende mais do que os criadores do Rust?&lt;/p&gt;

&lt;p&gt;E você que é de Java e também detesta testes, que tem essa noção atrasada de porque o Java compila, tem tipos estáticos, então se compila tá tudo certo. Você que acha que só linguagens dinâmicas não compiladas como Ruby ou Javascript que precisam de testes. Sabe onde nasceu o conceito de TDD? Foi com Kent Beck, que fez o primeiro jUnit, em Java. Desde os anos 90 a gente sabe que só porque alguma coisa compila, não quer dizer que funciona. Só porque compila não significa que bugs não foram introduzidos. E por ficarem cansados de toda hora ter que correr atrás do próprio rabo, de ficar corrigindo coisa que já funcionava, que os agilistas originais formalizaram o conceito de testes.&lt;/p&gt;

&lt;p&gt;Aliás, falei de Java e não mostrei Java. Só de exemplo, baixei o código fonte de um dos frameworks web mais usados que é o Spring Boot. Rodando o bom e velho &lt;code&gt;gradlew build&lt;/code&gt; olha só. Baixa todas as dependências, junto com a internet inteira duas vezes, e roda todos os testes automatizados na sequência. O build só é considerado ok se compila e os testes rodam sem erro até o fim. E são milhares de testes. Ué, mas não era só compilar que já tava garantido que funciona? Obviamente não.&lt;/p&gt;

&lt;p&gt;Muito antes de existir o conceito de TDD a gente já fazia testes automatizados de um jeito ou de outro. Toda empresa de software madura faz testes. É por isso que seus programas e seu sistema operacional não quebram o tempo todo, toda hora. E mesmo com tudo isso, ainda assim bugs acontecem. Significa que se não tivessem todos esses testes, teríamos ordens de grandeza mais bugs em tudo. Novas versões iam demorar ordens de grandeza mais pra sair. Ia precisar de ordens de grandeza mais pessoas testando. Mas não. Hoje em dia quase todo dia algum programa que você tem instalado ganha uma versão nova, mas você raramente é incomodado. Bugs acontecem, mas são raros. E a razão disso é que todo software maduro, proprietário ou de código aberto, tem testes.&lt;/p&gt;

&lt;p&gt;Veja meu blogzinho feito em Ruby on Rails. Ele é ridiculamente simples. Eu mesmo que fiz em 2012. Não lembro se foi em Rails versão 3 ou 4. De lá pra cá eu atualizei até o Rails 6 rodando em Ruby 3. Toda vez que sai versão nova da linguagem ou do framework, eu atualizo, pra ganhar as correções de segurança principalmente. Eu venho atualizando faz 10 anos. E toda vez atualizo as bibliotecas também. Mas diferente do tweet da pessoa que falou que o cara atualizou o Java e subiu pra produção. Antes eu rodo meus testes em Rspec. Se não passa, obviamente não subo. Faço pequenos ajustes quando precisa, se a API de alguma biblioteca mudou. Mas meus testes pegam a maior parte dos problemas. Uma vez por ano, gasto 30 minutos, atualizo as coisas e subo.&lt;/p&gt;

&lt;p&gt;Ter testes não é burocracia, não é perda de tempo. É um seguro. Não fazer testes é a mesma coisa que dirigir sem seguro. Na maior parte do tempo, não parece ter muito valor mesmo. Sempre dá pra fazer tudo rápido, sujo e na gambiarra. Você se sente o herói de entregar as coisas rápido. Mas é inevitável, sua produtividade, e da sua equipe, vão gradativamente caindo. Porque mais e mais se perde tempo corrigindo bugs do que fazendo coisa nova. Em breve, a maior parte do seu tempo é só corrigindo bugs. E o que o amador fala? Ahhh, vamos mudar pro framework mais novo. Vamos mudar pra linguagem mais da hora. Quantos de vocês conseguiram manter e atualizar o mesmo sistema por uma década?&lt;/p&gt;

&lt;p&gt;Não existe nenhuma linguagem inventada na história que só pelo fato de compilar garante que não precisa ter testes. Todo mundo que entendia isso já fazia testes de alguma forma. A gente fazia alguns scripts separados do código pra rodar partes que eram mais sensíveis se quebrasse, ou que sabíamos que sempre alguém fazia merda e quebrava. Era uma das coisas que separavam os amadores dos espertos. O que aconteceu foi que as comunidade ágeis deram nomes aos bois e criaram convenções e semânticas pra todo mundo falar a mesma língua. Mas não existiu uma data especial que divide antes, com zero testes, e depois, com testes. Foi gradual, onde décadas atirando no próprio pé naturalmente levaram os programadores mais experientes a chegar na mesma conclusão.&lt;/p&gt;

&lt;p&gt;Enfim. O problema é que a maioria de vocês assistindo detesta fazer testes. A desculpa é perda de tempo. Mas a verdade é porque vocês não conseguem fazer. Vocês acham que ninguém percebe. Todo mundo que se sente incapaz de fazer alguma coisa, tenta denegrir esse alguma coisa. E é óbvio, não sabe fazer porque nunca faz. Assim como tudo em programação, fazer testes depende de treino e prática. Não existe nenhum livro ou curso que dá pra ler e automaticamente sair fazendo testes perfeitos.&lt;/p&gt;

&lt;p&gt;Você que é front, assistiu um curso e já saiu criando interfaces do nível de complexidade de um Canvas instantaneamente? Ou precisou de semanas e semanas apanhando de CSS obscuro? Ou você que é back, só leu um tutorial e já saiu criando APIs perfeitas em GraphQL integrado com backend de Firebase? Em 5 minutos já saiu codando como se não houvesse amanhã? Duvido.&lt;/p&gt;

&lt;p&gt;Não adianta ficar só procurando coisa pra ler. Precisa praticar. Assim como tudo que já fez, todos os seus primeiros testes vão ser uma bosta. Mas é assim que começa. Eu citei vários projetos de código aberto. Dá uma olhada nos testes deles pra se inspirar. Tem centenas de outros projetos mais parecidos com o que estiver trabalhando agora. Baixa e lê os testes deles. Copia. Altera. Testa. Joga fora. Começa de novo. Repete. É assim mesmo.&lt;/p&gt;

&lt;p&gt;Outra desculpa é falar que o projeto já tá faz meses ou até anos sem nenhum teste. É o cúmulo da preguiça ouvir isso. Ótimo. Faça você o primeiro teste. Pelo menos essa uma funcionalidade que acabou de entregar, está coberto e vai ser mais difícil de aparecer bug aleatório pela falta de atenção de outro programador depois. E se alguém pisar em cima do seu código, só de rodar esse um teste, dá pra saber que quebrou, quem foi, em qual commit, e corrigir mais fácil. De 1 teste em 1 teste, a cobertura naturalmente aumenta.&lt;/p&gt;

&lt;p&gt;Alguém reportou um bug? Abriu um ticket? Faça o teste que simula o bug reportado. Agora corrige. Pronto, esse é 1 bug que dificilmente vai aparecer de novo. E olha só, do nada, você começou a andar pra frente em vez de ficar só olhando pra trás. E total de cobertura? Precisa ser 100%? Isso é outra pergunta idiota que sempre aparece numa discussão de quem é desleixado. Foda-se. Você não fez nem 1% ainda, pra que quer discutir 100%? Não interessa. Faz 50%, faz 500%. Só, faz, a, porra, dos, testes.&lt;/p&gt;

&lt;p&gt;O que acontece é o seguinte. Sua equipe tem 3 pessoas. Cada um consegue pegar 2 ou 3 tarefas do backlog por semana. 6 a 9 tarefas entregas toda semana. Se o sistema começa a ser usado por usuários de verdade, rapidamente começam a aparecer bugs e problemas. Surge 1 ticket, 2 tickets. De repente a equipe só consegue entregar 3 a 6 tarefas por semana, porque precisa resolver bugs urgentes. Mas continua entregando coisas. Só que essas coisas novas, causam mais bugs. Na próxima semana a produtividade caiu pra 2 a 4 tarefas. E o número de bugs aumenta em vez de diminuir. Em pouco tempo, a equipe male male entrega 1 ou 2 tarefas novas, o resto do tempo é só resolvendo bugs.&lt;/p&gt;

&lt;p&gt;E qual a decisão do gerente do projeto? &quot;Ah, é normal, precisa contratar mais programadores.&quot; Ele convence a diretoria. Dobra a equipe. E voilá, a equipe volta e entregar 6 a 9 tarefas novas por semana. E todo mundo continua sem se importar com testes ou qualidade em geral, é crunch, só entregar e entregar. O volume de código só aumenta, ninguém joga nada fora, vai só acumulando débito técnico. Mais código novo significa mais bugs. No final acontece a mesma coisa, nas semanas seguintes a produtividade cai pra 3 a 6 tarefas, depois pra 2 a 4 tarefas, e de novo, a nova equipe, com o dobro de integrantes, tá male male entregando 1 ou 2 coisas novas por semana, o resto é só apagando incêndio.&lt;/p&gt;

&lt;p&gt;Digamos que todo mundo é ruim em testes. Se lá no começo tivesse decidido entregar tudo com testes, em vez de 6 a 9 tarefas, a equipe estaria entregando só de 4 a 6 coisas, talvez menos. Parece bem ruim. A diferença é que vai ser 4 a 6 TODA semana. Surgiu um bug novo? O programador que pega pra corrigir adiciona testes, e segue todo o processo que descrevi antes. Esse bug em particular não aparece de novo. Entregou coisa nova? A suite de testes garante que o que tinha antes não quebrou por conta disso. E assim, toda semana, de forma sustentável, a equipe entrega 4 a 6 tarefas. Não é o máximo de 9, mas não cai pra 1 ou 2.&lt;/p&gt;

&lt;p&gt;Daí a empresa decide contratar mais gente e dobra a equipe. Mas a equipe é diligente e treina os novos integrantes sobre a importância de qualidade. A produtividade quase dobra, de 4 a 6 pra 8 a 10 tarefas. A produtividade só dobra se todo mundo segue o protocolo, ninguém pisa no calo de ninguém, e tudo que já foi feito e testado continua funcionando no futuro. Todo upgrade de bibliotecas e correções de segurança tem garantia que o sistema continua funcionando, porque a suite de testes tá passando. Daí nenhum deploy de upgrade feito na sexta feira causa desastre que vai precisar varar fim de semana corrigindo. Olha só que mágico.&lt;/p&gt;

&lt;p&gt;Muita gente vai inventar um monte de desculpas pra não fazer testes, e eu garanto que pra cada desculpa nós já temos soluções. Nenhum programador precisa rodar a suite de testes o tempo todo se essa suite demora muito pra executar. Basta rodar alguns poucos ao redor do trabalho que está fazendo naquele momento. Daí sobe um pull request que vai ser pego por um GitLab CI e esse servidor de CI é quem vai rodar a suite inteira de testes. Não só isso, se a suite passar, esse CI pode fazer CD, que é continuous deployment, e automaticamente subir em ambiente de staging e notificar a equipe de QA.&lt;/p&gt;

&lt;p&gt;Obviamente eu não posso mostrar projetos confidenciais dos meus clientes, mas posso mostrar esse projeto interno que nós usamos. Olha um pull request. Se olharmos o que mudou podemos ver que além do código, temos ajustes em specs também, que são testes. Quando o desenvolvedor subiu o código, o CI pegou e iniciou o job pra rodar os testes. Nesse caso levou pouco mais de 5 minutos. Passou com sucesso e, lógico, só porque todos os testes passaram não significa que não tem nenhum bug, só que não tem nenhum bug muito óbvio. Mais do que isso, no nosso caso tem configurado também checagens de segurança. No ecossistema Ruby temos coisas como Brakeman, que avalia o código por buracos de segurança conhecidos como injections.&lt;/p&gt;

&lt;p&gt;Essa aplicação sobe no Heroku, que tem uma funcionalidade chamada review apps. Podemos ter um deployment separado de testes pra cada pull request que ainda está aberta. Justamente pra alguém conseguir testar de verdade antes de aprovar. Um review app é um ambiente de staging, só que isolado pra cada pull request. Dá pra ter dezenas de review apps em paralelo. Assim não precisa ficar acumulando e integrando tudo no mesmo ambiente de staging pra testar, o que seria uma zona. Testa tudo isolado. Vai aprovando no ritmo do responsável pelos testes. E no final, quando estiver tudo testado, aprovado e mergeado no branch principal, daí faz um deployment pra produção aqui do lado.&lt;/p&gt;

&lt;p&gt;Neste exemplo, no dia que fui tirar foto de tela, não tinha nenhum pull request aberto. Mas eles apareceriam listados aqui. Cada um com uma URL separada de testes. Esse é outro motivo de porque eu sempre recomendo Heroku. Ele não é o mais barato, mas esse tipo de recurso facilita o fluxo de trabalho em ordens de grandeza. Só as horas de desenvolvimento que se economiza de cada desenvolvedor já se paga.&lt;/p&gt;

&lt;p&gt;Enfim, uma vez cada pull request de cada desenvolvedor sendo aprovado, tudo vai sendo integrado no branch principal e podemos fazer um deploy de tudo integrado pro ambiente de staging. Agora o pessoal de QA pode fazer o trabalho deles direito, que é testar as coisas novas, e não ficar retestando tudo de novo que já funcionava antes. Todo report de bugs volta pro programador, que faz a correção e ajusta os testes pra cobrir os casos que não tinha pensado.&lt;/p&gt;

&lt;p&gt;Com o pull request foi ajustado, os testes passam, QA já viu em staging, o sênior da equipe pode dar a última olhada e fazer o merge na branch principal. E aí alguém de devops ou alguém responsável pela produção pode pegar o que está na branch principal e fechar uma versão pra produção, com a segurança de saber que se está na branch principal, então está funcionando corretamente.&lt;/p&gt;

&lt;p&gt;E se estiver usando Heroku, ainda tem uma facilidade extra. Digamos que todo mundo fez tudo direitinho, mas mesmo assim surgiu um bug que só aparecem em produção. Basta ir nesta aba de atividades e clicar em roll back pra voltar pra versão anterior. Heroku é caro, mas é nessas horas que ele se paga. Medo de deploy na sexta-feira só existe em equipes disfuncionais que não fazem esse básico que acabei de falar.&lt;/p&gt;

&lt;p&gt;Falando em disfuncional, o maior desafio em toda equipe de projetos, é coordenar a comunicação. Ao ter um repositório de códigos como Git, um processo de testes de integração contínua, pareamento entre sêniors e juniors pra compartilhar conhecimento, e um protocolo como de pull requests, estabelecemos um conjunto simples e objetivo de entendimento nessa equipe. Não seguir esse protocolo é um desrespeito à equipe. Você não programa pra você mesmo, você contribui com sua equipe. O importante não é andar rápido, o importante é não estragar o que já funcionava.&lt;/p&gt;

&lt;p&gt;Da mesma forma, ninguém deve sair adicionando coisas no projeto que ninguém está sabendo. Por isso que no mundo ágil se inventou os tais rituais como de planejamento e revisão. Rituais não devem ser obrigações, mas pontos de coordenação. Se os rituais estão sendo meramente pras pessoas aparecerem mas não participar, então não servem pra nada e devem ser eliminados e repensados. Esse é um ponto não-técnico que não quero tentar detalhar demais, mas preciso falar de um ponto específico: estimativas.&lt;/p&gt;

&lt;p&gt;Se tem uma coisa que programador odeia fazer mais do que testes é estimar tarefas. Já disse isso em outros videos, mas existem duas palavras que todo mundo não-técnico confunde: estimar e prever. O programador estima, mas os gerentes ou diretores acham que receberam previsões. Estimativas são ordens de grandeza, não está certo nem errado. É uma ordem de grandeza. Previsões sim, é pra serem precisas. Quando um programador diz que pode levar uns 2 ou 4 dias, é isso que pode levar, 2 a 4 dias. Talvez leve 5. E aí o gerente que achava que tinha uma previsão na mão, fica puto, porque tava esperando 2 dias, recebeu em 5 e quer sair chutando tudo. E é isso que eu chamo de um idiota.&lt;/p&gt;

&lt;p&gt;Eu acho que todo gerente de projetos deveria gastar um tempo movimentando o próprio dinheiro no mercado financeiro. Comprar e vender ações, ou opções, ou criptomoedas, não importa. Porque todo mundo gostaria de saber pra onde vai o mercado, se vai subir ou se vai cair. É impossível acertar 100% do tempo. Se alguém soubesse prever isso, quebraria o mercado financeiro. O máximo que podemos fazer é dar chutes educados. Estimativas. A gente tenta acertar, comprar na baixa, vender na alta. Ou no mínimo planejar pra alta e se proteger da baixa. Criamos contingências, e sabemos que é impossível ganhar sempre. O máximo que podemos fazer é tentar minimizar perdas. E as perdas nunca vão ser zero.&lt;/p&gt;

&lt;p&gt;Gerenciamento de projetos é a mesma coisa. Um gerente inteligente não se interessa em tentar prever cada tarefa individualmente. Ele observa o movimento geral da equipe e do produto. Tá em tendência de alta de produtividade? Baixa de produtividade? Tem como influenciar a baixa? Qual é o obstáculo? Algum diretor deu carteirada e quebrou o planejamento? Alguém ficou doente? Um bom gerente está numa posição que um analista financeiro não tem: de influenciar as tendências.&lt;/p&gt;

&lt;p&gt;Lógico, quanto mais um programador ganha experiência e tem mais segurança no que realmente sabe e o que não sabe, mais e mais suas estimativa se aproximam de previsões. Ele já tem uma boa noção de quanto tempo leva a maioria das coisas. Quanto mais inexperiente, mais longe vai ser essa estimativa. Faz parte do crescimento profissional de cada um começar a quantificar suas próprias capacidades. Ninguém tem obrigação de &quot;acertar&quot; estimativas, mas claro, quanto mais precisas forem, mais todo mundo consegue confiar mais e mais em você. É uma das formas de subir na carreira. Entregar o que promete.&lt;/p&gt;

&lt;p&gt;Tarefas, stories ou seja lá como quer chamar, são meras formalidades. Quanto mais curtas, objetivas, direto ao ponto forem, melhor. O objetivo não é um concurso de literatura onde todo mundo tem que fazer uma dissertação. A quantidade de palavras deve ser o suficiente pra que toda a equipe envolvida saiba no que está sendo trabalhado. Só isso. Tem quinhentos blog posts, livros e cursos tentando ensinar &quot;a melhor forma&quot; de escrever requerimentos. E a maioria é bullshit. Quer dizer, até faz sentido, pode ser usado como inspiração, mas não existe nenhuma receita que dá pra só seguir. Assim como testes e estimativas, escrever requerimentos também depende de prática. Estude o que se sugere, mas não siga ao pé da letra.&lt;/p&gt;

&lt;p&gt;User Stories tem aquele formato idiota de &quot;eu, como usuário, quero conseguir adicionar um produto no carrinho e ter o total do pedido, para poder pagar minha compra&quot; ou algo assim. Pode ser, é bonitinho, tem o objetivo de forçar as pessoas a pensar em pra quem é a funcionalidade sendo feita, qual o objetivo e facilitar pensar na melhor forma de fazer. Tem coisa que precisa de mais detalhe que isso. Um protótipo de tela no Canvas. Um rascunho de conta de imposto. Tem coisa que nem precisa de tanto detalhe assim. Bugs costumam ser mais diretos &quot;quando o usuário abre no firefox de mac, o box de items fica em cima do box de cartão e não dá pra digitar o código de segurança do fucking pedido e a fucking compra não finaliza&quot;. Tá super claro.&lt;/p&gt;

&lt;p&gt;Eu tive um cliente que o imbecil do CTO mandava todo programador descrever em detalhes como ia programar, antes de escrever uma linha de códigos. Tinha que listar que classes novas ia criar, com que métodos, com quais nomes. Era a coisa mais idiota que eu já vi, só perde pra época que povo achava que precisava fazer diagramas de classe UML precisos meses antes de escrever uma linha de código. É idiótico. É da época que falei que se achava que programador era só pedreiro e havia uma forma mágica de descrever tudo, antes de escrever o código. É uma noção boçal.&lt;/p&gt;

&lt;p&gt;O resumo é simples: a descrição deve ser a mais concisa, com o menor número de palavras possível. A equipe deve conseguir se reunir e em pouco tempo entender o que precisa ser feito, quem vai fazer, quem precisa de ajuda. O gerente, ou Product Owner, e o equivalente tech lead ou programador mais sênior, deveriam estar alinhados. Daí quando seja lá qual programador entregar o pull request praquela funcionalidade, o tech lead consegue avaliar rápido e mandar pro QA. O PO e o QA precisam estar alinhados, pra ver se o que foi entregue resolve o que se queria resolver.&lt;/p&gt;

&lt;p&gt;Burocracia e formalidades em excesso aparecem em empresas onde esses personagens tem preguiça de conversar ou pior: não sabem o que estão fazendo. Porque se soubessem, poderiam ir direto ao assunto. Toda burocracia em excesso é defendida por aqueles que não sabem o que estão fazendo, porque aí podem usar a desculpa de &quot;mas, mas, eu segui os procedimentos, a culpa não é minha&quot;. Toda burocracia é uma muleta pra cabideiro que não deveria estar ocupando as posições que estão ocupando. É assim simples.&lt;/p&gt;

&lt;p&gt;Voltando pra estimativas. Não existe nenhuma fórmula nem procedimento que, dado uma descrição em texto de um caso de uso, ou user story ou seja lá o que for, chega num número de horas ou dias exatos pra resolver essa tarefa. Cocomo e outras bullshitagens são lixo. Não tem como. Só a experiência dos programadores envolvidos é capaz de dar uma ordem de grandeza, cuja qualidade vai ser proporcional à experiência da equipe. Se a equipe for super experiente, mesmo assim vai ser uma ordem de grandeza.&lt;/p&gt;

&lt;p&gt;Não tem nada mais retardado do que gente que pega esses números de estimativas, número de horas realizadas, vai pondo num gráfico e fica tentando derivar coisas desses números, fazer integral, aplicar fórmulas. Só porque se tem números, não signfica que existe cálculos que dê pra fazer. Eu posso sair jogando dados, colocar todos os resultados num gráficos e gerar um monte de conclusões. É assim que nasce pseudo-ciência, numerologia, astrologia e todas essas logias idiotas. Estimativa é um chute. Estimativas nunca estão erradas, porque não são previsões. Fazer contas com esses números não faz nenhum sentido.&lt;/p&gt;

&lt;p&gt;Dá pra saber que vai ser perto de 6 meses mas menos de 1 ano. Agora, se vai ser 5 meses, 2 semanas, 4 dias e 5 horas, exatamente. Isso é impossível. Gastar horas tentando estimar tudo é um exercício de futilidade. Não perca tempo tentando prever com exatidão. Aprenda a lidar com ordens de grandeza. Simplesmente escreva o que se sabe que realmente precisa fazer. E nem tente detalhar 6 meses de trabalho de uma só vez. É inútil. Descreva o curto prazo em detalhes, 1 mês, no máximo 2 meses de trabalho. Tenha a direçao do longo prazo em mente, mas ajuste enquanto vai andando. Deixe a equipe se dividir em quem faz o que. E siga o que falei até agora sobre pull requests e merge.&lt;/p&gt;

&lt;p&gt;Mesmo sem ter estimativa nenhuma, você como gerente ou como qualquer membro da equipe vai ver que numa semana a equipe entregou 10 tarefas, stories ou seja lá como quer chamar. Na semana seguinte, entregou 9. Na outra semana conseguiu entregar 11. Na semana seguinte entregou 7. O que o gerente ou facilitador dessa equipe começa a notar? Uma tendência. Parece que, seja lá como se descreve tarefas nessa equipe, eles conseguem entregar uma média de 8 a 9 unidades dessas &quot;coisas&quot;. E se for esperto, vai notar que quando entrega menos coincide quando sobe versão nova pra produção, daí o suporte recebe reports de bugs, que entram no backlog da equipe e isso diminui a quantidade de &quot;coisas&quot; entregues.&lt;/p&gt;

&lt;p&gt;O gerente fala com o tech lead, que revê o processo, e vê que ele tava deixando passar pull requests com testes não tão bons. Ele pareia com o programador júnior que tava fazendo errado. Ensina na prática onde tava errando. E agora ele passa a produzir melhor. E é assim que uma equipe evolui, um passo de cada vez. Não é via metodologia, nem aumentando formalidades e burocracias, é via bom senso. Resolvendo problemas na raíz em vez de ficar inventando band-aids, ou pior, mais burocracias inúteis. E o principal: reorganizado a lista de coisas a fazer pra garantir que as coisas mais importantes estão sendo feitas primeiro e tudo que é opcional ou de pouco valor está no fim da lista. Assim, caso não dê tempo de fazer tudo, pelo menos você sabe que o mais importante foi feito.&lt;/p&gt;

&lt;p&gt;Cuidado, não estou dizendo que não se deve perder tempo descrevendo tarefas ou que não se deve fazer estimativas. Estou falando que a formalidade não é tão importante, tem que começar a escrever e aceitar que vai estar mau escrito. Aceitar o feedback de que o programador fez errado porque do jeito que tava escrito ficou ambíguo. E aí vai ajustando os detalhes pra próxima vez, até o nível que faz sentido. &quot;Ah, se tivesse descrito essa conta com um exemplo, ele teria entendido&quot;, ou algo assim. Aceitar que uma estimativa nunca vai estar certa ou errada. É só uma referência. Não tente fazer projeções baseado em estimativas, porque isso só vai multiplicar um erro com outro erro.&lt;/p&gt;

&lt;p&gt;Em vez disso veja em ordens de grandeza como falei. Seja lá como organiza as tarefas da equipe, veja, no geral, que tipos de funcionalidades ou correções de bugs a equipe entrega num certo período de tempo, como toda semana. Daí nem precisa perguntar pra equipe se o que tem no backlog cabe ou não na semana. Você já sabe essa resposta pela velocidade que já conhece. Quando sabe que não vai caber, seja honesto e faça seu trabalho: corte ou simplifique. Não tem mágica. Se não cabe, não cabe. Se tentar fazer caber, obviamente a qualidade vai cair. É onde vai vir o &quot;se eu pular fazer testes, talvez caiba&quot;. E quando se cede uma vez, isso vai acumular. Na semana seguinte já começa a vir a conta. E uma decisão feita no desespero numa semana, vai prejudicar todo o resto do andamento daqui pra frente.&lt;/p&gt;

&lt;p&gt;Não existe almoço de graça. Toda vez que vai se deixando passar uma pequena coisa errada, isso vai acumulando. Em breve, vai estar tudo errado. Ser sustentável é exercitar disciplina. E não tem coisa mais maçante, mais entediante, do que ser disciplinado. Quando tudo é uma bagunça, é difícil elencar os problemas. Mas numa equipe disciplinada, fica fácil ver quem tá puxando a equipe pra trás. Sem ter a muleta da burocracia, fica fácil ver quem é a pessoa inconstante, aquele que promete demais, entrega de menos, e quando entrega outra pessoa precisa consertar. A equipe inteira sabe quem é essa pessoa. Eles devem ensinar, ajudar, mas quando a pessoa é resistente a trabalhar em equipe, ela deve ser removida. É assim simples.&lt;/p&gt;

&lt;p&gt;A decisão errada é manter essa pessoa e contratar outra pra aumentar produtividade. Muitas equipes que eu já vi teriam a produtividade aumentada só de cortar os maus elementos, porque agora eles não precisam ficar consertando o que o mau elemento tá entregando de mau jeito. Automaticamente a produtividade aumenta, o stress diminui, todo mundo trabalha melhor. Uma coisa que aconteceu durante esse período que já passou de vacas gordas, com montes de empresas contratando gente ruim no atacado, é que em vez de aumentar a produtividade, eles prejudicaram a qualidade do produto e das equipes. Imagina um monte de gente ruim, fazendo o contrário de tudo que falei até agora. Só acumulou um monte de código porcaria que funciona em produção na força bruta, com alguns poucos hiper estressados, varando noite, consertando a porcaria que recebe.&lt;/p&gt;

&lt;p&gt;Se a equipe não se importa com o que está entregando, nada do que eu falei até agora importa. E todo novo integrante, ao ver que ninguém tá realmente muito interessado, vai aprender do jeito errado: que o melhor é ligar o foda-se e fazer igual todo mundo. E é assim que um projeto entra em colapso, porque ninguém está ligando. Em empresas com dinheiro dos outros demais, é o que mais acontece. Mas se sua empresa realmente depende do código sendo feito, sinto dizer, rapidamente todo mundo vai perder o controle. E daí pra frente é só cavar o buraco mais fundo.&lt;/p&gt;

&lt;p&gt;De novo. Todo projeto é uma incerteza, especialmente nos primeiros dias. Especialmente se for uma equipe nova, recém constituída. Precisam aprender a trabalhar juntos ao mesmo tempo que precisam entender o que diabos vão construir. É uma montanha de incertezas. Por isso o ideal é garantir que o básico do básico está garantido: que pelo menos o que está sendo entregue vai continuar funcionando ao longo do tempo. Exercitem o processo de testes, pull requests, integração contínua, deployment em staging. Daí comecem a ver até que nível de detalhes realmente as tarefas precisam ser definidas. E vai ajustando.&lt;/p&gt;

&lt;p&gt;Não tente estabelecer regras escritas em pedra no dia um. É inútil. Comece no equivalente a fazer rascunhos. Avalie se o rascunho está num nível de detalhes suficiente. Se não estiver, vai ajustando. O PO, o UX, precisam ir refinando isso. Daí o QA e o suporte precisam também ir refinando o processo de testes. Esse processo volta pra equipe de desenvolvimento, que reflete se está cometendo deslizes técnicos que poderiam evitar bugs óbvios. Todo mundo ainda estava crú em fazer testes, e aos poucos vai aprendendo a realmente testar os casos que interessam e deixar de lado os casos que não precisa. Naturalmente vão se acostumando a saber até onde precisa ou não cobrir determinados tipos de funcionalidades.&lt;/p&gt;

&lt;p&gt;Sério. Exercite sua equipe em usar Git direito, em usar os recursos de testes que certamente já tem no seu framework. Como falei, Rails tem, Django tem, Laravel tem, Spring Boot tem, Express tem, React tem. Todo mundo tem suporte a testes que vocês só escolheram não usar. Só que vocês não são seres ungidos que fazem código sem erros. Pelo contrário, todo programador vai mais fazer erros do que acertar. Especialmente se for júnior. É normal. Só que hoje todo mundo deixa esses erros acumularem no repositório, porque os deploys pra produção acontecem só 1 vez por mês, ou 1 vez por bimestre. Daí toda vez que faz deploy fica todo mundo apreensivo, porque já sabe que vai dar merda. Todo mundo sabe que empurrou os problemas com a barriga. Daí o dia do deploy é o dia da pizza na madrugada. Todo mundo correndo atrás do próprio rabo pra consertar os erros que vão aparecendo. Um caos.&lt;/p&gt;

&lt;p&gt;Não seja esse clichê. Faz direito um pouco todo dia pro dia de deploy em produção ser só mais um dia normal, tedioso, sem nenhuma adrenalina, porque todo mundo tem segurança que tá tudo testado e funcionando. Fazer as coisas direito é muito chato, eu sei. Deixa pra fazer código porco gambiarrado nos seus projetinhos pessoais. Mas em projeto dos outros, aprenda a ser profissional. Imagina se seu médico também fizesse programação orientada a gambiarras sem testes como você faz. Imagine se o médico do seu filho fizesse como você faz. Acho que não ia gostar né? Cresce, e age como profissional. Se ficaram com dúvidas, mandem nos comentários abaixo. Se curtiram o video deixem um joinha, assinem o canal e compartilhem o video com seus amigos. A gente se vê, até mais!&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/6000</id>
    <published>2022-10-25T10:30:00-03:00</published>
    <updated>2022-10-25T10:38:54-03:00</updated>
    <link href="/2022/10/25/akitando-130-rant-projetos-testes-e-estimativa-rated-r-5097af48-c72f-42b7-bec4-486e24a86cfc" rel="alternate" type="text/html">
    <title>[Akitando] #130 - Rant: Projetos, TESTES e Estimativa??? | Rated-R</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/H_-7o_pLn1s&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Finalmente vou discutir um pouco sobre Programação Orientada a Testes e o que isso tem a ver com gerenciamento de projetos, equipes, e um pouco sobre como todo mundo ainda encara estimativas da maneira errada.&lt;/p&gt;

&lt;p&gt;Este video é pra você programador, gerente, cliente, empreendedor, que tem dificuldades de entregar projetos de qualidade. Vamos entender tudo que vocês estão fazendo de errado e porque dá errado.&lt;/p&gt;

&lt;p&gt;== Conteúdo&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;01:13 - CAP 01 - Suas Premissas estão Erradas - Esqueça Bullshit&lt;/li&gt;
&lt;li&gt;05:33 - Programadores não são Pedreiros!&lt;/li&gt;
&lt;li&gt;08:31 - CAP 02 - Provas de Conceito - Comecem a Fazer!&lt;/li&gt;
&lt;li&gt;13:01 - CAP 03 - Programação Orientada a Testes - Comecem a Fazer!&lt;/li&gt;
&lt;li&gt;17:53 - Demonstração de testes de Firefox&lt;/li&gt;
&lt;li&gt;19:02 - Demonstração de testes de React&lt;/li&gt;
&lt;li&gt;19:27 - Demonstração de testes de Tailwind&lt;/li&gt;
&lt;li&gt;19:57 - Demonstração de testes de Django&lt;/li&gt;
&lt;li&gt;20:28 - Demonstração de testes de Laravel&lt;/li&gt;
&lt;li&gt;21:06 - Demonstração de testes de VSCode&lt;/li&gt;
&lt;li&gt;22:14 - Demonstração de testes de Rust&lt;/li&gt;
&lt;li&gt;24:02 - Demonstração de testes de Spring Boot&lt;/li&gt;
&lt;li&gt;27:25 - Vocês não SABEM fazer testes!&lt;/li&gt;
&lt;li&gt;28:45 - CAP 04 - Como a falta de Testes afeta Produtividade? A Raíz do Ágil&lt;/li&gt;
&lt;li&gt;32:27 - Como é um fluxo de trabalho Ágil?&lt;/li&gt;
&lt;li&gt;37:01 - CAP 05 - O que significa Estimar? Estimar não é Prever&lt;/li&gt;
&lt;li&gt;47:27 - CAP 06 - Removendo Maus Elementos. Não acumule problemas.&lt;/li&gt;
&lt;li&gt;50:07 - CAP 07 - Resumindo os Princípios: O Mínimo pra Começar&lt;/li&gt;
&lt;li&gt;52:27 - Bloopers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Vamos dar uma pausa na série de videos técnicos. Se não assistiu ainda, fiz uma minissérie falando sobre redes e logo em seguida começando a falar sobre Linux. Como foi muita coisa técnica, eu me entediei, então resolvi mudar a marcha hoje pra falar sobre gerenciamento de projetos, em particular finalmente quero discutir sobre o famigerado desenvolvimento orientado a testes. É um puta tema cabeludo que daria pra fazer um canal inteiro sobre, mas a idéia é discutir só alguns pontos básicos, pra iniciantes no assunto mesmo. Então vamos lá.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Se você já tem experiência, trabalha em grandes empresas e grandes projetos, certamente vai discordar de algumas coisas que vou falar hoje e não está errado. Processos de gerenciamento de projetos de software variam bastante e devem ser customizados especificamente pro seu tipo de empresa, pro seu estilo de trabalho em equipe. Não existe nenhuma metodologia que vai se encaixar magicamente em todos os lugares. Simplesmente não existe, é inútil tentar fazer isso. Eu falei sobre isso no video de Esqueça Metodologias Ágeis. Recomendo assistir esse também depois.&lt;/p&gt;

&lt;p&gt;A expectativa é que existe algum conjunto de procedimentos que toda equipe consegue seguir pra manter os projetos estáveis e eficientes. Certamente deve existir algum tipo de PMI, Agile, CMMi, bla bla bla Alguma fórmula que o Spotify, o Netflix, a Microsoft, alguém inventou, que resolve todos os problemas. E não tem. Quanto mais cedo aceitarem isso, menos dor de cabeça vão ter. Se você se ver dizendo &quot;a gente segue o modelo Spotify&quot; ou segue qualquer modelo, certamente está fazendo errado. Veja no meu video do Guia Definitivo de Organizações.&lt;/p&gt;

&lt;p&gt;Além disso, se você é um empreendedor não-técnico, certamente assistiu um monte de palestras motivacionais, bullshit de startups e tá fazendo um monte de coisas erradas. Eu fiz 3 videos pra vocês também, o de Empreendendo Errado com Software e o video em duas partes dos 10 Mitos sobre Tech Startups. Vou tentar não repetir o que já disse lá, então assistam depois também.&lt;/p&gt;

&lt;p&gt;Vamos lá, no final do dia, você, como membro de uma equipe de desenvolvimento de software, precisa entregar alguma coisa que seu chefe ou seu cliente precisam no produto deles. Ou mesmo no seu próprio produto. Se for iniciante, é muito mais fácil integrar uma equipe que já existe, que tem membros experientes e processos já em andamento. Um iniciante sempre vai fazer errado da primeira vez, especialmente sozinho. Não tem como acertar de cara sem nunca ter feito, e isso não é um defeito, é natural. São muitas peças móveis que você nunca viu funcionando. É que nem eu dar um carro desmontado e mandar montar tudo e ainda funcionar direito no final. Impossível.&lt;/p&gt;

&lt;p&gt;Vai parecer que é simples, só encaixar as peças e apertar os parafusos. Mas não é só encaixar. Precisa monitorar cada pedaço, testar, ver se não tem vazamentos, se tá tudo alinhado, balanceado, etc. Porque se sair só montando tudo sem testar nada, quando terminar e tentar ligar o carro, vai explodir. Ou, o mais provável, nem vai ligar, e ninguém vai saber porque. E agora que tá tudo montado, vai precisar desmontar tudo e ir testando. No mínimo vai ser uma grande perda de tempo.&lt;/p&gt;

&lt;p&gt;Antes de mais nada, você precisa de pelo menos um membro da equipe um pouco mais experiente. Nem vou dizer sênior porque não quero ficar tentando definir hoje o que é junior ou sênior, mas alguém que já entregou no mínimo um projeto de sucesso. Porque essa pessoa precisa se responsabilizar pelas decisões técnicas. Decidir qual framework, bibliotecas, módulos, como as coisas vão ficar organizadas, como vai ser feito deployment. E lógico, todo mundo da equipe pode e deve contribuir com idéias, sugestões, mas alguém precisa bater o martelo e todo mundo seguir em frente.&lt;/p&gt;

&lt;p&gt;Senão toda hora se perde tempo com discussão inútil. Um grupo cheio de opiniões que leva horas ou dias pra decidir as coisas é ineficiente e o resultado sempre vai ser uma droga. Aliás, se isso acontece, é um bom sinal que o tal sênior responsável é ruim. Nenhuma equipe respeita alguém que só fala mas não faz. Um bom sênior fala menos e mostra mais, ensina como faz na prática. E o melhor é errar cedo, ter evidências de porque a primeira decisão deu errado, e isso servir de input pra uma segunda decisão melhor. Do que ficar horas discutindo o melhor caminho, pra no final dar errado de qualquer jeito.&lt;/p&gt;

&lt;p&gt;Isso talvez seja a coisa mais fundamental numa equipe. Errar antes e consertar rápido é infinitamente melhor do que horas e horas de discussão tentando chegar em consenso e bikeshedding. Ninguém tem certeza? Foda-se. Cara ou coroa. Faz um teste. Vê se funciona. Se falhar, agora veja porque falhou, conserta, e segue em frente. Essa é a essência do processo de lidar com o desconhecido. Todo mundo pensa que quando algo é desconhecido o certo é tentar planejar infinitamente até ter todos os passos minuciosamente detalhados. Mas tá errado. É desconhecido, qualquer planejamento que tentar fazer, quanto mais detalhado for, mais errado vai estar.&lt;/p&gt;

&lt;p&gt;Acho que esse é o ponto principal da discussão de hoje. Quando você e sua equipe são inexperientes, das duas uma, ou vão andar super lento porque vão tentar planejar em excesso, discutir em excesso, ou vão ser kamikazes retardados que vão começar a fazer e só ir fazendo sem parar pra ver se estão indo na direção certa. Ambos estão errados. Não consigo entender porque o desespero não leva ao mais óbvio, que é errar cedo, consertar rápido e o mais importante: garantir que o que já foi feito e está certo, não quebre à toa, porque isso seria o equivalente a andar pra trás.&lt;/p&gt;

&lt;p&gt;Não tem nenhum problema rascunhar o que precisa ser feito. Faça um checklist de funcionalidades. Faça post-its com cada tarefa. Faça uma planilha. Não importa. Todo mundo da equipe precisa ter uma noção geral do que precisa ser feito. Mas não tente detalhar em excesso. Só coloque detalhes no que estiver muito incerto, no momento em que precisa ser feito e não com meses de antecedência. Portanto, qualquer um que tenha estudado engenharia de software ou gerenciamento de projetos tradicional, e viu coisas como requisitos, casos de uso, ou até mesmo stories, pára. Você só precisa de uma lista, curta, com poucas palavras. Não é pra levar dias escrevendo, é pra levar horas, no máximo. Vou voltar nesse ponto mais pro fim.&lt;/p&gt;

&lt;p&gt;Não importa quantos diagramas, fluxogramas, photoshops ou words vocês produzam, nada disso é o software. Pra um amador, dá impressão que o ideal é ter o máximo de páginas de requerimentos detalhados quanto possível. Tá errado. Essa noção errada vem pelo seguinte: qualquer projeto de engenharia civil, pra fazer uma casa ou um prédio, precisa de meses de planejamento, várias plantas baixas, diagramas de encanamento, diagrama elétrico e tudo mais, pra garantir que o prédio seja construído perfeitamente dentro das especificações.&lt;/p&gt;

&lt;p&gt;Portanto, software deveria ser a mesma coisa. Vamos fazer quinhentas páginas de diagramas UML, que é o equivalente a planta baixa, porque daí, quando for escrever o software propriamente dito, vai sair tudo perfeitamente dentro das especificações. Essa premissa está errada. Todo mundo acha que um programador é um pedreiro. Não querendo desvalorizar o trabalho de pedreiros, mas um programador, na realidade, é o arquiteto. O trabalho de pedreiro, é 100% automático desde sempre. O pedreiro de software é o compilador ou interpretador. A planta baixa É o código fonte do software. Portanto todo programador, quer ele queira ou não, faz trabalho de um arquiteto.&lt;/p&gt;

&lt;p&gt;Lógico, se sua linha de trabalho é só instalar Wordpress pra diversos clientes. Não se está produzindo software novo, só copiando e colando o que já existe e esse trabalho, sim, parece mais uma linha de fábrica. Mas estamos falando de fazer software que não existe, quase do zero. Reusando vários componentes como frameworks ou bibliotecas, sim, mas o software final não seria possível fazer numa linha de fábrica. Cada passo no desenvolvimento desse software requer tomadas de decisões de cada programador envolvido. Não existe receita.&lt;/p&gt;

&lt;p&gt;O famoso exemplo de fazer um botão com cor na tela. Dá pra fazer de dezenas de jeitos diferentes. Posso fazer todo o CSS e HTML do zero. Posso escolher usar um Tailwind da vida. Mas se escolher isso, não pode ser só pra um botão, o certo é usar em todos os botões e elementos gráficos de todas as telas. Eu quero isso? Posso escolher fazer todo o sistema de eventos do zero ou usar React ou Vue. Mas de novo, isso vai afetar todo o sistema. Qual minha equipe tem mais facilidade pra usar? Já tinha começado como um sistema em Angular? Se eu decidir que quero usar Vue no lugar, quanto custa reescrever tudo? Vale a pena?&lt;/p&gt;

&lt;p&gt;Tudo são decisões. Muitas sem grandes consequências, mas algumas podem quebrar seu projeto inteiro se feito de forma irresponsável. Como se toma esse tipo de decisão? Não tem receita pra isso. Na maioria das vezes, seu prazo tá apertado, e o ideal é não escolher decisões que envolvam jogar tudo fora e fazer tudo de novo, então isso já elimina várias escolhas. Mas algumas são incertas, se der certo talvez vocês ganhem produtividade pra frente. Pra isso se fazem provas de conceito.&lt;/p&gt;

&lt;p&gt;Todo mundo ignora o poder das provas de conceito. Prova de conceito ou protótipos são pedaços de código, uma pequena parte da aplicação, que deve ser feito com o objetivo de potencialmente jogar fora depois. Portanto não precisa ser integrado ao código principal. Pode ser feito como uma branch do repositório, o que num GitHub chamamos de pull requests ou num GitLab chamamos de merge requests. Tecnicamente é um desvio no repositório de código. Enquanto todo mundo continua trabalhando no tronco principal, a branch &lt;code&gt;main&lt;/code&gt;, alguém trabalha nesse desvio, num galho, por alguns poucos dias, pra ver na prática se a mudança sugerida realmente trás benefícios ou não.&lt;/p&gt;

&lt;p&gt;Provas de conceito tem que ser um empreendimento curto, um micro-projeto de prazo fixo. Coisa de 2 dias, uma semana no máximo. Mesmo estando incompleto, no final desse período deve dar pra equipe ver na prática como a idéia se materializa em código, os impactos que causa, e se todo mundo está confortável em seguir em frente. Daí pode até jogar fora esse galho e todo mundo voltar pro tronco principal aplicando seja lá qual seja essa mudança técnica. Ou então se prova que a idéia era mais complexa do que se imaginava, e todo mundo descarta essa idéia. Mas isso não foi perda de tempo.&lt;/p&gt;

&lt;p&gt;Você tem 10 grandes dúvidas técnicas que podem custar muito tempo, coisas no nível: devemos usar React ou Vue? Devemos ficar no Postgres ou migrar pra Firebase? Devemos ficar só em APIs REST ou já começar tudo em GraphQL? Vamos dar deploy das coisas em Vercel ou criar Terraform pra AWS? Só que ninguém da equipe nunca teve muita experiência em nenhuma dessas coisas. Cada uma pode potencialmente custar 2 a 4 meses. Sua equipe só tem 3 pessoas. Então fácil, se for fazer tudo, não vai sair por menos que 10 meses pra equipe, e com a falta de experiência, fácil vai virar o ano.&lt;/p&gt;

&lt;p&gt;Em vez disso, pára tudo, cada um da equipe vai fazer provas de conceito de cada coisa, num prazo máximo de 2 semanas. E esse prazo é fixo, depois de 2 semanas vamos ver o que saiu. No final, mais da metade dessas coisas se provam realmente mais complicadas ou o custo claramente não vai compensar. Em vez de ir em frente com 10 coisas incertas que vai custar pelo menos 1 ano pra fazer, vocês concordam agora em desistir de 7 delas, porque a prova de conceito deu evidências que não vai dar certo. Por outro lado, 3 dessas coisas se provaram boas, e vocês vão seguir em frente.&lt;/p&gt;

&lt;p&gt;Ao custo de 2 semanas da equipe, vocês economizaram mais de meio ano de trabalho que ia dar errado, e vão prosseguir com 3 coisas que vão custar um mês da equipe e que todo mundo tem mais certeza que vai dar certo. Esse é o grande truque: não se comprometa com coisas de longo prazo que ninguém tem certeza. Pague um tempo curto pra descobrir se dá pra cobrir essa incerteza. Não é jogar tempo fora, é pagar um pouco pra não jogar muito mais fora. Essa noção deveria ser simples, mas na minha experiência, pouca gente parece entender que mais vale a pena jogar fora 2 semanas do que se comprometer cegamente com um projeto incerto que vai levar um ano.&lt;/p&gt;

&lt;p&gt;Na cabeça de muita gente, como 1 ano tá longe, fica aquela sensação de &quot;vamos indo, lá na frente a gente vê como faz&quot;. Especialmente em tech startups que vivem de dinheiro de investimento, esse tipo de decisão errada acontece muito, porque afinal é muito mais fácil gastar o dinheiro dos outros. E mesmo quem tem que gastar do próprio bolso, fica apreensivo com essa noção de &quot;jogar 2 semanas fora&quot;. Só que não é &quot;jogar fora&quot;, é o mesmo que pagar um seguro.&lt;/p&gt;

&lt;p&gt;Ninguém gosta de pagar mil reais, dois mil reais, todo ano, que ao final do ano, parece que não serviu pra nada. Só que se durante esse ano tiver um acidente, e você atropelar e machucar alguém, agora não é dois mil reais que vai custar, vai ser 100 mil, 200 mil reais. Quem faz economia porca e fica de bravata achando que &quot;não precisa, não vai acontecer&quot;, é o primeiro que se envolve em acidentes e depois fica falido. Em projetos é a mesma coisa. Se for coisa que você já faz várias vezes antes, o risco é baixo, já sabe o que tem que fazer, tem noção de quanto vai custar e mais ou menos sabe o que pode ou não dar errado, então consegue ir em frente. Mas se nunca fez, é melhor fazer um teste antes de decidir.&lt;/p&gt;

&lt;p&gt;E caindo no dia a dia de programação, é esse o mesmo argumento de porque falamos que todo programador deveria fazer, no mínimo, testes unitários de tudo que está trabalhando. Toda nova funcionalidade, deveria ser coberta de testes. Toda correção de bugs, deveria começar com um teste que simula o bug, pra que já comece falhando, e você sabe que corrigiu quando esse teste passa. Não só isso, esse bug em particular não vai acontecer de novo. Todo teste que você começa escrevendo antes de fazer o código, é como uma mini prova de conceito antes de tentar fazer uma coisa que não tem muita certeza. Se não tem certeza, comece escrevendo um teste.&lt;/p&gt;

&lt;p&gt;Muitas pessoas já me pediram pra fazer um video sobre testes, mas eu realmente não tenho vontade, porque testes é um assunto que tem já toneladas de documentação, livros, tutoriais, cursos, blog posts, PDFs gratuitos online, pra cada linguagem diferente, pra cada framework. Testes não é um troço que você aprende em 5 minutos e faz igual pra sempre. É um conjunto de técnicas. O ideal é um junior parear com alguém mais experiente e aprender na prática: quando precisa fazer, e com que nível de detalhes. É muito importante que sua equipe tenha pelo menos uma pessoa mais experiente que entende a importância de ter testes e oriente os demais a fazer.&lt;/p&gt;

&lt;p&gt;Deixa eu fazer o sales pitch de 2 minutos de porque todo programador deve fazer testes, de porque nenhum pull request pode ser mergeado sem testes, e porque uma pessoa que se diz sênior e é contra fazer testes, por definição, não deveria se considerar sênior. Toda regressão é tempo e dinheiro jogado fora. E o que é uma regressão?&lt;/p&gt;

&lt;p&gt;Regressão é exatamente o que o nome diz. Se até este momento, digamos que alguém de QA testou tudo e tinha zero bugs. Alguém me faz um pull request que ficou trabalhando sem integrar por duas semanas. Sem testar. E dá merge na master. Faz deploy, e o que acontece? Surgem um monte de bugs. Não só na funcionalidade que ele tava trabalhando, mas em outros lugares que antes funcionava. Esse amador regrediu o trabalho de todo mundo. Tudo deu passos pra trás.&lt;/p&gt;

&lt;p&gt;Agora o certo é dar revert nesse pull request, reverter mesmo. Só que esse pull request não tem mais como voltar atrás, porque dar deploy, ele alterou o schema do banco de dados e não tem como voltar mais ao schema anterior sem um grande downtime. Um desastre. E, claro, tenha certeza que esse indivíduo fez esse deploy na tarde de sexta-feira. Lógico. Cancela o happy hour porque a noite de sexta vai ser longa, fim de semana adentro, pra consertar essa cagada.&lt;/p&gt;

&lt;p&gt;O fluxo é simples: todo programador, seja fazendo testes-antes ou testes-depois, quando diz que concluiu seu pull request, também adicionou um mínimo de testes, fez rebase com a master, e indicou pra equipe que tá tudo pronto. Alguém da equipe sempre deve avaliar esse pull request, normalmente alguém mais experiente. Pelo menos pra bater o olho e ver que não tem nada que parece muito mau feito ou muito fora do lugar e, principalmente, se tem testes. Se não tiver, já rejeita.&lt;/p&gt;

&lt;p&gt;Se for uma equipe minimamente bem equipada, está usando repositórios privados de Git seja num GitHub ou GitLab. Ambos tem suporte a rodar testes automatizados num serviço separado. Nesse momento ele já puxou o que está nesse pull request, subiu um container de docker, e executou os testes automatizados do projeto. E quando o avaliador for checar o pull request, já vai saber inclusive se os testes passaram ou não. O GitLab tem a própria ferramenta que é o GitLab CI, de Continuous Integration. Tem diversos serviços externos que integram com GitHub como CircleCI, Travis CI, Semaphore e outros. É super simples configurar no seu projeto e não vejo nenhum motivo pra não usar.&lt;/p&gt;

&lt;p&gt;Se o pull request parece que está ok, e o CI diz que os testes passaram, só agora é permitido fazer merge desse código novo na master. No mínimo, sabemos que as funcionalidades que funcionavam antes, cobertas com testes, continuam funcionando depois, mesmo com a inclusão desse código novo. É um bom indicativo pra toda a equipe que as coisas estão andando pra frente, e não ficando com débito técnico acumulado pra trás. Eu vi um tweet alguns dias atrás, de alguém que tava relatando como um idiota da equipe resolveu atualizar a versão do Java, acho que do 16 pro 18 e fez deploy pra produção. Óbvio que deu pau. Óbvio que o pau apareceu na sexta-feira.&lt;/p&gt;

&lt;p&gt;Tem idiota que acha que porque usa uma linguagem compilada, com tipagem estática como Java, não tem problema não ter testes, porque se tudo compila, obviamente é porque funciona. Não tem nada que é mais certificado de júnior, do que pensar assim. Ahhnnnn como assim? Voltando ao exemplo de software livre. Vamos ver o projeto do navegador Firefox. Sabe Firefox? Sim, aquele Firefox. Eu baixei o código aqui na minha máquina. Firefox é feito na maior parte em C++, um pouco de Rust, um pouco de Javascript. É um navegador bem estável e quem usa acho que raramente tem algum problema dele renderizar errado, crashear ou coisas estranhas assim.&lt;/p&gt;

&lt;p&gt;Se você é desenvolvedor de front-end, certamente já usou o Developer Tools certo? Pra debugar o que está programando. E esse Developer Tools tem uma suite de testes, pra garantir que toda nova versão, antes de lançar pra você baixar na sua máquina, passa todos os testes e não introduz nenhuma regressão. Ou seja, tudo que funcionava numa versão continua funcionando na mais nova. No repositório de código fonte, só rodar &lt;code&gt;mach test devtools/*&lt;/code&gt; e olha só, abre a versão que acabei de compilar direto do código fonte e roda uma suite de testes de integração, simulando cada funcionalidade do Developer Tools. É assim que a Mozilla garante que código novo não introduz regressões óbvias.&lt;/p&gt;

&lt;p&gt;Se der uma fuçada no código, olha só, cada diretório tem um sub-diretório de testes. E muitos deles tem testes específicos pra bugs que foram reportados. Assim eles garantem que bugs corrigidos também não vão aparecer do nada de novo. Bugs que foram corrigidos e aparecem de novo porque não tinha um teste pra cobrir, se chama &quot;jogar dinheiro fora&quot;. Porque tem que corrigir mais de uma vez. Ao longo do tempo, você está perdendo tempo, dinheiro, eficiência, e mercado pra outro projeto que vai fazer melhor que você com menos recursos.&lt;/p&gt;

&lt;p&gt;Tem gente que acha que porque é programador front-end, não precisa se preocupar com testes. Acho que todo mundo gosta do projeto React do Facebook, quer dizer &quot;meta&quot;, certo? Eu baixei aqui o código fonte da biblioteca React. E olha só, se rodar &lt;code&gt;yarn test&lt;/code&gt;, o React tem uma suite de testes feita em Jest. Pelo mesmo objetivo. Mais de 7 mil testes unitários garantem que quando você atualiza o React, não vai quebrar tudo.&lt;/p&gt;

&lt;p&gt;Sabe aquela biblioteca de estilos, o TailwindCSS? Um amador poderia imaginar que é só uma coleção de CSS, obviamente não precisa de testes, certo? Errado. Eu baixei o código fonte do Tailwind e adivinha, se rodar &lt;code&gt;yarn test&lt;/code&gt;, mais testes unitários em Jest. Quase 900 testes unitários, fora checagens com eslint que por acaso deu alguns erros. Eu baixei do branch principal, ainda estão mexendo e na próxima versão esses bugs serão corrigidos. Mas está claro o que funciona e o que não funciona pra quem está contribuindo.&lt;/p&gt;

&lt;p&gt;Bah. Esse povo de front tá muito fresco. Ficar fazendo testes é coisa de hipster. Certamente povo de Python, mais calejado, mais raíz, não se importa com essas besteiras. Por isso eu baixei aqui o código fonte do framework Django. E olha só, tem um diretório chamado &lt;code&gt;tests&lt;/code&gt; e dentro eu posso rodar esse script chamado &lt;code&gt;runtests.py&lt;/code&gt;. E lá vai ele rodar centenas de testes pra garantir também que versões novas do Django não tá quebrando nada que já funcionava antes. Mais de 16 mil testes. Passando bonitinho.&lt;/p&gt;

&lt;p&gt;Foda-se povo de Javascript e Python. Vamos mudar pra PHP. PHP sempre foi conhecido por ser Quick and Dirty, rápido e sujo. Código raíz. Programador Cowboy que programa orientado a gambiarra. POG-zão na veia. Eu baixei aqui o código fonte do framework Laravel que é um dos mais usados hoje. Só que não. Eu posso rodar &lt;code&gt;composer exec phpunit&lt;/code&gt; e olha só, quase 8 mil testes, de novo, pra garantir que a versão mais nova não introduz nenhuma regressão. Por acaso falhou, povo ainda tá mexendo, próxima versão estável certamente já vai ter corrigido esse bug. Mas nós sabemos que tem o bug, porque tem suite de testes.&lt;/p&gt;

&lt;p&gt;Aliás, sabe o editor de textos mais usados por programadores hoje? O tal do Visual Studio Code? Provavelmente até você assistindo que não gosta de testes aí usa pra fazer seus códigos porcaria. Eu baixei o código fonte também e tem testes automatizados rodando &lt;code&gt;yarn smoketest&lt;/code&gt;, que segundo a documentação deles, é rodado antes de gerar o binário final que você baixa pra instalar. Igual o Firefox, tem testes de integração onde abre o aplicativo e vai testando cada funcionalidade que a gente aceita que simplesmente funciona em toda nova versão. E tudo só funciona, porque tem testes pra garantir isso.&lt;/p&gt;

&lt;p&gt;E voltando ao Firefox. Parte dele é feito em Rust. E pra você, seu infeliz, que ainda acha que só porque o código compila, tá tudo bem? Não era o Rust que tem o tal compilador mais inteligente de todos os tempos? Que muita gente acha que é mágica e garante não só binário super performático como se tivesse sido feito em C e sem bugs de gerenciamento de memória e problemas de segurança? Muita gente só falta dizer que o compilador escreve o código pra você, de tão bom que ele é. Certamente, se o código em Rust compila, não precisa de testes, certo?&lt;/p&gt;

&lt;p&gt;Bom, eu baixei o código fonte da biblioteca Tokio, que é o framework pra desenvolvimento assíncrono em aplicativos feitos em Rust. Todo projeto Rust acompanha um arquivo &lt;code&gt;Cargo.toml&lt;/code&gt;, assim como todo projeto Javascript acompanha um &lt;code&gt;package.json&lt;/code&gt;. E assim como existe npm ou yarn, em Rust temos o comando &lt;code&gt;cargo&lt;/code&gt;. &lt;code&gt;cargo install&lt;/code&gt; é equivalente a um &lt;code&gt;npm install&lt;/code&gt;. E adivinha, o próprio cargo, ferramenta padrão que já vem com o Rust, tem essa opção &lt;code&gt;test&lt;/code&gt;. E o que isso faz? Vamos ver. Só executar &lt;code&gt;cargo test&lt;/code&gt;. Olha só ele rodando a suite de testes da biblioteca Tokio.&lt;/p&gt;

&lt;p&gt;Ou seja, nem mesmo um projeto em Rust, que é armado com o compilador que é considerado a obra prima dos compiladores, evita os autores do Tokio de não fazer testes. Aliás, pode ver, ele sai rodando centenas de testes, não é um ou dois. Ahhh mas só de compilar já devia ser suficiente ... não, sua múmia paralítica, os criadores do Rust armaram o Cargo pra rodar testes. Você acha que você, logo você, entende mais do que os criadores do Rust?&lt;/p&gt;

&lt;p&gt;E você que é de Java e também detesta testes, que tem essa noção atrasada de porque o Java compila, tem tipos estáticos, então se compila tá tudo certo. Você que acha que só linguagens dinâmicas não compiladas como Ruby ou Javascript que precisam de testes. Sabe onde nasceu o conceito de TDD? Foi com Kent Beck, que fez o primeiro jUnit, em Java. Desde os anos 90 a gente sabe que só porque alguma coisa compila, não quer dizer que funciona. Só porque compila não significa que bugs não foram introduzidos. E por ficarem cansados de toda hora ter que correr atrás do próprio rabo, de ficar corrigindo coisa que já funcionava, que os agilistas originais formalizaram o conceito de testes.&lt;/p&gt;

&lt;p&gt;Aliás, falei de Java e não mostrei Java. Só de exemplo, baixei o código fonte de um dos frameworks web mais usados que é o Spring Boot. Rodando o bom e velho &lt;code&gt;gradlew build&lt;/code&gt; olha só. Baixa todas as dependências, junto com a internet inteira duas vezes, e roda todos os testes automatizados na sequência. O build só é considerado ok se compila e os testes rodam sem erro até o fim. E são milhares de testes. Ué, mas não era só compilar que já tava garantido que funciona? Obviamente não.&lt;/p&gt;

&lt;p&gt;Muito antes de existir o conceito de TDD a gente já fazia testes automatizados de um jeito ou de outro. Toda empresa de software madura faz testes. É por isso que seus programas e seu sistema operacional não quebram o tempo todo, toda hora. E mesmo com tudo isso, ainda assim bugs acontecem. Significa que se não tivessem todos esses testes, teríamos ordens de grandeza mais bugs em tudo. Novas versões iam demorar ordens de grandeza mais pra sair. Ia precisar de ordens de grandeza mais pessoas testando. Mas não. Hoje em dia quase todo dia algum programa que você tem instalado ganha uma versão nova, mas você raramente é incomodado. Bugs acontecem, mas são raros. E a razão disso é que todo software maduro, proprietário ou de código aberto, tem testes.&lt;/p&gt;

&lt;p&gt;Veja meu blogzinho feito em Ruby on Rails. Ele é ridiculamente simples. Eu mesmo que fiz em 2012. Não lembro se foi em Rails versão 3 ou 4. De lá pra cá eu atualizei até o Rails 6 rodando em Ruby 3. Toda vez que sai versão nova da linguagem ou do framework, eu atualizo, pra ganhar as correções de segurança principalmente. Eu venho atualizando faz 10 anos. E toda vez atualizo as bibliotecas também. Mas diferente do tweet da pessoa que falou que o cara atualizou o Java e subiu pra produção. Antes eu rodo meus testes em Rspec. Se não passa, obviamente não subo. Faço pequenos ajustes quando precisa, se a API de alguma biblioteca mudou. Mas meus testes pegam a maior parte dos problemas. Uma vez por ano, gasto 30 minutos, atualizo as coisas e subo.&lt;/p&gt;

&lt;p&gt;Ter testes não é burocracia, não é perda de tempo. É um seguro. Não fazer testes é a mesma coisa que dirigir sem seguro. Na maior parte do tempo, não parece ter muito valor mesmo. Sempre dá pra fazer tudo rápido, sujo e na gambiarra. Você se sente o herói de entregar as coisas rápido. Mas é inevitável, sua produtividade, e da sua equipe, vão gradativamente caindo. Porque mais e mais se perde tempo corrigindo bugs do que fazendo coisa nova. Em breve, a maior parte do seu tempo é só corrigindo bugs. E o que o amador fala? Ahhh, vamos mudar pro framework mais novo. Vamos mudar pra linguagem mais da hora. Quantos de vocês conseguiram manter e atualizar o mesmo sistema por uma década?&lt;/p&gt;

&lt;p&gt;Não existe nenhuma linguagem inventada na história que só pelo fato de compilar garante que não precisa ter testes. Todo mundo que entendia isso já fazia testes de alguma forma. A gente fazia alguns scripts separados do código pra rodar partes que eram mais sensíveis se quebrasse, ou que sabíamos que sempre alguém fazia merda e quebrava. Era uma das coisas que separavam os amadores dos espertos. O que aconteceu foi que as comunidade ágeis deram nomes aos bois e criaram convenções e semânticas pra todo mundo falar a mesma língua. Mas não existiu uma data especial que divide antes, com zero testes, e depois, com testes. Foi gradual, onde décadas atirando no próprio pé naturalmente levaram os programadores mais experientes a chegar na mesma conclusão.&lt;/p&gt;

&lt;p&gt;Enfim. O problema é que a maioria de vocês assistindo detesta fazer testes. A desculpa é perda de tempo. Mas a verdade é porque vocês não conseguem fazer. Vocês acham que ninguém percebe. Todo mundo que se sente incapaz de fazer alguma coisa, tenta denegrir esse alguma coisa. E é óbvio, não sabe fazer porque nunca faz. Assim como tudo em programação, fazer testes depende de treino e prática. Não existe nenhum livro ou curso que dá pra ler e automaticamente sair fazendo testes perfeitos.&lt;/p&gt;

&lt;p&gt;Você que é front, assistiu um curso e já saiu criando interfaces do nível de complexidade de um Canvas instantaneamente? Ou precisou de semanas e semanas apanhando de CSS obscuro? Ou você que é back, só leu um tutorial e já saiu criando APIs perfeitas em GraphQL integrado com backend de Firebase? Em 5 minutos já saiu codando como se não houvesse amanhã? Duvido.&lt;/p&gt;

&lt;p&gt;Não adianta ficar só procurando coisa pra ler. Precisa praticar. Assim como tudo que já fez, todos os seus primeiros testes vão ser uma bosta. Mas é assim que começa. Eu citei vários projetos de código aberto. Dá uma olhada nos testes deles pra se inspirar. Tem centenas de outros projetos mais parecidos com o que estiver trabalhando agora. Baixa e lê os testes deles. Copia. Altera. Testa. Joga fora. Começa de novo. Repete. É assim mesmo.&lt;/p&gt;

&lt;p&gt;Outra desculpa é falar que o projeto já tá faz meses ou até anos sem nenhum teste. É o cúmulo da preguiça ouvir isso. Ótimo. Faça você o primeiro teste. Pelo menos essa uma funcionalidade que acabou de entregar, está coberto e vai ser mais difícil de aparecer bug aleatório pela falta de atenção de outro programador depois. E se alguém pisar em cima do seu código, só de rodar esse um teste, dá pra saber que quebrou, quem foi, em qual commit, e corrigir mais fácil. De 1 teste em 1 teste, a cobertura naturalmente aumenta.&lt;/p&gt;

&lt;p&gt;Alguém reportou um bug? Abriu um ticket? Faça o teste que simula o bug reportado. Agora corrige. Pronto, esse é 1 bug que dificilmente vai aparecer de novo. E olha só, do nada, você começou a andar pra frente em vez de ficar só olhando pra trás. E total de cobertura? Precisa ser 100%? Isso é outra pergunta idiota que sempre aparece numa discussão de quem é desleixado. Foda-se. Você não fez nem 1% ainda, pra que quer discutir 100%? Não interessa. Faz 50%, faz 500%. Só, faz, a, porra, dos, testes.&lt;/p&gt;

&lt;p&gt;O que acontece é o seguinte. Sua equipe tem 3 pessoas. Cada um consegue pegar 2 ou 3 tarefas do backlog por semana. 6 a 9 tarefas entregas toda semana. Se o sistema começa a ser usado por usuários de verdade, rapidamente começam a aparecer bugs e problemas. Surge 1 ticket, 2 tickets. De repente a equipe só consegue entregar 3 a 6 tarefas por semana, porque precisa resolver bugs urgentes. Mas continua entregando coisas. Só que essas coisas novas, causam mais bugs. Na próxima semana a produtividade caiu pra 2 a 4 tarefas. E o número de bugs aumenta em vez de diminuir. Em pouco tempo, a equipe male male entrega 1 ou 2 tarefas novas, o resto do tempo é só resolvendo bugs.&lt;/p&gt;

&lt;p&gt;E qual a decisão do gerente do projeto? &quot;Ah, é normal, precisa contratar mais programadores.&quot; Ele convence a diretoria. Dobra a equipe. E voilá, a equipe volta e entregar 6 a 9 tarefas novas por semana. E todo mundo continua sem se importar com testes ou qualidade em geral, é crunch, só entregar e entregar. O volume de código só aumenta, ninguém joga nada fora, vai só acumulando débito técnico. Mais código novo significa mais bugs. No final acontece a mesma coisa, nas semanas seguintes a produtividade cai pra 3 a 6 tarefas, depois pra 2 a 4 tarefas, e de novo, a nova equipe, com o dobro de integrantes, tá male male entregando 1 ou 2 coisas novas por semana, o resto é só apagando incêndio.&lt;/p&gt;

&lt;p&gt;Digamos que todo mundo é ruim em testes. Se lá no começo tivesse decidido entregar tudo com testes, em vez de 6 a 9 tarefas, a equipe estaria entregando só de 4 a 6 coisas, talvez menos. Parece bem ruim. A diferença é que vai ser 4 a 6 TODA semana. Surgiu um bug novo? O programador que pega pra corrigir adiciona testes, e segue todo o processo que descrevi antes. Esse bug em particular não aparece de novo. Entregou coisa nova? A suite de testes garante que o que tinha antes não quebrou por conta disso. E assim, toda semana, de forma sustentável, a equipe entrega 4 a 6 tarefas. Não é o máximo de 9, mas não cai pra 1 ou 2.&lt;/p&gt;

&lt;p&gt;Daí a empresa decide contratar mais gente e dobra a equipe. Mas a equipe é diligente e treina os novos integrantes sobre a importância de qualidade. A produtividade quase dobra, de 4 a 6 pra 8 a 10 tarefas. A produtividade só dobra se todo mundo segue o protocolo, ninguém pisa no calo de ninguém, e tudo que já foi feito e testado continua funcionando no futuro. Todo upgrade de bibliotecas e correções de segurança tem garantia que o sistema continua funcionando, porque a suite de testes tá passando. Daí nenhum deploy de upgrade feito na sexta feira causa desastre que vai precisar varar fim de semana corrigindo. Olha só que mágico.&lt;/p&gt;

&lt;p&gt;Muita gente vai inventar um monte de desculpas pra não fazer testes, e eu garanto que pra cada desculpa nós já temos soluções. Nenhum programador precisa rodar a suite de testes o tempo todo se essa suite demora muito pra executar. Basta rodar alguns poucos ao redor do trabalho que está fazendo naquele momento. Daí sobe um pull request que vai ser pego por um GitLab CI e esse servidor de CI é quem vai rodar a suite inteira de testes. Não só isso, se a suite passar, esse CI pode fazer CD, que é continuous deployment, e automaticamente subir em ambiente de staging e notificar a equipe de QA.&lt;/p&gt;

&lt;p&gt;Obviamente eu não posso mostrar projetos confidenciais dos meus clientes, mas posso mostrar esse projeto interno que nós usamos. Olha um pull request. Se olharmos o que mudou podemos ver que além do código, temos ajustes em specs também, que são testes. Quando o desenvolvedor subiu o código, o CI pegou e iniciou o job pra rodar os testes. Nesse caso levou pouco mais de 5 minutos. Passou com sucesso e, lógico, só porque todos os testes passaram não significa que não tem nenhum bug, só que não tem nenhum bug muito óbvio. Mais do que isso, no nosso caso tem configurado também checagens de segurança. No ecossistema Ruby temos coisas como Brakeman, que avalia o código por buracos de segurança conhecidos como injections.&lt;/p&gt;

&lt;p&gt;Essa aplicação sobe no Heroku, que tem uma funcionalidade chamada review apps. Podemos ter um deployment separado de testes pra cada pull request que ainda está aberta. Justamente pra alguém conseguir testar de verdade antes de aprovar. Um review app é um ambiente de staging, só que isolado pra cada pull request. Dá pra ter dezenas de review apps em paralelo. Assim não precisa ficar acumulando e integrando tudo no mesmo ambiente de staging pra testar, o que seria uma zona. Testa tudo isolado. Vai aprovando no ritmo do responsável pelos testes. E no final, quando estiver tudo testado, aprovado e mergeado no branch principal, daí faz um deployment pra produção aqui do lado.&lt;/p&gt;

&lt;p&gt;Neste exemplo, no dia que fui tirar foto de tela, não tinha nenhum pull request aberto. Mas eles apareceriam listados aqui. Cada um com uma URL separada de testes. Esse é outro motivo de porque eu sempre recomendo Heroku. Ele não é o mais barato, mas esse tipo de recurso facilita o fluxo de trabalho em ordens de grandeza. Só as horas de desenvolvimento que se economiza de cada desenvolvedor já se paga.&lt;/p&gt;

&lt;p&gt;Enfim, uma vez cada pull request de cada desenvolvedor sendo aprovado, tudo vai sendo integrado no branch principal e podemos fazer um deploy de tudo integrado pro ambiente de staging. Agora o pessoal de QA pode fazer o trabalho deles direito, que é testar as coisas novas, e não ficar retestando tudo de novo que já funcionava antes. Todo report de bugs volta pro programador, que faz a correção e ajusta os testes pra cobrir os casos que não tinha pensado.&lt;/p&gt;

&lt;p&gt;Com o pull request foi ajustado, os testes passam, QA já viu em staging, o sênior da equipe pode dar a última olhada e fazer o merge na branch principal. E aí alguém de devops ou alguém responsável pela produção pode pegar o que está na branch principal e fechar uma versão pra produção, com a segurança de saber que se está na branch principal, então está funcionando corretamente.&lt;/p&gt;

&lt;p&gt;E se estiver usando Heroku, ainda tem uma facilidade extra. Digamos que todo mundo fez tudo direitinho, mas mesmo assim surgiu um bug que só aparecem em produção. Basta ir nesta aba de atividades e clicar em roll back pra voltar pra versão anterior. Heroku é caro, mas é nessas horas que ele se paga. Medo de deploy na sexta-feira só existe em equipes disfuncionais que não fazem esse básico que acabei de falar.&lt;/p&gt;

&lt;p&gt;Falando em disfuncional, o maior desafio em toda equipe de projetos, é coordenar a comunicação. Ao ter um repositório de códigos como Git, um processo de testes de integração contínua, pareamento entre sêniors e juniors pra compartilhar conhecimento, e um protocolo como de pull requests, estabelecemos um conjunto simples e objetivo de entendimento nessa equipe. Não seguir esse protocolo é um desrespeito à equipe. Você não programa pra você mesmo, você contribui com sua equipe. O importante não é andar rápido, o importante é não estragar o que já funcionava.&lt;/p&gt;

&lt;p&gt;Da mesma forma, ninguém deve sair adicionando coisas no projeto que ninguém está sabendo. Por isso que no mundo ágil se inventou os tais rituais como de planejamento e revisão. Rituais não devem ser obrigações, mas pontos de coordenação. Se os rituais estão sendo meramente pras pessoas aparecerem mas não participar, então não servem pra nada e devem ser eliminados e repensados. Esse é um ponto não-técnico que não quero tentar detalhar demais, mas preciso falar de um ponto específico: estimativas.&lt;/p&gt;

&lt;p&gt;Se tem uma coisa que programador odeia fazer mais do que testes é estimar tarefas. Já disse isso em outros videos, mas existem duas palavras que todo mundo não-técnico confunde: estimar e prever. O programador estima, mas os gerentes ou diretores acham que receberam previsões. Estimativas são ordens de grandeza, não está certo nem errado. É uma ordem de grandeza. Previsões sim, é pra serem precisas. Quando um programador diz que pode levar uns 2 ou 4 dias, é isso que pode levar, 2 a 4 dias. Talvez leve 5. E aí o gerente que achava que tinha uma previsão na mão, fica puto, porque tava esperando 2 dias, recebeu em 5 e quer sair chutando tudo. E é isso que eu chamo de um idiota.&lt;/p&gt;

&lt;p&gt;Eu acho que todo gerente de projetos deveria gastar um tempo movimentando o próprio dinheiro no mercado financeiro. Comprar e vender ações, ou opções, ou criptomoedas, não importa. Porque todo mundo gostaria de saber pra onde vai o mercado, se vai subir ou se vai cair. É impossível acertar 100% do tempo. Se alguém soubesse prever isso, quebraria o mercado financeiro. O máximo que podemos fazer é dar chutes educados. Estimativas. A gente tenta acertar, comprar na baixa, vender na alta. Ou no mínimo planejar pra alta e se proteger da baixa. Criamos contingências, e sabemos que é impossível ganhar sempre. O máximo que podemos fazer é tentar minimizar perdas. E as perdas nunca vão ser zero.&lt;/p&gt;

&lt;p&gt;Gerenciamento de projetos é a mesma coisa. Um gerente inteligente não se interessa em tentar prever cada tarefa individualmente. Ele observa o movimento geral da equipe e do produto. Tá em tendência de alta de produtividade? Baixa de produtividade? Tem como influenciar a baixa? Qual é o obstáculo? Algum diretor deu carteirada e quebrou o planejamento? Alguém ficou doente? Um bom gerente está numa posição que um analista financeiro não tem: de influenciar as tendências.&lt;/p&gt;

&lt;p&gt;Lógico, quanto mais um programador ganha experiência e tem mais segurança no que realmente sabe e o que não sabe, mais e mais suas estimativa se aproximam de previsões. Ele já tem uma boa noção de quanto tempo leva a maioria das coisas. Quanto mais inexperiente, mais longe vai ser essa estimativa. Faz parte do crescimento profissional de cada um começar a quantificar suas próprias capacidades. Ninguém tem obrigação de &quot;acertar&quot; estimativas, mas claro, quanto mais precisas forem, mais todo mundo consegue confiar mais e mais em você. É uma das formas de subir na carreira. Entregar o que promete.&lt;/p&gt;

&lt;p&gt;Tarefas, stories ou seja lá como quer chamar, são meras formalidades. Quanto mais curtas, objetivas, direto ao ponto forem, melhor. O objetivo não é um concurso de literatura onde todo mundo tem que fazer uma dissertação. A quantidade de palavras deve ser o suficiente pra que toda a equipe envolvida saiba no que está sendo trabalhado. Só isso. Tem quinhentos blog posts, livros e cursos tentando ensinar &quot;a melhor forma&quot; de escrever requerimentos. E a maioria é bullshit. Quer dizer, até faz sentido, pode ser usado como inspiração, mas não existe nenhuma receita que dá pra só seguir. Assim como testes e estimativas, escrever requerimentos também depende de prática. Estude o que se sugere, mas não siga ao pé da letra.&lt;/p&gt;

&lt;p&gt;User Stories tem aquele formato idiota de &quot;eu, como usuário, quero conseguir adicionar um produto no carrinho e ter o total do pedido, para poder pagar minha compra&quot; ou algo assim. Pode ser, é bonitinho, tem o objetivo de forçar as pessoas a pensar em pra quem é a funcionalidade sendo feita, qual o objetivo e facilitar pensar na melhor forma de fazer. Tem coisa que precisa de mais detalhe que isso. Um protótipo de tela no Canvas. Um rascunho de conta de imposto. Tem coisa que nem precisa de tanto detalhe assim. Bugs costumam ser mais diretos &quot;quando o usuário abre no firefox de mac, o box de items fica em cima do box de cartão e não dá pra digitar o código de segurança do fucking pedido e a fucking compra não finaliza&quot;. Tá super claro.&lt;/p&gt;

&lt;p&gt;Eu tive um cliente que o imbecil do CTO mandava todo programador descrever em detalhes como ia programar, antes de escrever uma linha de códigos. Tinha que listar que classes novas ia criar, com que métodos, com quais nomes. Era a coisa mais idiota que eu já vi, só perde pra época que povo achava que precisava fazer diagramas de classe UML precisos meses antes de escrever uma linha de código. É idiótico. É da época que falei que se achava que programador era só pedreiro e havia uma forma mágica de descrever tudo, antes de escrever o código. É uma noção boçal.&lt;/p&gt;

&lt;p&gt;O resumo é simples: a descrição deve ser a mais concisa, com o menor número de palavras possível. A equipe deve conseguir se reunir e em pouco tempo entender o que precisa ser feito, quem vai fazer, quem precisa de ajuda. O gerente, ou Product Owner, e o equivalente tech lead ou programador mais sênior, deveriam estar alinhados. Daí quando seja lá qual programador entregar o pull request praquela funcionalidade, o tech lead consegue avaliar rápido e mandar pro QA. O PO e o QA precisam estar alinhados, pra ver se o que foi entregue resolve o que se queria resolver.&lt;/p&gt;

&lt;p&gt;Burocracia e formalidades em excesso aparecem em empresas onde esses personagens tem preguiça de conversar ou pior: não sabem o que estão fazendo. Porque se soubessem, poderiam ir direto ao assunto. Toda burocracia em excesso é defendida por aqueles que não sabem o que estão fazendo, porque aí podem usar a desculpa de &quot;mas, mas, eu segui os procedimentos, a culpa não é minha&quot;. Toda burocracia é uma muleta pra cabideiro que não deveria estar ocupando as posições que estão ocupando. É assim simples.&lt;/p&gt;

&lt;p&gt;Voltando pra estimativas. Não existe nenhuma fórmula nem procedimento que, dado uma descrição em texto de um caso de uso, ou user story ou seja lá o que for, chega num número de horas ou dias exatos pra resolver essa tarefa. Cocomo e outras bullshitagens são lixo. Não tem como. Só a experiência dos programadores envolvidos é capaz de dar uma ordem de grandeza, cuja qualidade vai ser proporcional à experiência da equipe. Se a equipe for super experiente, mesmo assim vai ser uma ordem de grandeza.&lt;/p&gt;

&lt;p&gt;Não tem nada mais retardado do que gente que pega esses números de estimativas, número de horas realizadas, vai pondo num gráfico e fica tentando derivar coisas desses números, fazer integral, aplicar fórmulas. Só porque se tem números, não signfica que existe cálculos que dê pra fazer. Eu posso sair jogando dados, colocar todos os resultados num gráficos e gerar um monte de conclusões. É assim que nasce pseudo-ciência, numerologia, astrologia e todas essas logias idiotas. Estimativa é um chute. Estimativas nunca estão erradas, porque não são previsões. Fazer contas com esses números não faz nenhum sentido.&lt;/p&gt;

&lt;p&gt;Dá pra saber que vai ser perto de 6 meses mas menos de 1 ano. Agora, se vai ser 5 meses, 2 semanas, 4 dias e 5 horas, exatamente. Isso é impossível. Gastar horas tentando estimar tudo é um exercício de futilidade. Não perca tempo tentando prever com exatidão. Aprenda a lidar com ordens de grandeza. Simplesmente escreva o que se sabe que realmente precisa fazer. E nem tente detalhar 6 meses de trabalho de uma só vez. É inútil. Descreva o curto prazo em detalhes, 1 mês, no máximo 2 meses de trabalho. Tenha a direçao do longo prazo em mente, mas ajuste enquanto vai andando. Deixe a equipe se dividir em quem faz o que. E siga o que falei até agora sobre pull requests e merge.&lt;/p&gt;

&lt;p&gt;Mesmo sem ter estimativa nenhuma, você como gerente ou como qualquer membro da equipe vai ver que numa semana a equipe entregou 10 tarefas, stories ou seja lá como quer chamar. Na semana seguinte, entregou 9. Na outra semana conseguiu entregar 11. Na semana seguinte entregou 7. O que o gerente ou facilitador dessa equipe começa a notar? Uma tendência. Parece que, seja lá como se descreve tarefas nessa equipe, eles conseguem entregar uma média de 8 a 9 unidades dessas &quot;coisas&quot;. E se for esperto, vai notar que quando entrega menos coincide quando sobe versão nova pra produção, daí o suporte recebe reports de bugs, que entram no backlog da equipe e isso diminui a quantidade de &quot;coisas&quot; entregues.&lt;/p&gt;

&lt;p&gt;O gerente fala com o tech lead, que revê o processo, e vê que ele tava deixando passar pull requests com testes não tão bons. Ele pareia com o programador júnior que tava fazendo errado. Ensina na prática onde tava errando. E agora ele passa a produzir melhor. E é assim que uma equipe evolui, um passo de cada vez. Não é via metodologia, nem aumentando formalidades e burocracias, é via bom senso. Resolvendo problemas na raíz em vez de ficar inventando band-aids, ou pior, mais burocracias inúteis. E o principal: reorganizado a lista de coisas a fazer pra garantir que as coisas mais importantes estão sendo feitas primeiro e tudo que é opcional ou de pouco valor está no fim da lista. Assim, caso não dê tempo de fazer tudo, pelo menos você sabe que o mais importante foi feito.&lt;/p&gt;

&lt;p&gt;Cuidado, não estou dizendo que não se deve perder tempo descrevendo tarefas ou que não se deve fazer estimativas. Estou falando que a formalidade não é tão importante, tem que começar a escrever e aceitar que vai estar mau escrito. Aceitar o feedback de que o programador fez errado porque do jeito que tava escrito ficou ambíguo. E aí vai ajustando os detalhes pra próxima vez, até o nível que faz sentido. &quot;Ah, se tivesse descrito essa conta com um exemplo, ele teria entendido&quot;, ou algo assim. Aceitar que uma estimativa nunca vai estar certa ou errada. É só uma referência. Não tente fazer projeções baseado em estimativas, porque isso só vai multiplicar um erro com outro erro.&lt;/p&gt;

&lt;p&gt;Em vez disso veja em ordens de grandeza como falei. Seja lá como organiza as tarefas da equipe, veja, no geral, que tipos de funcionalidades ou correções de bugs a equipe entrega num certo período de tempo, como toda semana. Daí nem precisa perguntar pra equipe se o que tem no backlog cabe ou não na semana. Você já sabe essa resposta pela velocidade que já conhece. Quando sabe que não vai caber, seja honesto e faça seu trabalho: corte ou simplifique. Não tem mágica. Se não cabe, não cabe. Se tentar fazer caber, obviamente a qualidade vai cair. É onde vai vir o &quot;se eu pular fazer testes, talvez caiba&quot;. E quando se cede uma vez, isso vai acumular. Na semana seguinte já começa a vir a conta. E uma decisão feita no desespero numa semana, vai prejudicar todo o resto do andamento daqui pra frente.&lt;/p&gt;

&lt;p&gt;Não existe almoço de graça. Toda vez que vai se deixando passar uma pequena coisa errada, isso vai acumulando. Em breve, vai estar tudo errado. Ser sustentável é exercitar disciplina. E não tem coisa mais maçante, mais entediante, do que ser disciplinado. Quando tudo é uma bagunça, é difícil elencar os problemas. Mas numa equipe disciplinada, fica fácil ver quem tá puxando a equipe pra trás. Sem ter a muleta da burocracia, fica fácil ver quem é a pessoa inconstante, aquele que promete demais, entrega de menos, e quando entrega outra pessoa precisa consertar. A equipe inteira sabe quem é essa pessoa. Eles devem ensinar, ajudar, mas quando a pessoa é resistente a trabalhar em equipe, ela deve ser removida. É assim simples.&lt;/p&gt;

&lt;p&gt;A decisão errada é manter essa pessoa e contratar outra pra aumentar produtividade. Muitas equipes que eu já vi teriam a produtividade aumentada só de cortar os maus elementos, porque agora eles não precisam ficar consertando o que o mau elemento tá entregando de mau jeito. Automaticamente a produtividade aumenta, o stress diminui, todo mundo trabalha melhor. Uma coisa que aconteceu durante esse período que já passou de vacas gordas, com montes de empresas contratando gente ruim no atacado, é que em vez de aumentar a produtividade, eles prejudicaram a qualidade do produto e das equipes. Imagina um monte de gente ruim, fazendo o contrário de tudo que falei até agora. Só acumulou um monte de código porcaria que funciona em produção na força bruta, com alguns poucos hiper estressados, varando noite, consertando a porcaria que recebe.&lt;/p&gt;

&lt;p&gt;Se a equipe não se importa com o que está entregando, nada do que eu falei até agora importa. E todo novo integrante, ao ver que ninguém tá realmente muito interessado, vai aprender do jeito errado: que o melhor é ligar o foda-se e fazer igual todo mundo. E é assim que um projeto entra em colapso, porque ninguém está ligando. Em empresas com dinheiro dos outros demais, é o que mais acontece. Mas se sua empresa realmente depende do código sendo feito, sinto dizer, rapidamente todo mundo vai perder o controle. E daí pra frente é só cavar o buraco mais fundo.&lt;/p&gt;

&lt;p&gt;De novo. Todo projeto é uma incerteza, especialmente nos primeiros dias. Especialmente se for uma equipe nova, recém constituída. Precisam aprender a trabalhar juntos ao mesmo tempo que precisam entender o que diabos vão construir. É uma montanha de incertezas. Por isso o ideal é garantir que o básico do básico está garantido: que pelo menos o que está sendo entregue vai continuar funcionando ao longo do tempo. Exercitem o processo de testes, pull requests, integração contínua, deployment em staging. Daí comecem a ver até que nível de detalhes realmente as tarefas precisam ser definidas. E vai ajustando.&lt;/p&gt;

&lt;p&gt;Não tente estabelecer regras escritas em pedra no dia um. É inútil. Comece no equivalente a fazer rascunhos. Avalie se o rascunho está num nível de detalhes suficiente. Se não estiver, vai ajustando. O PO, o UX, precisam ir refinando isso. Daí o QA e o suporte precisam também ir refinando o processo de testes. Esse processo volta pra equipe de desenvolvimento, que reflete se está cometendo deslizes técnicos que poderiam evitar bugs óbvios. Todo mundo ainda estava crú em fazer testes, e aos poucos vai aprendendo a realmente testar os casos que interessam e deixar de lado os casos que não precisa. Naturalmente vão se acostumando a saber até onde precisa ou não cobrir determinados tipos de funcionalidades.&lt;/p&gt;

&lt;p&gt;Sério. Exercite sua equipe em usar Git direito, em usar os recursos de testes que certamente já tem no seu framework. Como falei, Rails tem, Django tem, Laravel tem, Spring Boot tem, Express tem, React tem. Todo mundo tem suporte a testes que vocês só escolheram não usar. Só que vocês não são seres ungidos que fazem código sem erros. Pelo contrário, todo programador vai mais fazer erros do que acertar. Especialmente se for júnior. É normal. Só que hoje todo mundo deixa esses erros acumularem no repositório, porque os deploys pra produção acontecem só 1 vez por mês, ou 1 vez por bimestre. Daí toda vez que faz deploy fica todo mundo apreensivo, porque já sabe que vai dar merda. Todo mundo sabe que empurrou os problemas com a barriga. Daí o dia do deploy é o dia da pizza na madrugada. Todo mundo correndo atrás do próprio rabo pra consertar os erros que vão aparecendo. Um caos.&lt;/p&gt;

&lt;p&gt;Não seja esse clichê. Faz direito um pouco todo dia pro dia de deploy em produção ser só mais um dia normal, tedioso, sem nenhuma adrenalina, porque todo mundo tem segurança que tá tudo testado e funcionando. Fazer as coisas direito é muito chato, eu sei. Deixa pra fazer código porco gambiarrado nos seus projetinhos pessoais. Mas em projeto dos outros, aprenda a ser profissional. Imagina se seu médico também fizesse programação orientada a gambiarras sem testes como você faz. Imagine se o médico do seu filho fizesse como você faz. Acho que não ia gostar né? Cresce, e age como profissional. Se ficaram com dúvidas, mandem nos comentários abaixo. Se curtiram o video deixem um joinha, assinem o canal e compartilhem o video com seus amigos. A gente se vê, até mais!&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5998</id>
    <published>2022-10-11T09:47:00-03:00</published>
    <updated>2022-10-11T09:48:39-03:00</updated>
    <link href="/2022/10/11/akitando-129-apanhando-do-gentoo-melhor-linux" rel="alternate" type="text/html">
    <title>[Akitando] #129 - Apanhando do Gentoo | Melhor Linux???</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/cSyTjCUFx2A&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Finalmente vamos tentar instalar e configurar uma das distros Linux mais trabalhosas de todas. O lendário Gentoo foi pensado pra você tirar o máximo proveito do seu hardware, customizando e compilando tudo do zero especificamente pra sua máquina. A primeira parte do video vai cobrir um pouco o que significa instalar uma distro assim. E na segunda e última parte vamos discutir se vale a pena todo o trabalho.&lt;/p&gt;

&lt;p&gt;== Errata&lt;/p&gt;

&lt;p&gt;Os capítulos 9 e 10 aparecem errado no video mas está correto no índice de tempo abaixo.&lt;/p&gt;

&lt;p&gt;== Conteúdo&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;01:03 - CAP 1 - Mesma coisa que Slackware - Instalando Gentoo&lt;/li&gt;
&lt;li&gt;06:22 - CAP 2 - CHROOT - Quebrando a Prisão&lt;/li&gt;
&lt;li&gt;11:32 - CAP 3 - Depois do Stage3 - Terminando a instalação&lt;/li&gt;
&lt;li&gt;17:59 - CAP 4 - Compilando a Kernel - Customizando seu Gentoo&lt;/li&gt;
&lt;li&gt;25:34 - CAP 5 - Resolvendo seu Primeiro Problemão - Não boota!&lt;/li&gt;
&lt;li&gt;29:24 - CAP 6 - Tentando Instalar GNOME - Mais um Problemão&lt;/li&gt;
&lt;li&gt;35:49 - CAP 7 - GDM, WebKit, Reflexões - Final da Instalação&lt;/li&gt;
&lt;li&gt;37:35 - CAP 8 - Mais um Problema de SystemD - OpenRC&lt;/li&gt;
&lt;li&gt;39:33 - CAP 9 - Por que e Quando usar Gentoo? É pra mim?&lt;/li&gt;
&lt;li&gt;42:52 - CAP 10 - Qual a Melhor Distro? Eu sei &quot;Linux&quot;?&lt;/li&gt;
&lt;li&gt;45:57 - Bloopers&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;== Links&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Gentoo AMD64 Handbook (https://wiki.gentoo.org/wiki/Handbook:AMD64/pt-br)&lt;/li&gt;
&lt;li&gt;Quick Installation Checklist (https://wiki.gentoo.org/wiki/Quick_Installation_Checklist#UEFI.2FGPT)&lt;/li&gt;
&lt;li&gt;GNOME/Guide (https://wiki.gentoo.org/wiki/GNOME/Guide)&lt;/li&gt;
&lt;li&gt;GNOME/GNOME without systemd/Gentoo (https://wiki.gentoo.org/wiki/GNOME/GNOME_without_systemd/Gentoo)&lt;/li&gt;
&lt;li&gt;GCC optimization (https://wiki.gentoo.org/wiki/GCC_optimization)&lt;/li&gt;
&lt;li&gt;Gentoo Cheat Sheet (https://wiki.gentoo.org/wiki/Gentoo_Cheat_Sheet)&lt;/li&gt;
&lt;li&gt;Upgrading Gentoo (https://wiki.gentoo.org/wiki/Upgrading_Gentoo#Updating_packages)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Vamos continuar a saga de Linux por baixo do capô e como prometido, desta vez o video vai ser mostrando o lendário Gentoo. Mesmo quem nunca tentou instalar acho que já ouviu falar dele. A distro onde todo o sistema é compilado na sua máquina em vez de baixar pacotes binários pré-compilados.&lt;/p&gt;

&lt;p&gt;Não é a distro mais difícil, mas certamente é chatinha, talvez ainda mais chatinha que o Slackware, e muito demorada, vocês vão ver. Só pra ter uma idéia, num ambiente controlado que é uma máquina virtual, mesmo assim eu recomecei a instalação do zero múltiplas vezes e ainda tive que fazer muita tentativa e erro até conseguir bootar.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;O projeto original do Gentoo se chamava Enoch Linux, lá no começo deste século, e o objetivo era criar uma distro customizável que não instalava nenhum binário e sempre compila tudo direto na sua máquina. Consigo imaginar as vantagens disso. Mesmo assim, ainda não recomendo pra todo mundo. Esperem até o final que vou explicar porquê.&lt;/p&gt;

&lt;p&gt;Sem mais delongas, mesma coisa que Ubuntu ou Slackware, vamos no site, baixar a ISO pra gravar no pendrive e bootar o instalador. No caso do Gentoo tem uma peculiaridade. Não esquece de baixar esse arquivo chamado stage3 aqui embaixo. Se quiser uma instalação pra servidor, ou seja, sem interface gráfica, baixe um dos dois primeiros arquivos. Se quiser instalar em desktop com gerenciador gráfico de janelas, escolha o profile de desktop. Em ambos os casos, você pode escolher se prefere usar systemd ou openrc.&lt;/p&gt;

&lt;p&gt;Já expliquei um pouco sobre init systems e o systemd no episódio anterior de Slackware, mas de novo, se já está acostumado a usar uma distro mais popular como Ubuntu ou Fedora, eles usam systemd e talvez você prefira instalar systemd aqui também, pra não ter que reaprender muita coisa. Por outro lado, justamente porque estamos já tendo o trampo de instalar e configurar Gentoo, por que não usar OpenRC? Já que é uma das poucas distros que tem a opção de não usar systemd. Experimentar coisas novas e diferentes sempre vai agregar, no mínimo você vai ter outra perspectiva.&lt;/p&gt;

&lt;p&gt;Eu resolvi escolher o stage3 de profile desktop com openrc por padrão. De resto, mesma coisa, colocamos a ISO no CD-ROM virtual da máquina virtual, bootamos e deixamos carregar. E diferente do Slackware, que cai no login em modo texto, aqui caímos em modo gráfico abrindo KDE. A tela está pequena porque não carregou serviço das ferramentas de guest da vmware pra ajustar resolução. Depois resolvemos isso.&lt;/p&gt;

&lt;p&gt;Mas meio que tanto faz pra agora, porque vamos abrir o terminal e fazer tudo dele de qualquer jeito, já que Gentoo também não usa o instalador gráfico Calamares e nos obriga a configurar tudo na mão, bem old school mesmo, e por isso escolhi falar dele pra mostrar algumas coisas que não expliquei na instalação de Slackware.&lt;/p&gt;

&lt;p&gt;Uma das grandes vantagens do Gentoo é que o site deles é muito bem documentado. Tem um wiki bem completo que, embora não seja do tamanho do wiki do Arch Linux, mesmo assim consegui encontrar quase tudo que queria. Vale a pena experimentar Gentoo só como desculpa pra estudar a wiki e aprender mais sobre Linux em geral. Em particular, como estou instalando dentro do VMWare, vou precisar usar a página de Wiki específica sobre VMWare, que também é meio válido pra VirtualBox. Então lembrem de ler caso queiram experimentar no VirtualBox de vocês.&lt;/p&gt;

&lt;p&gt;Eu vou seguir primariamente esta página que é o &quot;Quick Installation Checklist&quot;. Como disse no episódio passado, este video não é um tutorial pra seguir passo a passo. Vou usar a instalação como desculpa pra explicar conceitos de Linux e também uma parte do processo de tentativa e erro normal. Assistam tudo primeiro, depois leiam as páginas da wiki, e depois se aventure a instalar direto na sua máquina ou num VirtualBox.&lt;/p&gt;

&lt;p&gt;E não se preocupe, você não vai acertar na primeira vez. Eu mesmo tiver que reinstalar umas três ou quatro vezes na minha máquina virtual até conseguir terminar, foi um dia inteiro de tentativas e erro. Errar e recomeçar faz parte do aprendizado. Se quiser uma instalação fácil que acerta tudo de primeira, e não aprender nada, baixa Ubuntu.&lt;/p&gt;

&lt;p&gt;Enfim, a wiki já começa sendo legal com você. Se sua máquina for muito antiga, de mais de 10 anos pra trás, pode ser que tenha BIOS em vez de UEFI e só suporte boot com MBR. Se for um pouco menos velha talvez seja BIOS com GPT, e se for mais moderna vai ser UEFI e GPT. Já falei disso no video passado e na minissérie sobre armazenamento. Assistam depois. Se tiver assistido, já sabe a diferença de BIOS e UEFI e MBR e GPT. No meu caso vou escolher UEFI e GPT, só garantir que na minha máquina virtual selecionei UEFI também, por padrão ele simula uma BIOS antiga, cuidado com isso.&lt;/p&gt;

&lt;p&gt;O resto é a mesma coisa que já mostrei no video de Slackware. Uso &lt;code&gt;cfdisk&lt;/code&gt; pra particionar o HD e depois comandos como &lt;code&gt;mkfs.ext4&lt;/code&gt; e &lt;code&gt;mkswap&lt;/code&gt; pra formatar as partições. A única diferença é que no video de Slackware fui preguiçoso e só criei 3 partições, 1 de boot efi, 1 de swap e a principal. Esse wiki manda criar 4, que é mais correto, 1 partição separada pro mount point de &quot;/boot&quot;, outra pra &quot;/boot/efi&quot; dentro, daí as mesmas de swap e outra pra principal.&lt;/p&gt;

&lt;p&gt;Não vou explicar linha a linha tudo de novo. Veja o episódio anterior pra detalhes disto. Mas é esta sequência de comandos aqui do lado. Primeiro estas, depois do cfdisk, pra formatar as partições e depois esta outra sequência pra montar as partições dentro de &quot;/mnt/gentoo&quot;, que vai ser a raíz da instalação.&lt;/p&gt;

&lt;p&gt;Agora vem a hora daquele Stage3 que falei no começo, o zipão de 500 megas. O wiki manda entrar no diretório &quot;/mnt/gentoo&quot; e usar wget pra baixar o zipão. É o jeito mais fácil mesmo se o LiveCD detectou seu wifi ou sua rede cabeada e tá conectado na internet. Se não estiver, baixa em outro computador, coloca num pendrive, e copia do pendrive pra dentro desse diretório.&lt;/p&gt;

&lt;p&gt;No meu caso, abro o firefox, e na página de download seleciono o stage3 pra desktop com openrc. Agora é descompactar com o comando &lt;code&gt;tar xpf stage3*&lt;/code&gt;. Vai parecer que travou, mas é porque o arquivo é grandão e leva alguns minutos pra descompactar tudo, paciência.&lt;/p&gt;

&lt;p&gt;Pronto. Se der um &lt;code&gt;ls&lt;/code&gt;, olha só, tem os diretórios que você já deve ter visto em qualquer instalação de Linux, o &quot;/usr&quot;, &quot;/etc&quot;, &quot;/bin&quot;, &quot;/var&quot; e tudo mais. Tecnicamente o Gentoo já tá instalado, agora vem a parte mais complicada, que é configurar tudo antes de bootar. E a próxima seção do Wiki se chama &quot;Chroot&quot;. Então chegou a hora de fazer uma tangente pra explicar o que é isso.&lt;/p&gt;

&lt;p&gt;Deixa eu bootar meu Ubuntu em outra máquina virtual e abrir um terminal. Agora vou criar um diretório qualquer e chamar de &quot;jail&quot; ou &quot;prisão&quot; mesmo. Tenha paciência que vou executar vários comandos que podem não fazer sentido, mas vai fazer. Lembra que executáveis de Linux costumam ter dependência de outros objetos binários ou bibliotecas compartilhadas? Que nem em Windows que você tem DLLs?&lt;/p&gt;

&lt;p&gt;Lembra também quando falei sobre PATH? Quando executamos &lt;code&gt;ls&lt;/code&gt;, onde que realmente tá esse executável? Podemos saber usando o comando &lt;code&gt;which ls&lt;/code&gt; e descobrimos que o caminho completo é &quot;/usr/bin/ls&quot;. Fazemos a mesma coisa pro shell Bash, que é a linha de comando onde estamos. &lt;code&gt;which bash&lt;/code&gt; e tá em &quot;/usr/bin/bash&quot;. Beleza, vou precisar desses binários.&lt;/p&gt;

&lt;p&gt;Uma ferramenta que nunca tinha mostrado é o &lt;code&gt;ldd&lt;/code&gt;. Ela analisa um binário e lista as bibliotecas compartilhadas que depende. Fazemos &lt;code&gt;ldd /usr/bin/bash&lt;/code&gt; e &lt;code&gt;ldd /usr/bin/ls&lt;/code&gt;. Estes são os objetos binários que eles precisam. Agora vem a parte estranha. Recriamos esses diretórios dentro do &quot;jail&quot; com &lt;code&gt;mkdir -p jail/bin&lt;/code&gt; depois &lt;code&gt;mkdir -p jail/lib/x86_64-linux-gnu&lt;/code&gt;, &lt;code&gt;mkdir -p jail/lib64&lt;/code&gt;. Copiamos os binários do bash e ls pra &quot;jail/bin&quot; e depois, um a um, copiamos as bibliotecas pra dentro de &quot;jail/lib&quot; e &quot;jail/lib64&quot;. Trabalhinho. Já entenderam o que vou fazer?&lt;/p&gt;

&lt;p&gt;Pronto, listando com &lt;code&gt;ls jail&lt;/code&gt;, temos todos os bits necessários pra rodar bash e ls. Mas qual o objetivo de duplicar essas coisas? Agora vem o truque: rodamos &lt;code&gt;sudo chroot jail /bin/bash&lt;/code&gt;. E olha só, abriu um shell de bash. Duh, grande bosta. Mas presta atenção, estamos rodando o bash que copiamos dentro do diretório &quot;jail&quot;.&lt;/p&gt;

&lt;p&gt;Fazendo &lt;code&gt;ls /&lt;/code&gt; pra listar a raíz, cadê meu Ubuntu, cadê todos os outros diretórios? Estou preso dentro do jail, enxergando como se esta prisão fosse a raíz do sistema. Parece uma máquina virtual né? Só que não é. &lt;code&gt;chroot&lt;/code&gt; literalmente quer dizer &quot;change root&quot; ou &quot;mude a raíz&quot;. Ele faz o sistema operacional mentir pro meu usuário achar que a raíz do drive agora é só isso. A grosso modo, é como se eu tivesse bootado pra dentro de outra máquina, sem realmente ter feito um boot de verdade.&lt;/p&gt;

&lt;p&gt;Eu chamei o diretório de &quot;jail&quot; porque neste momento meu usuário está preso dentro dessa cadeia. Ele acredita que meu HD de verdade é o que está dentro da cadeia. Por isso copiei as bibliotecas do bash e ls pra dentro, senão eles nem iam conseguir executar. Se der exit, pronto, voltamos ao normal. &lt;code&gt;ls /&lt;/code&gt; agora lista minha partição de verdade.&lt;/p&gt;

&lt;p&gt;Sei que é um pouco confuso isso, mas vocês tem que perder a noção de associar a raíz do sistema com seu HD físico. A raíz do sistema é um ponto de montagem, por acaso eu associo &quot;/&quot; com a partição usando o sistema de arquivos ext4, é o que tá configurado no arquivo &quot;/etc/fstab&quot;. Mas posso mudar isso dinamicamente mesmo depois do boot. Linux é um sistema dinâmico, não pense nele como algo congelado e fixo.&lt;/p&gt;

&lt;p&gt;Chroot é bem útil pra várias aplicações, mas um uso errado é segurança. Se procurar no Google vai achar tutoriais, por exemplo, pra fazer um usuário se conectando via SSH cair dentro de um chroot. Alguns pensam que é uma boa forma de evitar que o usuário consiga enxergar o sistema de arquivos de verdade da máquina por baixo. Uma forma de segurança. Mas chroot nunca foi projetado pra ser usado pra segurança.&lt;/p&gt;

&lt;p&gt;Antigamente, coisa de mais de 20 anos atrás, a gente usava chroot em processos como servidor web Apache, na esperança que se o apache fosse invadido, o invasor ia cair dentro de um chroot e não ter acesso à partição toda do servidor. De novo, chroot não é mecanismo de segurança. Deixa eu provar com o tiro pode sair pela culatra. Chroot é executado como root e permanece com o setuid de root dentro. Vamos tentar estourar essa prisão. Nada nessa manga, nada nessa outra. Minto na verdade eu tenho um programinha feito em C que achei no Google. Esse aqui.&lt;/p&gt;

&lt;p&gt;`#include &amp;lt;sys/stat.h&gt;&lt;/p&gt;

&lt;h1&gt;include &amp;lt;stdlib.h&gt;&lt;/h1&gt;

&lt;h1&gt;include &amp;lt;unistd.h&gt;&lt;/h1&gt;

&lt;p&gt;//gcc break_chroot.c -o break_chroot&lt;/p&gt;

&lt;p&gt;int main(void)
{
    mkdir(&quot;chroot-dir&quot;, 0755);
    chroot(&quot;chroot-dir&quot;);
    for(int i = 0; i &amp;lt; 1000; i++) {
        chdir(&quot;..&quot;);
    }
    chroot(&quot;.&quot;);
    system(&quot;/bin/bash&quot;);
}`&lt;/p&gt;

&lt;p&gt;Vou compilar, copiar esse binário pra dentro do diretório jail/bin, entro no chroot de novo, estou preso, tá vendo? Não enxergo nada fora. Agora é só executar e .. boom, olha só, não só estourei pra fora do jail como invadi o usuário de root dessa máquina. Isso que se chama escalação de privilégios.&lt;/p&gt;

&lt;p&gt;O que ele faz é simples. Esse programinha poderia ter sido feito em python, shell script ou o que quiser. Ele só cria um novo diretório com mkdir, dentro dele faz um loop com equivalente a &lt;code&gt;cd ..&lt;/code&gt; mil vezes, e sabemos que &quot;..&quot; é pra voltar pro diretório anterior. Mas ele explora algum bug do chroot que ao fazer isso um tanto de vezes e depois criar outro chroot dentro desse chroot, ele estoura pra fora e mantém os privilégios do usuário que criou o primeiro chroot, que foi o root.&lt;/p&gt;

&lt;p&gt;Enfim, essa tangente foi só apresentar chroot e pra ilustrar o ponto que apesar de parecer um mecanismo de segurança ao isolar o usuário num diretório mais limitado, na realidade não é, é uma conveniência que serve justamente pra coisas como instalar de Linux, que é justamente o que o tutorial do wiki do Gentoo vai nos mandar fazer no próximo passo.&lt;/p&gt;

&lt;p&gt;Veja que na página de Wiki a próxima seção se chama justamente &quot;Chroot&quot;. Entramos no diretório &lt;code&gt;cd /mnt/gentoo&lt;/code&gt; que foi onde descompactamos aquele arquivão stage3 e instalamos todos os arquivos do sistema. Criamos pontos de montagem pra diretórios virtuais especiais de Linux usando parâmetros especiais do comando &lt;code&gt;mount&lt;/code&gt;. Pra proc, sys, dev e run, tudo dentro do /mnt/gentoo.&lt;/p&gt;

&lt;p&gt;Com tudo montado, no final copio o &quot;/etc/resolv.conf&quot; que tem configuração de DNS e faço um chroot pro diretório corrente, executando o &lt;code&gt;/bin/bash&lt;/code&gt; que tá dentro desse chroot. A partir de agora, pro meu usuário, o novo &quot;/&quot; é o que tem no diretório &quot;/mnt/gentoo&quot;, sacaram? Estou dentro do meu gentoo recém instalado, sem precisar bootar. Saí da partição do LiveCD e entrei na partição do novo Linux instalado no meu HD. Assim posso mexer lá dentro sem ter que bootar ainda. E nem daria pra bootar, porque essa instalação ainda não tem kernel e nem bootloader configurado, que é o que fazemos agora.&lt;/p&gt;

&lt;p&gt;De novo, a partir deste ponto é como se eu já estivesse no novo Gentoo instalado no meu HD de verdade, sem ter precisado bootar. Isso é um jeito super rudimentar e incompleto de container, como um Docker. Você já está num Linux funcionando, a gente só muda a raíz do usuário pra outra raíz em outra partição e de lá de dentro é um Linux com ferramentas diferentes, embora usando a mesma kernel que já tava carregada. Ainda não é hoje que vou falar de containers, mas pra chegar lá achei útil explicar que chroot existe, e apesar de ter um efeito parecido, não é usado em containers de verdade.&lt;/p&gt;

&lt;p&gt;Agora que estamos dentro do nosso novo Linux, o que todo mundo sempre fala pra fazer logo que instala um Linux novo? Atualizar os pacotes com o repositório online pra ver se tem pacotes novos pra instalar. Num Ubuntu da vida, no terminal, é só executar &lt;code&gt;apt update &amp;amp;&amp;amp; apt upgrade&lt;/code&gt;. Mas Gentoo não usa Apt e sim Portage e o comando é &lt;code&gt;emerge&lt;/code&gt;. Gentoo não instala software, ele &quot;emerge&quot; software .. Enfim, o equivalente ao &lt;code&gt;apt update&lt;/code&gt; é &lt;code&gt;emerge-webrsync&lt;/code&gt;. Pra aprender outros comandos de emerge, a wiki tem outra página com um cheat sheet, explicando as principais opções. Vou deixar o link na descrição abaixo.&lt;/p&gt;

&lt;p&gt;Feito isso, até agora só temos o usuário de root funcionando, mas sem senha. Então precisamos criar uma senha forte pro root com o comando &lt;code&gt;passwd&lt;/code&gt;. Por acaso a política de senhas do Gentoo é mais restrita que de um Ubuntu, ele exige senhas um pouco mais fortes, então cuidado aqui. Depois disso, só criar um usuário normal não-administrador com o comando &lt;code&gt;useradd&lt;/code&gt; e mudar a senha dele também.&lt;/p&gt;

&lt;p&gt;Precisamos editar alguns arquivos de configuração, então próximo passo é instalar um editor de textos de linha de comando decente. Por isso &lt;code&gt;emerge -vq vim&lt;/code&gt;. Se ainda não sabe usar vim, tudo bem, usa um nano da vida, qualquer editor serve, mas você precisa aprender a usar um editor de linha de comando. VSCode não vai te ajudar quando precisar dar SSH num servidor. Depois disso precisamos editar o arquivo &quot;/etc/fstab&quot;, que já expliquei no video passado, é onde declaramos os pontos de montagem principais, ligando partições como &quot;/dev/sda1&quot; com pontos de montagem como &quot;/boot&quot;.&lt;/p&gt;

&lt;p&gt;Como expliquei no começo, o conceito todo do Gentoo é baixar pacotes de código-fonte dos softwares e compilar na hora de instalar. Significa que temos que configurar o compilador com parâmetros de compilação e a configuração global do Portage fica no arquivo &quot;/etc/portage/make.conf&quot;. Lendo as páginas de instalação do Wiki, mantenho tudo igual e só no final adiciono as seguintes variáveis de ambiente:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
ACCEPT_LICENSE=&quot;*&quot;
GRUB_PLATFORMS=&quot;efi-64&quot;
USE &quot;-qt5 -kde X gtk gnome&quot;
VIDEO_CARDS=&quot;vmware&quot;
INPUT_DEVICES=&quot;libinput synaptics&quot;
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;De novo, isto não é pra ser um tutorial completo de instalação, certamente tem mais configurações importantes que eu não vi, então faça sua pesquisa. Pra mim, nesse cenário de máquina virtual, isso foi o suficiente. E no meu caso vou instalar GNOME como gerenciador de janela, por isso tem &quot;-qt5 -kde&quot;, pra priorizar opções em gtk e gnome.&lt;/p&gt;

&lt;p&gt;Vale a pena explicar é esse Accept License pra asterisco. Acho que o emerge restringe instalar pacotes de softwares com licenças não livres, daí ele mascara esses pacotes e eu tive problema de não conseguir instalar por causa disso. Mas dizendo pro Portage aceitar as licenças, daí consigo instalar. E veja também em Video Cards que isso é necessário no meu caso porque estou num Vmware. Se você tiver placa de video NVIDIA teria colocado &quot;nvidia&quot; ou se fosse AMD seria &quot;amdgpu&quot; e assim por diante.&lt;/p&gt;

&lt;p&gt;Além disso, o grande lance do Gentoo é baixar pacotes de código fonte e compilar, por isso nesse arquivo tem CFLAGS e CXXFLAGS, que são opções pro compilador GCC. Tem outra página no Wiki explicando essas e outras flags que eu recomendo que vocês leiam e não saiam só copiando e colando coisa aqui sem saber porque. Por exemplo, o flag &quot;-O2&quot; é o nível de otimização que o compilador vai usar. &quot;-O1&quot; é o mais básico e &quot;-O3&quot; é o máximo. Tem gente escrevendo em fórum que conseguiu um executável mais rápido usando um &quot;-O9&quot; da vida. É mentira, não existe esse nível, o máximo é 3.&lt;/p&gt;

&lt;p&gt;Tem gente recomendando usar flags como &quot;-funroll-loops&quot; pra otimizar mais, e de novo, como o Wiki explica, o próprio manual do GCC fala pra não usar isso, porque você vai acabar com um binário maior e mais lento. Flags agressivas devem ser usadas só pro programadores que realmente sabe o que elas fazem, em software que foi escrito pra tirar vantagem disso. Não devem ser usadas em pacotes aleatórios e muito menos pro sistema inteiro. Se ficar usando flags aleatórias, vai acabar com binários mais pesados, mais lentos e mais instáveis e seu sistema vai ficar uma bosta. E a culpa não é o Gentoo, é você que sai copiando e colando coisa sem saber.&lt;/p&gt;

&lt;p&gt;Seguindo nosso guia do Wiki, tem configurações opcionais que vou passar acelerado. É só seguir o que diz no Wiki. Mas seria configuração de locale, hostname, keymap, timezone. Nenhum detalhe importante pra agora.... E finalmente chegamos na parte importante que diferencia o Gentoo dos demais, na minha opinião, a seção de customização da Kernel agora na instalação. Na real, você pode customizar kernel em qualquer distro, mas no Gentoo isso faz parte do processo de instalação.&lt;/p&gt;

&lt;p&gt;Pra isso precisamos do código-fonte. Executamos &lt;code&gt;emerge -av sys-kernel/gentoo-sources sys-kernel/linux-firmware&lt;/code&gt;. Note que pacotes em Gentoo são organizados em namespaces como &quot;sys-kernel&quot;. Enfim, quando terminar, entramos no diretório de código-fonte que é o &quot;/usr/src/linux&quot; e a versão.&lt;/p&gt;

&lt;p&gt;Se nunca teve a oportunidade de fuçar o código fonte da kernel do Linux, eis sua chance. Depois abra um editor de textos qualquer e perca algumas horas vasculhando o código, talvez aprenda algo. Não tenha medo de código só porque parece complicado. Justamente porque parece complicado é que deveria ser fascinante. Na prática, uma boa parte do código vai ser de drivers de dispositivos.&lt;/p&gt;

&lt;p&gt;Apesar do código fonte do Linux ser um enorme monolito, tudo organizado num mesmo repositório, o binário em si é mais ou menos modular. Existem vários drivers que são compilados estaticamente no mesmo binário da kernel. Acho que o driver do sistema de arquivos ext4, por exemplo, fica dentro do binário da kernel. Porém existem drivers que não são carregados imediatamente no boot. São compilados separadamente, como objetos ou bibliotecas, e podem ser carregados depois, com comandos como modprobe.&lt;/p&gt;

&lt;p&gt;Sem mais delongas, do diretório de código fonte, vai ter um Makefile, como mostrei no episódio passado, lembram? Todo programador de C sabe lidar com Makefiles e aqui não é diferente. Vai ter uma tarefa chamada &lt;code&gt;menuconfig&lt;/code&gt;, que carrega com &lt;code&gt;make menuconfig&lt;/code&gt;. Abre esse programa interativo em modo texto, acho que usando ncurses. É uma lista de categorias de módulos de kernel.&lt;/p&gt;

&lt;p&gt;Tudo que tá desmarcado, é porque não vai ser compilado. O que estiver com &quot;*&quot; vai ser compilado e linkado estaticamente, ou seja, vai estar dentro do binariozão da kernel. E o que estiver com &quot;M&quot; vai ser compilado e mantido desativado num arquivo separado até ser carregado por comandos como &lt;code&gt;modprobe&lt;/code&gt;. Você quer manter o máximo de pacotes desmarcados, se possível.&lt;/p&gt;

&lt;p&gt;Por exemplo, sei lá porque, por padrão vem marcado suporte pra PCCards, padrão PCMCIA ou Cardbus. Eu nunca mais vi esse tipo de dispositivo nos últimos 20 anos. Pra quem não sabe, antes de ter conectores como Thunderbolt ou USB, uma das formas de adicionar um hardware externo num notebook dos anos 90, era com cartão PCMCIA. Eram coisas como essas. Até imagino que deva ter hardware especializado em alguma indústria que ainda precise disso, mas pra pessoas normais como eu ou vocês aí, nunca vamos precisar disso. Não faz sentido a kernel sempre carregar esse driver e ficar ocupando espaço na RAM. Posso desmarcar.&lt;/p&gt;

&lt;p&gt;Na real, vamos sair e fazer outra coisa antes. Note que estamos ainda no ambiente de LiveCD do pendrive. E tá tudo funcionando bonitinho, ou seja, a detecção automática de hardware funcionou até que bem. Seja lá quais drivers foram carregados, tá fazendo nossa máquina funcionar. Então, por que não selecionar os mesmos drivers que já estão carregados? Pra isso podemos rodar o comando &lt;code&gt;make localyesconfig&lt;/code&gt; pra atualizar o arquivo &lt;code&gt;.config&lt;/code&gt; com esses drivers. E aí sim, voltamos pro &lt;code&gt;make menuconfig&lt;/code&gt; e tiramos suporte a PCMCIA e outras coisas que sabemos que não vamos precisar.&lt;/p&gt;

&lt;p&gt;Podemos sair tirando mais coisas, como suporte a hardware de Chromebooks, ou suporte a notebooks Surface da Microsoft. Normalmente o que você mais vai mexer é a categoria de Device Drivers. Nas outras opções, a menos que tenha certeza absoluta, não mexa, especialmente em categorias como Criptografia, Gerenciamento de Memória. Isso é bom pra você fuçar se estiver pesquisando sobre sistemas operacionais mais a fundo. Mas pra usuários iniciantes, o ideal é mexer bem pouco.&lt;/p&gt;

&lt;p&gt;No meu caso, agora entra aquela outra página de Wiki que falei no começo, que lista alguns módulos específicos pro caso onde estou numa máquina virtual VMWare. Então eu gostaria de carregar drivers de Vmware. Por exemplo, em Device Drivers, Network device support, preciso selecionar o driver VMware VMXNET3 ethernet. Nessa página tem essa listinha sugerindo módulos pra checar e ativar. Depois que terminar, só selecionar lá embaixo a opção de &quot;Save&quot; e sair.&lt;/p&gt;

&lt;p&gt;De volta à linha de comando, agora é hora de compilar a kernel. Vamos compilar com o comando &lt;code&gt;make -j4&lt;/code&gt;. No tutorial ele sugere &quot;-j2&quot;. Essa opção &quot;j&quot; significa &quot;jobs&quot; ou trabalhos em paralelo, que é quantos arquivos vai compilar ao mesmo tempo. &quot;j2&quot; é o mínimo, vai usar só duas threads da sua CPU. No meu caso, eu dei 2 cores da minha CPU de verdade com 2 threads cada, então posso rodar com &quot;-j4&quot;.&lt;/p&gt;

&lt;p&gt;Se estivesse nativo no meu AMD Ryzen 9 de 32 threads, eu poderia rodar com &quot;-j32&quot;. Quanto mais cores tiver pra gastar, mais rápido vai ser a compilação, porque vai tudo em paralelo. Porém, quanto mais rodar em paralelo, mais RAM precisa ter. Se for agressivo demais em jobs paralelos e tiver 4 giga ou menos de RAM, provavelmente vai crashear, então cuidado.&lt;/p&gt;

&lt;p&gt;Esse arquivo vmlinuz que gera no final é o binário da kernel. Se nunca viu um Linux de verdade na sua frente, muito prazer, este é &quot;O&quot; Linux que todo mundo tanto fala mas ninguém nunca viu. E não só isso, esse é um kernel feito sob medida especialmente pra você, sem carregar muita coisa desnecessária. Numa distro normal como Ubuntu ou Manjaro, vem com uma kernel mais genérica que precisa suportar os mais diferentes tipos de hardware, então eles tentam manter mais drivers ativos do que realmente é necessário, consumindo mais RAM.&lt;/p&gt;

&lt;p&gt;Esse é o custo da comodidade. Um usuário normal não quer ter que configurar sua própria kernel do zero, então se contenta em desperdiçar um pouco de RAM e um pouco até de processamento, carregando uma kernel mais gorda do que precisa, só pro caso de um dia precisar de um hardware mais exótico. Mas a filosofia do Gentoo é justamente compilar tudo do zero pra termos a oportunidade de escolher tirar tudo que sabemos que não vamos precisar.&lt;/p&gt;

&lt;p&gt;Agora que temos a kernel, precisamos colocar na partição de boot junto com um bootloader. Lembram de dois episódios atrás que expliquei sobre boot? Vamos instalar o bootloader com &lt;code&gt;emerge --ask sys-boot/grub&lt;/code&gt;. Uma vez instalado, como estamos num ambiente EFI, rodamos &lt;code&gt;grub-install --target=x86_64-efi --efi-directory=/boot/efi&lt;/code&gt; pra instalar o bootloader na partição de boot. E instalamos a kernel com &lt;code&gt;grub-mkconfig -o /boot/grub/grub.cfg&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;E pronto. Pra terminar, só instalar o suporte a rede pra no próximo boot conseguir carregar wifi, rodar dhcp e tudo mais. O wiki sugere essa linha de comando grandão aqui, mas como estou numa máquina virtual, sei que não vai ter wifi, porque estou simulando uma rede cabeada na máquina virtual, por isso só rodo &lt;code&gt;emerge --ask sys-apps/iproute2 net-misc/dhcpcd&lt;/code&gt;. E acabou a parte do LiveCD. Saímos com &lt;code&gt;exit&lt;/code&gt; e damos um reboot de uma vez.&lt;/p&gt;

&lt;p&gt;E que estranho. Cadê a opção da kernel que acabamos de bootar? O menu do Grub tá vazio. Ele entra no menu UEFI da máquina. Bom, obviamente fiz alguma coisa errada, mas por aqui podemos bootar de novo pelo LiveCD no CD-ROM virtual. Vamos voltar pro ambiente que estávamos antes. Lembra que eu falei que o LiveCD serve pra diagnosticar e resolver problemas caso o boot falhe? Vamos ver isso na prática.&lt;/p&gt;

&lt;p&gt;De volta àquele KDE do LiveCD, abrimos o terminal e vamos criar os pontos de montagem das nossas partições de novo. Pra não ficar repetitivo é essa sequência igual tá no começo da página de wiki. Ir criando os diretórios, daí &lt;code&gt;mount&lt;/code&gt; de coisas como proc, run, sys e finalmente o chroot de novo. E enquanto fazia isso que me toquei. Eu terminei de compilar a kernel e logo depois fui instalar o Grub, mas esqueci de instalar a kernel. Vamos voltar pro diretório &quot;/usr/src/linux&quot;. Faltou rodar &lt;code&gt;make modules_install&lt;/code&gt; e &lt;code&gt;make install&lt;/code&gt;. Burro.&lt;/p&gt;

&lt;p&gt;Sempre que compila e instala uma nova kernel, tem que lembrar de atualizar o Grub. Só rodar &lt;code&gt;grub-mkconfig -o /boot/grub/grub.cfg&lt;/code&gt;. Pronto, podemos dar exit pra sair e reboot. Esperamos alguns segundos e agora sim, temos a opção do nosso Gentoo novinho. Escolhemos ele, começa a sequência de boot do sistema init. E ... caraca, que porra é essa?&lt;/p&gt;

&lt;p&gt;A mensagem diz que a CPU foi desabilitada e a máquina parou de bootar. Literalmente ou a kernel deu crash ou deu uma instrução HLT de halt and catch fire. Nunca tinha visto isso em nenhuma outra instalação de Linux. Certamente fizemos alguma coisa errada na hora de compilar a kernel que agora ela não boota. Isso vai acontecer bastante nas primeiras vezes que você tentar compilar sua própria kernel e acabar desabilitando coisa que não devia.&lt;/p&gt;

&lt;p&gt;E eu estou deixando esses erros aparecer no video de propósito pra vocês saberem que sim, esses erros acontecem o tempo todo. E sim, em videos como esse todo youtuber corta fora as partes de erro. Por isso parece que no video tudo é perfeito e só você que deve ser ruim porque no seu dá erro. Mas pra todo mundo dá erro, a gente corta senão o video ia ficar muito cansativo.&lt;/p&gt;

&lt;p&gt;Temos que resetar à força de novo, escolher o menu da UEFI no Grub pra poder escolher bootar pelo LiveCD. E lá vamos nós outra fez, cair no KDE do LiveCD, abrir o terminal, refazer os pontos de montagem e dar chroot ...  Alguma coisa eu fiz errado quando desmarquei coisas do kernel. Vamos abrir o menuconfig outra vez ... (5 min later ...)&lt;/p&gt;

&lt;p&gt;Se olharmos a página de Wiki sobre Vmware, fala que preciso checar se estes módulos aqui estão selecionados, e eu vi que estão. Muito estranho. Selecionei de volta tudo que lembro que tinha tirado. Vamos compilar de novo com &lt;code&gt;make -j4&lt;/code&gt; .... (5 min later), pronto, agora é fazer outra vez &lt;code&gt;make modules_install&lt;/code&gt; e &lt;code&gt;make install&lt;/code&gt; e &lt;code&gt;grub-mkconfig&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Pra garantir, acho que não faz diferença pra agora, mas vamos aproveitar pra instalar o pacote &lt;code&gt;open-vm-tools&lt;/code&gt;, que é a versão open source do antigo pacote proprietário &lt;code&gt;vmware-tools&lt;/code&gt;, as ferramentas da Vmware pra Guests. Ele configura coisas como resolução da tela, rede, clipboard compartilhado com o host, sincronia da hora com o host. Talvez tenha algum driver que falta. Pra instalar é só fazer &lt;code&gt;emerge --ask app-emulation/open-vm-tools&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Se fosse systemd como num Ubuntu, precisaria fazer &lt;code&gt;systemctl start vmware-tools&lt;/code&gt; mas como escolhi o stage3 com OpenRC precisamos fazer &lt;code&gt;rc-service vmware-tools start&lt;/code&gt; e olha só, dinamicamente carregou o driver de video da Vmware e ajustou a resolução da máquina. Bom sinal. Pra garantir que vai carregar no próximo boot, só fazer &lt;code&gt;rc-service add vmware-tools&lt;/code&gt;. Mas tá vendo, systemd ou openrc, pro usuário final, é mais diferença de sintaxe entre os comandos systemctl e o rc-service.&lt;/p&gt;

&lt;p&gt;Podemos sair, rebootar e ... Caraca, mesma coisa. Crash. Vamos bootar outra vez pelo LiveCD .. (5 min later) .. pronto. Tudo que a gente modifica naquela tela interativa do menuconfig no final é gravado no arquivo &lt;code&gt;.config&lt;/code&gt;. Deixa eu abrir direto ele. Comparando com a Wiki, ele fala NET_VENDOR_AMD, vamos procurar e sim, tá yes. AMD8111_ETH, tá yes também. Vamos checar um a um (5 min later), FUSION ... ué, tá comentado. Mas eu juro que no menuconfig o Fusion MPT que é driver de Scsi Host, tava habilitado. Estranho, vamos descomentar e dar yes.&lt;/p&gt;

&lt;p&gt;Gravamos e damos &lt;code&gt;make -j4&lt;/code&gt; de novo. E olha só, ele vai me dando opção de configurar outras opções novas, como Fusion SPI, que também precisa estar habilitado. Vamos lá então. Vai continuar a compilação como antes. No final, repetimos a mesma sequência, &lt;code&gt;module_install&lt;/code&gt;, &lt;code&gt;install&lt;/code&gt;, &lt;code&gt;grub-mkconfig&lt;/code&gt; ... saímos com exit, e reboot com dedos cruzados.&lt;/p&gt;

&lt;p&gt;Máquina rebootando ... e caraca, agora passou!! Como eu mexi mais coisas além do Fusion SPI, não sei se foi isso que resolveu ou não, mas pqp. Fiquem espertos, talvez vocês rodem num Vmware também e não tenham problema nenhum, talvez tenham problemas diferentes, mas enfim, vai ter problemas desse tipo e vai ter que ficar indo e voltando como eu fiz. Agora sim, termina o boot e chega até a o prompt de login em modo texto, ou seja, runlevel 3 de multi-user. Vamos instalar uma interface gráfica pra gente.&lt;/p&gt;

&lt;p&gt;Entramos com &lt;code&gt;sudo su&lt;/code&gt; pra continuar como root. Gentoo tem conceito de profiles, sets e coisas assim que ainda não sei muito bem os detalhes. Mas segundo o Wiki, preciso me certificar que estou no profile de desktop gnome com openrc. Pra isso posso ver os profiles disponíveis com &lt;code&gt;eselect profile list&lt;/code&gt;. Seleciono o certo com &lt;code&gt;eselect  profile set default/linux/amd64/17.1/desktop/gnome/systemd&lt;/code&gt;. Pronto.&lt;/p&gt;

&lt;p&gt;Antes de continuar, vamos mexer na configuração do Portage em &quot;/etc/portage/make.conf&quot;. Tinha faltado INPUT_DEVICES. Colocamos libinput e synaptics que é pra touch pads. E no final configuramos o parâmetro USE pra ignorar qt5 e kde e prioriza X, gtk e gnome. E é isso. Pra instalar o gnome é só fazer &lt;code&gt;emerge --ask gnome-base/gnome&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mas, já temos um erro. No caso um erro de dependência circular. Vira e mexe vai esbarrar nisso em Gentoo e não tem uma resposta automática. Nesse caso tem dois sistemas de áudio concorrentes e ele não sabe qual instalar. Mas como falei antes, leia a porra do erro. Já diz que é ou pra escolher &quot;minimal&quot; ou &quot;pulseaudio&quot;. Pra corrigir basta mudar o comando pra &lt;code&gt;USE=&quot;minimal&quot; emerge --ask gnome-base/gnome&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Só que deu erro de novo. Leia com atenção, é um erro diferente. Agora fala que não consegue instalar o pacote &lt;code&gt;x11-base/xorg-server&lt;/code&gt;. Gnome, Kde, Xfce e qualquer gerenciador de janelas roda em cima do servidor gráfico X11, ou em distros mais novas, em cima do Wayland. Então precisa instalar o X11, só que o erro diz que ele não consegue instalar com essa flag &quot;minimal&quot; que acabamos de colocar. Uma das formas de resolver é instalar o xorg-server sozinho com &lt;code&gt;emerge --ask x11-base/xorg-server&lt;/code&gt;. Não sei se é a melhor forma mas, é isso, tá instalando. Temos que esperar alguns minutos porque o X11 é bem grandinho e compilar leva tempo ... (5 min later)&lt;/p&gt;

&lt;p&gt;Com o xorg instalado, podemos tentar instalar o gnome de novo com o mesmo comando de antes e agora passa! Baixa o código fonte, começa a compilar e pode ir almoçar, vai na academia, vai ler um livro. Sério. Um gerenciador de janelas, Gnome, Kde ou qualquer um é composto por dezenas de pacotes, milhões de linhas de código, e o Gentoo vai compilar tudo do zero.&lt;/p&gt;

&lt;p&gt;Eu configurei esta máquina virtual com 2 cores e 2 threads e 8 giga de RAM, uma configuração bem fraquinha. Se estivesse nativamente no meu Ryzen 9 de 32 threads com 64 giga de RAM, ia acabar em poucos minutos, mas numa máquina fraca dessas, vai levar mais de 2 horas fácil. Nem tô brincando ... Até logo, a gente se vê mais tarde&lt;/p&gt;

&lt;p&gt;(1 year later ...)&lt;/p&gt;

&lt;p&gt;Ufa, parece que agora terminou. Caraca, sério, é muito tempo. Nem tô brincando. Vamos no wiki ver o que falta fazer. Ele manda atualizar as variáveis de ambiente com &lt;code&gt;env-update &amp;amp;&amp;amp; source /etc/profile&lt;/code&gt;. Daí manda colocar nosso usuário no grupo plugdev com &lt;code&gt;getent group plugdev&lt;/code&gt; primeiro pra checar, e sim, tá lá. Então &lt;code&gt;gpasswd -a akitaonrails plugdev&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finalmente, precisamos habilitar o DM, no caso o GDM. E o GDM precisa do elogind no boot. Opa, falei grego, eu sei. Que diabos é DM? É acrônimo pra Display Manager ou, mais corretamente, Login Manager ou gerenciador de Login. É aquela primeira tela gráfica que pede seu login e senha depois do boot. É ele que se encarrega de carregar o GNOME propriamente dito. Por padrão o X carrega o XDM, então precisamos trocar pro GDM porque o XDM não consegue carregar Gnome.&lt;/p&gt;

&lt;p&gt;Pra isso primeiro precisamos configurar o serviço elogind pra carregar antes no boot com &lt;code&gt;rc-update add elogind boot&lt;/code&gt;. Agora precisamos instalar o serviço que inicia o display manager com &lt;code&gt;emerge --ask --noreplace gui-libs/display-manager-init&lt;/code&gt; e editamos o arquivo &lt;code&gt;/etc/conf.d/display-manager&lt;/code&gt; com vim. Tá vendo? Lá embaixo o padrão é &quot;xdm&quot; então trocamos pra &quot;gdm&quot; e salvamos.&lt;/p&gt;

&lt;p&gt;Por fim, configuramos o serviço pra iniciar automático, colocando no runlevel padrão com &lt;code&gt;rc-update add display-manager default&lt;/code&gt;. Teoricamente é só isso, só rebootar agora ... e ... falhou, por alguma razão ainda tá tentando carregar o XDM, que não existe, e sai fora. E lá vamos de novo investigar. (5 min later) Fiquei fuçando aqui e checando se aquele arquivo de configuração tá com &quot;gdm&quot; mesmo, o serviço elogind também carregou normalmente, tá difícil.&lt;/p&gt;

&lt;p&gt;Mas achei estranho que também não encontrei o gdm. Uma hipótese é que inicializador tenta carregar o GDM, não acha, e como alternativa tenta carregar o XDM, que também não tá instalado. Imaginei que o pacote &quot;gnome-base/gnome&quot; já automaticamente instalava, mas pelo jeito não, ou deu algum erro que eu passei batido e não vi, então vamos instalar com &lt;code&gt;emerge --ask gnome-base/gdm&lt;/code&gt;. De novo, hora de mais um café, porque esse também demora pra compilar ... (1 year later)&lt;/p&gt;

&lt;p&gt;(levanta .. CONTINUA ... volta ...)&lt;/p&gt;

&lt;p&gt;Dessa vez estou cortando fora as várias horas que eu levei apanhando do GDM. Na real, da dependência do GDM e do GNOME que é o webkit-gtk. Puta pacote bosta. Ele ficava dando erro, eu tentava de novo e nada. Rechequei configuração, flags de compilação e gastei um tempo lendo fóruns que achei no Google, até num deles tive um insight. Lembra que falei que se tentar compilar muito agressivo, numa máquina fraca, pode faltar RAM? Acho que foi isso, o compilador crasheava por falta de memória, dava erro e parava. Só que o compilador não diz que faltou memória, ele fala que deu erro numa linha de um script que voltou false.&lt;/p&gt;

&lt;p&gt;Mas sim, voltou false porque deu crash. Aumentei pra 16 giga e vamos compilar de novo. Nos vemos em duas horas ... (2 hours later) …&lt;/p&gt;

&lt;p&gt;Será que agora vai? Opa, sem erro na deteção do GDM e ... e ... ha! Carregou o GDM!! Vamos logar ... opa, senha errada ... de novo. Pronto! Caraca, um dia inteiro pra no fim descobrir que era falta de memória. Vai se fuder. E olha só, isso é um GNOME zerado, sem nenhuma opção extra, sem nenhum aplicativo pré-instalado. Agora eu posso instalar exatamente só os programas gráficos que eu quiser, e não ter que ficar com aquele monte de lixo que toda distro já pré-instala pra mim. Eu não preciso de cliente de email, não preciso de gerenciador de foto. Nesse sentido, eu gosto disso de poder instalar minhas próprias coisas.&lt;/p&gt;

&lt;p&gt;A moral da história é: não tente instalar um gerenciador de janela pesado como GNOME numa máquina fraca com menos de 8 giga de RAM. Provavelmente tinha flags de GCC que eu podia ficar mexendo, mas na prática só ia deixar a compilação ainda mais lenta. E sério, puta componente lento de compilar. Toda vez que tentava de novo era fácil mais de 1 hora até terminar em crash. Configurar um Gentoo não é pros fracos. E se você tem máquina fraca, escolha um gerenciador de janela mais leve, como XFCE. Apesar que a culpa nem foi do GNOME, foi do webkit. De curiosidade, vocês sabem o que é Webkit?&lt;/p&gt;

&lt;p&gt;Webkit é o framework de navegadores, a engine de HTML, CSS que roda por trás do Safari de Mac e que inicialmente estava nos Chrome. Acho que o Chrome é meio que um fork ou rewrite de Webkit e evolui separado. Mas o Webkit não é o navegador, ele é usado dentro de navegadores como o Konqueror, e todo aplicativo desktop que exibe HTML, como cliente de email. Justamente por isso é provavelmente o componente mais pesado que você vai instalar na sua máquina. Sério, olha o tanto de hora que leva pra compilar essa porra. Uma engine de web não é um troço pequeno, porque a web hoje é desnecessariamente complicada. Valeu Apple e Google, por tornar a web pior.&lt;/p&gt;

&lt;p&gt;Aliás, falei que não veio nenhum aplicativo instalado aqui. Inclusive não tem nem loja de aplicativos e nem terminal. Como que eu instalado um terminal sem ter o terminal? Lembrem dos meus videos anteriores, vocês tem 5 segundos pra adivinhar. 4, 3, 2, 1 ... vamos mudar pro runlevel 3. Ctrl + Alt + F3. Pronto, de volta pro console em modo texto. Agora daqui eu posso instalar qualquer aplicativo de terminal que eu quiser, como o Alacritty, que é meu preferido. E quando terminar, volto pro modo gráfico com Ctrl + Alt + F5.&lt;/p&gt;

&lt;p&gt;Enfim, finalmente, temos GNOME bootando bonitinho neste Gentoo quentinho saído do forno, configurado com OpenRC em vez de systemd pra gente ter um ambiente pra estudar melhor OpenRC e se é melhor mesmo do que o Systemd. Falando nisso, tem essa outra página no Wiki que fala de GNOME sem Systemd. Lá explica que antes dava pra carregar GNOME sem problemas tanto com systemd quanto openrc. Mas isso mudou.&lt;/p&gt;

&lt;p&gt;Como expliquei no episódio anterior, o systemd faz mais do que só gerenciar resolução de dependências e start de serviços. Agora gerencia também coisas como economia de energia e outras atividades do sistema, inclusive gerenciar o diretório /dev de dispositivos, lembra dele? Por causa disso surgiram dois projetos que extraem essas coisas do systemd, projetos como elogind que instalamos antes e eudev. Assim é possível usar só essas partes do systemd e substituir por OpenRC e conseguir subir o Gnome.&lt;/p&gt;

&lt;p&gt;Essa é outra das críticas do systemd: que ele faz muito mais do que deveria. Um sistema de init deveria só gerenciar inicialização. Software pra controlar &quot;/dev&quot; deveria ser separado, em outro pacote, de tal forma que desse pra instalar um, sem instalar o outro. Mas o systemd é um código-fonte monolito, ou compila tudo junto, ou nada funciona.&lt;/p&gt;

&lt;p&gt;Pra extrair projetos como elogind e eudev, dá um puta trampo, porque toda vez que o systemd atualiza e muda as coisas, esses outros projetos precisam extrair de novo o que mudou pra nada quebrar. É realmente uma grande perda de tempo e consigo entender porque muitos desenvolvedores de Linux estão putos com isso.&lt;/p&gt;

&lt;p&gt;De qualquer forma, pra gente que contribui no desenvolvimento de distros, é bom saber que essa discussão existe e se tiver interesse depois procura fóruns de desenvolvimento pra ver como as coisas estão evoluindo. Eu pessoalmente não tenho muito interesse porque não contribuo com software livre, mas quem contribui precisa entender essa novela. No mínimo é um bom estudo pra saber como a comunidade lida com conflitos assim.&lt;/p&gt;

&lt;p&gt;Sobre o Gentoo em si. É um excelente exercício se desafiar em controlar essa besta selvagem. É o extremo oposto de instalar um Ubuntu da vida. Você realmente vai perder muitas horas, muitos dias, em tentativas e erros, lendo muita documentação no Wiki deles, e no mínimo serão horas de muito estudo e aprendizado que uma hora vai ser útil em outras distros de Linux. Todo mundo que tem alguma vontade de brincar no mundo de devops, esse é o hello world do hello world. Se não consegue nem se virar com a instalação de um Gentoo, que tem documentação bem organizada, imagina clusters de servidores mais avançados ou sistemas embarcados mais restritos.&lt;/p&gt;

&lt;p&gt;Porém, sobre a filosofia de sempre compilar tudo do zero, acho que é uma faca de dois gumes. Pensando exclusivamente no meu caso. Eu tenho uma máquina parruda, um Ryzen 9 com 64 giga de RAM e todo o espaço em disco que eu precisar no meu NAS de 10 gigabits, com velocidade superior a SSDs. Pra fazer os últimos episódios instalei dúzias de máquinas virtuais de teste. No final, o Gentoo é o que realmente dá sensação de mais responsivo, um pouco mais rápido, e isso provavelmente se deve a ter compilado tudo exclusivamente pra minha máquina, eliminando muita gordura no processo.&lt;/p&gt;

&lt;p&gt;Mas como falei antes, instalar X11, Gnome, GDM, na verdade o pacote webkit, custou literalmente umas 3 horas, a CADA tentativa, e até eu descobrir o problema, foram VÁRIAS tentativas. Foi quase um dia inteiro repetindo esse processo até resolver o erro. Enquanto ficava esperando, escrevi quase o script inteiro desse episódio, com tempo sobrando pra revisão. Não tô brincando, é muito tempo. Tudo bem que foi numa máquina virtual. Se fosse no meu hardware nativo, num dual boot, teria sido ordens de grandeza mais rápido. Mas é justamente esse o ponto.&lt;/p&gt;

&lt;p&gt;Quem tem hardware parrudo como o meu, não se importa tanto se tá ganhando 1 milissegundo a mais de performance por ter compilado tudo nativo. Os milissegundos que vou ganhar, não compensam a espera em compilar tudo do zero toda hora. Minha máquina já é rápida o suficiente. Mesmo se o binário for mau feito e pesado, vai rodar de boas. Quem realmente precisa da otimização é quem tem máquinas mais lentas ou antigas. Só que agora a compilação também vai ser ordens de grandeza mais lento, nessa faixa de levar mais de 3 horas pra conseguir instalar um simples GNOME.&lt;/p&gt;

&lt;p&gt;Ou seja, quem mais precisa da performance extra, vai toda hora perder muita produtividade quando precisar instalar pacotes, porque todo novo software, precisa baixar o código fonte e esperar compilar tudo. Nem sempre é demorado assim, a maioria dos softwares são pequenos. Mas imagina agora quero instalar Firefox, lá se vai mais uma ou duas horas. Agora preciso instalar Ruby, ou Python, mais 1 hora. Você tem que ser disciplinado de não tentar instalar nada grande no meio do expediente, senão acabou o dia pra você. Sério. É meio duvidoso até onde tem vantagem.&lt;/p&gt;

&lt;p&gt;O benefício de um Gentoo vem quando você tem uma máquina fraca que não vai ficar usando no dia a dia. Por exemplo, um PC fraco que quer usar como servidor multimídia, ou um roteador de rede, ou um NAS. Uma máquina com um único propósito, idealmente sem nem precisar de interface gráfica, que é a compilação mais pesada. Por isso falei em aplicações de sistemas embarcados, ou IoT. Aí sim, você precisa economizar o máximo de recursos, e quando terminar de instalar, não vai ficar adicionando mais nada. Um Gentoo é perfeito pra isso.&lt;/p&gt;

&lt;p&gt;Outro objetivo dos exercícios dos últimos videos é mostrar o seguinte: qual é a melhor distro? Não existe essa resposta. Todas são boas, e em todas roda praticamente todos os mesmos software, é tudo software livre. Em alguns dá mais trabalho ou menos trabalho instalar esse ou aquele software, mas na prática dá no mesmo. Se tiver um dispositivo periférico muito exótico, no final vai ter que compilar kernel na mão de qualquer jeito, e em qualquer distro pode fazer isso. Não é só em Gentoo que pode customizar kernel. No Ubuntu também dá. Só baixar o código fonte, subir o menuconfig, compilar e mandar o grub configurar no boot.&lt;/p&gt;

&lt;p&gt;Pra quem é iniciante, povo recomenda instalar Ubuntu ou PopOS da vida porque são triviais, por causa do Calamares, e por serem populares, tem documentação de tudo na internet. Tem fóruns de discussão ativos. Tem salas de chat com gente discutindo a respeito. Vai ser mais fácil de achar solução pra qualquer problema neles. Por outro lado, se usar um distro dessas, não vai aprender nada sobre Linux. Clicar com mouse e arrastar janela não te ensina absolutamente nada. Esquece colocar no currículo &quot;Sei Linux&quot; só porque instalou um Ubuntu. Sabe bosta nenhuma. Só sabe abrir o Chrome e navegar. Isso você faz em Windows ou Mac igual.&lt;/p&gt;

&lt;p&gt;Saber Linux é no mínimo estar confortável com tudo que eu fiz nos últimos videos. Saber o que é chroot, o que são pontos de montagem, o que é escalonamento de privilégios, como se customiza a kernel, como configura um bootloader, como diagnostica dependências de binários, como monta um pacote. Olha só tudo que a gente viu até agora. Isso minimamente &quot;saber Linux&quot;, e o que mostrei até agora, foi só o hello world, os conceitos básicos. Começa a ler a Wiki do Gentoo ou Arch. Aí você vai ver o quanto ainda falta pra realmente &quot;saber Linux&quot;.&lt;/p&gt;

&lt;p&gt;Por isso, sim, tente instalar Slackware, tente instalar Gentoo, tente instalar Arch. Esse é o exercício que todo aprendiz de Linux precisa passar. O fim de semana inteiro varando noite de sábado pra domingo, saindo de uma mensagem de erro pra outra mensagem de erro, até ter a satisfação de ver tudo bootando e funcionando no final. Aí sim você sabe que aprendeu alguma coisa de verdade.&lt;/p&gt;

&lt;p&gt;Aliás, eu até entendo porque no final tem um grupo de pessoas que defente um Gentoo com unhas e dentes, porque depois de tantas horas e dias dedicado em fazer tudo funcionar na sua máquina, horas e horas esperando tudo compilar e tendo que repetir tudo de novo quando dá erro. Dá muita dó jogar fora e mudar de distro. É meio que síndrome de Estocolmo, já sofreu tanto, agora fica com ele. Mas eu sou um psicopata altamente funcional. Gastei dias configurando tudo, no final jogo tudo fora e pulo pra outra coisa. Quem consegue desapegar de coisas que investiu horas em cima, sempre vai ter mais vantagem, pense nisso.&lt;/p&gt;

&lt;p&gt;E o assunto ainda não chegou ao fim, o próximo video vai ser mais conceitos de Linux que você não sabe. Então, se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e compartilhem o video com seus amigos. A gente se vê, até mais!&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5997</id>
    <published>2022-09-25T12:49:00-03:00</published>
    <updated>2022-09-25T12:53:22-03:00</updated>
    <link href="/2022/09/25/akitando-128-entendendo-pacotes-com-slackware-deb-apt-tarbals" rel="alternate" type="text/html">
    <title>[Akitando] #128 - Entendendo Pacotes com Slackware | Deb, Apt, Tarbals</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/iQkBbRPkASo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Como gerenciadores de pacotes como APT funcionam? O que tem num pacote? Vamos instalar um Slackware, uma das distros de Linux mais antigas, e ver como evoluímos de gerenciar tarballs pra gerenciar resolução de dependências com pacotes modernos como formato DEB.&lt;/p&gt;

&lt;p&gt;Hoje você vai começar a entender como software num Linux é organizado de verdade.&lt;/p&gt;

&lt;p&gt;== Conteúdo&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;01:21 - CAP 1 - Como nós, reviewers, trapaceamos&lt;/li&gt;
&lt;li&gt;04:15 - CAP 2 - Instalação como nos Anos 90: Slackware&lt;/li&gt;
&lt;li&gt;06:24 - CAP 3 - Particionando o Drive: CFDISK&lt;/li&gt;
&lt;li&gt;10:32 - CAP 4 - Filesystems e Mount Points&lt;/li&gt;
&lt;li&gt;16:36 - CAP 5 - Como APT instala pacotes? Formato DEB&lt;/li&gt;
&lt;li&gt;25:14 - CAP 6 - Formato de Pacotes Original: pacotes no Slackware&lt;/li&gt;
&lt;li&gt;31:57 - CAP 7 - Mantenedores de Distros = Mantenedores de Pacotes Binários: SlackBuilds.org&lt;/li&gt;
&lt;li&gt;40:31 - CAP 8 - Slackware nos dias de hoje: Nostalgia&lt;/li&gt;
&lt;li&gt;43:02 - Bloopers&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;== Links&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Build &amp;amp; Install Slackware Packages Automatically (https://medium.com/netdef/build-install-slackware-packages-automatically-b2986d2f86f9)&lt;/li&gt;
&lt;li&gt;SlackBuilds.org (https://slackbuilds.org/)&lt;/li&gt;
&lt;li&gt;SBOPKG (https://sbopkg.org/)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Vamos continuar a minissérie de Linux falando mais sobre o que tem por baixo do capô. No video anterior vimos um pouco sobre como é a sequência de boot, desde ligar o computador até chegar na tela de login. E hoje quero apresentar mais conceitos de Linux destacando alguns trechos da instalação da distribuição Linux mais antiga ainda em funcionamento hoje: o lendário Slackware. Vamos lá.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Eu vinha dizendo em redes sociais e até alguns videos anteriores que eu recomendo Arch Linux pra você começar a aprender um pouco mais sobre como Linux funciona. Tanto porque a instalação de um Arch é menos fácil que de um Ubuntu ou Fedora, mas também porque o Wiki do Arch é uma das documentações online mais completa sobre todos os aspectos de um Linux.&lt;/p&gt;

&lt;p&gt;Distros modernas como Ubuntu, PopOS, Mint e coisas assim são hiper triviais de instalar, graças a LiveCDs e instalador gráfico como Calamares, que já fazem o grosso do trabalho pra você. Mas mesmo o Arch não é tão complicado assim e na verdade ele já esconde muito da complexidade. Portanto, instalar Linux moderno hoje em dia não ensina quase nada, a menos que seu hardware seja muito exótico e tenha pouco suporte, daí você vai ser obrigado a descer mais fundo nas suas pesquisas até conseguir fazer tudo funcionar.&lt;/p&gt;

&lt;p&gt;Canais de reviews também trapaceiam um pouco, e eu vou fazer a mesma coisa. A coisa mais chata na instalação de qualquer sistema operacional, seja Windows ou qualquer distro Linux, é suporte a hardware porcaria que não tem drivers bons disponíveis. Dispositivos de marcas chinesas duvidosas compradas numa Shopee ou AliExpress, webcam baratinha que, quando você conecta, seu sistema nem detecta automaticamente e coisas assim. O maior problema em qualquer sistema operacional é ter bons drivers que desestabilizem tudo.&lt;/p&gt;

&lt;p&gt;Mas a trapaça que eu falo é que a gente que grava video, não vai ficar testando instalação em vários tipos diferentes de combinações de hardware. Normalmente temos uma máquina separada só pra isso que não é nosso PC primário. Claro, sem nenhum componente muito exótico, que a maioria dos instaladores vai auto detectar de primeira sem problema. Ou vamos usar máquinas virtuais. E a vantagem de usar um VirtualBox, VMWare, QEMU/KVM é que o hardware virtual dessas máquinas é bem documentado e tem bons drivers disponíveis. Qualquer instalador de Linux detecta que tá numa máquina virtual e instala os corretos sem nenhum problema. Por isso que você assiste video de canal de YouTube e parece que foi tão fácil.&lt;/p&gt;

&lt;p&gt;Entenda que hoje em dia, mesmo Windows, até que funciona direitinho. Quando você tem problemas, normalmente são drivers ruins, ou atualização automática que baixou uma versão nova de driver com bugs. Campeão de bugs são drivers de placas de video como NVIDIA. Saem atualizações com bastante frequência, são pacotes grandes, e vira e mexe chega um bug novo que te fode. Gamers sabem bem o que é isso. E por isso também notebooks de boas marcas costumam ter poucos problemas, porque os fabricantes tendem a arredondar bem os drivers dos componentes da máquina, e fazem pouca ou nenhuma atualização depois que tudo já funciona.&lt;/p&gt;

&lt;p&gt;Aí você só vai ter problema se o sistema operacional ganhar um upgrade que quebra compatibilidade com seus drivers. Mas no fim, o problema sempre são drivers. Por isso sempre pesquise se todos os componentes que você compra costuma ter drivers disponíveis que são atualizados com uma frequência razoável pra corrigir bugs. Se você tá pesquisando, digamos, uma placa de captura de video, vai no site deles. Se descobrir que a última vez que eles lançaram um driver novo foi uns 3 anos atrás, desiste dessa compra.&lt;/p&gt;

&lt;p&gt;Isso tudo dito, eu já tinha mostrado no video anterior uma instalação rápida de Ubuntu numa máquina virtual, e em poucos minutos tava tudo detectado e instalado. Zero problemas. Por isso quero instalar um Slackware hoje. Slackware foi a primeira distro que eu tentei instalar, lá por volta de 1995, acho que ainda era 1.0, que vinha num CD junto com um livro importado ensinando sobre Linux. Eu acho que na época não consegui instalar e foi super frustrante, eu só voltei a tentar Linux de novo quando um amigo da faculdade me emprestou um Red Hat versão 4, com um instalador um pouco menos difícil e consegui ir até subir o X pela primeira vez.&lt;/p&gt;

&lt;p&gt;Se você só instalou Linux modernos, hoje vai ter um choque, porque o Slackware moderno ainda é muito parecido com o que eu lembro de mais de 20 anos atrás. E como disclaimer, o video de hoje não é um tutorial. Não dá pra seguir passo a passo. Quero aproveitar a instalação pra ir destacando alguns conceitos que acho importante saber. Então vamos bootar o Slackware numa máquina virtual, e logo de cara você já vai tomar um susto. Não abre interface gráfica, cai direto numa linha de comando. E sim, LEIA a porra dos textos que aparecem. 90% dos erros de iniciantes é ser preguiçoso e não ler com atenção o que tá escrito. Se alguma coisa escreve na sua tela é porque você deveria ler.&lt;/p&gt;

&lt;p&gt;Pessoal pensa que LiveCDs que são esses Linux que bootam de pendrive servem só pra instalar a distro no seu HD. Mas na realidade, todo LiveCD tem uma segunda utilidade: ser usado pra conseguir recuperar o Linux do seu PC caso faça alguma bobagem e não consiga mais bootar dele. Essa máquina virtual, por exemplo, tá vazia, mas já estamos dentro de um ambiente Linux. Como ele explica aqui no texto, daqui você pode montar o HD da sua máquina que já tinha uma instalação e no mínimo ter acesso aos seus arquivos, caso não esteja conseguindo bootar normalmente. Aqui ele ensina como fazer isso.&lt;/p&gt;

&lt;p&gt;Damos enter e olha o que expliquei no episódio anterior: executa o binariozão da kernel do Linux, cria uma partição em RAM que é o initramfs e descomprime a imagem initrd nela pra fazer o segundo estágio do boot. Aí a kernel executa um gerenciador de serviços, um init system, que é processo número 1 que vai iniciar os serviços do runlevel certo. Lembram? Expliquei tudo isso no video anterior. E quando terminar o boot, vai perguntar se quer trocar o layout do seu teclado, caso esteja usando um layout brasileiro, por exemplo.&lt;/p&gt;

&lt;p&gt;Como é um LiveCD, tem só o usuário root sem senha e de novo, leia a porra do texto, porque já te dá a dica que o próximo passo pra continuar é que vai precisar criar uma partição e formatar antes de conseguir instalar qualquer coisa. E uma coisa que imagino que ninguém vai ter é problema de pouca memória, mas digamos que seu PC seja tão velho que tenha menos de 4 gigas de RAM, daí ele recomenda também criar uma partição de swap. E vamos fazer as duas coisas agora.&lt;/p&gt;

&lt;p&gt;Pra criar partições, nos anos 90 eu seria obrigado a usar a antiga ferramenta &lt;code&gt;fdisk&lt;/code&gt;. Era horrível, porque precisava calcular manualmente a quantidade de megas baseado na multiplicação de coisas como tamanho de setor, cilindros, cabeças, coisas que expliquei no video sobre partições. Mas hoje em dia temos uma ferramenta melhor, o bom e velho &lt;code&gt;cfdisk&lt;/code&gt;. A opção mais padrão agora é escolher a etiqueta de GPT, que também explico no video que falei.&lt;/p&gt;

&lt;p&gt;Normalmente um Linux dá o nome de &quot;/dev/sda&quot; pro primeiro HD que detecta. Se tiver um segundo HD o nome seria &quot;/dev/sdb&quot;. No caso temos 80 giga de espaço livre e vamos criar a primeira partição que vai se chamar &quot;/dev/sda1&quot;. Com setinhas, mudo os itens de menu abaixo, escolho &quot;new&quot; e digito que tamanho quero que essa partição tenha. Posso digitar &quot;100M&quot;, onde &quot;M&quot; quer dizer &quot;megabytes&quot;. Essa vai ser a partição de boot. 100 mega acho que chutei pouco, poderia ser mais. Pra estar seguro, digita uns 150 megas num PC de verdade.&lt;/p&gt;

&lt;p&gt;Daí no menu embaixo, movo pra direita pra escolher a opção de &quot;tipo&quot; e mudo pra &quot;EFI System&quot;. Tá vendo? Se você assistiu meus videos sobre armazenamento, tudo isso de fdisk, partição, tipos, EFI, já expliquei tudo antes. Agora eu movo a setinha pra baixo pra escolher o resto do espaço livre, escolho a opção de nova partição, e chuto uns 4G, onde &quot;G&quot; significa &quot;gigabytes&quot;. Essa vai ser a partição &quot;sda2&quot;, daí mudo o tipo pra ser Linux Swap. Não existe tamanho certo pra essa partição. Se tem bastante RAM, um swap pequeno só pra emergência tá bom. Se tem pouca RAM, recomendo comprar mais RAM. Pouca RAM é abaixo de 8 giga. Um swap de 4 giga ou menos tá de bom tamanho.&lt;/p&gt;

&lt;p&gt;Finalmente, selecionamos o espaço que sobrou e criamos a maior partição, que aqui vai ser a &quot;sda3&quot; com mais de 70G e escolhemos o tipo de &quot;Linux filesystem&quot;. Esses tipos são só etiquetas. Elas não mudam nada, é que nem uma hashtag de twitter. Só pra depois ficar mais fácil saber o que é cada partição. E notem como a ordem dessas partições não faz diferença. Só a partição de boot que é bom ser a primeira, mas eu poderia ter invertido e deixado a principal como &quot;sda2&quot; e o swap como &quot;sda3&quot;. Poderia ter criado uma quarta partição pra ser onde ficaria meu diretório home, pra facilitar coisas como backup e recuperação depois. Tem várias estratégias, mas o que fiz agora é o hello world, o mais simples. A maioria dos instaladores gráficos, por baixo dos panos, faz a mesma coisa sem você saber.&lt;/p&gt;

&lt;p&gt;Partição é só uma forma de deixar gravado nos primeiros bytes do HD que do byte X até o byte Y está reservado pra partição 1, do byte Y até o byte Z é a partição 2, e assim por diante. É só isso. Mas pra usar essas partições, o sistema operacional precisa saber que formato é pra usar dentro dela, e pra isso que formatamos. E em linux, cada sistema de arquivos tem uma ferramenta &quot;mkfs&quot; que é literalmente &quot;make filesystem&quot;, por exemplo, vamos formatar a primeira partição de boot &quot;sda1&quot; como FAT32 com o comando &lt;code&gt;mkfs.fat -F 32 /dev/sda1&lt;/code&gt;. Pronto, apesar desses avisos, não deu erro e agora tá formatado.&lt;/p&gt;

&lt;p&gt;A segunda partição &quot;sda2&quot; é o swap. Lembram? Swap é usado quando tamos ficando com pouca memória RAM, daí o Linux passa a mapear parte do disco como memória adicional pra não crashear tudo. Só que HD é sempre MUITO mais lento que RAM, por isso um bom sistema operacional como Linux só vai usar swap se realmente precisar muito. Swap só vale a pena se estiver usando um SSD NVME, que é rápido o suficiente pra não deixar sua máquina muito lenta. De qualquer forma, precisamos formatar essa partição com um sistema de arquivos especial que vai simular memória, e pra isso usamos o comando &lt;code&gt;mkswap /dev/sda2&lt;/code&gt; que é literalmente &quot;make swap&quot;.&lt;/p&gt;

&lt;p&gt;Mas não basta formatar, temos que dizer pro sistema que é pra usar como swap e pra isso tem o comando &lt;code&gt;swapon /dev/sda2&lt;/code&gt;, que é literalmente &quot;swap on&quot;, de ligar. Se quisermos desligar pro sistema não usar mais essa partição tem o comando oposto que é &lt;code&gt;swapfoff&lt;/code&gt;. E pronto, temos swap configurado. Finalmente, vamos formatar a partição principal que é onde o Slackware vai se instalar e onde vai ficar os arquivos do sistema. A opção padrão mais fácil é formatar em ext4.&lt;/p&gt;

&lt;p&gt;Se for um notebook, recomendo antes encriptar a partição com LUKS e dentro criar um volume formatado com ext4 ou usar um sistema de arquivos mais moderno como btrfs ou ZFS que começou a vir nos Ubuntu a partir da versão 19 ou 20. Vou falar um pouco mais sobre isso em outro video. Mas na dúvida, escolha ext4 que é o filesystem mais usado no mundo Linux e todo ferramental que vem em todas as distros já assume que está usando ext4. De novo, se nunca ouviu falar de ext4, btrfs, zfs, fat, é porque não assistiu meu video sobre sistemas de arquivos. Anota aí pra assistir depois.&lt;/p&gt;

&lt;p&gt;Pronto, agora o HD foi particionado, e cada partição foi formatada com sistemas de arquivos que um Linux entende. Agora precisamos dar acesso a essas partições. E a forma de fazer isso é criando um mount point. Vou falar mais de mount points no próximo video, mas por hoje entenda o seguinte. Um &quot;/dev/sda3&quot; é só o caminho pra um linguição de bits. O sistema não entende arquivos e diretórios, só entende ler blocos de bits e gravar blocos de bits. Lembra o que eu sempre falo? Um dispositivo de armazenamento é um block device, um dispositivo de blocos. O sistema operacional precisa carregar um intérprete pra traduzir conjuntos de blocos em arquivos. Esse é o papel de um sistema de arquivos como ext4.&lt;/p&gt;

&lt;p&gt;E pra carregar esse intérprete, fazemos um ponto de montagem. Um mount point é só um diretório onde digo: a partir deste diretório, interprete os blocos do dispositivo como uma árvore de diretórios. Use o formato ext4 pra ler e gravar diretórios e arquivos. Então podemos usar o comando &lt;code&gt;mount /dev/sda3 /mnt&lt;/code&gt;. Tá implícito aqui, mas o comando mais completo seria &lt;code&gt;mount -t ext4 /dev/sda3 /mnt&lt;/code&gt;. De qualquer forma, a partir de &quot;/mnt&quot; é a raíz da partição sda3.&lt;/p&gt;

&lt;p&gt;A partir daqui, tudo que gravar em &quot;/mnt&quot; na realidade vai gravar dentro da partição sda3. E dentro crio o diretório &quot;/boot/efi&quot;, onde vou montar a primeira partição de boot, o sda1 com &lt;code&gt;mount /dev/sda1 /mnt/boot/efi&lt;/code&gt;. Isso realmente não teria como você saber de cabeça se nunca configurou EFI. E, de novo, se não sabe a diferença de BIOS e EFI, de MBR e GPT, é porque não assistiu meu outro video sobre isso. Mas saiba que toda instalação automática de Linux tá criando esses mount points por baixo dos panos pra você. Só que o Slackware me obriga a fazer isso manualmente.&lt;/p&gt;

&lt;p&gt;Sendo honesto, eu não precisava ter formatado as partições na linha de comando com os comandos de &quot;mkfs&quot;. O Slackware tem um instalador rudimentar que sabe fazer essas coisas. Mas eu quis mostrar como é manualmente. Apesar que partição de boot EFI você ia ter que fazer na mão de qualquer jeito porque o instalador do Slackware não configura isso, então não fizemos nada muito extra. Mas agora podemos digitar &lt;code&gt;setup&lt;/code&gt; e vai abrir esse menu de opções. Na opção TARGET, olha só, detectou minhas partições e sabe que &quot;sda2&quot; é o swap porque no cfdisk demos o tipo de &quot;linux swap&quot;, lembra? Daí ele se oferece pra fazer o mkswap e swapon, mas como já fizemos isso antes, só dar &quot;não&quot; pra continuar.&lt;/p&gt;

&lt;p&gt;Agora ele achou a partição principal &quot;sda3&quot; e diz que vai usar pra ser o mount point &quot;/&quot; que é a raíz do sistema de arquivos, o diretório &quot;/&quot; principal. Se tivesse feito outras partições, aqui poderia escolher pra montar como &quot;/home&quot;, por exemplo. O que essa ferramenta tá fazendo é preparando o arquivo &quot;/etc/fstab&quot;, que é como o sistema init no boot sabe como montar as partições do seu HD automaticamente. De novo, ele se oferece pra formatar essa partição, mas como já fizemos isso, damos &quot;no&quot; de novo.&lt;/p&gt;

&lt;p&gt;Depois de ter as partições devidamente configuradas é hora de copiar os arquivos do sistema pra lá. A primeira opção é justamente copiar tudo a partir do seu pendrive que usou pra bootar, ou no caso da minha máquina virtual, direto a partir do arquivo de ISO que baixamos. Mas de curiosidade veja essas outras opções aqui embaixo. Ele tem opção de instalar a partir de um diretório compartilhado na rede ou via servidor de arquivos FTP. Quem é dos anos 90 vai se lembrar disso, porque nos anos 90 não ia ter como dar boot via pendrive como fazemos hoje. Pendrive nem existia ainda.&lt;/p&gt;

&lt;p&gt;Normalmente ia ter 2 disquetes ou floppy disks de 1.44 megabytes cada. Um era o disco de boot, que carrega a kernel e o segundo era o disco de root, que acho que é onde tinha o initrd e as ferramentas de instalação. Daí precisava ter o resto dos pacotes num CD ou DVD. Veja o tamanho deste arquivo de ISO, mais de 3 gigabytes. Um CD na época tinha menos de 700 megabytes, então esse Slackware ia precisar de mais de 4 CDs pra instalar completo.&lt;/p&gt;

&lt;p&gt;Lá pelo meio dos anos 90, DVD também não era acessível. Então íamos precisar de 2 disquetes, mais uns 4 CDs. Mas se estivéssemos numa boa faculdade ou empresa moderna, talvez tivesse os pacotes num servidor FTP. Daí dava pra instalar pela rede em vez de ter que ficar trocando CD. A gente que curtia Linux tinha tubos com dezenas de CDs. E CD é lento, levava horas pra instalar. Por isso eu falo que hoje tá fácil. Em menos de 1 hora dá pra sair do zero pra um Linux todo configurado e funcionando. Bons tempos.&lt;/p&gt;

&lt;p&gt;Enfim, agora é só ir escolhendo as opções padrão, que é basicamente não ficar tirando nada e deixar o Slackware instalar tudo completo. O objetivo todo de instalar Slackware foi justamente pra falar um pouco sobre os primórdios do que chamamos de pacotes. Olha só o instalador, ele tá nervoso instalando um pacote atrás do outro, mas o que diabos é isso de pacote? Vamos fazer uma tangente.&lt;/p&gt;

&lt;p&gt;Se você é programador e já usou um Ubuntu da vida, deve estar acostumado a copiar e colar comandos como &lt;code&gt;apt install docker&lt;/code&gt;, acender uma vela, e torcer pra não dar nenhum erro, porque você não tem idéia do que fazer se der erro. Se estiver num Fedora, já deve ter digitado &lt;code&gt;dnf install docker&lt;/code&gt;, se estiver num openSuse, digita &lt;code&gt;zypper install docker&lt;/code&gt;, ou se estiver num Arch Linux vai ser &lt;code&gt;yay -S docker&lt;/code&gt;. Apt, Dnf, Zypper, Yay, Pacman, Portage, Apk, são o que chamamos de &quot;gerenciadores de pacotes&quot;, justamente porque eles sabem como baixar e instalar ou desinstalar os softwares que vem dentro desses pacotes. Cada distro diferente costuma ter gerenciadores diferentes.&lt;/p&gt;

&lt;p&gt;Slackware vem de uma época que precede o conceito de gerenciar pacotes. Vamos entender pacotes primeiro. Enquanto o Slackware fica ali instalando pacotes, vamos abrir outra máquina virtual com o Ubuntu 22 que mostrei no episódio anterior. Eu tenho instalado aqui um programa que todo programador já deve ter visto, o htop. Olha só ele mostra informações do sistema, quanto de carga cada core da minha CPU tá puxando, quanto de memória tá sobrando, e embaixo uma lista com todos os processos rodando na máquina, quanto de recursos cada um tá usando. É uma excelente ferramenta de monitoramento pra você saber o que tá acontecendo na sua máquina.&lt;/p&gt;

&lt;p&gt;Onde fica o binário executável do htop? Do terminal podemos usar o comando &lt;code&gt;which htop&lt;/code&gt;, que vai vasculhar os PATHs que tão configurados no seu shell e ele acha o binário em &lt;code&gt;/usr/bin/htop&lt;/code&gt;. Se não tivesse essa variável PATH configurada, precisaria digitar o caminho completo &lt;code&gt;/usr/bin/htop&lt;/code&gt; pra executar. O PATH é uma conveniência pra facilitar nossa vida na hora de digitar comandos. Enfim, o htop não tá todo contido nesse binário. Ele tem dependência com outros objetos binários, bibliotecas instaladas no sistema. A gente pode saber quais são usando o comando &lt;code&gt;ldd /usr/bin/htop&lt;/code&gt; e olha só, ele depende desse objeto binário libtinfo.so.6, a mais comum em todos os binários que é a libc ou libc.so.6 e assim por diante.&lt;/p&gt;

&lt;p&gt;Vamos estragar o htop. Deixa eu mover o libncursesw.so.6 pro diretório /tmp e tentar executar o htop ... e olha só, reclama que não consegue localizar a biblioteca compartilhada libcurses e crasheia. Então vamos mover o ncurses de volta pro lugar com o comando &lt;code&gt;mv&lt;/code&gt; inverso ... e pronto, agora o htop consegue carregar como antes. Mas o exercício aqui foi pra vocês terem noção que um executável tem o que chamamos de dependências. E resolução de dependências é um problema recorrente que vai acompanhar sua vida inteira de programador. Quanto mais cedo entender sobre isso, melhor.&lt;/p&gt;

&lt;p&gt;Agora, como instalar um software como htop? Simples, vai no terminal e digita &lt;code&gt;sudo apt install htop&lt;/code&gt; e pronto. Mas você sabe o que tá acontecendo? Já parou pra pensar nisso? Como esse comando &lt;code&gt;apt&lt;/code&gt; sabe de onde baixar e o que instalar no seu sistema? Deixa eu dar um resuminho. No caso de todo derivado de Debian, como um PopOS ou este Ubuntu, tudo começa no arquivo &quot;/etc/apt/sources.list&quot;, vamos dar &lt;code&gt;cat&lt;/code&gt; pra ver o conteúdo. E temos uma lista com várias URLs. Ele já foi inteligente pra deixar pré-configurado domínios brasileiros, que são servidores espelho, ou seja, cópias dos servidores oficiais da Canonical, que ficam aqui no Brasil, pra ser mais rápido fazer download.&lt;/p&gt;

&lt;p&gt;Quando você executa aquele comando que parece que não faz nada, o &lt;code&gt;apt update&lt;/code&gt;, o que ele faz é baixar um zip de um desses servidores listados. Esse zip é um arquivo texto com a lista de todos os pacotes oficialmente suportados pela Canonical. O que faz um programador bom de verdade? Ele é curioso, se tem uma URL, a gente abre. Vamos abrir aqui essa primeira URL e carrega uma página com diretórios. Agora navegamos aqui em dists, que acho que é distribuições. E dentro temos os codenomes das distros de Ubuntu. Como mostra aqui no arquivo sources.list ele declara pra procurar o codenome &quot;jammy&quot;. Sabemos que Jammy Jellyfish é o codenome do Ubuntu 22.&lt;/p&gt;

&lt;p&gt;Dentro temos o diretório &quot;main&quot;, depois &quot;binary-amd64&quot;, que são binários de 64-bits compatíveis com AMD ou Intel, então escolhemos ele e finalmente achamos um arquivo suspeito de 1.7 mega chamado &quot;Packages.gz&quot;. Vamos baixar. &quot;gz&quot; é a extensão de um arquivo compactado com &quot;gzip&quot;, então vamos pro terminal e fazemos &lt;code&gt;gunzip Packages.gz&lt;/code&gt; e bingo, descompacta um arquivo de 6 mega que podemos abrir num editor de textos como meu vim.&lt;/p&gt;

&lt;p&gt;Agora vamos procurar o htop que queremos, não é esse, não é esse outro, ah, achamos, olha só esse trecho. Várias informações sobre o htop, mas o que nos interessa é esse &quot;Filename&quot; aqui embaixo, que é o caminho pro pacote htop bla bla ponto deb. Mas onde fica esse diretório &quot;pool&quot;? Acho que já vi antes. Vamos voltando algumas páginas. Aha, olha só, tá aqui nesse servidor mesmo. A gente foi no &quot;dists&quot; lá em cima, mas tem esse &quot;pool&quot; aqui embaixo. Então vamos copiar a URL do servidor e ir pro terminal. Podemos usar a ferramenta &lt;code&gt;wget&lt;/code&gt; pra facilitar, colando o endereço do servidor e copiando e colando esse caminho poll etc pro arquivo deb.&lt;/p&gt;

&lt;p&gt;Pronto, baixamos o pacote do htop. Mas o que diabos é um arquivo ponto deb? Não é a mesma coisa mas é como se fosse um arquivo zip. Só que em vez de usar o comando &quot;gzip&quot; o padrão pra deb é usar o comando &quot;ar&quot;, que literalmente significa &quot;archive&quot;. Vamos fazer &lt;code&gt;ar x htop bla bla ponto deb&lt;/code&gt;. Opção &quot;x&quot; significa extract. Olha só o que abriu, apareceu arquivos &quot;debian-library&quot; mas mais importante, temos esse &quot;control.tar.zst&quot; e &quot;data.tar.zst&quot;. Tar eu já expliquei no video de Ubuntu, é o comando de Tape Archive, que foi criado na época que se usava fitas magnéticas pra gravar arquivos.&lt;/p&gt;

&lt;p&gt;Em resumo, o tar serve pra concatenar arquivos um atrás do outro num único linguição de bits, um único arquivão, pra mandar pra fita. É que nem um zip, mas sem necessariamente compactar. Mas nesse caso ele tá compactado, e de novo, não é um zip e sim um zstd. Se nunca ouviu falar, zstd é mais uma alternativa a gzip. Por ser mais moderno, usa algoritmos mais refinados de compressão e faz arquivos menores que gzip e é mais performático. Perfeito pra um gerenciador de pacotes.&lt;/p&gt;

&lt;p&gt;Podemos descompactar fazendo &lt;code&gt;tar -I unzstd -xvf control.tar.zst&lt;/code&gt; e bingo, ganhamos um arquivo chamado &quot;control&quot;. Podemos ver o conteúdo com o comando &lt;code&gt;cat&lt;/code&gt; e olha só? Parece familiar? Sim, é exatamente o que tinha naquele arquivo Packages que baixamos antes. O arquivo Packages é montado automaticamente com o conteúdo desse arquivo control dos vários pacotes da distro. Agora descompactamos o outro arquivo com &lt;code&gt;tar -I unzstd -xvf data.tar.zst&lt;/code&gt;. Esse é o arquivo que vai ter o binário executável do htop e outros arquivos que depende. Olha só, surgiu um diretório &quot;usr&quot; e vasculhando vemos que em &quot;usr/bin&quot; temos o binário do &quot;htop&quot;.&lt;/p&gt;

&lt;p&gt;De curiosidade, naquele arquivo de control, veio também um arquivo chamado &quot;md5sum&quot;, vamos ver dentro. É uma lista de hashes md5 dos arquivos que tava no data.tar. Vamos checar? Só rodar &lt;code&gt;md5sum usr/bin/htop&lt;/code&gt; e pronto, é exatamente o mesmo hash. Isso garante que o binário que temos aqui não foi corrompido nesse processo todo de download, descompressão. Lembra do episódio sobre detecção e correção de erros? Poderia ter um raio cósmico que flipou um bit, ou o meu HD poderia estar velho e falhado na hora de gravar um bit. Com essa checagem podemos garantir que o binário que está no meu disco tem 100% dos seus bits intactos.&lt;/p&gt;

&lt;p&gt;Mas note uma coisa. Lembra que quando apagamos a biblioteca libncurses o htop não funciona? Cadê essa biblioteca? Não aparece nessa lista, se listamos os sub-diretórios que descomprimimos do data.tar, não existe &quot;lib&quot;, só &quot;usr&quot;. E é pra isso que serve aquele arquivo &quot;control&quot;. Vamos listar o conteúdo dele de novo e olhar com mais calma. Olha só como tem um item chamado &quot;Depends&quot;. Aqui mostra quais outros pacotes precisam ser instalados juntos e quais versões mínimas. Por baixo dos panos o comando &lt;code&gt;apt&lt;/code&gt; vai também fazer esse mesmo processo que fizemos manualmente pra todos esses outros pacotes.&lt;/p&gt;

&lt;p&gt;Se o apt não fosse inteligente pra fazer resolução dessas dependências, precisaríamos fazer na mão um comando bem maior, por exemplo &lt;code&gt;apt install htop libc6 libncursesw6 libnl-3-200, libnl-gen-3-200 libtinfo6&lt;/code&gt;, mas como o apt sabe encontrar essa lista de dependências dentro do arquivo de &quot;control&quot;, dentro do pacote deb do htop, já instala automaticamente pra gente. A única coisa que resta é copiar os arquivos dentro desse diretório &quot;usr&quot; pra cima do diretório &quot;/usr&quot; do nosso HD, e é assim que o binário vai parar em &quot;/usr/bin/htop&quot;.&lt;/p&gt;

&lt;p&gt;Em resumo bem resumido, essa é a anatomia de um pacote deb de Ubuntu. Um arquivão que dentro tem dois outros arquivões. Um é o data.tar que tem os binários do software. O outro é o control.tar que são metadados, informações como a descrição do software, autores, mantenedores, e dependências de outros pacotes. Num Fedora se usa pacotes em outro formato, o RPM que significa &quot;Red Hat Package Manager&quot; e foi criado na época das distros comerciais da Red Hat. Hoje o formato RPM ainda é usado pelo CentOS, Fedora, openSuse. As distros Arch Linux como Manjaro ou Garuda usam o formato PKG de pacote. E agora vamos voltar a falar do Slackware, que pacotes ele usa?&lt;/p&gt;

&lt;p&gt;Na realidade, o Slackware original não usa nenhum formato especial de pacotes. Deixa eu mostrar o jeito antigo, o jeito padrão e o jeito moderno de instalar software num Slackware. Primeiro, vamos ver como anda a instalação. Olha só, ainda tá instalando como se não houvesse amanhã, é pacote pra caramba. O Slackware é disparado a instalação que mais ocupa espaço em disco comparado com qualquer outro. Quando terminar de instalar e baixar as atualizações todas, vai ocupar quase 20 gigabytes no disco.&lt;/p&gt;

&lt;p&gt;Em comparação, o Ubuntu que instalei na versão mínima, sem softwares tipo Office, ocupa menos de 4 gigabytes. Mesmo se instalar os softwares opcionais, uma distro tipo Ubuntu ou Manjaro vai ocupar menos de 10 gigas, menos da metade do Slackware completo. E porque o Slack ocupa tanto espaço? Porque a filosofia é a seguinte: pra que ter uma ferramenta complicada tipo Apt ou Pacman pra ficar resolvendo dependências? Basta já ter tudo pré-instalado, todas as bibliotecas como aquela libncurses que o htop precisava, daí quando for instalar o htop, já vai ter o que precisa e não tem que se preocupar. E mesmo se não tiver tudo, dá pra instalar manualmente o pouco que falta.&lt;/p&gt;

&lt;p&gt;Alguém poderia dizer &quot;porra, quer dizer que vai desperdiçar mais de 10 giga de coisas que talvez nem use?&quot;. Sim, vai mesmo. Se fosse uns 10 anos ou mais pra trás, essa reclamação teria mais peso, mas hoje em dia, 10 gigas a mais, 10 gigas a menos, meio que não faz diferença nenhuma. Com menos de 200 reais você compra um SSD barato de 240 gigabytes. Estamos literalmente falando de 10 real a mais ou 10 real a menos. Literalmente não faz diferença.&lt;/p&gt;

&lt;p&gt;Especialmente se considerar que qualquer jogo Triple A no Steam, como um Red Dead Redemption 2 ou GTA 5 ocupam mais de 100 gigabytes. O remaster do Spider-man que saiu faz pouco tempo, um God of War ou Cyberpunk 2077 custam mais de 60 gigabytes cada. O que é 10 gigas a mais do Slackware? Nada. Merreca. Então, o argumento de desperdiçar espaço, em 2022, meio que tanto faz.&lt;/p&gt;

&lt;p&gt;O problema é que mesmo tendo um monte de pacotes já pré-instalados, isso ainda não resolve o problema de dependências. A partir daqui vou adaptar o exemplo de um blog post que esbarrei 2 anos atrás. Vou deixar o link na descrição abaixo pra referência. O exercício é instalar a biblioteca &quot;jq&quot;, que é um processador de JSON de linha de comando. Quem mexe com APIs provavelmente já usou essa ferramenta, e se não usou, recomendo pesquisar depois.&lt;/p&gt;

&lt;p&gt;Mas vamos lá, digamos que estamos nos anos 90, quando o conceito de gerenciador de pacotes ainda não era comum. Não temos APT nem Pacman nem nada disso. Não temos repositórios pra baixar pacotes. Como fazemos? Primeiro, procuramos o repositório de código. Antigamente estaria em sites como Sourceforge.net, hoje em dia provavelmente vai estar num GitHub. E de fato, aqui está a página. Agora vamos no link de releases e baixamos o tarball, que é o arquivo tar compactado com gzip do código fonte. Esse é o jeito antigo, nem existia Git nos anos 90, por isso não vamos usar Git.&lt;/p&gt;

&lt;p&gt;Vamos pro terminal, diretório de downloads e fazemos o bom e velho &lt;code&gt;tar xvfz jq.tar.gz&lt;/code&gt;. Entramos no diretório e olha só, temos o código fonte completo dessa ferramenta. E quem é das antigas já sabe o que tem que fazer. Rodar o bom e velho &lt;code&gt;./configure&lt;/code&gt;. Todo código fonte de C costuma vir com um script executável chamado &quot;configure&quot;, que vai checar se temos todas as dependências de compilação nos lugares certos e gerar um arquivo de &quot;Makefile&quot; que rodamos com o comando &quot;make&quot; pra realmente pegar todo o código fonte, usar o compilador GCC e gerar o binário. Veja que o configure vai checando e em toda linha vai dizendo &quot;yes&quot;, &quot;yes&quot;, pra reportar que tá tudo ok. É isso que esperamos ver.&lt;/p&gt;

&lt;p&gt;Mas antes do fim, erro. Fala que não achou o Oniguruma. Quem é experiente conhece o Oniguruma, que é uma biblioteca de Regex. Vamos listar o que tem nesse &quot;modules/oniguruma&quot; e, por coincidência, o autor desse projeto foi legal e já deixou copiado o código fonte do Oniguruma nesse diretório. Se não tivesse feito isso eu precisaria ir caçar o repositório do Oniguruma, baixar o tarball e descompactar aqui. Mas o erro aconteceu porque ele não achou o script de configure atualizado lá. E isso porque eu não li a documentação do README, como deveria. Se tivesse lido ia ver que o autor me manda rodar o comando &lt;code&gt;autoreconf&lt;/code&gt; ANTES de rodar o &lt;code&gt;./configure&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Autoreconf vai rodar outras ferramentas por baixo como autoconf, autoheader e mais. Quem é desenvolvedor de C e C++ conhece isso. Autoreconf vai gerar o script atualizado de configure. Então vamos rodar &lt;code&gt;autoreconf -fi&lt;/code&gt; como manda. E agora sim, repetimos o &lt;code&gt;./configure&lt;/code&gt;. Esperamos um pouquinho e pronto: agora sim, terminou sem problema. Significa que nosso Makefile tá pronto, então é só rodar o comando &lt;code&gt;make&lt;/code&gt;. Pra quem é de Ruby o make é o avô de &lt;code&gt;rake&lt;/code&gt;. Pra quem é de Javascript rodar make é tipo rodar &lt;code&gt;npm build&lt;/code&gt; ou &lt;code&gt;yarn&lt;/code&gt;. É um executador de tarefas. Ele vai automatizar rodar o GCC pra cada arquivo de código fonte C, depois pegar os objetos binário e linkeditar num binariozão executável.&lt;/p&gt;

&lt;p&gt;E pronto, terminou de compilar. Se listarmos os arquivos, olha aqui o binário &quot;jq&quot;. Pra terminar de instalar precisa copiar esse binário pra um diretório do sistema como &quot;/usr/local/bin&quot;. Mas não precisa fazer manualmente, no Makefile tem uma tarefa chamada &quot;install&quot; justamente pra isso. Como precisa instalar em diretórios do sistema, o certo seria rodar com &lt;code&gt;sudo&lt;/code&gt;, mas como é só um exemplo, eu não quero instalar de verdade. Vamos rodar &lt;code&gt;make install&lt;/code&gt; sem sudo e olha só, dá erro, claro, porque não tem permissão pra copiar arquivos pra diretórios como &quot;/usr/local/lib&quot;. Ele tentou copiar a libonig que é a biblioteca do oniguruma que foi compilado junto.&lt;/p&gt;

&lt;p&gt;Se executasse com &lt;code&gt;sudo&lt;/code&gt;, iria copiar o libonig.so.4.0.0 pro /usr/local/lib e no final o jq pra /usr/local/bin. E pronto, finalizado. Era assim que a gente instalava software antigamente. Baixava o tarball com o código fonte, rodava direto &lt;code&gt;./configure&lt;/code&gt; ou &lt;code&gt;autoreconf&lt;/code&gt; antes pra gerar o script de configure. Checava se faltava alguma dependência. Caçava a dependência, baixava o tarball dele, e ia fazendo assim na munheca. Roda &lt;code&gt;./configure&lt;/code&gt;. Compila tudo com &lt;code&gt;make&lt;/code&gt;, instala os binários com &lt;code&gt;sudo make install&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Antigamente isso de compilar tudo a partir do código fonte era mais necessário porque além de Intel x86 tinha vários tipos de CPUs diferentes. Tinha chips RISC Power da IBM. Chips Sparc de workstations Sun. PA-RISC da HP. DEC, Motorola. Fora a transição de 32-bits pra 64-bits. Como a comunidade de Linux era pequena, não tinha como ficar mantendo binários especializados de todas as versões pra todos esses chips. Era mais fácil fornecer o código fonte, todo mundo tinha compilador GCC, então cada um compilava o binário pra sua arquitetura de CPU.&lt;/p&gt;

&lt;p&gt;Mas felizmente a gente evoluiu. Empresas como Red Hat apareceram e começaram a manter servidores com repositórios de binários já pré-compilados. E em vez de baixar código fonte, a gente passou a baixar pacotes RPM direto com os binários. Melhor ainda, como tinha um gerenciador de pacotes como o antigo Yum, dava pra manter um banco de dados com todos os pacotes instalados. Se quisesse desinstalar, tinha o manifesto com os arquivos de cada software, era só automatizar apagar. Por exemplo, pra apagar esse &quot;jq&quot; que acabamos de tentar instalar, antigamente, tinha que manualmente apagar o &quot;/usr/local/bin/jq&quot; e lembrar que também copiou arquivos &quot;libonig*&quot; em &quot;/usr/local/lib&quot;, e apagar manualmente. Sempre ficava coisa sobrando.&lt;/p&gt;

&lt;p&gt;Hoje em dia, todo novo software povo já deixa preparado pra gerar pacotes como RPM ou DEB pelo menos. Daí um servidor automatizado baixa o tarball mais novo, compila os binários no servidor, e atualiza a lista de pacotes. É isso que empresas como Red Hat ou Canonical fazem pra manter suas distros. É pra isso que eles servem. O autor do jq se cadastra lá e fala: &quot;toda vez que sair versão nova, vocês podem baixar o tarball dessa URL aqui&quot;, e pronto. Daí em outros servidores eles montam a distro, rodam testes automatizados, e garantem que a versão nova dos pacotes não quebra nada. Se quebrar, dá rollback e descarta.&lt;/p&gt;

&lt;p&gt;Vamos voltar pro Slackware. Como disse antes, não tem oficialmente nenhum equivalente de Apt ou Dnf pra gerenciar pacotes. No máximo tem scripts pra lidar com pacotes como pkgtool ou installpkg, que são bem simples. Ele assume que tudo que vamos precisar já tá instalado. Mas a realidade é que tem um monte de software que não vem na instalação. Esse &quot;jq&quot; é um exemplo.&lt;/p&gt;

&lt;p&gt;Pra instalar software por fora, poderíamos fazer o que acabei de mostrar, que é baixar os tarballs, compilar e tals. Mas a comunidade Slack criou um site chamado Slackbuilds.org. Ele é um repositório que cadastra softwares como o &quot;jq&quot; e oferece um script em formato SlackBuild. No tal blog post que falei que vou usar de referência, em 2020 que é quando foi escrito, o jq declarava que tinha dependência com o oniguruma, como pode ver nessa foto de tela que tinha no blog.&lt;/p&gt;

&lt;p&gt;Mas em 2022, se navegarmos pro site slackbuilds.org e pesquisar &quot;jq&quot;, vemos que não tem mais essa dependência. E já sabemos porque. Quando baixamos o tarball do jq, vimos que o autor já copia o codigo fonte do oniguruma junto. Daí não precisa mais compilar separado. O ponto do blog post era mostrar que pra instalar o jq, era necessário instalar o oniguruma separado na mão. Isso ainda é verdade pra vários outros softwares, mas por acaso pra esse exemplo não é mais verdade e menciono aqui justamente pra mostrar que blog posts envelhecem e software muda. Por isso você não pode ser um tapado que só vai copiando e colando, tem que saber o que tá acontecendo.&lt;/p&gt;

&lt;p&gt;Agora sei que posso pular esse trecho todo do blog post e ir direto no slackbuild do jq. Eu tenho que baixar o código fonte original, que já fizemos agora pouco, e baixar esse jq.tar.gz que é o script pra gerar o pacote. Voltamos pro terminal, &lt;code&gt;tar xvfz jq.tar.gz&lt;/code&gt;, entramos nesse diretório &quot;jq&quot; e movemos o tarball do código fonte aqui pra dentro. O segredo é esse script &quot;jq.SlackBuild&quot;. O que fazemos com ele? Vamos ver o que tem dentro ué.&lt;/p&gt;

&lt;p&gt;É um scriptzão que configura variáveis de ambiente aqui no começo, acerta permissão dos arquivos e olha só, vai executar &lt;code&gt;./configure&lt;/code&gt; com algumas opções a mais, rodar &lt;code&gt;make&lt;/code&gt; pra compilar, depois &lt;code&gt;make check&lt;/code&gt; que vai rodar os testes automatizados pra garantir que compilou certo. Pra você, babaca que acha que testes são desnecessários, o mundo open source só é estável como é hoje graças a testes automatizados. Versões mais novas do seu Ubuntu conseguem sair mais rápido, porque ele roda os testes de todos os pacotes antes de gerar a ISO que você faz download.&lt;/p&gt;

&lt;p&gt;Finalmente o script roda aquele &lt;code&gt;make install&lt;/code&gt;, mas manda instalar num sub-diretório, que é o que vai usar pra gerar o pacote final. Vamos executar aqui. &lt;code&gt;./jq.SlackBuild&lt;/code&gt;. Só reclama que precisa de sudo, então repetimos com sudo. E pronto, no final fala que gerou um pacote jq bla bla ponto tgz. Pacote de SlackBuild é nada mais, nada menos que um zipão. E pra instalar, abrimos um shell de root com &lt;code&gt;sudo su&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Aqui podemos rodar o script &lt;code&gt;installpkg /tmp/jq bla bla ponto tgz&lt;/code&gt; e pronto, tá instalado. O &quot;jq&quot; roda bonitinho. Se dermos &lt;code&gt;which jq&lt;/code&gt; fala que tá em &quot;/usr/bin&quot;. E a grande vantagem de instalar dessa forma em vez de fazer &lt;code&gt;sudo make install&lt;/code&gt; na mão é que pra desinstalar fica mais fácil também. Vamos desinstalar. Só rodar &lt;code&gt;removepkg jq&lt;/code&gt; e olha só, apagou todos os arquivos direitinho. Além do binário &quot;jq&quot; tinha arquivos de documentação, tinha libs, tudo que ia ficar pra trás sujando meu sistema se não tivesse uma forma automatizada de apagar.&lt;/p&gt;

&lt;p&gt;E eu falei script, e não programa, porque installpkg é só um script de shell. Damos um &lt;code&gt;which installpkg&lt;/code&gt; pra ver onde tá. Agora podemos fazer &lt;code&gt;cat /sbin/installpkg&lt;/code&gt; e olha só. Só um script. Por exemplo, podemos ver que esse script suporta pacotes em 4 formatos diferentes de compressão. O nosso foi final tgz, mas ele suporta extensões como tbz, tlz e txz que respectivamente podem usar bzip, lzip ou xz em vez de gzip. Existem várias variações do lempel-ziv que zip usa, incluindo o zst que é o zstandard que o Debian usa em pacotes ponto deb. Se quiser aprender mais sobre scripts de shell, leia e tente entender esse script. Não é muito difícil.&lt;/p&gt;

&lt;p&gt;Esses scripts installpkg e removepkg e pkgtool que eu não mostrei, são o equivalente à ferramenta dkpg do Debian e rpm dos derivados de RedHat. Um Apt usa dkpg por baixo. Um Dnf ou Zypper usa o rpm por baixo. Dpkg serve pra, dado que já tenho um arquivo ponto deb, ele sabe como abrir o pacote, como instalar e como desinstalar. O apt serve pra procurar e baixar o pacote, daí chama o dpkg pro resto.&lt;/p&gt;

&lt;p&gt;Como falei antes, Slackware tem equivalente ao dkpg mas não tem equivalente ao Apt. Mas não significa que a única solução é ficar indo manualmente no site slackbuilds.org pra procurar os softwares que precisamos e gerar os pacotes manualmente. Já que o Slackware não tem nada oficial, a comunidade criou uma ferramenta, chamada de sbopkg. Sbo que significa SlackBuilds.org. Vamos instalar. No momento que estou escrevendo esse episódio, é a versão 0.38.2.&lt;/p&gt;

&lt;p&gt;Só pegar o link, ir no terminal e baixar com wget. Agora entramos no shell de root e instalamos o pacote tgz com &lt;code&gt;installpkg&lt;/code&gt;, como fizemos antes com o jq. E agora é só rodar &lt;code&gt;sbopkg -r&lt;/code&gt;. Esse traço &quot;r&quot; vai fazer um rsync com o site slackbuilds.org pra ter uma cópia local dos pacotes disponíveis lá, pra possibilitar procurar pacotes sem ter que abrir o site num navegador. Por exemplo. Vamos instalar o mesmo &quot;jq&quot; usando essa ferramenta.&lt;/p&gt;

&lt;p&gt;Ainda no shell de root podemos fazer &lt;code&gt;sbopkg -p jq&lt;/code&gt;. Vai criar um arquivo em &quot;/var/lib/sbopkg/queues&quot;. Ele tem o conceito de fila, onde podemos ir enfileirando vários pacotes pra instalar de uma só vez. Mas aqui vamos direto já rodar &lt;code&gt;sbopkg -B -i jq.sqf&lt;/code&gt; e pronto. Tá instalado. Se fosse um pacote com várias dependências, não precisaríamos instalar uma por uma na mão como antes, o sbopkg vai se encarregar de instalar tudo sozinho. Vamos instalar algo mais complexo, como um neovim, mas um pouco diferente agora. Digite só o comando &lt;code&gt;sbopkg&lt;/code&gt; sem nenhum parâmetro e olha só, abre uma interface interativa em ncurses.&lt;/p&gt;

&lt;p&gt;Podemos dar search pra achar o neovim, selecionar o pacote que quero, e dar install. E pronto. Ele se vira. É tipo a loja de software do seu Ubuntu só que em modo texto. Tem opções pra desinstalar, pra atualizar todos os pacotes e mais. Com isso o slackware também fica um pouco mais moderno e parecido com outras distros.&lt;/p&gt;

&lt;p&gt;E a quantas anda nossa instalação de Slackware? Vamos ver, puts, continua copiando pacotes. Eu vou parar por aqui, porque no final a única coisa que vai sobrar é ele me perguntando que gerenciador de janelas queremos usar, se queremos KDE ou XFCE mas a curiosidade é que ele vem também instalado com o antigo Window Maker, que é a base da interface de MacOS desde a primeira versão &quot;X&quot; ou &quot;10&quot;, a versão open source do que vinha nos computadores da Next do Steve Jobs. E também vem com Motif, que é interface gráfica que vinha em workstations UNIX como da Sun ou Silicon Graphics. Vale testar de curiosidade, mas são bem rudimentares. No final escolhe um KDE da vida que tá de bom tamanho. Reboota e pronto, tá igual qualquer outra distro.&lt;/p&gt;

&lt;p&gt;O Slackware é provavelmente a distro mais antiga ainda em atividade hoje e eu escolhi falar dela pela oportunidade de explicar vários conceitos que acontecem por baixo dos panos, escondido, em distros mais modernas, e que vocês dificilmente iam aprender sozinhos. Eu sei tudo isso porque comecei literalmente usando Slackware e Red Hat nos anos 90 quando nenhuma das distros modernas existia. Ubuntu só viria a aparecer lá por 2005 ou 2006. Arch só aparece depois de 2001. Todas as distros derivadas de Ubuntu como PopOS só aparecem depois de 2010.&lt;/p&gt;

&lt;p&gt;Pra mim sempre tudo fez sentido justamente porque aprendi numa época em que a única coisa que a gente tinha era um tarball de código fonte e o GCC pra compilar e mais nada. Por isso foi mais fácil pra mim do que é pra vocês. É muito mais difícil entender as ferramentas de hoje porque tem misturado ferramentas do passado e é confuso saber o que é pra usar quando. Um usuário novato que escolher Slackware vai bater cabeça sem saber como instalar software, porque não entende a filosofia de tarballs e ia ter dificuldade de entender a diferença do script &lt;code&gt;installpkg&lt;/code&gt; pro projeto &lt;code&gt;sbopkg&lt;/code&gt;, sem saber que o primeiro é fundação pro segundo.&lt;/p&gt;

&lt;p&gt;De qualquer forma, eu acho interessante a idéia de já ter um monte de coisa pré-instalada. Espaço hoje em dia é barato. E mesmo assim ele não fica pesado porque apesar de estar instalado, nada que você não pedir vai carregar. Por exemplo, se já tiver apache instalado mas eu não habilitar o serviço pra carregar, então não vai consumir memória ou CPU a mais só por estar lá. Porém, do ponto de vista de segurança eu não gosto. Em segurança queremos sempre ter o mínimo possível de software pré-instalado, porque não dá pra saber se um software antigo, sem manutenção, não tem um bug de segurança que pode ser usado pra algum hacker escalar privilégios pra root e tomar conta da nossa máquina. Eu recomendo sempre só ter instalado o que realmente precisa usar.&lt;/p&gt;

&lt;p&gt;De qualquer forma, vale tentar instalar e usar por um tempo, pra aprender mais uma distro diferente de Linux. Tendo um sbopkg, o uso é parecido com um apt de Ubuntu ou dnf de Fedora, então não deve dar muito trabalho de usar no dia a dia. Muitos de vocês já instalaram dezenas de software num Ubuntu da vida, sem saber que tava baixando pacotes DEB, sem saber que esses pacotes nada mais são que um zipão. No próximo episódio vamos falar mais conceitos de Linux. Se ficaram com dúvidas, mandem nos comentários abaixo. Se curtiram o video deixem um joinha, assinem o canal pra não perder a continuação, e compartilhem o video com seus amigos. A gente se vê, até mais!&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5996</id>
    <published>2022-09-12T18:05:00-03:00</published>
    <updated>2022-09-12T18:07:06-03:00</updated>
    <link href="/2022/09/12/akitando-127-como-funciona-o-boot-de-um-linux-o-que-tem-num-livecd" rel="alternate" type="text/html">
    <title>[Akitando] #127 - Como Funciona o Boot de um Linux? | O que tem num LiveCD?</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/5F6BbhgvFOE&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Instalar distros Linux hoje em dia é muito fácil. Só bootar do pendrive, clicar em &quot;próximo&quot;, &quot;próximo&quot; e pronto, tá tudo instalado e funcionando. Qualquer um consegue instalar na maioria das configurações modernas de hardware.&lt;/p&gt;

&lt;p&gt;Porém, isso não te ensina nada e ao final você continua não sabendo absolutamente nada sobre o que é um Linux e o que de fato ele faz. Como é o processo de boot? Quais softwares estão envolvidos nesse processo? E mais importante: o que posso fazer quando alguma coisa dá errado? Quais ferramentas tenho à minha disposição?&lt;/p&gt;

&lt;p&gt;Hoje é o dia de finalmente você começar a entender como sua máquina realmente funciona.&lt;/p&gt;

&lt;p&gt;== Conteúdo&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;01:11 - CAP 1 - Instalar Linux é Fácil | Ubuntu 22.04&lt;/li&gt;
&lt;li&gt;06:11 - CAP 2 - O que é um Mount Point? | Tudo em Linux são Arquivos&lt;/li&gt;
&lt;li&gt;11:45 - CAP 3 - Firmwares e UEFI | Quem começa o boot?&lt;/li&gt;
&lt;li&gt;15:43 - CAP 4 - O que é uma “imagem” ou ISO? | O que tem num LiveCD?&lt;/li&gt;
&lt;li&gt;26:03 - CAP 5 - O “meta-Linux” antes do Linux | Init RAM&lt;/li&gt;
&lt;li&gt;31:26 - CAP 6 - Depois do Linux: Daemons | Systemd&lt;/li&gt;
&lt;li&gt;33:31 - CAP 7 - O que são “Run Levels”? | O jeito SysV antigo&lt;/li&gt;
&lt;li&gt;40:40 - CAP 8 - O fim do Init SysV? | O jeito novo do Systemd&lt;/li&gt;
&lt;li&gt;45:04 - Bloopers&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;== Links&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gnome-cedilla-fix (https://github.com/marcopaganini/gnome-cedilla-fix)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;== SCRIPT&lt;/p&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Por hora, finalizei finalmente a minissérie sobre redes e internet e agora é hora de falar um pouco mais sobre Linux  pra complementar os videos de Ubuntu e WSL que já fiz aqui no canal. Tem uma playlist de Software Livre onde falo de assuntos relacionados, caso ainda não tenha assistido e vou adicionar os novos videos nessa mesma playlist. Nos próximos videos vou mostrar um pouco mais em detalhes sobre instalação de algumas distros consideradas mais exóticas, mas hoje quero explorar mais conceitos de Linux e software em geral que talvez vocês não conheçam usando como desculpa a explicação de como um Linux faz boot. Vamos começar a olhar um pouco mais debaixo do capô.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Pra escolher distros, hoje em dia é fácil. Basta ir em canais de reviews que tem de tonelada, como do camarada DioLinux ou outros que vem crescendo como do Chris Tut. O comportamento padrão vai ser você vendo qual tem o visual que mais te atrai e daí baixar um Deepin ou PopOS da vida, ou se quiser se sentir mais &quot;edgy&quot; e &quot;gamer hipster&quot; da vida, um Garuda. É assim que 99% de todo mundo que usa Linux faz pra escolher.&lt;/p&gt;

&lt;p&gt;Além disso, a instalação de qualquer distro tá super trivial. Você baixa o arquivo de ISO no Windows mesmo, usa um programa como o Rufus pra gravar essa ISO num pendrive. Faz boot pelo pendrive e boom, tem um ambiente ao vivo de Linux já funcionando. Ele detecta sozinho praticamente todo o seu hardware. Se nesse ambiente do pendrive parecer que tá tudo funcionando, daí tem um ícone de instalação. Vai abrir o Calamares, que é o programa de instalação que quase toda distro moderna usa. Por isso mesmo meio que tanto faz qual distro escolher, o processo de instalação é muito parecido.&lt;/p&gt;

&lt;p&gt;O passo a passo que o Calamares abre é sempre o mesmo. Começa pedindo pra escolher que língua usa e que layout de teclado prefere. Eu sempre escolho inglês e o layout US international que, até hoje, continua com o mesmo problema de não suportar acento agudo no &quot;c&quot; pra gerar cedilha, mas isso é fácil de consertar com um script que se acha fácil no Google. No caso do Ubuntu, pede pra escolher se quer instalar um monte de aplicativos como LibreOffice, Gimp, aplicativo de fotos e coisas assim, ou uma instalação mínima, que é o que eu prefiro. Eu gosto de já escolher pra baixar os pacotes mais atualizados via internet e também instalar software proprietário de terceiros, como codecs multimídia e drivers de placa de video da NVIDIA.&lt;/p&gt;

&lt;p&gt;Se você é iniciante, nem dá bola pra isso, mas no mundo de software livre é uma discussão importante. Por causa de licença de software, toda distro faz um esforço pra ter por padrão somente o que é de verdade software livre, com licenças como GPL ou BSD. Porém, muitos dispositivos, em particular a NVIDIA, não libera o código fonte dos drivers, somente o binário fechado. Mas se você instala isso no seu PC, está concordando com a licença restritiva da NVIDIA e contaminando seu ambiente com software fechado. Do ponto de vista filosófico, não é recomendado. Mas pra nós, usuários finais, meio que tanto faz isso. Por isso sempre habilito, porque eu não sou ativista de software livre. E antes que você vá nos comentários pra dizer que viu notícia que a NVIDIA liberou código dos drivers, nem perca tempo, pra variar é matéria de jornalista bosta que não sabe ler e não viu que eles liberaram só uma pequena parte do código só pra marketing. A parte importante continua fechada e, mais importante, com licença restritiva de uso.&lt;/p&gt;

&lt;p&gt;Enfim, o próximo passo costuma ser pra escolher em qual SSD ou HD quer instalar, como quer criar partições e com qual sistema de arquivos. O Calamares costuma ter essa primeira opção que diz o seguinte &quot;eu sei que você não tem idéia do que tá fazendo, então deixa que o papai aqui cuida de tudo pra você&quot;. É a opção pra idiotas, quero dizer, pessoas não-técnicas. Ele vai apagar o disco todo e criar as partições automaticamente pra você. No caso do Ubuntu, recomendo pelo menos tentar instalar com o sistema de arquivos ZFS em vez do ext4. Se você não tem idéia do que é isso, não esqueça que eu fiz uma minissérie inteira explicando o que são partições, volumes, como o PC faz boot via EFI, e o que diabos é ext4 ou ZFS. Então vai assistir depois.&lt;/p&gt;

&lt;p&gt;Finalmente, o Calamares vai mandar preencher um formulário com seu nome e que usuário e senha quer cadastrar pra poder fazer login depois. Obviamente você deveria escolher uma senha aleatória e forte e não &quot;senha123&quot; ou &quot;teste123&quot;. Também te dá a opção de criar uma senha diferente pro usuário &quot;root&quot; que é o administrador da máquina. Mas aqui estou escolhendo a opção preguiçosa que é usar a mesma pros dois usuários e fazer login automático porque só eu vou usar essa máquina mesmo.&lt;/p&gt;

&lt;p&gt;E pronto. Acabou. Agora o Calamares vai copiar os arquivos do pendrive pra partição que acabou de criar. Vamos acelerar porque essa é a parte que demora mais, de 15 minutos a meia hora dependendo do seu PC. No final ele configura a carga do boot, e agora é só reiniciar e tá tudo instalado. É absurdamente fácil instalar qualquer distro Linux que usa Calamares hoje em dia. Olha só, vou acelerar aqui o boot, tirar o pendrive, e pronto. Um Ubuntu quentinho recém saído do forno, com tudo funcionando. Se seu PC não for muito bosta, já vai estar tudo detectado, placa de vídeo, som, seu teclado, mouse, wifi e tudo mais. Não precisa fazer mais nada.&lt;/p&gt;

&lt;p&gt;Muita gente acha o Ubuntu feio, e eu não discordo muito. Apesar que eles tem melhorado o tema visual e até que hoje, pra mim, não cheira nem fede. Não teria nenhum problema de usar um Ubuntu sem mudar muita coisa. A partir daqui, se você nunca mexeu com Linux antes, veja primeiro o meu video só sobre Ubuntu onde também explico os comandos e conceitos mais básicos que vou assumir que todo mundo já sabe pra continuar este video.&lt;/p&gt;

&lt;p&gt;Se escolher um Linux Mint, Elementary, Budgie, Zorin ou PopOS, o funcionamento não vai diferir porque todos são derivados de Ubuntu. Por isso que muito tutorial que funciona no Ubuntu vai funcionar neles também. Cada um tem alguma característica pra se diferenciar. Seja ter um gerenciador de janelas customizado diferente, seja já ter coisas pré-instaladas e pré-configuradas pro usuário não ter que configurar manualmente depois. Mas na prática, pra todos os efeitos e propósitos, é quase a mesma coisa que usar direto Ubuntu.&lt;/p&gt;

&lt;p&gt;Antes de continuar, vamos olhar o que fizemos até agora. Um bom programador precisa se acostumar a fazer essas perguntas. Você baixou um arquivo de ISO, gravou num pendrive, e ele logou uma versão light de Ubuntu, sem nem ter instalado nada. Como isso é possível, já parou pra se perguntar isso? Vai, pensa alguns segundos. Como que faz pra bootar um Linux de um pendrive a partir de um mero arquivão com extensão ponto ISO no final?&lt;/p&gt;

&lt;p&gt;Se você assistiu minha minissérie sobre armazenamento e sistemas de arquivos talvez já saiba parte da resposta. Deixa eu aproveitar pra mostrar alguns conceitos de Linux pra vocês. Um dos fundamentos inventados no UNIX original e herdado pelo Linux é a idéia de tentar fazer tudo num computador ser representado como se fossem diretórios e arquivos. Por exemplo, talvez já tenha usado o comando &lt;code&gt;ps&lt;/code&gt; pra listar os programas rodando. Só &lt;code&gt;ps&lt;/code&gt; sem nada mostra os processos que seu usuário iniciou, mas se usar opções como &lt;code&gt;ps aux&lt;/code&gt; podemos ver todos os processos que foram iniciados desde o boot pelo usuário &lt;code&gt;root&lt;/code&gt; também.&lt;/p&gt;

&lt;p&gt;Já expliquei em outros videos sobre processos e como eles tem um PID, que é um process ID, um número que identifica um processo e como podemos mandar sinais sigterm, como &lt;code&gt;kill -9 pid&lt;/code&gt; pra matar um processo forçadamente. Agora, lembrem que todo Linux costuma ter um diretório na raíz chamado &lt;code&gt;/proc&lt;/code&gt;. Posso fazer &lt;code&gt;ls /proc/pid&lt;/code&gt; e olha só, aparece um monte de arquivos e diretórios falsos. O que tem dentro desse diretório não são arquivos de verdade, são informações sobre esse processo representado como arquivos. Por exemplo, posso usar um comando normal de arquivos como o &lt;code&gt;cat&lt;/code&gt; que se usa pra listar o conteúdo de um arquivo texto.&lt;/p&gt;

&lt;p&gt;Fazendo &lt;code&gt;cat /proc/pid/status&lt;/code&gt; temos várias informações sobre esse processo, como quanto de memória ele tá usando. Se eu quiser fazer uma ferramenta parecida com &lt;code&gt;ps, top ou htop&lt;/code&gt; da vida, posso consultar as informações de cada processo nessa árvore embaixo de &lt;code&gt;/proc&lt;/code&gt;. Fica de exercício pra vocês fazer uma versão parecida com o comando top ou htop, mas usando python ou javascript ou o que quiser, só lendo do diretório /proc, sem usar syscalls pra kernel. Esse diretório não tem arquivos de verdade, só representações virtuais. Quando der boot, esse diretório desaparece. Se você remover esse HD e ligar em outro PC como HD externo USB por exemplo, não vai ter nada dentro.&lt;/p&gt;

&lt;p&gt;Esse diretório é o que chamamos de ponto de montagem, um mount point. Tente digitar o comando &lt;code&gt;mount&lt;/code&gt; no terminal. Vai aparecer uma lista como essa. Esses são os pontos de montagem ativos atualmente. E que diabos é um ponto de montagem? Num Windows pontos de montagem costumam ser letras como C:, D:, E:. Em Linux são sub-diretórios. Pra explicar isso você precisa entender que, uma coisa é o hardware de armazenamento como um HD ou pendrive. Outra coisa é como o sistema operacional acessa esse hardware.&lt;/p&gt;

&lt;p&gt;O que é um dispositivo de armazenamento? Cansei de repetir em vários videos que são dispositivos que gerenciam blocos de bits, ou block devices. Seja um CD, um pendrive USB ou um SSD, todos são dispositivos de bloco. E por acaso, num Linux temos comandos como &lt;code&gt;ls&lt;/code&gt; que a essa altura você já sabe que significa &lt;code&gt;listar&lt;/code&gt; e &lt;code&gt;blk&lt;/code&gt; ou seja listar block devices. E agora sim temos os nomes dos dispositivos plugados no meu PC, por exemplo, o HD principal se chama &lt;code&gt;sda&lt;/code&gt;. Um segundo HD se chama &lt;code&gt;sdb&lt;/code&gt;, um pendrive, quando plugar vai aparecer como &lt;code&gt;sdc&lt;/code&gt; ou &lt;code&gt;sde&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Além de &lt;code&gt;/proc&lt;/code&gt; existe outro ponto de montagem que toda distro Linux monta automaticamente no boot, que é o &lt;code&gt;/dev&lt;/code&gt;. De novo, o que tem dentro não são diretórios nem arquivos de verdade. Ele representa os dispositivos de armazenamentol. Por isso em muitos tutoriais por aí você já deve ter esbarrado num &lt;code&gt;/dev/sda&lt;/code&gt; da vida. Isso não é um arquivo, é um canal pra falar com o HD físico “via” um arquivo virtual. No Windows funciona assim também, o HD principal costuma ter o nome de &lt;code&gt;\\.\PhysicalDisk1&lt;/code&gt;. Abre um Powershell no Windows e digita &lt;code&gt;Get-PhysicalDisk&lt;/code&gt;. Por exemplo, esse meu NVME Samsung 970 EVO Plus de 2TB se chama PhysicalDisk2. Num Linux um nome equivalente seria &lt;code&gt;/dev/sdc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Beleza, o Linux tá enxergando os dispositivos, mas agora como navego dentro desses dispositivos? Agora eu preciso de um mount point. O ponto de montagem é justamente dizer que a partição 1 do drive &lt;code&gt;/dev/sda&lt;/code&gt; vai ser o diretório raíz &quot;/&quot; e é pra interpretar os bits desse canal usando o sistema de arquivos ext4. Leia o arquivo &quot;/etc/fstab&quot; que significa tabela de file system. Em cada linha vai ter dizendo a partição do disco, o ponto de montagem, o sistema de arquivo e opções, por exemplo, pra abrir só pra leitura, caso seja um CD-ROM.&lt;/p&gt;

&lt;p&gt;Quando espeto um pendrive nesse meu Ubuntu, automaticamente aparece aqui no meu desktop. Isso porque tem um serviço rodando em background que vai rodar pra mim o comando &lt;code&gt;mount -t vfat /dev/sdc /run/media/akitaonrails/pendrive&lt;/code&gt;, por exemplo. Em distros mais antigas, eu precisaria rodar esse comando manualmente no terminal se quisesse acessar o pendrive. E o equivalente a dar um eject é rodar o comando &lt;code&gt;umount /run/media/akitaonrails/pendrive&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Sacaram? Montar é dizer pro sistema operacional como esse dispositivo foi formatado e onde quero que ele apareça e com qual sistema de arquivos é pra interpretar os bits dentro. No Windows isso é feito em aplicativos com o Disk Management, onde posso mudar o ponto de montagem, que no caso é só uma letra de drive, como E:.&lt;/p&gt;

&lt;p&gt;Vamos recapitular o que já expliquei nos videos sobre volumes e partições. Num computador moderno, logo que liga, quem vai carregar primeiro é um software chamado firmware, que muitos ainda chamam erroneamente de BIOS. Mas é o software que não está no HD e sim num chip de memória na placa mãe. Por isso que mesmo num PC sem HD nenhum, se você ficar apertando uma tecla como &quot;delete&quot; várias vezes, entra naquele software de configuração, onde pode atualizar a data e outras configurações do sistema.&lt;/p&gt;

&lt;p&gt;Antigamente esse software era chamado BIOS mesmo, que significa Basic Input/Output System, ou sistema básico de entrada e saída que literalmente tem o básico só pra conseguir identificar seu monitor, teclado e mouse pra você conseguir interagir com o computador antes de tentar carregar o sistema operacional do disco. Mas BIOS era um sistema básico até demais, que só conseguia lidar com identificadores de 16-bits, o que impossibilita usar HDs com mais do que 4 partições. Nos anos 90 isso já era pouco e representava um problema.&lt;/p&gt;

&lt;p&gt;Por isso hoje todo PC moderno tem um firmware UEFI que significa Universal Extensible Firmware Interface. UEFI é um padrão pra firmwares. Além disso antigamente partições eram organizadas com MBR ou Master Boot Record, que é o registro mestre de boot no HD, que são os bits de boot do sistema operacional que a BIOS vai tentar executar logo na sequência. Hoje em dia é UEFI e GPT que é o GUID Partition Table. Eu falei que antigamente estávamos limitados a 4 partições com identificadores de 16-bits. Em GPT, em vez de números, usamos GUIDs que é uma sequência de 32-bits. Números parecidos com esse aqui que você já deve ter visto. Parece aleatório e não são sequenciais. E com isso hoje podemos ter quantas partições de quantos tipos quisermos. Expliquei tudo isso em mais detalhes no video de tudo que você queria saber sobre dispositivos de armazenamento, depois assiste lá.&lt;/p&gt;

&lt;p&gt;Gerenciadores de boot como LILO ou Grub foram criados pra conseguir lidar com as limitações de BIOS e MBR. Com UEFI e GPT nem precisa mais deles, o próprio firmware tem capacidade pra fazer boot de qualquer partição GPT, basta ter o bootloader numa partição que o UEFI consiga ler, formatado em fat32, por exemplo. E isso me faz voltar aqui pro nosso Linux de exemplo. Vamos ver o arquivo &quot;/etc/fstab&quot; de novo. Olha só, tem uma partição &lt;code&gt;/dev/sda1&lt;/code&gt; cujo ponto de montagem é &quot;/boot&quot;. E de um terminal normal mesmo, usando comandos mundanos como &quot;ls&quot; podemos ver que tem um diretório chamado &lt;code&gt;/boot/efi&lt;/code&gt; e é aqui que fica o bootloader, ou literalmente o carregador de boot que o firmware UEFI vai carregar depois do POST.&lt;/p&gt;

&lt;p&gt;É nesse ponto que o firmware UEFI pára de executar e dá controle pro bootloader. O bootloader é quem vai de fato carregar a kernel do sistema operacional, no caso o Linux. E é logo um diretório pra trás, em &quot;/boot&quot; que está o Linux de verdade. É um binário executável chamado “vmlinuz” até que pequeno, de pouco mais de 10 megabytes, já compilado com os principais drivers que precisa pra iniciar o sistema, como driver pra ler HD e abrir a partição formatada nele, onde vai estar o resto do sistema operacional. O bootloader vai se encarregar de executar o binário e é nesse estágio que a tal kernel do Linux finalmente vai começar a exercer controle sobre a máquina.&lt;/p&gt;

&lt;p&gt;A kernel vai começar a carregar drivers pra ter acesso à memória, aos discos, a periféricos como teclados, monitor e tudo mais. Uma vez que agora a kernel tem controle, ela vai criar um espaço em memória com uma partição virtual, usando um initramfs, literalmente file system de ram de inicialização. Sabe seus diretórios de Linux na raíz do seu HD? Pensa uma versão menor disso, mas montado em RAM. Esse sistema de arquivos estava comprimido junto com a kernel naquele arquivo em &quot;/boot&quot;, é esse outro arquivo chamado “initrd.img”, veja que ele é bem maior com uns 100 megabytes. Mas deixa eu fazer uma tangente pra ficar mais claro.&lt;/p&gt;

&lt;p&gt;Vamos abrir o terminal de novo. Existe uma ferramenta que costumamos usar pra fazer clones de HDs ou pendrives no nível dos blocos, independente se tá formatado em ext4, ntfs ou fat32. É o comando &quot;dd&quot;. E o que significa &quot;dd&quot;? Significa Copy and Convert .. eh, deveria ser &quot;cc&quot; só que CC em Linux já significa &quot;C Compiler&quot;, então escolheram a letra seguinte .. eh, programador é uma bosta pra dar nome pras coisas. Mas enfim, dd funciona assim, tem um parâmetro &quot;if&quot; que não sei se era &quot;input file&quot;, é onde coloco o caminho do dispositivo que quero copiar, tipo &quot;/dev/sdc&quot; e no parâmetro &quot;of&quot; que talvez seja &quot;output file&quot; coloco o path de pra onde quero copiar.&lt;/p&gt;

&lt;p&gt;Aquele programa de Windows chamado Rufus? É mais ou menos a versão gráfica desse comando. Por exemplo, se do terminal de um Linux eu quiser gravar aquele arquivo de ISO no meu pendrive, poderia ter feito &lt;code&gt;dd if=ubuntu.iso of=/dev/sdc&lt;/code&gt; e pronto. Lógico, tenho que garantir que “/dev/sdc” realmente é meu pendrive, porque se eu confundir com uma partição do meu HD, esse comando vai gravar a ISO por cima e já era meus arquivos que tavam lá. Agora, só pra complicar deixa mostrar mais alguns truques de Linux que talvez vocês não saibam. Naquele ponto de montagem &quot;/dev&quot; que tem representado dispositivos como meus HDs ou pendrives, também temos alguns dispositivos virtuais especiais como &quot;/dev/null&quot; ou &quot;/dev/urandom&quot;.&lt;/p&gt;

&lt;p&gt;De novo, podemos usar comandos como &lt;code&gt;cat&lt;/code&gt; com eles. Vamos fazer &lt;code&gt;cat /dev/null&lt;/code&gt; e, uau, nada. Como o próprio nome diz, esse dispositivo devolve nulo. Mas muitos scripts gostam de usar ele pra mandar coisas que quero jogar no lixo. Por exemplo, digamos que eu rode um comando que imprima várias informações na tela que não estou interessado. Posso redirecionar o stdout do comando pra /dev/null pra ele ser engolido e mandado pro limbo. Um exemplo besta é o comando &quot;echo&quot; que só imprime na tela. Coloco um sinal de &quot;maior&quot; pra redirecionar pra &quot;/dev/null&quot; e pronto, engoliu o saída. Se começar a fuçar scripts de shell, vai esbarrar mais nisso.&lt;/p&gt;

&lt;p&gt;O &quot;/dev/urandom&quot; por outro lado é um dispositivo que cospe bits aleatórios. Muitas funções de muitas linguagens, incluindo as funções da kernel do Linux e bibliotecas de criptografia usam esse dispositivo quando precisam gerar números aleatórios. Tecnicamente, isso é um PRNG ou pseudo random number generator. É um gerador de números pseudo aleatórios. Em computador não existe aleatoriedade verdadeira, só uma aproximação. Teríamos que explorar conceitos como entropia e tudo mais pra justificar isso, mas não é objetivo de hoje, só entenda que existe isso.&lt;/p&gt;

&lt;p&gt;Pra hoje, mais importante é outro dispositivo virtual, o &quot;/dev/zero&quot;. Assim como o &quot;/dev/null&quot; se eu redirecionar a saída stdout de qualquer comando pra ele, vai engolir tudo. Por outro lado se der um &lt;code&gt;cat&lt;/code&gt; ou ficar lendo dele, vou receber literalmente nada, só caracteres nulos. Vai cuspindo nulos infinitamente até eu forçar a parada com Ctrl+C. E pra que diabos isso serve? Porque posso usar numa outra ferramenta que aceita dispositivos como parâmetro, como nossa ferramenta &quot;dd&quot;.&lt;/p&gt;

&lt;p&gt;E se em vez de ler de um arquivo de ISO, eu ler desse dispositivo &quot;/dev/zero&quot;? Vamos voltar ao comando &quot;dd&quot; e fazer &lt;code&gt;dd if=/dev/zero of=hello.img bs=1M count=10&lt;/code&gt; que significa, leia bits do dispositivo &quot;/dev/zero&quot; e grave num arquivo chamado &quot;hello.img&quot; usando blocos de 1 megabyte de tamanho e contando até 10, ou seja, até gerar um arquivo de 10 megabytes. Pronto. Criei um arquivo cujo conteúdo é só de zeros. Olha só, &lt;code&gt;ls -la&lt;/code&gt; e o arquivo tem mesmo 10 megabytes. Se usar o comando &lt;code&gt;cat&lt;/code&gt;, não imprime nada, porque dentro é tudo bits vazios. E mesmo se usar um comando que lê binários como &lt;code&gt;hexdump&lt;/code&gt;, ele mostra que os primeiros bits são todos 0 e esse asterisco significa que todo o resto é igualzinho à primeira linha até o endereço 10 megabytes.&lt;/p&gt;

&lt;p&gt;E pra que diabos preciso de um arquivo totalmente vazio ocupando espaço com bits zero? Bom, em Linux temos comandos pra formatar partições do seu HD como &lt;code&gt;mkfs.ext4&lt;/code&gt; ou &lt;code&gt;mkfs.vfat&lt;/code&gt; onde &quot;mkfs&quot; literalmente quer dizer &quot;make file system&quot;. Sabe no Windows quando espeta um pendrive, vai no explorar em cima da letra do drive e com o botão direito do mouse escolhe a opção de &quot;formatar&quot;? É a mesma coisa só que na linha de comando de Linux. Vamos formatar esse arquivo então.&lt;/p&gt;

&lt;p&gt;No caso, vou formatar com o &lt;code&gt;mkfs.vfat&lt;/code&gt; pra formatar em FAT32, como eu faria com um pendrive que gostaria de conseguir ler independente de que sistema operacional espetar ele. Só fazer &lt;code&gt;mkfs.vfat -F 32 hello.img&lt;/code&gt;. Não sei se vocês estão conseguindo entender, normalmente se usa um comando como &lt;code&gt;mkfs.vfat&lt;/code&gt; na partição de um dispositivo hardware de verdade, como em &quot;/dev/sda1&quot;, mas dá pra formatar dentro de um arquivo. E lembra como usando comandos como &lt;code&gt;cat&lt;/code&gt; ou mesmo &lt;code&gt;hexdump&lt;/code&gt; só voltava vazio? E agora? O que tem nesse arquivo? Vamos ver.&lt;/p&gt;

&lt;p&gt;E temos o cabeçalho de uma partição FAT32. Olha só do lado direito o que ele reconhece como caracteres ASCII com mensagens de erro e dizendo que é FAT32. Essa é exatamente a mesma sequência de bytes que encontraríamos lendo direto de um pendrive. Vamos ver? Vamos plugar o pendrive e fazer o mesmo comando de &quot;hexdump&quot; direto do &quot;/dev/sdb1&quot; e olha só, esse é o pendrive. São os mesmos tipos de bytes. E já sabemos que a partir de um &quot;/dev/sdb1&quot; podemos montar num diretório como &quot;/run/media&quot; da vida. E esse arquivo que acabamos de formatar, dá pra montar também?&lt;/p&gt;

&lt;p&gt;Basta criar um diretório qualquer como &lt;code&gt;mkdir hello&lt;/code&gt; e fazer &lt;code&gt;mount -t vfat hello.img hello&lt;/code&gt;. Podemos dar &lt;code&gt;cd hello&lt;/code&gt; e agora estamos DENTRO do arquivo hello.img. Sacaram? Deixa eu criar um arquivo texto qualquer chamado &quot;blabla.txt&quot; com qualquer coisa dentro e salvar. Vamos listar e olha só o arquivo. Agora vamos sair desse diretório com &quot;cd ..&quot; e desmontar isso. Só fazer &lt;code&gt;umount hello&lt;/code&gt;. Se tentarmos listar o conteúdo do diretório hello agora, não vai ter nada, porque o arquivo blabla ficou dentro do hello.img que acabamos de ejetar, como se fosse um pendrive que desconectamos. O arquivo blabla não foi criado dentro desse diretório. O diretório só tava servindo como ponto de montagem PRA DENTRO do hello.img.&lt;/p&gt;

&lt;p&gt;Se eu fizer um hexdump no hello.img olha aqui no fim, os bytes do arquivo que criamos tá dentro dele. Este arquivo hello.img, do ponto de vista do sistema operacional é a mesma coisa que um pendrive, só que sem a parte hardware, só os bits, o linguição de bits. Se ficou confuso, vamos repetir. Lembra o arquivo de ISO que baixamos do site do Ubuntu e gravamos no pendrive? É um arquivão que chamamos de “imagem”. Vamos montar aqui no mesmo diretório hello. Só fazer &lt;code&gt;mount -t iso9660 -o ro ubuntu.iso hello&lt;/code&gt;. No caso, um CD-ROM é formatado no padrão iso9660, não é fat nem ext4. E coloquei a opção de &quot;ro&quot; porque CD-ROM é read-only, só de leitura, não dá pra modificar nada dentro.&lt;/p&gt;

&lt;p&gt;Vamos listar o que tem em hello e olha só, todos os arquivos que estão dentro do ISO. É isso que é gravado pelo Rufus num pendrive que queremos dar boot de Ubuntu. Tem o diretório &quot;boot&quot; com o bootloader, o kernel e tudo mais que precisa pra conseguir bootar. É isso que é um LiveCD. Mas o objetivo de toda essa história é pra vocês começarem a largar noções que existe uma entidade física chamada &quot;dispositivo&quot;, como um HD ou pendrive, e entender que disposivo de bloco é literalmente qualquer sequência de bits, qualquer linguição de bits, como um arquivo, ou uma conexão de rede. Basta ser formatado de uma forma que dê pra montar com o comando &lt;code&gt;mount&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;O sistema operacional não tem preconceitos. Pra ele foda-se se você espetou um pendrive ou se montou um arquivão de imagem. Pra ele só interessa: eu consigo ler e gravar blocos de bits? Então tá, pra mim é um drive. E é assim que consigo montar coisas como um Google Drive ou Dropbox como se fosse um pendrive virtual também. Só que diferente de gravar os bits num arquivo local, o driver de sistema de arquivo do Google Drive vai mandar os bits pela rede pra sua conta no Google. Mas pro sistema operacional, foda-se. Ele mandou blocos, o driver de sistema de arquivos recebeu e disse que gravou. O sistema operacional tá pouco se lixando o que acontece com os bits, ele delega e só espera ouvir um &quot;ok, tá gravado&quot; e fica feliz.&lt;/p&gt;

&lt;p&gt;O que fizemos com a ferramenta &quot;dd&quot; dá pra fazer o equivalente no Windows com a ferramenta Disk Management. Olha só, temos essa opção de &quot;Criar VHD&quot; que é virtual hard disk, que é o arquivão vazio. Vamos criar um simples de uns 10 megabytes. Uma vez criado posso anexar esse VHD e tornar ele disponível como um PhysicalDisk numerado. Agora ele aparece aqui embaixo, olha só. Ele vai se comportar como se fosse um pendrive ou HD externo. Eu posso escolher a opção pra formatar como NTFS e no final posso montar com um drive, digamos letra P. Agora o P: é um drive virtual e podemos mover arquivos, criar diretórios, o que quiser. E no final é só desanexar, que é o equivalente de ejetar.&lt;/p&gt;

&lt;p&gt;De dentro do Linux posso usar aquele comando hexdump nesse arquivão VHD. E parecido com o arquivão hello.img que criamos com &quot;dd&quot;, é basicamente um linguição de zeros igualzinho. Olha a linha de zeros e o asterisco embaixo. Mas lá embaixo no arquivo, do endereço a00020 hexa em diante, tem um metadado pra identificar esse arquivão como um VHD. Só por causa desses bytes, que o arquivão que criei com &quot;dd&quot; não se enquadra como um VHD. Mas na prática é quase a mesma coisa: um linguição de bits zeros.&lt;/p&gt;

&lt;p&gt;Voltando pro boot do Linux, parei no ponto onde o bootloader EFI o binário do kernel no diretório “/boot”. Esse arquivo normalmente é chamado de vmlinuz, muitas vezes o nome segue com a versão da kernel, como 5.15. A gente costuma chamar uma distro como Ubuntu ou Fedora de “Linux”, mas na verdade Linux mesmo é só esse um arquivo binário, a kernel. O resto do sistema operacional, o certo é chamar de GNU/Linux, ou seja, ferramentas GNU que rodam em cima de um kernel Linux. Quando o bootloader tem acesso a esse arquivo é quando iniciamos a segunda fase de boot.&lt;/p&gt;

&lt;p&gt;Normalmente, no mesmo diretório &quot;/boot&quot; vai ter um outro arquivo, um &quot;initrd.img&quot;. Esses nomes vão variar de distro pra distro, mas no caso do Ubuntu e todo mundo que é derivado de Debian, o arquivo vmlinuz e o initrd.img são links simbólicos, tão vendo? O arquivo de verdade é esse que termina com &quot;generic&quot; no nome. A vantagem de ser link simbólico é porque posso querer deixar kernels mais antigas depois de um upgrade do sistema. Vai que a kernel mais nova veio com algum bug ou incompatibilidade e ferra meu sistema, impedindo bootar?&lt;/p&gt;

&lt;p&gt;Não tem problema, se isso acontecer, basta resetar o PC. Vai voltar pro firmware UEFI da placa mãe que, por sua vez vai carregar o bootloader em &quot;/boot/efi&quot; ou o GRUB, que é aquela tela que tem no começo do boot da maioria das distros Linux. Eu sei que você só dá enter direto sem pensar muito em pra que ela serve. Nessa tela costuma ter uma opção pra justamente poder escolher outra kernel e outras opções de recuperação. Assim dá pra bootar na kernel da versão anterior, que sabemos que funcionava, e ter a oportunidade de consertar alguma coisa. Basta mudar o link simbólico pra kernel e initrd.img anterior e vai voltar a bootar normalmente até sair uma correção pra kernel mais nova. É raro acontecer, mas se acontecer existe essa opção.&lt;/p&gt;

&lt;p&gt;Enfim, o arquivo initrd.img é um zipão comprimido, pode ser qualquer formato, a distro que decide. No caso do Ubuntu, eu falei “zipão”, mas não é exatamente um zip como feito pela ferramenta &quot;gzip&quot; mas sim um arquivo formato CPIO comprimido com LZ4, o Lempel-Ziv 4. Eu expliquei o que é Lempel-Ziv no video &quot;De 5 Tera a 25 Giga&quot;, depois assiste lá pra saber como compressão funciona. Enfim, o importante é que em Ubuntu podemos usar a ferramenta &lt;code&gt;unmkinitramfs&lt;/code&gt; pra descomprimir essa imagem em qualquer lugar.&lt;/p&gt;

&lt;p&gt;E olha só, se listarmos o que descomprimiu, vai parecer familiar com a estrutura de diretórios do seu Linux de verdade. Isso é o mínimo que a kernel precisa pra conseguir terminar o boot. Só que aqui descomprimimos essa imagem no nosso HD local. O grande lance é que depois do bootloader conseguir carregar a kernel, ela precisa ter acesso a esses arquivos pra conseguir carregar coisas como driver do seu HD. Por exemplo, e se seu HD tiver partição encriptada com LUKS? Ou se estiver em configuração de RAID dentro de um volume LVM? Como o kernel vai conseguir carregar o driver de RAID pra conseguir montar a partição se já não estiver compilada estaticamente?&lt;/p&gt;

&lt;p&gt;São drivers como esse que estão nesse initrd. Só que o initrd em si precisa ser descomprimido em algum lugar. E aqui vou continuar simplificando, mas o que acontece é que a kernel pode não ter acesso ao seu HD no boot. Pode não ter os drivers pra isso. Mas ele tem acesso à RAM. Lembra o lance de criar um um arquivo com o comando &quot;dd&quot; vazio e formatar pra ser um pendrive virtual? Podemos fazer a mesma coisa na RAM, criar um espaço de bits que vai servir pra dar mount num sistema de arquivos e descomprimir o conteúdo do arquivo initrd pra lá.&lt;/p&gt;

&lt;p&gt;Por isso que mesmo se você mandar encriptar sua partição principal ou configurar volumes em RAID, sempre vai ter pelo menos essa pequena partição de boot logo no começo do HD que é formatado em FAT32. Aí o firmware da máquina consegue acesso. Não desperdiça muito espaço, é super pequeno, com não muito mais que uns 100 megabytes. Por isso o initrd é um arquivo compactado, pra caber nessa partição. No Windows mesmo, se você abrir a ferramenta de Disk Management e olhar seu HD, veja, tem essa partição maior onde fica o Windows, e seria o que um BitLocker encriptaria se você escolher.&lt;/p&gt;

&lt;p&gt;Mas olha aqui no começo, tem um partição pequena de EFI, que é o equivalente no Linux ao &quot;/boot&quot; que estamos analisando. Você não enxerga essa partição de boot EFI no Windows porque ele não monta com nenhuma letra de drive, pra não aparecer pros usuários e correr o risco deles estragarem o boot. No caso do Windows, lá no fim do HD tem outra partição escondida sem letra de drive que é a partição de recuperação. Ele é como se fosse o pendrive do Ubuntu que bootamos pra instalar. É o que boota quando você pede pro Windows iniciar em modo de recuperação pra conseguir diagnosticar algum problema que tá bloqueando o boot do Windows principal. É outro Windows, com kernel e drivers separados, em versão reduzida só com ferramentas de diagnóstico e recuperação.&lt;/p&gt;

&lt;p&gt;No caso de Linux não temos uma partição assim, porque normalmente usamos pendrive mesmo. Mas é a mesma coisa, a gente gasta um espaço no HD pra ter a comodidade de não ter que sair procurando pendrive se uma hora seu Windows parar de bootar depois de um Windows Update. Já tá tudo no seu HD. Como espaço de HD é barato, podemos desperdiçar algumas centenas de megas. Se você tá pensando em apagar essa partição pra ter um pouco mais de espaço, é hora de pensar em comprar um HD maior, isso sim. Em Macs também tem uma partição escondida com um MacOS reduzido só com ferramentas de manutenção que você consegue acessar se na hora do boot deixar apertado Command + R.&lt;/p&gt;

&lt;p&gt;Enfim. Nesse segundo estágio de boot, vamos acabar com um drive virtual em RAM, chamado initramfs ou file system de RAM de inicialização, onde o conteúdo do initrd.img vai ser descomprimido e com isso temos um mini Linux inicializado em memória. Agora sim, a kernel vai ter acesso a drivers extras que precisa e diversas outras ferramentas que pode precisar pra pular pra próxima fase.&lt;/p&gt;

&lt;p&gt;Estamos na terceira e última fase do boot. Agora que começa aquela etapa que já viu no boot do seu Linux, quando vai correndo um monte de linhas que eu sei que você nunca parou pra ler. Aquele monte de linhas é o gerenciador de serviços reportando cada um dos serviços que tá conseguindo carregar naquele momento. Esse gerenciador pode ser o famigerado systemd que tem na grande maioria das distros modernas, ou o OpenRC que tem em distros como Slackware ou Gentoo, ou o Runit que acho que é um dos mais antigos, derivados da era dos UNIX originais. Acho que distros baseadas em BSD ainda usam também. Mas seja lá qual for, esse é o primeiro processo que a kernel vai carregar no seu sistema, o famoso processo com ID 1, o PID número 1.&lt;/p&gt;

&lt;p&gt;Agora sim, esse sistema de inicialização vai se encarregar de carregar todo o resto dos serviços necessários pra ter um sistema usável. Agora ele consegue acessar a partição principal do seu HD e ler o arquivo &quot;/etc/fstab&quot;, que é a tabela de file systems onde tá declarado que partição é pra montar como &quot;/&quot;, a raíz do seu sistema operacional. Ele vai dar mount em tudo que precisa, incluindo os mounts especiais como o /proc, /dev, /sys. Vai carregar os serviços de USB pra achar seu mouse ou sua webcam, serviço de bluetooth que vai conectar com seu fone de ouvido, serviço de network que vai carregar seu wifi, serviço de dhcp client que vai pedir IP pro servidor de DHCP do seu roteador de internet e assim por diante.&lt;/p&gt;

&lt;p&gt;Eu zoei que você nunca prestou atenção nesse monte de linhas subindo no boot porque vai rápido demais mesmo. Pra ver em detalhes o que aconteceu, use o comando &lt;code&gt;dmesg&lt;/code&gt; depois do boot que lá vai ter essas linhas. Se um serviço deu pau, use o comando `systemctl statusad  e o nome do serviço pra ver se tem alguma dica. Esses serviços tem mais um detalhe na realidade. Logo que a kernel carrega um systemd ou runit, ele tem que ver em qual runlevel vai rodar. Isso também meio que varia, especialmente entre Linux e UNIX ou BSD, mas todos tem conjuntos diferentes de serviços que carregam em runlevels diferentes. Mas, Runlevel? Aí fodeu, deixa eu voltar na história um pouquinho.&lt;/p&gt;

&lt;p&gt;Vamos voltar pro boot, a kernel tá carregada, ele carrega seu primeiro programa, de process ID 1, que antigamente era chamado simplemente de &quot;init&quot;. Esse init ia num diretório como &quot;/etc/rc3.d&quot; e lá ia achar vários links simbólicos pra scripts que costumavam ficar em &quot;/etc/init.d&quot;. Então cada sub-diretório &quot;rc1.d&quot; ou &quot;rc3.d&quot; podia ter coleções diferentes de links simbólicos de scripts.&lt;/p&gt;

&lt;p&gt;Os scripts em si ficavam em &quot;/etc/init.d&quot;. É o que a gente chama de serviços ou daemons. Eram scripts de shell, que aceitavam parâmetros como &quot;start&quot; ou &quot;stop&quot; e dentro configuravam e executavam algum binário, por exemplo, programa e impressora pra montar o spool de impressão, ou o programa X que carrega a interface gráfica e assim por diante. Um daemon é um programa executável normal, com a diferença que tem esse script que inicia ou termina o programa. E o sistema de init executa esses scripts pra inicializar os serviços durante o boot do sistema.&lt;/p&gt;

&lt;p&gt;Agora, nesse tal de &quot;/etc/rc3.d&quot; o “rc” é acrônimo pra &quot;run commands&quot; ou rode comandos do &quot;runlevel&quot; ou nível de execução 3. Parece complicado mas originalmente cada runlevel tem uma função específica, o runlevel 0 é pra procedimento de shutdown, o runlevel 6 pra reboot. Runlevel 1 é pra bootar em modo de um usuário só. Runlevel 3 é pra multi-usuário com rede. Runlevel 5 costuma ser pra bootar com interface gráfica.&lt;/p&gt;

&lt;p&gt;Se você é de Windows, ele vem com pelo menos 2 runlevels, o do boot normal que cai no Windows e aquele &quot;Modo de Segurança&quot; ou &quot;Safe Mode&quot;, lembra? Que boota o Windows sem carregar todos os drivers e serviços, daí você fica sem rede, com resolução baixa, tudo pra quando tiver algum problema no boot, ter um ambiente pra bootar sem carregar quase nada, pra te dar a oportunidade de diagnosticar e consertar o problema. Runlevel é isso, quais conjuntos de serviços vai carregar ou não.&lt;/p&gt;

&lt;p&gt;Vamos ver isso na prática. Abrindo o terminal de novo, podemos digitar o comando &lt;code&gt;runlevel&lt;/code&gt; e vai dizer que estamos com o runlevel 5 carregado. Digamos que instalamos algum programa gráfico instável que travou tudo. Teclado não funciona, mouse não funciona, nenhuma janela se mexe. Vou simular carregando o Firefox. Faz de conta que foi ele que travou tudo. Nesse ponto a maioria de vocês ia apertar o botão de reset e rebootar. Mas a gente que é de Linux tem outra saída: usar a combinação de teclas Ctrl + Alt + F3 pra mudar pro runlevel 3 que é de multi-usuário com rede em modo texto.&lt;/p&gt;

&lt;p&gt;Olha isso, saímos do modo gráfico, que continua rodando no runlevel 5. Agora fazemos login em modo texto e podemos usar comandos como &lt;code&gt;ps aux | grep firefox&lt;/code&gt;. Lembra? Na nossa historinha, ele travou tudo, então vamos matar forçadamente com o comando &lt;code&gt;kill -9&lt;/code&gt; e o PID dele. Pronto, agora podemos voltar pro modo gráfico. Normalmente eu diria que é só apertar a combinação Ctrl + Alt + F5 mas por alguma razão quando tava escrevendo o script do episódio, isso não funciona, mas Ctrl + Alt + F1 funciona. Enfim, veja como o programa de terminal continua aqui, mas o Firefox morreu. A resolução que zoou aqui, mas deve ser porque tá rodando numa máquina virtual. Esse é um dos jeitos de tentar destravar alguma coisa antes de ser drástico e dar reset na máquina toda.&lt;/p&gt;

&lt;p&gt;Dá pra mudar de runlevels no terminal também usando o comando &lt;code&gt;telinit&lt;/code&gt; com o número do runlevel. O runlevel 1 na realidade é usado pra manutenção e diagnósticos. E se você mudar pro runlevel 6, isso vai dar reboot na máquina, vamos ver? &lt;code&gt;sudo telinit 6&lt;/code&gt; e olha só, rebootou. Sabe quando você usa no terminal comandos como &lt;code&gt;shutdown&lt;/code&gt; ou &lt;code&gt;reboot&lt;/code&gt;? É isso que ele tá fazendo, mudando pro runlevel 6. E por que isso é importante? Runlevels determinam quais conjuntos de serviços iniciar ou desligar.&lt;/p&gt;

&lt;p&gt;Você nunca quer forçadamente apertar reset. Tem vários programas rodando em background, coletando arquivos de log, tem partições montadas que podem ainda não ter feito flush de caches. Você quer notificar todos os programas rodando pra fechar e limpar as coisas antes de realmente desligar a máquina, é o que chamamos de graceful shutdown ou desligamento gracioso. Se ficar dando reset forçado, uma hora vai corromper arquivos ou partições no sistema, por isso nunca force desligar, sempre dê halt, que é mudar pro runlevel 0. Vamos desligar? &lt;code&gt;sudo shutdown -h now&lt;/code&gt; ou simplesmente &lt;code&gt;sudo telinit 0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Na prática, quando o sistema boots, a kernel carrega e inicia um processo 1 como o antigo &quot;init&quot; ou o atual &quot;systemd&quot;, ele vai carregar o conjunto de serviços declarados pro runlevel padrão, que costuma ser o runlevel 3 em servidores que não tem interface gráfica ou o runlevel 5 em desktops e PCs como de vocês assistindo, que vai bootar os serviços pro modo gráfico. Na prática significa que você tem diversas formas diferentes de bootar, só que o padrão pra maioria das instalações é entrar no runlevel 5 que é gráfico.&lt;/p&gt;

&lt;p&gt;Vamos fazer outro exercício. Digamos que de jeito nenhum consigo entrar no modo gráfico depois de um upgrade ou script mal feito que rodei na máquina, sei lá. Não boota. Nesse ponto a maioria das pessoas já ia pensar &quot;que bosta, cadê meu pendrive, vou reinstalar tudo do zero&quot;. Calma! No Ubuntu, ele faz como o Windows, tenta ir direto pra tela gráfica. Mas se antes do computador ligar deixarmos a tecla shift apertada, a tela do GRUB vai aparecer.&lt;/p&gt;

&lt;p&gt;GRUB é acrônimo pra Grand Unified Bootloader. Como o nome diz, é um bootloader. É ele que o computador vai carregar no boot, e depois se encarregar de achar a kernel do Linux e passar o controle pra ele. O Ubuntu sempre boota com Grub, mas ele carrega quieto, por isso que nunca vemos. Com a tecla shift apertada forçamos a aparecer. Por padrão vai bootar com essa primeira opção aqui, mas podemos apertar a tecla &quot;e&quot; pra editar os comandos de boot da kernel. Daí vamos pra essa linha aqui embaixo e no final escrevemos o número 3, indicando que queremos bootar no runlevel 3 em vez do padrão que é o 5. Com ctrl + x saímos e vai bootar os serviços do runlevel 3, e olha só, não carrega mais o modo gráfico, podemos logar em modo texto e tentar consertar seja lá o que eu fiz de errado. Daí, no próximo boot vai normalmente pro runlevel padrão que é o 5, de modo gráfico.&lt;/p&gt;

&lt;p&gt;Então a sequência é sempre essa: computador liga, carrega o firmware UEFI da placa mãe que vai inicializar o hardware e os dispositivos. Ele vai procurar a primeira partição no HD ou pendrive que tenha o diretório /boot/efi. No nosso caso, dali vai carregar o bootloader do GRUB. O GRUB por sua vez tem uma configuração, que foi o que editamos agora, que diz onde tá a kernel e onde tá a imagem do initrd. Executa a kernel, monta uma partição temporária na RAM chamada initramfs e descomprime a imagem do initrd lá.&lt;/p&gt;

&lt;p&gt;Finalmente, a kernel vai carregar um gerenciador de serviços como o systemd como processo número 1 que, por sua vez, vai começar a carregar o resto dos serviços, conforme declarado na runlevel padrão. Cada serviço tem declarado quais outros serviços ele precisa esperar carregar, qual dependência tem, e é isso que vai aparecendo naquele monte de linhas que vai subindo na sua tela no boot. Até finalmente terminar de carregar o servidor Xorg, e o gerenciador de login, no nosso caso, o GDM do GNOME e é aí que aparece a tela gráfica de login.&lt;/p&gt;

&lt;p&gt;Esses conceitos de runlevels, diretórios de scripts de serviços em &quot;/etc/init.d&quot;, links simbólicos pros scripts em diretórios como &quot;/etc/rc5.d&quot; e o processo chamado &quot;init&quot; que era o antigo gerenciador de serviços, é o sistema que chamamos System V ou SysV que nasceu nos UNIX originais. Foi assim que eu aprendi em UNIX dos anos 90, e é assim que gerenciadores como o Runit e acho que o OpenRC ainda funcionam. Mas em distros modernas, nada disso vale mais, pode esquecer tudo que eu falei.&lt;/p&gt;

&lt;p&gt;Distros como Ubuntu, Fedora, OpenSuse, Manjaro e muitos outros, especialmente se usam gerenciador de janelas GNOME, aderiram ao SystemD e ele descarta todos esses conceitos. Não existe runlevels em systemd e sim targets. E apesar de ter diretórios como &quot;rc3&quot; ou &quot;rc5&quot; eles não são mais usados. Em vez disso existem diretórios como &quot;graphical.target&quot; ou &quot;multi-user.target&quot;. E eles tão espalhados em diversos diretórios como &quot;/etc/systemd/system&quot; ou &quot;/usr/lib/systemd/system&quot; e assim vai. Ele tenta simular o comportamento dos runlevels pra manter compatibilidade com comandos como &lt;code&gt;telinit&lt;/code&gt; que usamos antes.&lt;/p&gt;

&lt;p&gt;É uma das razões de porque tanta gente odeia o systemd e você vai achar fios de reddit e grupos em discord de gente xingando. Eu mesmo não entendo o systemd a fundo, nem de perto. Ele faz bem mais que o Init de SysV antigo e tenta fazer muito mais coisas por baixo dos panos. Eu me acostumei a usar, mas quem desenvolve pra Linux deve ter mais motivos técnicos de porque isso incomoda. Só o fato de mudar conceitos tradicionais com runlevels pra targets, deve dar um trabalho extra pra manter software pra diferentes distros que não aderiram a esse novo padrão.&lt;/p&gt;

&lt;p&gt;Por exemplo, em &quot;/usr/lib/systemd/system&quot; tem esses links simbólicos que tenta equiparar o que seriam os antigos runlevel numerados com os atuais targets. Veja, o runlevel5 é um link simbólico pra graphical.target. O runlevel6 é link pro reboot.target. Vamos ver o que é o graphical.target e olha só, é uma configuração que fala que requer carregar a mesma coisa que o multi-user.target, o antigo runlevel 3, mas também quer carregar o display-manager.service. Faz sentido, o modo gráfico nada mais é do que o modo multi-usuário que carrega em texto, mas carregando o serviço de tela pra mostrar interface gráfica.&lt;/p&gt;

&lt;p&gt;No terminal, com o comando &quot;runlevel&quot; ainda consigo ver os runlevels rodando agora, e com o novo comando &lt;code&gt;systemctl get-default&lt;/code&gt; consigo ver que o target padrão de boot é o graphical.target. Se quiser mudar pra das próximas vezes bootar em modo texto, posso usar o comando &lt;code&gt;systemctl isolate multi-user.target&lt;/code&gt;, que é mais ou menos o equivalente antigo a usar o comando &quot;telinit&quot;. Cuidado ao fazer isso porque ele vai rebootar o sistema, mas não vai mudar o target padrão, que ainda vai ser graphical.target.&lt;/p&gt;

&lt;p&gt;Pra todas as vezes bootar só em modo texto, posso mudar o target padrão fazendo &lt;code&gt;systemctl set-default multi-user.target&lt;/code&gt;. Esse comando &quot;systemctl&quot; você vai ver em diversos tutoriais. Quando instala Docker por exemplo, o tutorial costuma mandar habilitar o serviço pra carregar em todo boot fazendo &lt;code&gt;systemctl enable docker&lt;/code&gt;, que vai adicionar no target de graphical.target. E pra carregar o serviço na hora pode fazer &lt;code&gt;systemctl start docker&lt;/code&gt;. Pra ver se o serviço tá carregado, basta fazer &lt;code&gt;systemctl status docker&lt;/code&gt;, e pra ver todos os serviços que estão carregados agora é só rodar &lt;code&gt;systemctl --type service&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A maioria dos tutoriais hoje assume que você está usando uma distro com systemd e por isso sempre vai pedir pra rodar comandos como &lt;code&gt;systemctl start&lt;/code&gt;. O systemd em si faz bem mais coisas que iniciar ou rebootar serviços, então recomendo que leia sobre ele em wikis como do site do Arch ou Gentoo, que costumam ter exemplos de como usar e mais detalhes do funcionamento. Como disse antes, eu mesmo não sei todos os detalhes dele.&lt;/p&gt;

&lt;p&gt;Enfim, o objetivo de hoje foi juntar o antigo video de Ubuntu e um pouco dos videos de WSL2 com a minissérie sobre armazenamento. Recomendo que assistam agora se ainda não viram. Pra continuar a minissérie, no próximo episódio quero explicar mais conceitos de Linux mostrando como uma instalação realmente funciona, então se ficaram com dúvidas, mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal pra não perder o próximo episódio e compartilhe o video com seus amigos. A gente se vê, até mais!&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5994</id>
    <published>2022-08-29T10:01:00-03:00</published>
    <updated>2022-08-29T10:03:04-03:00</updated>
    <link href="/2022/08/29/akitando-126-criando-uma-rede-segura-introducao-a-redes-parte-6-vpn-e-nas" rel="alternate" type="text/html">
    <title>[Akitando] #126 - Criando uma Rede Segura | Introdução a Redes Parte 6 - VPN e NAS</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/EOmzo5d0F9Y&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Hoje vamos fechar a minissérie de Introdução a Redes. Meu objetivo todo foi poder falar sobre minha nova solução de armazenamento e edição de videos com NAS e na sequência, como criei uma rede segura pra conseguir acessar coisas como meu Media Server, mesmo estando do meu celular numa rede insegura fora de casa, que é a mesma solução que empresas tem usado pra conseguir dar infraestrutura pra funcionários remotos. Hoje vamos entender como VPNs funcionam.&lt;/p&gt;

&lt;h2&gt;Conteúdo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;00:44 - Cap 1. Meu Primeiro NAS de backup&lt;/li&gt;
&lt;li&gt;04:38 - Cap 2. Meu Setup Novo de Gravação&lt;/li&gt;
&lt;li&gt;07:24 - Cap 3. O Novo NAS&lt;/li&gt;
&lt;li&gt;14:17 - Cap 4. DNS Privado com Pi-Hole&lt;/li&gt;
&lt;li&gt;18:31 - Cap 5. Meu “Netflix” Particular&lt;/li&gt;
&lt;li&gt;21:56 - Cap 6. Recapitulando Redes e dispositivos virtuais&lt;/li&gt;
&lt;li&gt;27:49 - Cap 7. Redes Seguras com Zero Tier&lt;/li&gt;
&lt;li&gt;36:57 - Cap 8. Como funciona uma VPN?&lt;/li&gt;
&lt;li&gt;43:51 - Conclusão&lt;/li&gt;
&lt;li&gt;45:59 - Bloopers&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;WHAT HAPPENED TO GEEKBRIEF TV? (https://www.thegeekpub.com/4173/what-happened-to-geekbrief-tv/)&lt;/li&gt;
&lt;li&gt;Pi-Hole cloudflared (DoH) (https://docs.pi-hole.net/guides/dns/cloudflared/)&lt;/li&gt;
&lt;li&gt;Media Server Setup Pt. 3b: Prowlarr/Sonarr/Radarr (https://morrismotel.com/servarr-pt3b-prowlarr-sonarr-radarr/)&lt;/li&gt;
&lt;li&gt;Jellyfin (https://jellyfin.org/)&lt;/li&gt;
&lt;li&gt;Plex Media Server (https://www.plex.tv/media-server-downloads/)&lt;/li&gt;
&lt;li&gt;Getting Started with Software-Defined Networking and Creating a VPN with ZeroTier One (https://www.digitalocean.com/community/tutorials/getting-started-software-defined-networking-creating-vpn-zerotier-one)&lt;/li&gt;
&lt;li&gt;Docker Networking Overview (https://docs.docker.com/network/)&lt;/li&gt;
&lt;li&gt;Hamachi VPN (https://www.vpn.net/)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Finalmente, hoje vou voltar a falar do meu novo NAS e na segunda metade dar mais um tópico sobre redes que me interessa, como acessar meu NAS fora de casa, sem comprometer a segurança da minha rede de casa. Na verdade, eu acho que o objetivo todo dessa mini-série de redes que ando fazendo foi justamente pra chegar no episódio de hoje pra falar do meu NAS novo. De volta aos meus videos sobre armazenamento, lá explico tudo sobre como HDs funcionam e como eu posso configurar vários HDs num único volume pra ter redundância e mais segurança. Se não assistiram, recomendo ver depois. Entender arquivos e entender redes é o mínimo do mínimo que um programador precisa saber.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Foi lá por 2009 que eu comprei meu primeiro NAS caseiro. Naquela época eu usava Mac e nos primórdios de podcast e webcast eu assistia uma reviewer de tech chamada Cali Lewis, que fez propaganda de uma nova marca muito da hora chamada Drobo. Desde aquela época eles faziam um lance mais inteligente do que RAID normal. Um RAID clássico precisa de HDs de mesma capacidade pra montar o array. Significa que se eu colocar um HD diferente maior, o espaço que sobra ele desperdiça. Mas o Drobo usava um sistema proprietário que conseguia usar o máximo de discos de tamanho diferente. Foi o que me chamou a atenção.&lt;/p&gt;

&lt;p&gt;Por quase 10 anos eu usei esse Drobo, primeiro acho que foram uns 4 HDs de 512 GB ou 1 TB cada. Com o tempo eu ia trocando pra HDs maiores um de cada vez. Mas acho que nunca usei nada maior que uns 4 TB por HD. E durante essa quase uma década eu já tive problema de HD realmente falhar e graças ao Drobo eu conseguia só trocar esse HD falhando e ele conseguia se reconstruir sem perder nada. Porém a interface dele acho que era USB 2, com seus meros 480 megabits, o que não dá muito mais que 60 megabytes por segundo.&lt;/p&gt;

&lt;p&gt;Mas como era mais pra backup, e não pra acessar o tempo todo, eu deixava fazendo backup a noite, aí não importava tanto se era muito lento. Eu tive uma pane no Drobo inteiro uma vez que ele não ligava mais. Comprei outro Drobo, a versão 5N, que já era USB 3. Só tirei os HDs do primeiro que quebrou, coloquei no novo na mesma ordem, e tudo voltou a funcionar. E com a vantagem que com USB 3 já era até 10 vezes mais rápido que o USB 2. Chegava perto dos 100 megabytes por segundo. Que continua sendo 2 a 3 vezes mais lento que um bom HD mecânico, e pelo menos 4 ou 5 vezes mais lento que um SSD SATA.&lt;/p&gt;

&lt;p&gt;Durante a pandemia resolvi experimentar com um novo NAS e peguei um Synology DS420j, de tamanho similar ao Drobo. Depois de mais de 10 anos, o Drobo meio que parou no tempo. O software dele é super limitado e não oferece nada além do básico. Diversas outras marcas como QNap, Synology e mesmo opções open source como TrueNAS pra você montar seu próprio NAS num gabinete de PC evoluíram e deixaram o Drobo no chinelo. Resolvi ver o que os concorrentes estavam fazendo. E o Synology eu achei bacana porque é basicamente um Linux, com Linux Raid rodando mdadm por baixo e suportando filesystems open source como ext4 ou btrfs.&lt;/p&gt;

&lt;p&gt;Ele não é tão sofisticado quanto um TrueNAS, que tem ZFS e consegue facilmente suportar armazenamento em nível de data center, com arrays de dezenas de HDs e petabytes de dados. Mas TrueNAS não é pra amadores. Não basta só seguir um tutorial na internet e fazer funcionar da primeira vez. Se você nunca precisou recuperar um TrueNAS com problemas, seus dados vão estar inseguros quando você mais precisar. Não adianta nada um software sofisticado que você vai cometer erros de amador e potencialmente corromper seus dados no meio do caminho. E não adianta só fazer funcionar da primeira vez. O teste de fogo é quando alguma coisa der pane e você realmente precisa recuperar dados. Se nunca passou por isso, considere que vai fazer errado da primeira vez.&lt;/p&gt;

&lt;p&gt;Como eu não quero virar administrador de sistemas nem trabalhar em data center, eu queria uma solução que usasse componentes open source que eu confio como Linux Raid, mas com suporte comercial decente e softwares maduros que me ajude a não cometer erros de amador. E a Synology oferece tudo isso. Sim, é mais caro que montar um do zero com TrueNAS, mas é mais confiável, e pra isso eu pago mais caro. Valor é relativo. Não adianta nada ser barato pra montar mas na hora de recuperar dados, você cometer um erro e perder tudo. Aí saiu super caro. Todo amador comete esse erro de só considerar o valor inicial das coisas.&lt;/p&gt;

&lt;p&gt;O DS420j foi bacana, mas ele tinha o mesmo problema do Drobo, interfaces lentas. Ele tinha ou USB 3 ou conexão de ethernet 1 gigabit. Como eu disse no primeiro episódio desta série de redes, 1 gigabit é ok. Pra maioria dos casos é velocidade suficiente. Mas por causa do canal, mais e mais eu estava lidando com arquivos de centenas de gigabytes. Eu gravo meus videos em 4K com codec de edição DNxHR, isso dá uma média de quase 100 gigabytes por hora de gravacão. Alguns episódios eu preciso ficar umas 2 horas ou mais gravando, então toda vez da 200 gigas ou mais. E não, se deu vontade de comentar &quot;ah, porque você não grava de jeito X ou jeito Y&quot;, obrigado pelo palpite, mas se eu gravo nesta configuração é porque eu já sei que é a melhor. E isso nem é considerado muito. Estúdios profissionais que gravam em 8K estão acostumados a lidar com terabytes por projeto.&lt;/p&gt;

&lt;p&gt;Agora pensa, transferir 200 gigas a uma velocidade de uns 120 megabytes por segundo significa quase meia hora pra transferir tudo pro Synology. 120 megabytes por segundo é quase saturar a conexão ethernet de 1 gigabit. É o máximo que meu Synology conseguia ir. E no Drobo era mais lento ainda. E isso só pra fazer backup. Eu precisava ter terabytes de SSD no meu PC pra editar isso. Pra editar video 4K desse tamanho, HD mecânico é muito lento. E assistindo videos de outros youtubers como Linus Tech Tips, MKBHD e outros eu sabia que todos editam videos direto dos NAS deles, usando redes acima de 2.5 gigabits.&lt;/p&gt;

&lt;p&gt;Em 2021 eu atualizei meu equipamento de gravação. Antes minha câmera Sony A7III gravava em SD cards. Mas ele tem aquela maldita limitação de parar de gravar automaticamente depois de 30 minutos. Toda câmera fotográfica faz isso por causa de mais uma maldita lei da Europa. Europeu é campeão de criar leis imbecis. De qualquer forma SD cards não são confiáveis e um dia tive problema de corromper meus arquivos e perdi um episódio. Cansado dessas limitações eu comprei um Atomos Ninja V. Agora ligo a saída HDMI da minha câmera nesse Atomos, que por sua vez tem um SSD SATA e ele grava direto do HDMI em arquivo no SSD, sem limitações. Isso resolveu o problema de video. De novo, se tava indo comentar &quot;porque não grava direto no PC&quot;, novamente, pra mim não era uma boa solução. Obrigado pelo palpite.&lt;/p&gt;

&lt;p&gt;Daí um belo dia meu gravador de áudio, que era uma Tascan velha, comecou a dar pau e mastigar os arquivos de áudio. Cansei dele e comprei um gravador novo, um Zoom H5. Agora ele grava tanto em SD card, mas eu ligo a saída de áudio na entrada do Atomos, então o Atomos grava tanto o video da câmera quanto o áudio do gravador tudo junto e eu ainda tenho uma segunda cópia no gravador, via SD Card. Daí resolvi o audio também.&lt;/p&gt;

&lt;p&gt;Agora eu queria resolver a edição e backup dos videos originais. E sim, eu guardo e sempre vou guardar os originais. Novamente, não precisa ir nos comentários dizer pra apagar os originais pra economizar espaço. Um profissional nunca joga fora originais. E pra isso eu queria transferir os arquivos do SSD do Atomos direto pro meu NAS em rede 10 gigabit. 10 gigabit seria um pico de mais de 1 gigabyte por segundo. E esse meu Synology DS420j que é um modelo caseiro, não oferece upgrade de hardware, tipo colocar placa de 10 gigabits. Então resolvi que era hora de investir num NAS profissional, o modelo DS1821+. A série que termina com  &quot;j&quot; acho que é de entrada, é bom pra quem tá começando. A série 21 plus é profissional. Tem modelos ainda maiores, mas pra mim esse seria suficiente.&lt;/p&gt;

&lt;p&gt;Pra começar, esse modelo tem 8 baias de HDs. Então decidi ir all in e comprar nada menos que 6 HDs Seagate IronWolf de 12 Terabytes. IronWolf são HDs feitos pra NAS, eles tem mais durabilidade, aguentam mais vibrações, tem cache maior. É como um HD caseiro, tipo modelo Barracuda, mas muito mais resistente, e obviamente mais caros. Liguei os 6 no novo Synology, e montei um volume com 5 deles em modo SHR que é Synology Hybrid RAID. Em vez de um RAID normal que exige HDs de tamanho igual, ele consegue aproveitar o espaço de HDs diferentes melhor de um jeito proprietário dele. Lembra o que eu falei que o Drobo fazia? Hoje em dia outros fabricantes já superaram. No meu caso, isso virou um volume de 42 terabytes, e o 6o HD fica desligado como hot spare. Se um dos 5 HDs falhar, o NAS automaticamente desliga o que falhou, liga o hot spare no lugar e começa a reconstruir o volume sem perder nada.&lt;/p&gt;

&lt;p&gt;E como é pra ir all in, resolvi fazer todos os upgrades que esse modelo permite. Coloquei 2 NVMEs de 1 terabyte cada dedicados só pra ser cache. Assim arquivos mais acessados ficam no NVME, em vez de ter que ir nos HDs mecânicos. E nem é muito que precisava porque apesar de serem HDs mecânicos e muita gente ficar me perguntando se um RAID feito com SSDs não seria mais rápido. A resposta deveria ser óbvia: não, o custo simplesmente não compensa. Mas na cabeça de vocês, se imagina que porque estou lidando com HDs mecânicos, vai ser mais lento que um SSD. E é aí que estão errados. Em IOPS talvez, mas em velocidade não. Cada HD desses é capaz sozinho de mais de 200 megabytes por segundo.&lt;/p&gt;

&lt;p&gt;Lembra da explicação de dispositivos de bloco e como arquivos são divididos em blocos e como esses blocos são dividos nos vários HDs de um RAID? Cada HD no RAID contribui com mais ou menos metade da velocidade pra leitura e escrita. Então cada um contribui com pelo menos 100 megabytes por segundo pra velocidade total, indo pra mais de 500 megabytes por segundo. Pra dar perspectiva isso é mais de 4000 gigabits. E eu estou arredondando por baixo, em benchmarks eu estou conseguindo ir até 6000 ou 7000 gigabits. Sua rede cabeada de casa é só 1 gigabit, mas na minha que é 10 gigabits, consigo suportar isso.&lt;/p&gt;

&lt;p&gt;Pra conseguir essas velocidades na rede eu comprei a placa de rede 10 gigabits e comprei cabos de rede cat 6a com conectores blindados. Não adianta nada placa de rede boa com cabos que não aguentam a velocidade. Se eu falei que consigo ir na faixa acima de 7000 gigabits por segundo, às vezes 8000, significa que estou usando a maior parte do que minha rede consegue. E pra tudo falar nessa velocidade, também comprei uma placa de 10 gigabits pro meu PC e um pequeno switch TP-Link que suporta 10 gigabits em todas as portas.&lt;/p&gt;

&lt;p&gt;Aliás, cuidado com switches com marketing duvidoso. Eu vi caixas de switches baratos dizendo que suportam 5 gigabits, e tem 5 portas, aí você pensa que cada porta suporta 5 gigabits, mas não, em letras miúdas eles falam que o aparelho suporta até o máximo de 5 gigabits ou seja, cada porta é só 1 gigabit. No meu caso são 5 portas de 10 gigabits cada, então o aparelho todo suporta 50 gigabits. E isso é importante, porque lembra todo o processamento de checagem de erros, roteamento e tudo mais que um switch precisa fazer? Tudo isso exige CPU que consiga processar o tráfego de 50 gigabits por segundo. Ele fica bem quente quando tá ligado trabalhando.&lt;/p&gt;

&lt;p&gt;Finalmente, o último upgrade desse all in foi trocar os 4 gigabytes de RAM que o DS1821 vem e colocar 16 giga de RAM ECC, que tem correção de erros, pra evitar single event upset de raios cósmicos, por exemplo, como expliquei no segundo video da série, de correção de erros. Minha memória ECC provavelmente tá rodando Hamming Code, pra recuperar qualquer bit flip acidental. Tudo isso pra garantir que nenhum dado nessa caixa seja corrompida de jeito nenhum.&lt;/p&gt;

&lt;p&gt;E agora sim, eu consigo gravar os videos crus depois da gravação direto pro NAS em velocidade máxima e consigo usar o DaVinci Resolve pra editar videos direto do NAS, sem precisar ter nada em SSD local. Vou te falar que a qualidade de vida é incrível, meu fluxo de trabalho fica bem mais fácil. Isso tudo já é equipamento que um pequeno estúdio de gravação teria, pra ter 2 ou 3 editores de video trabalhando direto do NAS. Pra estúdio maior, eles teriam racks de HDs, cada uma com mais de uma porta de 10 gigabits, talvez numa rede de 100 gigabits por segundo. Nos videos do Linus ele tem o projeto petabyte, onde montam rack com TrueNAS e dezenas de HDs ou SSDs com ZFS que suportaria uma dúzia de editores de video tudo trabalhando por rede 2.5 gigabit, lidando com videos 8K de terabytes de tamanho. O que eu montei é a versão junior disso.&lt;/p&gt;

&lt;p&gt;Dá pra ir muito longe. Mas isso foi só pra solucionar meu fluxo de trabalho de video que, honestamente nem é tão pesado assim. Eu sou o único editor e faço tudo sozinho. Portanto sim, esse investimento foi bem mais do que eu precisaria. Só pra ter uma idéia, o NAS, os HDs, os upgrades de SSD, rede e RAM, deram um total de bem mais que 30 mil reais. Se fosse lá fora teria custo fácil mais de 3 mil dólares. Em dólar nem parece tão caro. Um computador moderno como um Mac Studio custa mais que isso. Mas falando em reais, sim, é um terço de um carro popular. Qualidade é proporcional ao preço.&lt;/p&gt;

&lt;p&gt;Mas, eu não ia usar só pra videos. A vantagem desse NAS é que ele vem com um bom processador, um AMD Ryzen quad-core de 2.2 Ghz cada. Não chega nem aos pés dos 16 core que meu Ryzen 9 5850 do meu PC tem, mas pra um NAS tá sobrando. Um NAS é nada mais, nada menos, que um PC com Linux. Na teoria você não deveria, mas pode rodar qualquer outro software dentro dele.&lt;/p&gt;

&lt;p&gt;Não deveria porque a função de um NAS é ser dedicado a lidar com arquivos. Num ambiente onde outras pessoas estivessem trabalhando e puxando arquivos o tempo todo pra editar, a CPU ia ficar trabalhando só pra entregar os blocos o mais rápido possível. Fora que você não quer que, por acidente, um aplicativo com bug acabe corrompendo seus dados. Então nunca rode nada diferente num NAS de trabalho. Mas como é meu NAS caseiro onde eu sou a única pessoa com acesso, quis aproveitar o processamento sobrando pra mais coisas.&lt;/p&gt;

&lt;p&gt;O software da Synology já suporta Docker nativamente, com interface gráfica facinha de usar e acesso a terminal via ssh se eu quiser rodar manualmente. Então comecei a fuçar coisas pra rodar nele. Em outro episódio vou finalmente explicar mais detalhes sobre Docker e containers, mas por hoje entenda Docker como uma forma fácil, controlada e isolada de instalar software no meu NAS sem grandes riscos.&lt;/p&gt;

&lt;p&gt;Vamos recapitular o básico de redes. Toda vez que eu navego em sites, normalmente não me preocupo porque todos oferecem HTTPS, então tudo que trafega entre meu navegador e o site é criptografado. Mas isso é depois que já sei o endereço IP do site. Pra saber o endereço IP baseado no domínio como google.com ou apple.com, alguém precisa me dizer que apple.com é o IP x.y.z. E quem sabe disso é um DNS, lembram?&lt;/p&gt;

&lt;p&gt;No terceiro episódio da série eu disse que meu PC ganha endereço IP privado atrás de um NAT graças ao serviço de DHCP do meu roteador, que funciona como um síndico do prédio. Eu ligo meu PC e ele pergunta pro síndico, &quot;ow, que endereço ip tem sobrando pra mim&quot;, e o DHCP responde &quot;tem esse 192.168.1.200, beleza?&quot; e aí o PC assume esse endereço. Junto com o endereço vem outras configurações, como o endereço de DNS, que eu disse pra mudar pra ser 1.1.1.1 da CloudFlare ou 8.8.8.8 do Google e evitar usar o DNS que o provedor já pré-configurou pra você caso se preocupe com privacidade, porque DNS de provedor não oferece DoH, que é DNS over HTTPS.&lt;/p&gt;

&lt;p&gt;O navegador abre conexão encriptada via TLS com o site depois que sabe o endereço ip do site, mas a requisição pro DNS, perguntando &quot;qual ip que é apple.com&quot; vai em texto puro, aberto, sem criptografar. E se você deixou o DNS do provedor, ele sabe que sites você navegou, que horas, a partir de qual lugar. Não chega a ser nenhuma informação tão crucial assim, mas pra quem se importa com privacidade, é mais uma informação sobre você que está vazando. Tem gente que diz &quot;foda-se&quot; e tudo bem, não precisa comentar isso, seu palpite é irrelevante. Estou falando com quem se importa.&lt;/p&gt;

&lt;p&gt;Não só isso. A gente usa plugins de adblock pra bloquear propagandas em sites, mas eles não conseguem pegar tudo sempre. Toda ajuda a mais é bem vinda. Por tudo isso resolvi que o primeiro container que queria instalar no NAS seria do aplicativo Pi-Hole. Ele se chama assim porque foi originalmente feito pra instalar num raspberry-pi, que é como eu usava antes. Mas ele é levinho e mover pro NAS significa um equipamento a menos que preciso dar manutenção.&lt;/p&gt;

&lt;p&gt;O Pi-Hole é um servidor de DNS e que oferece uma aplicação web pra dar um monitoramento gráfico pra gente. O que eu faço é configurar pra usar o DNS da CloudFlare 1.1.1.1 e ele faz cache de tudo. E ele também se sincroniza com alguns bancos de dados mantidos pela comunidade de domínios reconhecidos como sites de propaganda ou mesmo de malwares. Daí se algum site onde navego pede alguma coisa desses domínios, o Pi-Hole rejeita e não me deixa navegar pra lá.&lt;/p&gt;

&lt;p&gt;Daí o que eu faço é colocar o endereço IP do NAS como DNS no DHCP do meu roteador. Daí todos os equipamentos de casa, meu PC, minhas TVs, meus consoles de video game, meu celular, até minhas lâmpadas inteligentes, recebem a mesma configuração e passam a usar o Pi-Hole como servidor de DNS. A outra vantagem é que ele consegue fazer requisições de DoH ou DNS over HTTPS. Lembram que eu falei que toda requisição de pergunta pra DNS vai em texto aberto? Com DNS over HTTPS ele primeiro abre uma conexão segura via HTTPS e manda a pergunta criptografada. Agora tudo é criptografado, dificultando alguém saber meu padrão de navegação na web.&lt;/p&gt;

&lt;p&gt;Pra isso precisei de um segundo container de Docker, oferecido pela própria Cloudflare, que é o serviço cloudflared. Lembrando que &quot;d&quot; no final é de daemon. Não vou entrar em detalhes porque a documentação diz tudo que precisa saber mas o container em si já vem pré-configurado apontando pros servidores de DNS que suportam HTTPS da Cloudflare. Daí eu aponto meu Pi-Hole pra esse daemon e pronto, meu navegador pergunta pro Pi-Hole, que abre uma conexão segura com a Cloudflare e resolve o domínio via HTTPS e nesse percurso, ninguém no meio do caminho, incluindo meu provedor, conseguem saber em quais sites estou navegando. Se gostaram da solução, vou deixar links pro que falei na descrição do video abaixo.&lt;/p&gt;

&lt;p&gt;O próximo container eu falei um bocado nas stories do meu Instagram então quem acompanhou sabe. Faz décadas que venho baixando animes, série e filmes. Tem coisas que eu assistia nos anos 80 e 90 e que não existem mais em nenhuma plataforma de streaming de hoje, em particular animes antigos. Tem outros que existem no Japão, mas nunca vieram pro Ocidente. Enfim, tem milhares de conteúdos que se você só assiste Netflix, nunca ficou sabendo que existe. Aliás, a seleção do Netflix é bem ruim e incompleta. Sinto pena de quem só conhece o que tem lá.&lt;/p&gt;

&lt;p&gt;Sim, tecnicamente é pirataria. Não estou incentivando mas eu também acho que quem quer piratear vai piratear independente. Avisar que não pode nunca fez nenhum efeito. É que nem foto de câncer em pulmão em caixa de cigarro. Isso nunca funcionou, como ex-fumante eu sempre ignorei completamente aquilo. Cada um é adulto e lide com as consequências. Quer fumar, fume. Só não incomode os outros. Quer piratear, pelo menos pirateia direito.&lt;/p&gt;

&lt;p&gt;Que atire a primeira pedra quem nunca baixou nada via bittorrent. Em outro episódio falo sobre ele, mas por hoje, todo mundo pelo menos sabe que é o melhor jeito de piratear. Eu sou da época do LimeWire, eDonkey, Kazaa, Napster e outros, mas bittorrent é animal. E existem softwares open source como Jellyfin, Radarr, Sonarr e Prowlarr que fazem o seguinte.&lt;/p&gt;

&lt;p&gt;Jellyfin é um projeto em cima de outro conhecido, o Emby, é basicamente um software que organiza sua mídia, videos, música e tudo mais numa interface parecida com da Netflix, com tocador e tudo mais. Vale a pena pesquisar. Ele vasculha suas pastas de mídia e faz índice, baixa capas dos filmes, notas do Rottentomato, informações de elenco num IMDB e organiza bonitinho pra você.&lt;/p&gt;

&lt;p&gt;Mas você ainda tem que ficar baixando as coisas manualmente, renomeando, colocando nas pastas certas. E se desse pra ser tudo automático? É pra isso que servem o Radarr, Sonarr e Prowlarr. Você pode assinar o RSS de bittorrent de séries que ainda estão saindo e o povo de fansub ainda tá legendando, e quando ficam disponíveis, baixa sozinho pra você, renomeia, baixa capa e tudo mais e quando você liga a TV, já está organizado no seu Jellyfin. Dependendo da série que tá acompanhando, se for muito famosa, no mesmo dia que saiu na Amazon Prime, algumas horas depois já aparece no seu Jellyfin. É literalmente como se você tivesse seu próprio Netflix particular, só que melhor.&lt;/p&gt;

&lt;p&gt;Hoje não vou mostrar como configura tudo isso mesmo porque eu sou das antigas e uso mais pra baixar coisas que não se encontra fácil. Séries novas eu prefiro assistir direto dum Crunchyroll da vida. Eu assino todos os serviços de qualquer forma. Mesmo filmes que gosto muito compro em Blu-Ray, então quando baixo, normalmente já tinha comprado, só baixo porque é mais fácil pra assistir. E em vez de Jellyfin, prefiro usar o Plex. Ele é meio chatinho com planos pagos e tudo mais, mas é o que tem a interface mais bem feita, melhores apps pra celular e TV, e realmente funciona igual uma Netflix. Dêem uma olhada. Eu tenho séries de anime, ele organiza as temporadas, episódios que já assisti, posso começar a assistir no PC e continuar na TV, perfeito.&lt;/p&gt;

&lt;p&gt;O único problema é que ele só funciona dentro de casa, na minha rede local. O Plex é o concorrente do Jellyfin, Emby, Kodi e outros, tem planos pagos se eu quiser acessar de fora, mas digamos que prefira continuar usando só os recursos gratuitos, como faço pra poder assistir meus animes quando estiver viajando, na casa dos meus pais ou em qualquer lugar fora de casa?&lt;/p&gt;

&lt;p&gt;E agora vem a última coisa que queria explicar de redes pra vocês. Por causa da pandemia, muita gente ganhou a opção de trabalhar remotamente de casa e aqui o problema é oposto. Como você faz pra poder ter acesso a servidores da empresa por exemplo? É aqui que entra a solução de uma VPN, Virtual Private Network ou redes virtuais privadas.&lt;/p&gt;

&lt;p&gt;VPN existem diversos tipos e de novo,não sou especialista em redes então quero compartilhar com vocês só o básico. Até aqui a gente sempre assume que redes funcionam com dispositivos de hardware como placas de rede, que fisicamente tem um buraco pra espetar um cabo ethernet com conector RJ-45. Ou uma placa Wifi que você conecta num ponto de acesso. Mas em ambos os casos se trata de dispositivos físicos de hardware.&lt;/p&gt;

&lt;p&gt;Pro sistema operacional falar com esses e outros dispositivos eles precisam de um pedaço de software chamado driver. Esses drivers normalmente são carregados juntos com a kernel do seu sistema operacional, seja Linux, Windows, Mac, Android, iOS, não importa. Se tem um hardware, a kernel precisa de um driver. Num Windows você enxerga os dispositivos no gerenciador de dispositivos e pode ver detalhes, tentar atualizar os drivers por lá e tudo mais. Em Linux pode usar comandos como &lt;code&gt;lspci&lt;/code&gt; pra listar os dispositivos ligados no bus ou barramento de PCI. Enfim, o importante é a kernel conseguir enxergar todos eles.&lt;/p&gt;

&lt;p&gt;Mas tecnicamente pro sistema operacional, meio que não importa se o hardware existe de verdade, contanto que exista um driver que diz que existe. Presta atenção. Um driver pode mentir pro sistema. É o que acontece quando se instala uma máquina virtual com um VirtualBox da vida. Dentro dela manda instalar o Windows e ela vai instalar drivers que dizem pra esse Windows que tem uma placa gráfica, embora não tenha hardware disso. O driver está emulando um hardware e desenhando na janela do VirtualBox. O Windows virtualizado acha que é uma placa de verdade. Ele não tem como saber a diferença, ele tá dentro da Matrix. E isso acontece com todos os dispositivos. Dentro da máquina virtual os drivers dizem que tudo existe, mas na prática os drivers conversam com o VirtualBox do lado de fora e assim possibilita compartilhar meu teclado de verdade, mouse, som e tudo mais.&lt;/p&gt;

&lt;p&gt;E se fosse possível eu ter placas de redes virtuais? E aqui entra os conceitos de TUN e TAP. Lembra a idéia que pacotes de rede são divididos em camadas? No modelo OSI temos camada 1 que é física, camada 2 que é data link, que é o que ethernet se preocupa, tem a camada 3 que é network que é onde a parte IP de TCP/IP se preocupa. Na prática TUN é um dispositivo de rede virtual que atua na camada 3 de rede e TAP cria um dispositivo de rede virtual que atua na camada 2. Isso é uma tecnicalidade que pra gente não importa muito, mas só pra saber. Nos diversos componentes de uma infraestrutura de rede, cada um lida com pacotes em camadas diferentes. A gente que é programador acaba só lidando com camada de aplicação, mas coisas como placas de rede ou switches atual da camada 3 pra baixo, não se importando com as camadas superiores.&lt;/p&gt;

&lt;p&gt;Numa máquina virtual, se instala um driver de rede, pra simular que existe rede, e o Windows virtualizado fala com uma placa que não existe, um TAP, que por baixo cria um bridge, ou ponte, entre essa placa virtual e a placa real do PC que é host. Graças a essa placa virtual, dentro da máquina virtual ele ganha um IP tipo 172 ponto alguma coisa. E tudo que é roteado por essa placa, chega na placa real e é roteado pra internet. Na prática é como se existisse uma nova sub-rede privada dentro da rede interna.&lt;/p&gt;

&lt;p&gt;Eu uso VMWare na minha máquina, e se abrir as configurações avançadas de rede, olha só o que eu acho, várias placas de rede. Quase todas virtuais. Essa Ethernet aqui é a minha placa de 10 gigabits de verdade que falei antes. Essa outra é a de 2.5 gigabit que vem embutida na placa mãe. Só essas são as de verdade. Mas essas outras aqui são placas virtuais pra coisas como minhas máquinas virtuais. Mesmo num Docker você pode subir container em dois modos de rede, host ou bridge. Se for host, vai subir como se fosse qualquer outro programa no seu PC, usando as mesmas configurações de rede local. Mas se escolher bridge, o container vai falar com uma placa de rede virtual TAP, que vai criar uma sub-rede separada, e rotear os pacotes por essa ponte até meu PC host.&lt;/p&gt;

&lt;p&gt;A vantagem de usar bridge é o seguinte. Digamos que eu suba dois containers de web, que internamente vai carregar um programa servidor web que quer se conectar na porta 80. Ambos estão em modo host. O primeiro vai carregar e se pendurar na porta 80 do meu PC. Mas o segundo container vai falhar, porque vai tentar se conectar na porta 80, mas já tá ocupado, então o sistema operacional recusa e rejeita o pedido do container. Agora, se rodar ambos em modo bridge, eles ganham uma rede vazia com outro range de endereços IP estilo 172 alguma coisa que do lado de fora ninguém enxerga. É como sua rede 192.168 local que do lado de fora ninguém enxerga por causa do NAT.&lt;/p&gt;

&lt;p&gt;Cada container, nessa rede 172 privada, consegue se ligar na porta 80 interna do container, porque é como se estivessem numa máquina vazia com uma placa de rede só pra eles. Mas do lado de fora podemos mapear a porta 80 de dentro do container com a porta 8080 de fora do meu PC, e o segundo container sube na porta 80 interna e podemos mapear pra porta 8081 do mesmo PC. Ambos os programas acreditam que estão sozinhos, conseguem ligar em portas 80 virtuais, mas do lado de fora estamos remapeando as portas virtuais pra outras portas livres de verdade. Máquinas virtuais e containers são formas de eu mentir pros programas acharem que estão rodando numa máquina sozinhos. E do lado de fora eu controlo o que quero expor pra elas. De novo, containers de Docker estão na Matrix e não sabem disso.&lt;/p&gt;

&lt;p&gt;Não são só tecnologias de virtualização e containers que usufruem de placas de rede virtuais. E agora entra os TUN que significa tunel. A parte que eu falei em outros episódios que não queria explicar é roteamento e agora vou mostrar só o mínimo pra vocês entenderem onde quero chegar. Eu mostrei já no episódio 3 desse mini-série como pacotes conseguem sair da minha rede pelo roteador do provedor e ir pra internet. E mostrei rapidamente uma tabela de rotas. Todo sistema operacional com TCP/IP mantém uma tabela de rotas.&lt;/p&gt;

&lt;p&gt;Ela que diz pra mandar todo tráfego pro roteador, que costuma ter o endereço IP mais baixo da sub-rede interna como 192.168.1.1 ou 10.0.0.1 ou algo assim. No meu caso é esse TP-Link que deixo do lado do modem. Ele recebe os pacotes e também tem uma tabela de rotas como essa só que no caso dele a saída padrão é mandar pro modem, e o modem por sua vez passa pra frente pra outros servidores na internet. Essa é a tarefa de um default gateway, a porta padrão de saída.&lt;/p&gt;

&lt;p&gt;Lembra no episódio anterior onde mostrei o exemplo onde estou numa rede de empresa rígida que restringe uso da internet e fecha quase todas as portas e nem deixa eu navegar em todos os sites? E como usando um servidor da Digital Ocean eu furei um buraco no firewall e abri um túnel seguro encriptado usando SSH? Uma VPN é um programa diferente de SSH mas em conceito funciona parecido, furando buraco em firewall ou no meu caso no CGNAT da Vivo, se conectando num servidor de fora e abrindo um túnel.&lt;/p&gt;

&lt;p&gt;Mas em vez de eu abrir o túnel manualmente com o comando SSH uma VPN costuma instalar uma placa de rede virtual TUN. No caso de Linux cria um dispositivo como &lt;code&gt;/dev/tun&lt;/code&gt;. Esse dispositivo ganha um endereço IP inválido de internet como 172 alguma coisa. E o software de VPN faz mais uma coisa, cria entradas nessa tabela de roteamento que falei.&lt;/p&gt;

&lt;p&gt;Dá pra configurar de varias formas. Por exemplo, posso adicionar na tabela que toda vez que do meu PC de casa eu tentar acessar um IP que só existe na rede da empresa, os pacotes são direcionados não pra minha placa de rede de verdade, mas sim pra esse dispositivo virtual TUN que na realidade é um tunel criptografado ligado no servidor de VPN da empresa.&lt;/p&gt;

&lt;p&gt;Vamos dar um exemplo. Como já falei existem dezenas de formas diferentes de VPN. Existe OpenVPN, existe Microtik, soluções baseadas em Wireguard como Tail Scale, mas eu escolhi usar a Zero Tier, que achei simples, fácil de usar, e resolve o meu problema de expor meu Plex de anime fora de casa. Pra começar crio uma conta lá e uma nova rede. Essa rede tem um código identificador que, obviamente estou escondendo uma parte. Ela começa com a84.&lt;/p&gt;

&lt;p&gt;Agora vou no meu NAS e instalo o docker da ZeroTier, que é um programa cliente que vai se conectar no servidor deles. Seja no meu NAS, seja num PC com Linux, primeiro preciso carregar o driver pra criar o tal dispositivo virtual de TUN. Eu me conecto no NAS via SSH e, como root, crio o seguinte script:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;echo -e '#!/bin/sh -e \ninsmod /lib/modules/tun.ko' &amp;gt; /usr/local/etc/rc.d/tun.sh
chmod a+x /usr/local/etc/rc.d/tun.sh
/usr/local/etc/rc.d/tun.sh
ls /dev/net/tun
/dev/net/tun&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Estou só escrevendo um arquivo com o comando insmod e o caminho pro módullo de tun pra kernel carregar. Na sequência estou dando permissão pra esse script executar. Eu falei nos videos de Ubuntu pra estudar chmod e permissões lembram? Daí rodo da primeira vez manualmente, no próximo boot já vai carregar sozinho. No final checo se o dispositivo virtual apareceu com o comando ls pra listar e de fato, tá lá. Agora tenho o equivalente a uma placa de rede virtual, de mentira.&lt;/p&gt;

&lt;p&gt;Ainda do terminal, agora baixo e rodo o container da zerotier que é um programa cliente que vai se conectar no servidor deles, furando um buraco em firewall e CGNAT.  Relembrando, o conceito de furar é porque do lado de fora, meu PC é inacessível por conta de firewall ou NAT do provedor. O servidor da ZeroTire não tem como abrir uma conexão comigo. Mas eu consigo me conectar com o servidor deles. Pra entrar está bloqueado, mas pra sair está liberado. Agora que eu abri a conexão, o servidor da ZeroTier consegue falar de volta comigo por essa mesma conexão. Isso que significa &quot;furar&quot;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker run -d           \
  --name zt             \
  --restart=always      \
  --device=/dev/net/tun \
  --net=host            \
  --cap-add=NET_ADMIN   \
  --cap-add=SYS_ADMIN   \
  -v /var/lib/zerotier-one:/var/lib/zerotier-one zerotier/zerotier-synology:latest&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Nessa configuração eu faço esse container enxergar o dispositivo TUN que acabamos de criar, configuro rede pra ser host, ou seja, sem ser ponte, configuro um volume externo pra configuração que é onde o container vai gravar coisas que ele precisa e que não podem desaparecer se o container bootar. E é isso. Quando terminar de subir vai se conectar e vai ganhar um número identificador da zero tier.&lt;/p&gt;

&lt;p&gt;Ainda pelo terminal, posso mandar comandos pra esse container e o que preciso fazer é mandar se conectar naquela rede da ZeroTier que criei que começa com a84 lembra? É um numerozão. Se você se registrar no site e criar uma nova rede, obviamente o número vai ser diferente. Então vamos conectar esse cliente que instalei via Docker com minha rede privada no servidor da ZeroTire.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker exec -it zt zerotier-cli join e5cd7a9e1cae134f&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Pronto. Do lado do NAS é só isso. Agora vou no meu celular e no meu PC e baixo o aplicativo da ZeroTier e instalo nos dois. É o equivalente ao docker que instalei no NAS, um aplicativo cliente da ZeroTire. No meu celular abre esse aplicativo feioso aqui. Mesma coisa, eu clico em Add Network e cadastro a mesma rede que começa com a84 e já posso conectar nela. No meu PC depois que instalei e rodei, aparece aqui embaixo na barra de tarefas. Mesma coisa, me junto na mesma rede. Note que em cada dispositivo diferente ele tem um outro número identificador. No meu celular lá embaixo tem esse outro número que eu escondi que começa com &lt;code&gt;eeb&lt;/code&gt;, é o identificador do meu celular. E no app do PC mesma coisa, ele tem esse &quot;My Address&quot; que começa com &lt;code&gt;3d7&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Pronto, todo mundo configurado e conectado na mesma rede da minha conta na Zero Tire. Último passo, volto pro site, abro a configuração da rede &lt;code&gt;a84&lt;/code&gt; e lá embaixo vai ter uma lista de dispositivos que se conectaram. E olha só, meu PC que começa com &lt;code&gt;3d7&lt;/code&gt; apareceu, meu NAS que começa com &lt;code&gt;d46&lt;/code&gt; aparece e meu celular que começa com &lt;code&gt;eeb&lt;/code&gt; apareceu. Agora só preciso autorizar todos com esses checks do lado e pronto. Tá todo mundo conectado na mesma rede privada. Nessa nova sub-rede todo mundo tem endereços 172.22 ponto alguma coisa. Por exemplo, meu NAS ganhou o endereço 172.22.61.42, o mesmo vale pra todos os dispositivos, cada um com um endereço IP. Esses endereços não são válidos na internet, só dentro dessa rede privada.&lt;/p&gt;

&lt;p&gt;Agora vamos entrar no meu celular com o ZeroTier desligado. Se eu abro o app do Plex, que não pago a assinatura, olha que tristeza. Vazio. Vamos fechar, abrir o ZeroTier e conectar na rede dele. Agora abro o app do Plex de novo e voilá!! Olha aqui minhas bibliotecas, e todos os meus animes bonitinho aparecendo! Estou via 4G, fora do Wifi de casa. Estou conseguindo falar com o docker de Plex no meu NAS porque estamos conectados na mesma sub-rede privada da ZeroTier via VPN.&lt;/p&gt;

&lt;p&gt;Mesma coisa no meu notebook. Se estivesse fora de casa, em vez de usar o endereço IP da minha rede que é 192.168 ponto alguma coisa, ligo o ZeroTier e no navegador digito aquele endereço 172.22 porta 32400 e olha só, estou navegando no Plex de dentro do NAS, mesmo estando fora da minha rede. Mas como isso é possível? E aí vem o truque que a ZeroTier e outros softwares de VPN fazem. Vamos abrir o terminal do Windows.&lt;/p&gt;

&lt;p&gt;Digitando &lt;code&gt;route print -4&lt;/code&gt; pra mostrar rotas de ipv4, tenho rotas pra todo endereço 192.168 interno da minha rede. É com essa tabela que ele sabe pra onde mandar pacotes. Mas depois que habilito o ZeroTier, podemos rodar o comando de route de novo e vemos que adiciona novas entradas nessa tabela. Olha só, ele passa a reconhecer endereços 172 ponto alguma coisa e o default gateway é esse dispositivo 172.22.121.226. Vamos de volta na janela de configurações avançadas de rede do Windows, onde lista dispositivos de rede e olha aqui o dispositivo virtual TUN da ZeroTier. É como se fosse uma placa de rede normal, mas ela não existe, é o driver mentindo pro Windows, ela é virtual.&lt;/p&gt;

&lt;p&gt;Botão direito e propriedades. Vamos ver que configurações de IPV4 ele tem e olha o endereço, 172.22.121.226, que é o default gateway que registrou na tabela de roteamento. Por isso que toda vez que tento navegar, por exemplo, pro meu Plex, com o endereço 172.22.61.142, o sistema pega os pacotes, manda pro dispositivo virtual, que por sua vez é só um software cliente que tá conectado no servidor da ZeroTier. Ele faz forward desses pacotes, ou seja, redireciona pra fora, pra ZeroTier. E no mesmo servidor está conectado o cliente de docker que instalei no NAS, que recebe os pacotes por essa conexão.&lt;/p&gt;

&lt;p&gt;O que eu tinha mostrado de abrir túnel SSH no episódio anterior é o modelo que se chama de client to site ou acesso remoto. Mas nesse caso eu tenho dois clientes atrás de uma rede privada conversando como se estivessem na mesma rede, ambos com clientes conectados num servidor de VPN, é o modelo site to site ou gateway to gateway. Depois estudem esses termos. Mas na prática, essa solução da ZeroTier cria um túnel criptografado entre meu celular ou meu notebook com o NAS usando um serviço no meio que permite burlar as restrições de cada uma das redes, seja firewalls, seja CGNATs.&lt;/p&gt;

&lt;p&gt;Isso que mostrei da ZeroTier é uma solução boa quando você precisa acessar um servidor específico na sua casa ou na empresa, dentro de uma rede privada virtual. Mas isso deve soar um pouco diferente do que você já deve ter ouvido falar de VPN. Se eu tentar navegar na Web, esse tráfego não vai pra ZeroTier. Só endereços 172.22 que vão ser enviados pelo túnel, todo o resto é roteado normalmente pela sua rede local, sem mudar nada.&lt;/p&gt;

&lt;p&gt;O que você conhece de VPN é o contrário, que todo tráfego Web passa pelo túnel da VPN e sai pelos servidores deles. Muita gente que vê propaganda de VPN pensa neles como uma forma de burlar o filtro de região de sites como Netflix ou Amazon Prime pra conseguir assistir séries que não passam no Brasil. Ou o uso mais correto que é conseguir navegar com segurança mesmo quando estiver em redes não-confiáveis como o Wifi do Starbucks.&lt;/p&gt;

&lt;p&gt;Falando sobre redes públicas, obviamente, nunca, jamais, digite sua senha de qualquer coisa enquanto estiver conectado em rede da faculdade, do aeroporto, do starbucks ou o que for. Lembra no terceiro episódio da minissérie quando falei sobre o problema de hubs, modo promíscuo e tudo mais? Você não sabe quem está escutando seus pacotes numa rede aberta. Sempre que estiver numa rede pública, use uma VPN pra acessar coisas como seu banco. Assim você garante que vai estar tudo criptografado e mesmo se tiver alguém fuçando seus pacotes de rede, não vai conseguir ler nada. Se usa um celular android da Samsung, eles já tem embutido um serviço de VPN barato que ativa automaticamente quando se conecta em Wifi público. Recomendo testar se puder, mas qualquer um serve. NordVPN, ExpressVPN, qualquer um.&lt;/p&gt;

&lt;p&gt;O que acontece quando você usa um cliente de VPN? Por acaso eu uso o NordVPN. Ele oferece opção de se conectar em servidores que eles mantém pelo mundo todo. Então vou me conectar num dos Estados Unidos. E pronto, conectado. Agora navego na web normalmente e parece que nada mudou. Vamos praquele site que mostra qual é meu ip, what is my ip. E olha só, fala que o endereço que ele enxerga de mim é um endereço nos Estados Unidos. Justamente porque todos os pacotes da minha máquina estão sendo tunelados pra um servidor da NordVPN nos Estados Unidos e de lá indo pros sites que quero navegar. Do ponto de vista dos sites, é um computador nos Estados Unidos que tá conectado.&lt;/p&gt;

&lt;p&gt;Foi isso que fizemos com SSH no episódio anterior, lembra? Local Forwarding? É a mesma coisa, só que mais fácil de usar em vez de ter que digitar aquele comando grande no terminal. Mas o que de fato tá acontecendo? Pra isso vamos abrir a tabela de rotas de novo na linha de comando com &lt;code&gt;route print&lt;/code&gt;. Veja a primeira linha e a última linha. Estou simplificando mas ele basicamente diz pra qualquer endereço IP que quero acessar de 0 até 255, pra mandar pro default gateway 192.168.1.1 que é meu roteador como mostrei no terceiro episódio. Mas agora vamos ver quando estou conectado na NordVPN.&lt;/p&gt;

&lt;p&gt;Olha o que apareceu. Logo no começo e lá no final da tabela de rotas, agora tem novas entradas apontando pra interface 10.5.0.2. E quem diabos é isso? Ainda na linha de comando digitamos &lt;code&gt;ipconfig /all&lt;/code&gt; no Windows pra ver todos os dispositivos de rede e olha só. Tem um dispositivo NordLynx Tunnel. Um dispositivo virtual de rede. O Windows acha que é uma placa de rede, mas não tem hardware por baixo. Ele cria um túnel e roteia os pacotes pela minha placa de rede de verdade, só que criptografado e sempre pro servidor que eu conectei na rede da Nord.&lt;/p&gt;

&lt;p&gt;Como que o sistema operacional decide se deve mandar pacotes pra rota anterior do meu roteador 192.168 ou mandar pra rota nova da VPN em 10.5? É por causa desse número de métrica aqui no fim, que é um número de prioridade. Ele vai mandar pro menor número. E de curiosidade, esse “on-link” significa que são endereços que podem ser resolvidos localmente, sem precisar rotear pra fora. Claro, são dispositivos virtuais locais, estão na mesma máquina. Tem um tanto de outras rotas aqui mas é porque meu PC tem máquinas virtuais instaladas, e elas também tem placas de rede virtuais que precisam ser roteadas. Na sua máquina, vai ser uma tabela diferente, mas o princípio é o mesmo, a primeira coisa que vai ter é a rota pro seu roteador. É assim que os pacotes saem da sua máquina e sabem pra onde ir.&lt;/p&gt;

&lt;p&gt;Toda vez que você aperta o botão pra conectar na VPN, o que acontece por baixo dos panos é que ele adiciona essas novas entradas na tabela de roteamento. Quando desconecta, apaga essas linhas, aí seus pacotes voltam a trafegar pelo seu roteador como sempre. É isso que um cliente de VPN faz, cria um dispositivo virtual e reconfigura sua tabela de rotas pra desviar os pacotes por esse dispositivo virtual.&lt;/p&gt;

&lt;p&gt;Mas voltando pra conversa da ZeroTier e meu servidor de anime. Sem uma solução de VPN, todos esses dispositivos não conseguiriam se conectar através da internet, só seriam acessíveis na mesma rede local, no caso, dentro da minha casa. Mas agora meu NAS fica sempre conectado na rede da ZeroTier, esperando pacotes virem de lá até a placa de rede virtual TUN que criamos, e a partir daí, pro resto do sistema e aplicações, é como se fosse uma conexão local na mesma rede. E é assim que muitas empresas conseguiram ter funcionários trabalhando de casa, em redes restritas atrás de CGNAT, abrindo conexão em servidores que estão nas empresas, de forma segura, porque todas as soluções de VPN são criptografadas.&lt;/p&gt;

&lt;p&gt;Por muito tempo se usou protocolos como PPTP que é Point to Point Tunneling Protocol, pra criar túneis ponto a ponto seguros, mas se não estou muito errado ninguém mais usa PPTP, já é obsoleto e acho que considerado inseguro. Se esbarrar num tutorial com PPTP pode pular porque deve ser velho. Muita gente ainda usa soluções derivadas de OpenVPN, que é um software complexo, escrito em cima de outro software complexo e controverso que é o OpenSSL. Mas no mundo Linux pelo menos, acho que povo tá migrando pro Wireguard, com código mais moderno e mais leve, que agora vem direto na kernel do Linux. Eu mesmo não sei muitos detalhes, mas se o assunto de redes e segurança te atrai, definitivamente estude Wireguard.&lt;/p&gt;

&lt;p&gt;Como disse antes, a ZeroTier usa alguma solução proprietária que parece que é criptograficamente um pouco menos seguro que Wireguard, mas não significa que seja inseguro. Se quiser um serviço equivalente que use Wireguard por baixo, tem a Tail Scale, que funciona mais ou menos do mesmo jeito que mostrei. E no caso de soluções pra empresas, acho que muitos usam os produtos da Microtik. Só lembrando que nada disso sai de graça, e não digo no custo de assinatura dos serviços. Pacotes tendo que passar por outras máquinas intermediárias no meio do caminho, em túnel criptografado, adiciona tempo de processamento e tempo de rota, por isso seus pacotes vão chegar no destino levando mais tempo e por isso conexões atrás de VPN tem velocidade menor e latência maior. O que varia nos diversos serviços por aí é a oferta de servidores próximos de você que podem ajudar a diminuir essa latência. Pra navegar na web não faz diferença mas pra coisas como gamers, latência é importante.&lt;/p&gt;

&lt;p&gt;Enfim, no frigir dos ovos consegui fazer o que queria. Configurei um NAS super rápido, o suficiente pra conseguir editar video pesado direto dele em rede de 10 gigabits local e consigo acessar coisas do meu NAS como meu Plex, mesmo estando fora de casa, usando uma VPN da ZeroTier. Tudo isso foi super simples de configurar porque já estou mais que familiarizado com VPNs, túneis e tudo mais. Do momento que decidi usar o ZeroTier, já estava com tudo configurado em todos os dispositivos em uns 15 minutos.&lt;/p&gt;

&lt;p&gt;Mesmo você não sendo alguém de infraestrutura, é importante saber esses conceitos porque em pouquíssimo tempo dá pra criar soluções pra problemas simples como o meu, que era expor meu NAS na internet de forma segura. Eu falo como é impressionante o quanto se aprende quando brinca de pirataria. Mesmo gamers já tiveram que lidar com esse tipo de tecnologia. Quem aqui nunca arrancou os cabelos tentando fazer o Hamachi funcionar? Pra quem não sabe Hamachi é que nem o NordVPN, um serviço de VPN da LogMeIn. Por alguma razão gamers tendem a usar Hamachi.&lt;/p&gt;

&lt;p&gt;Eu poderia ter criado tudo na mão, um servidor particular numa Amazon ou Digital Ocean, um túnel local forwarding com SSH ou mesmo uma VPN com Wireguard, tudo configurado via linha de comando no terminal, na mão. E é uma boa opção pra quem quer aprender. Mas eu queria algo plug and play. Como exercício deixo de lição de casa pra vocês que tem um PC em casa e um notebook, criar uma VPN usando Wireguard ou SSH, sem usar um software mais fácil como ZeroTier ou NordVPN. Acho que só o tanto de documentação pra ler e tentativa e erro que vão ter que fazer vai valer mais que muito curso de redes que tem por aí.&lt;/p&gt;

&lt;p&gt;E com isso finalmente consegui concluir a mini-série de redes. Em seis episódios mostrei só o básico do básico do que considero introdução ao assunto de redes e internet. Ainda tem bem mais coisas e muito mais detalhes, como VLANs. Mas vou deixar pra falar sobre essas coisas mais pra frente. Por agora cansei de falar de redes. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e não deixem de compartilhar o video com seus amigos. A gente se vê, até mais!&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5993</id>
    <published>2022-08-17T10:32:00-03:00</published>
    <updated>2022-08-17T10:33:42-03:00</updated>
    <link href="/2022/08/17/akitando-125-burlando-proxies-e-firewalls-introducao-a-redes-parte-5-ssh" rel="alternate" type="text/html">
    <title>[Akitando] #125 - Burlando Proxies e Firewalls | Introdução a Redes Parte 5 - SSH</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/T-jHuFnxZ2k&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Ainda não é um episódio &quot;de segurança&quot; mas vou começar mostrando como é fácil injetar um bug de segurança tendo a melhor das intenções e depois como manipular tunelamento de SSH pra abrir buracos em firewalls e bypassar proxies. Coisas que podem ser muito úteis na vida real.&lt;/p&gt;

&lt;h2&gt;Conteúdo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;00:57 - todo programa é como uma função&lt;/li&gt;
&lt;li&gt;02:52 - CAP 1 - fazendo um site com injection&lt;/li&gt;
&lt;li&gt;06:34 - One Liners&lt;/li&gt;
&lt;li&gt;08:25 - Sanitização&lt;/li&gt;
&lt;li&gt;09:57 - CAP 2 - furando firewall&lt;/li&gt;
&lt;li&gt;13:47 - criando um droplet na DigitalOcean&lt;/li&gt;
&lt;li&gt;15:41 - reconfigurando sshd&lt;/li&gt;
&lt;li&gt;18:06 - ssh remote forwarding&lt;/li&gt;
&lt;li&gt;21:26 - CAP 3 - furando proxies&lt;/li&gt;
&lt;li&gt;26:25 - ssh local forwarding&lt;/li&gt;
&lt;li&gt;30:45 - CAP 4 - consertando a vaca&lt;/li&gt;
&lt;li&gt;33:17 - conclusão&lt;/li&gt;
&lt;li&gt;34:27 - bloopers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Finalmente, agora que posso assumir que assistiram meus últimos quatro videos sobre introdução a redes e internet, posso começar a falar de coisas mais interessantes. Vocês já sabem como informação trafega na rede, como processos se ligam a portas, sockets e tudo mais. Então vamos falar rapidinho sobre alguns conceitos de segurança em rede. Só pra esclarecer, o tema de hoje não é o geral de segurança, só algumas brincadeiras que eu acho interessante saber.&lt;/p&gt;

&lt;p&gt;A primeira parte da história vai envolver um pouco de programação web e um dos buracos de segurança mais comum. E na segunda parte vou mostrar alguns truques que dá pra fazer com SSH em alguns cenários. A idéia não é ser um video completo sobre segurança, só mencionar dois aspectos que podem ser úteis. Eu já fiz um video inicial de segurança e em episódios futuros talvez eu entre em mais detalhes. Mas por hoje são coisas que programadores iniciantes ainda nem sabem que dá pra fazer.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Pense assim, um programa é como se fosse uma função. Ele pode receber algum input seu, do usuário, processar alguma coisa dentro, e cuspir alguma resposta. Sempre são essas três grandes partes, input, processamento e output. Mesmo no design de um chip de CPU temos as três grandes partes de fetch, decode e execute. Se for um programa long lived, que persiste e fica rodando em background, como expliquei no episódio passado, é um Big Loop esperando algum input seu na forma de coisas que digita ou que clica na tela. Daí processa alguma coisa, e cospe alguma coisa na sua tela. Em resumo, programas mastigam seu input e cospem um output. Também podemos pensar em programas como alguma coisa que transforma o input em um output. Por exemplo, um Spotify transforma os cliques de mouse na interface em música, que é seu output.&lt;/p&gt;

&lt;p&gt;Antes de internet e antes de redes, você era a única pessoa que entregava algum input pro programa que ia rodar na própria máquina. E a maioria de nós não tem interesse em digitar comandos que destruam a própria máquina. Ninguém vai digitar &lt;code&gt;sudo rm -Rf /&lt;/code&gt; e dar a senha. Esse meme é antigo mas deve ter gente que ainda não sabe o que é isso. Sério, se você não sabe vai assistir meu episódio de Ubuntu. Tudo que você manda sua máquina fazer é responsabilidade sua saber o que vai acontecer. Se não sabe, Google tá aí pra isso.&lt;/p&gt;

&lt;p&gt;Eu gosto de pensar nas minhas máquinas como extensões de mim. Da mesma forma que seu corpo vai estragar se só consomir um monte de refrigerante, doces e pizza, sua máquina também vai estragar se só sair dando copy e paste de comandos e rodando no terminal. Fast food é fast food. Mas o ponto que quero chegar é que partindo da premissa que quem vai executar alguma coisa sabe o que tá fazendo, os programas antigamente eram bem mais ingênuos. Até porque não tinha tanto poder de processamento pra fazer coisas consideradas supérfluas, então o código sempre era o mais simples possível.&lt;/p&gt;

&lt;p&gt;Vamos pegar o exemplo de hello world de node.js que mostrei no episódio anterior e adicionar algumas coisas. Acho que foi no episódio de aprendizado na beira do caos que mencionei de um programinha besta que tem em Linux chamado &lt;code&gt;cowsay&lt;/code&gt; lembram? Você instala e no terminal executa assim &lt;code&gt;cowsay hello&lt;/code&gt; e ele cospe uma vaquinha desenhada com caracteres ASCII mais a sua mensagem. Aí nos primórdios da Web, alguém poderia pensar, &quot;ulha, seria legal eu ter esse cowsay na web&quot;.&lt;/p&gt;

&lt;p&gt;E por que não? Meu hello world do episódio passado é muito chato. Só imprime a mesma mensagem. Eu quero que o usuário consiga fazer a vaca dizer qualquer mensagem. Todo mundo precisa ver a carinha da vaca pra alegrar o dia. Eu quero poder digitar no navegador &lt;code&gt;http://localhost:3000?message=bom+dia&lt;/code&gt; e ver a vaca me dar bom dia. Todo mundo precisa de um bom dia da vaca pra começar bem o dia.&lt;/p&gt;

&lt;p&gt;A primeira coisa é conseguir pegar esse parâmetro de mensagem que vai na URL depois da interrogação. Em HTTP a URL não serve só pra passar o endereço do servidor, mas podem ir mais informações, como pares de chave e valor separado por &amp;amp; comercial depois de uma interrogação. Todo mundo já deve ter visto isso, especialmente se já copiou e colou links de produtos em ecommerces como a Amazon, que notoriamente tem uns links sujos pra cacete. Pra pegar esses parâmetros, primeiro vou importar a biblioteca &lt;code&gt;url&lt;/code&gt; pra processar a URL. E lá embaixo, na função que recebe a estrutura de request, faço o parsing da URL pra me devolver um objeto de query strings.&lt;/p&gt;

&lt;p&gt;Pronto. Com isso tenho acesso àquele parâmetro que chamei de &quot;message&quot; no exemplo da URL. Agora quero conseguir executar o comando &lt;code&gt;cowsay&lt;/code&gt; que instalei no meu Linux. Pra isso todas as linguagens tem a capacidade de instanciar um novo processo filho, literalmente conseguem rodar outro programa. No caso do Node carrego a biblioteca &lt;code&gt;child_process&lt;/code&gt; e pego o comando &lt;code&gt;exec&lt;/code&gt; dele. Já expliquei sobre forks, processos e coisas assim nos episódios de concorrência e paralelismo e no video de Ubuntu.&lt;/p&gt;

&lt;p&gt;Agora vem o principal. Eu chamo a função &lt;code&gt;exec&lt;/code&gt; com a query string &quot;message&quot; que tirei da URL e monto a string do comando de &lt;code&gt;cowsay&lt;/code&gt; como fiz no terminal, usando interpolacão de strings. Toda linguagem tem o equivalente a interpolação de strings, se não sabem nem isso, estudem mais. Essa função &lt;code&gt;exec&lt;/code&gt; vai executar o comando cowsay, e quando terminar chama essa outra função anônima que passei. Ela vai preencher pra mim o que o programa retornou, separado em objetos de erro, standard output ou stdout e standard error ou stderr.&lt;/p&gt;

&lt;p&gt;Como é um exemplo besta, nem vou me preocupar com os casos de erro e ignorar completamente. Obviamente isso não é uma boa prática, mas é só pra simplificar neste exemplo. Quero só pegar o que o programa cuspiu no standard output e preencher a estrutura de response com ela. Pronto, agora salvo, volto pro terminal e executo o programa com o Node. Ele vai dar bind e ficar escutando na porta 3000 como antes. Podemos voltar pro navegador e digitar aquela URL que mostrei antes, localhost 3000 message igual bom dia.&lt;/p&gt;

&lt;p&gt;E boom, olha que bonitinho, mostrou a vaquinha no meu navegador como eu queria. E era assim que muitos sites nos primórdios da Web faziam. Delegavam alguma coisa pra algum programa já instalado no seu sistema, pegavam o que cuspia no stdout, e devolvia no meio de um HTML pro usuário. No meu exemplo besta, nem me preocupei com HTML, só cuspi um texto puro mesmo. Iniciantes podem achar que acabou, mas agora que começam os problemas. Se você não viu nenhum problema no que eu fiz, sabemos que realmente ainda é um iniciante. Não se preocupe, vamos abrir seus olhos.&lt;/p&gt;

&lt;p&gt;Vamos voltar pro terminal no Linux. Se não sabia disso, um shell como Bash ou ZSH ou todos eu acho, permite digitar múltiplos comandos numa mesma linha, um atrás do outro, basta separar por ponto e vírgula. Vamos tentar? Deixa eu digitar &lt;code&gt;cowsay hello; ls -la&lt;/code&gt; e olha o que acontece. Ele executa o cowsay e printa na tela, mas logo em seguida executa o &lt;code&gt;ls -la&lt;/code&gt; e printa embaixo. É um jeito de você fazer one-liners, que são programinhas que cabem numa única linha antes de dar enter. Pra scripts isso é muito útil. Todo mundo que mexe com Linux sabe disso.&lt;/p&gt;

&lt;p&gt;Não precisa ser muito astuto pra pensar. “Hmm, será que eu consigo fazer isso do navegador?” Vamos tentar. Depois da mensagem posso digitar ponto e vírgula e algum comando. E olha só o que voltou!! O meu programa em Node executou o comando seguinte ao cowsay. E agora, eu consigo navegar por todos os arquivos do servidor e executar comandos arbitrários!!! E você pode pensar “ah, mas só dá pra listar coisas, por que isso seria um problema?”. Ok, vamos fazer outro exemplo http://localhost:3000?message=fodeu;cat ~/.ssh/id_rsa.&lt;/p&gt;

&lt;p&gt;Sabe o que eu fiz? Acabei de pegar a chave privada de SSH desse usuário, que é um arquivo que jamais deveria poder ser exposto fora desse diretório, por motivos de segurança. Com ela consigo invadir conta de GitHub, Amazon e tudo mais que essa chave conseguir usar. Eu posso até baixar arquivos binários, basta passar pelo comando &lt;code&gt;base64&lt;/code&gt; pra transformar num texto, vai aparecer no navegador, copio e colo com Notepad em algum lugar e faço decode do base64 de volta pra string. Expliquei isso no video de detecção e correção de erros. Consigo imaginar várias coisas que dá pra fazer só com isso. Se dá pra executar qualquer comando, eu já tenho controle sobre essa máquina inteira. Eu ownei essa máquina. É game over.&lt;/p&gt;

&lt;p&gt;Esse meu programa é exatamente o que um programador amador poderia pensar em fazer: o mínimo de código pra fazer a idéia funcionar. E de fato funcionou, a vaquinha aparece como deveria. E normamente esse mínimo nunca assume que alguém teria a idéia de tentar coisas que o programa não foi desenhado pra fazer. Mas é assim que pensa todo hacker: como eu posso explorar os programas de formas que o autor não previu? E um programador minimamente mais experiente já sabe: eu deveria ter sanitizado o que veio como query string.&lt;/p&gt;

&lt;p&gt;Hoje não é o dia de falar sobre vulnerabilidades, mas isso que acabei de fazer se chama injection, ou injeção, porque estou injetando comandos arbitrários num parâmetro que não foi devidamente tratado, ou sanitizado. Sanitizar significa considerar que tudo que vem do mundo externo de um usuário, por definição, é podre, sujo, e precisamos sanitizar, antes de usar. Sanitizar significa remover caracteres especiais, como esse ponto e vírgula, ou escapar esses caracteres pra serem tratados como string e não como comandos. Existem dezenas de bibliotecas que fazem isso.&lt;/p&gt;

&lt;p&gt;Frameworks como o Express, que é feito em cima do Node, justamente adiciona coisas como sanitização já sabendo que a maioria dos programadores não se preocupa com isso. Mas o objetivo desse exemplo besta foi só pra demonstrar como nós fazemos programas ingênuos quando somos iniciantes, e se for só pra nós mesmos, pra usar no nosso próprio computador, tudo bem. Mas numa aplicação web que vai pra produção e vai ter gente de verdade usando, todo cuidado é pouco.&lt;/p&gt;

&lt;p&gt;Mas o ponto é que programas podem ser usados de formas pras quais eles não foram desenhados. No final vou voltar nesse código, mas esta próxima parte é voltando pros conceitos de rede que vim explicando nos últimos episódios. Em particular a idéia de sockets e portas. Como disse, antigamente a gente era bem mais ingênuo e não parava pra pensar que existiam pessoas maliciosas ou simplesmente só curiosas. É o oposto do que se aprende em cursos e tutoriais.&lt;/p&gt;

&lt;p&gt;Agora imagina. Povo tá empolgado com o nascimento da internet e começa a colocar programinhas ingênuos como o que eu fiz aberto em portas de servidores com endereço IP público válido, como nas universidades da época. Servidores que agora qualquer pessoa poderia acessar. E no meio deles tá esse site de vaquinha. O objetivo nem era deixar público, só queria mostrar pros colegas da faculdade, mas sem querer agora deixei um buraco de segurança enorme aberto. Por isso que nunca um código de junior pode ir pra produção sem ter passado pela revisão de um ou mais sêniors, no mínimo.&lt;/p&gt;

&lt;p&gt;Existem várias formas de um administrador de sistemas bloquear essas tentativas de ataque mesmo sem a colaboração do autor da vaquinha. Uma delas e a mais comum é criar políticas como, pra estar na porta 80 ou 443, precisa ser um site feito por alguém de confiança da faculdade ou instituição. Sites feitos alunos ou outras pessoas, até podem rodar internamente em portas diferentes, dentro do servidor do departamento, mas não vão ser expostos na internet pública. Todo mundo pode subir sites nesse servidor hipotetico, em portas como a 3000 que eu usei ou qualquer outra como 4000 ou 8080.&lt;/p&gt;

&lt;p&gt;O site oficial roda na porta 80 desse servidor. Todas as outras portas deveriam estar fechadas pro público. E assim nasce o conceito de firewalls. Firewall é um programa, como qualquer outro, que roda com privilégios de administrador ou root do sistema. Isso porque ele precisa interceptar todos os pacotes que chegam ou saem pela placa de rede e filtrar pra saber se podem ou não prosseguir. Sem entrar em detalhes, existem dois tipos básicos de regras, uma pra permitir coisas e outra pra rejeitar coisas, allow ou deny.&lt;/p&gt;

&lt;p&gt;Sem nenhum firewall é como se tivesse um firewall com nenhuma regra de deny e allow asterisco, ou seja, aceita tudo. Segurança completa é deny tudo e nenhum allow, mas aí o servidor seria meio inútil. No caso do servidor dessa faculdade de exemplo, a regra poderia ser deny tudo mas allow in, ou entrada, na porta 80. Assim, eu poderia ter meu site de vaquinha interno na porta 3000, mas se alguém de fora, da internet pública, tentasse acessar a porta 3000, a regra do firewall diz que é pra rejeitar, então rejeita todos os pacotes endereçados pra 3000 e nunca chega no meu sitezinho.&lt;/p&gt;

&lt;p&gt;Mesmo numa rede interna de faculdade ou empresa, isso ainda não é suficiente. Porque em segurança a gente suspeita de tudo e de todos, incluindo pessoas internas. De novo, não é porque eu acho que todo mundo tem más intenções, mas porque justamente as pessoas de boa intenção, que não enxergam mau nos outros, são os mais fáceis de enganar com coisas como malwares. Você nunca sabe se um aluno instalou um malware que veio por email sem querer e agora no notebook dele tem um malware pendurado numa porta qualquer e vasculhando por portas abertas.&lt;/p&gt;

&lt;p&gt;Por isso os firewalls também bloqueiam que um notebook numa rede consiga tentar se conectar numa porta de outro notebook na mesma rede. Ele dá deny de todas as portas acima de 1024, por exemplo. Assim, mesmo se eu subir a vaquinha na porta 3000 da minha máquina, outro usuário na mesma rede não vai conseguir acessar, mesmo sabendo meu endereco IP. O firewall vai bloquear. E é assim mesmo até hoje em muito ambiente corporativo. As regras costumam ser as mais fechadas possível pra justamente evitar o acidente de alguém largar uma vaquinha dando bandeira e alguém malicioso se aproveitar disso.&lt;/p&gt;

&lt;p&gt;Agora faz de conta, sou um aluno dessa faculdade. Eu fiz lá a vaquinha e subi no servidor na porta 3000 e sei que tá bloqueada do lado de fora. Mas eu queria porque queria poder mostrar isso pros meus amigos lá fora. Só que o administrador da rede nunca que vai deixar eu pendurar meu site publicamente do servidor da faculdade. Por acaso eu tenho uma máquina virtual de grátis que ganhei pra testar da DigitalOcean, que é um serviço de VPS ou servidores virtuais de aluguel. Eu poderia instalar lá, mas sabe? Tô com preguiça, porque já tá funcionando aqui no servidor da faculdade e só depois descobri que as portas tavam fechadas. O que eu posso fazer? Tá tudo bloqueado, mas ao mesmo tempo eu sei que consigo navegar na web. Então nem tudo tá bloqueado, no mínimo portas 80 e 443 tão abertas pra sair. Eu posso usar isso a meu favor.&lt;/p&gt;

&lt;p&gt;De novo vou repetir, portas são só números. Por convenção 80 é pra HTTP e 22 é pra SSH. Vamos rapidinho criar uma nova máquina na DigitalOcean. Vou escolher um Ubuntu numa máquina fraquinha, no data center dos Estados Unidos. Espero terminar. E copio o endereço IP público valido que ele me dá. Depois de ter criado uma máquina numa DigitalOcean da vida, consigo logar usando o comando &lt;code&gt;ssh root@ip da máquina&lt;/code&gt; e como não indiquei nenhuma porta, o ssh vai tentar na porta 22 padrão. Coloco minha senha, e pronto, entrei na máquina remota. Estou mostrando o endereço IP de verdade porque depois de hoje vou destruir essa máquina, então não vai mais funcionar se você tentar conectar agora.&lt;/p&gt;

&lt;p&gt;A DigitalOcean fornece endereços válidos na internet pra máquina que eu alugo. Numa máquina baratinha ele fica mudando o endereço pra outro. Se eu quiser ter sempre o mesmo, posso pagar a mais pra reservar pra mim. Lembra minha explicação que não existem endereços IPv4 pra todo mundo? É por isso. Pra ter um válido, precisa pagar mais caro. Mas o ponto é que qualquer um na internet consegue enxergar, ou seja, rotear pacotes pra essa máquina.&lt;/p&gt;

&lt;p&gt;Enfim, o programa em si que tá rodando no servidor pendurado na porta 22 se chama &lt;code&gt;sshd&lt;/code&gt; ou ssh daemon. Daemon é todo programa que o sistema operacional gerencia, ele inicializa depois do boot e fica checando se ele crashear, daí tem regras se tenta carregar de novo e assim por diante. Estude sobre systemd, OpenRC ou runit, que são gerenciadores de daemons que a maioria das distros Linux roda. Mas enfim, saiba que eles que são responsáveis por garantir que programas como o SSH carreguem sozinhos após cada boot. Graças a esse programa daemon aberto e pendurado na porta 22 que do meu notebook consigo executar o programa ssh apontando pro endereço de lá e abrir uma conexão segura de terminal remoto.&lt;/p&gt;

&lt;p&gt;Agora vai ter alguns detalhezinhos mas não é importante decorar, só acompanhar o raciocínio. O que eu vou fazer é o seguinte, de dentro dessa máquina remota, vou editar o arquivo &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt; e habilitar essas opções comentadas de Allow Tcp Forwarding e Gateway Ports pra yes, e lá embaixo vou mudar a porta de 22 pra 80. Viu, eu posso pendurar programas em qualquer porta e eu escolhi mudar o daemon de SSH pra porta 80. Agora salvo e uso o comando do systemd &lt;code&gt;systemctl restart sshd&lt;/code&gt; pra reinicializar o daemon e carregar as novas configurações, agora posso sair do ssh com exit.&lt;/p&gt;

&lt;p&gt;Se eu tentar o mesmo comando &lt;code&gt;ssh&lt;/code&gt; pra conectar no servidor, vai falhar, porque como eu disse, vai tentar por padrão na porta 22. Só que mudei pra porta 80. E agora? Como faz? Normalmente esses programas de rede sempre tem uma opção pra manualmente escrever a porta, só colocar &lt;code&gt;-p 80&lt;/code&gt; no final e bingo, olha só, conectou direitinho. Agora que a brincadeira começa.&lt;/p&gt;

&lt;p&gt;Aquela minha vaquinha tá rodando localmente no tal servidor hipotético da faculdade na porta 3000. Faz de conta que meu PC aqui de desenvolvimento é esse servidor da faculdade. Por isso vou no navegador e digito 127.0.0.1 ou localhost, que são os endereços locais da minha máquina.&lt;/p&gt;

&lt;p&gt;Não adianta eu passar esse endereço pra ninguém, porque significa &quot;minha máquina&quot; como expliquei no episódio anterior. Na máquina do seu vizinho, se ele digitar a mesma coisa, não vai carregar nada porque não tem nada pendurado na porta 3000 da máquina dele. Mas eu quero que meus amigos de fora consigam ver a vaquinha. Pra isso preciso de um endereço IP público válido e que tenha portas abertas sem bloqueio de um firewall.&lt;/p&gt;

&lt;p&gt;Pra isso vou usar um dos super poderes do SSH, o recurso chamado Remote Forwarding. No terminal do servidor da faculdade, eu digito &lt;code&gt;ssh -R&lt;/code&gt; pra remote forwarding e agora fica complicadinho &lt;code&gt;ip do servidor:7000&lt;/code&gt; depois &lt;code&gt;127.0.0.1:3000&lt;/code&gt;. Estou falando o seguinte, toda vez que alguém tentar se conectar no endereço do meu servidorzinho na DigitalOcean, nessa porta 7000, faça forwarding, redirecione o tráfego, pra minha máquina local 127.0.0.1 porta 3000, que é onde tá rodando a vaquinha.&lt;/p&gt;

&lt;p&gt;Daí termino com as mesmas informações de antes pro login, usuario root arroba endereço e &lt;code&gt;-p&lt;/code&gt; pra porta 80 que é onde o sshd tá ouvindo. Eu sei, ficou um comandão enorme, mas depois revejam com calma e entendam cada parâmetro. O que esse comando diz é: conecte no meu programa de sshd na porta 80 do servidor da DigitalOcean e se pendure na porta 7000 de lá e fique escutando. Toda vez que vier alguma requisição por lá, faça forwarding, redirecione todo o trafego, pra porta 3000 da minha máquina local.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ssh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Explicando fica complicado. Então vamos mostrar na prática. Deixa eu abrir aqui meu celular que tá conectado via 4G, ou seja, ele não teria acesso ao tal servidor hipotético da faculdade. Vamos digitar o endereco IP público do servidor da Digital Ocean, válido na internet e colocar dois pontos 7000. E voilá, olha a vaquinha aqui, aparecendo em público!  Como pode isso? O firewall não tava travando? O que aconteceu?&lt;/p&gt;

&lt;p&gt;Vou repetir. No navegador do celular eu acessei um endereço IP válido da Digital Ocean. Essa máquina virtual tá vazia. É um Ubuntu recém instalado. A única modificação que eu fiz foi configurar o programa de sshd pra se pendurar na porta 80 em vez da 22 que seria padrão. Dessa forma, eu, pobre aluno da faculdade que está atrás de um firewall bem rígido que bloqueia portas como 22, consigo acessar via a porta 80, que está aberta porque a faculdade pelo menos permite as pessoas navegar na web. E pra isso, obrigatoriamente o firewall é obrigado a deixar passar saída na porta 80. Entenderam? O firewall bloqueia qualquer coisa de entrar na rede, mas precisa permitir pacotes saírem pras pessoas conseguirem navegar na Web, e pra isso no mínimo saída pela porta 80 estava aberta. Portanto, havia esse buraco que poderia ser explorado.&lt;/p&gt;

&lt;p&gt;O firewall deixa pacotes sair, mas não deixa nada entrar de jeito nenhum, então mesmo o servidor da faculdade tendo um endereço IP valido, o firewall não deixaria ninguém se conectar na minha porta 3000. O que eu fiz com SSH é o que se chama de poking a hole, ou literalmente fazer um buraco no firewall, é assim que se fura um firewall do lado de dentro. O grande lance é que do servidor da DigitalOcean ele não consegue me achar na internet e se conectar em mim, mas eu consigo achar e me conectar nele, via a única porta aberta de saída, a 80. E uma vez aberta a conexão, daí meu servidor de fora consegue falar comigo via essa conexão que eu estabeleci com o programa SSH.&lt;/p&gt;

&lt;p&gt;Se pra você isso é novidade, respire um segundo e reveja. O servidor da DigitalOcean bem como qualquer outra pessoa na internet não tem como se conectar no servidor porque o firewall bloqueia, mas se a conexão se inicia do lado de dentro, desse servidor, uma vez estabelecida a conexão pra fora, agora ambas as pontas conseguem se comunicar.&lt;/p&gt;

&lt;p&gt;Esse é um truque que eu já usei inúmeras vezes ao longo dos anos. A primeira vez que aprendi a fazer isso acho que nem existia SSH. A gente usava um programinha chamado HTTP Tunnel, que faz o oposto do que acabamos de fazer, o recurso de Local Forwarding do SSH. Deixa eu dar um outro exemplo. Acontece que só fechar todas as portas mas manter a porta 80 e 443 de Web abertas, continua sendo um enorme risco de segurança, porque vai saber quem dentro da empresa que não tá entrando em site pornô, site de pirataria e uma hora esbarrando em sites com malwares. Pra evitar isso acho que até hoje tem empresas que obrigam todo mundo a usar um proxy.&lt;/p&gt;

&lt;p&gt;Se você abrir um navegador recém instalado numa empresa mais segura, não vai conseguir navegar pra lugar nenhum. O administrador da rede da empresa vai te instruir a abrir a configuração de proxy do seu sistema operacional e colocar um endereço IP e uma porta que aponta pro programa de proxy. Isso pode ser no nível do sistema operacional ou nas configurações do navegador.&lt;/p&gt;

&lt;p&gt;Lembra no episódio passado que fiz um exemplo de conectar num servidor web local usando telnet e digitando manualmente o cabeçalho de requisicão HTTP? Lembra? Comando GET seguido de barra e o nome da pagina html que quiser, terminando com HTTP 1.0? Um cabeçalho mais completo, que um navegador enviaria, seria parecido com isso aqui.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;GET / HTTP/1.1
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
DNT: 1
Accept-Encoding: gzip, deflate, sdch
Accept-Language: pt-BR,en;q=0.8&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Ele dá bem mais detalhes pro servidor, como se apresentar dizendo que navegador que é, dizendo que línguas que aceita e coisas assim. É uma das formas de como serviços como Google Analytics conseguem saber coisas como qual navegador mais acessa seu site. Enfim. Se tiver um proxy configurado, o navegador não vai mais se conectar direto com o site que você quer, em vez disso sempre vai mandar todas as requisições pro servidor de proxy. E o pacote vai ser um pouquinho diferente. Assim.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;GET http://www.mega.io/ HTTP/1.1
Host: www.mega.io
Proxy-Connection: keep-alive
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&lt;/code&gt;
DNT: 1
Accept-Encoding: gzip, deflate, sdch
Accept-Language: pt-BR,en;q=0.8`&lt;/p&gt;

&lt;p&gt;Olha só, no comando GET vai a URL inteira, com o domínio, uma nova linha dizendo que host estamos querendo nos conectar e uma configuração específica de Proxy. O Proxy vai receber essa requisição e modificar pra ficar igual à original sem os detalhes de proxy. E é proxy quem vai se conectar com o servidor do Mega, no caso. O site do Mega vai gerar um HTML, o pacote de response e devolver pro Proxy. Daí o proxy redireciona a resposta de volta pro meu navegador. Um proxy atua como um intermediário entre eu e os sites que quero navegar. Do ponto de vista do usuário, estou navegando normamente. Mas do ponto de vista dos sites que estou acessando, ele só enxerga o proxy como usuário.&lt;/p&gt;

&lt;p&gt;Mais ou menos o efeito é parecido com o que eu expliquei de NAT e CGNAT no terceiro episódio da mini-serie. Internamente temos um endereço IP 192.168 que não é válido na internet. Todo pacote passa pelo roteador do provedor e é convertido num endereço válido, que é o que os sites enxergam. Mas todo mundo atrás do NAT vai ser visto com o mesmo endereço IP. Pros sites, é difícil dizer quem são os usuários só baseado nisso.&lt;/p&gt;

&lt;p&gt;Mesma coisa com proxy, só que restrito ao tráfego de Web. Em vez de NAT os pacotes passam pelo Proxy, e os sites só enxergam um endereço IP público da empresa, sem saber com certeza os usuarios atrás. A gente rastreia usuários com outros dados, como o cookie do navegador, que tem o login de cada usuário. Outra hora explico sobre cookies, mas foi só pra dizer que em termos de conceito, NAT e Proxy tem resultados semelhantes do ponto de vista dos sites.&lt;/p&gt;

&lt;p&gt;A vantagem disso pra empresa é óbvia. Proxies tem filtros com whitelists e blacklists. Meio como um firewall ele pode dizer o seguinte: ninguém pode navegar em nenhum site da internet, com exceção dos domínios que o administrador configurar na whitelist. Assim ele pode restringir em quais sites os funcionários podem navegar. Sites pornô e de entretenimento provavelmente estão na blacklist da empresa. E adivinha, se o administrador não tiver colocado stackoverflow.com, você se fodeu.&lt;/p&gt;

&lt;p&gt;Mais do que isso, ele pode monitorar o conteúdo de tudo que tá passando e registrar. Isso é mais difícil hoje porque usamos HTTPS e aí ele tem mais dificuldades de vigiar o conteúdo do que você tá navegando porque vai tudo criptografado. Mas como expliquei antes, na requisição fica aberto que site você tá tentando acessar, e o Proxy consegue registrar isso. Só de restringir onde pode navegar já é um pé no saco. É compreensível, muita empresa não gosta de ver os funcionários perdendo tempo em redes sociais, então bloqueiam coisas como YouTube ou Facebook. É um pensamento super retrógrado, que era comum no começo dos anos 2000 e eu sei que tem empresas que pensam assim até hoje, infelizmente.&lt;/p&gt;

&lt;p&gt;Pois bem. No exemplo da faculdade eu dei um cenário hipotético onde o administrador fez um firewall que bloqueia a saída de pacotes pra 100% das portas com exceção das de web. Mas na prática acho que quase ninguém faz isso. Seria muito pouco produtivo. Você acabaria bloqueando portas úteis como as acima de 49 mil até 65 mil que servem pra coisas como Zoom e outros software de reunião online ou mesmo portas como 25, 193 que são pra clientes de email. Mas as portas de web ficam fechadas e te obrigam a configurar proxy. Então dessa vez não temos portaa 80 de saída pra trabalhar.&lt;/p&gt;

&lt;p&gt;Não vou mostrar na prática, mas a solução é simples. Com o mesmo servidor na Digital Ocean que eu tenho, posso configurar aquele daemon sshd não pra porta 80 mas sim pra uma porta como 50000. Uma porta alta que provavelmente não vai estar bloqueado nos firewalls. O remote forwarding que eu fiz, com aquela opção &lt;code&gt;ssh -R&lt;/code&gt;, permite alguém de fora acessar um site meu rodando numa máquina privada dentro da rede. Mas o oposto também é possível. Eu posso rodar o seguinte comando, um pouco mais complicadinho: &lt;code&gt;ssh -D 1337 -q -C -N root@ip do servidor de ssh -p 50000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Opções como &lt;code&gt;-q&lt;/code&gt; é quiet mode, pra não ficar imprimindo nada no meu terminal, &lt;code&gt;-C&lt;/code&gt;maiúsculo  é pra comprimir o conteúdo que passar por esse programa. &lt;code&gt;-N&lt;/code&gt; é pra não abrir uma linha de comando pro servidor. Mas o mais importante é esse &lt;code&gt;-D 1337&lt;/code&gt;. Isso vai pendurar esse programa, fazer bind, nessa porta 1337 na minha máquina local. Poderia ser qualquer outro número de porta. O resto é a mesma coisa pra conectar no sshd apontando pra porta 50000 do servidor, o que provavelmente vai conseguir furar o firewall.&lt;/p&gt;

&lt;p&gt;Agora, no meu sistema operacional ou direto no navegador, eu procuro a configuração de proxy de novo, mas em vez de colocar o endereço do proxy da empresa, coloco localhost e porta 1337. E pronto, agora posso navegar normalmente por qualquer site que quiser, mesmo se a empresa achar que está me bloqueando. O que fiz foi criar um túnel via SSH, que fica exporto como um serviço SOCKS5, que é o protocolo de um Proxy Web, e pendurado na porta local 1337 da minha máquina. Eu fiz um proxy pessoal, que recebe pacotes HTTP normalmente, como um servidor web, só que ele dá forward, redireciona o tráfego, pro meu servidor da Digital Ocean via esse túnel criptografado via ssh.&lt;/p&gt;

&lt;p&gt;O serviço sshd de lá recebe os pacotes que vieram pelo túnel e navega usando a internet da Digital Ocean, que tá toda aberta. O site que eu quis navegar devolve a resposta HTTP pro servidor, e o ssd redireciona o pacote de volta pra mim pelo mesmo túnel. E assim eu burlo toda tentativa de me restringir de navegar. Posso navegar onde quiser. Mesmo a saída da porta 80 na empresa estando fechada por firewall, mesmo sendo instruído a usar o servidor de proxy da empresa, eu criei o meu próprio servidor de proxy saindo por uma porta alta que provavelmente tá aberta no firewall e passei a navegar sem restrição nenhuma. Isso é um exemplo do que se chama de tunelamento.&lt;/p&gt;

&lt;p&gt;Mais uma curiosidade. Antes, se eu fosse num site como what is my ip, ia mostrar o endereço IP do proxy da empresa. É a mesma coisa como no caso de CGNAT de provedor. Meu computador tem um endereço privado tipo 192.168 da vida, que não é válido pra rotear na internet. Então todo mundo da rede da empresa vai ter o mesmo endereço IP público, seja por causa de NAT, seja por causa de Proxy. Agora, se navego via o túnel pro servidor da Digital Ocean, o que o site what is my ip vai ser é o endereço IP do servidor. Não só isso, se ele tentar encontrar a posição geográfica desse IP vai ver que está em Nova Iorque, porque eu escolhi montar meu servidor no data center de Nova Iorque. Como programador é importante entender isso: os dados de usuário que sua aplicação web recebe normalmente não representa o computador do usuário, porque a maioria está escondido em redes privadas de NAT ou atrás de Proxy de empresas.&lt;/p&gt;

&lt;p&gt;De qualquer forma, com esse truque de tunelamento de SSH, significa que não só burlei as restrições da tal empresa hipotética, como agora posso assistir Netflix como se estivesse nos Estados Unidos, tendo acesso a conteúdo que não tem no Brasil. Então eu burlei também a restrição de região do Netflix. E se o que eu acabei de falar pareceu com aquelas propagandas sobre VPN, é porque tem a ver.&lt;/p&gt;

&lt;p&gt;Esse é só um exemplo de coisas que conseguimos fazer quando sabemos um pouco sobre redes. No próximo episódio vou falar um pouquinho de VPN. Por hoje queria mostrar um pouco mais de que tipo de coisas você está perdendo quando se recusa a entender um pouco mais sobre redes. A pessoa hipotética que fez o site de vaquinha e conseguiu disponibilizar na internet com túnel de SSH eu poderia dizer que é o oposto da maioria dos programadores: um péssimo programador, que fez um site cheio de buracos de segurança, mas um bom cara de redes que soube ultrapassar as restrições de firewall e abrir buracos no bloqueio.&lt;/p&gt;

&lt;p&gt;Mas no geral ele ganha nota negativa, porque alguém colocar um site que não entende que é inseguro, à força na internet o torna um péssimo profissional. Eu sou o tipo que gosta de quebrar as regras e sempre fiz isso, mas quando faço eu sei os riscos que estou correndo. Não seja esse cara irresponsável, aprenda de tudo um pouco e saiba onde pode ou não pode usar cada coisa. Por exemplo, lá no começo eu disse que faltou sanitizar a mensagem do usuário pra impedir ele de tentar injetar comandos arbitrários pra rodar no servidor. Como que faz isso? Antes de terminar o episódio vamos pelo menos fazer isso.&lt;/p&gt;

&lt;p&gt;Existem dezenas de bibliotecas e frameworks como Express já vem preparado pra isso. Mas nesse exemplo besta, vamos fazer nossa própria função. Veja esta função &lt;code&gt;sanitize&lt;/code&gt;, ela recebe uma mensagem e passar por uma expressão regular, uma regex. Se não conhecem regex, precisa estudar, é obrigatório saber. Essa expressão procura na mensagem tudo que não é letra minúscula, letra maiúscula ou números e com a função &lt;code&gt;replace&lt;/code&gt; ele vai trocar por uma string vazia. Ou seja, coisas com o ponto e vírgula vai ser substituída por string vazia. Agora lá embaixo eu passo a mensagem que peguei da URL por essa função e só depois concateno com o comando &lt;code&gt;cowsay&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Vamos no terminal, rodar essa nova versão do programa e voltar pro navegador. Chamamos http localhost interrogação mensagem com ponto e virgula e um comando como ls e olha só, ele não executa mais o comando, porque eu quebrei a sintaxe. Se alguém achar esse site no ar e tentar executar comandos, não vai mais conseguir, porque só com letras e números, não tem como concatenar outros comandos pra executar e com isso bloqueamos todas as tentativas de injeção. Essa função não é a melhor, porque ela apaga todo caracter especial, então não consigo usar pontuação nas mensagens. Isso é só um exemplo, tem jeitos melhores de fazer isso, mas foi só pra mostrar como é o jeito mais simples e mais drástico de sanitizar.&lt;/p&gt;

&lt;p&gt;O recado é simples: tudo que um usuário manda, seja como parâmetros na URL, seja como campos num formulário, tudo deve ser sempre considerado suspeito e deve passar por sanitização. Todo usuário é mau e malicioso até prova em contrário, é assim que devemos considerar todo mundo que acessa seu site. Todo mundo que vai usar seu programa vai tentar quebrar ele. Como programador você tem que estar ciente disso. E é simples: em algum lugar no seu programa você permite o usuário enviar alguma informação? Essa informação é material radioativo, trate ele como tal.&lt;/p&gt;

&lt;p&gt;Não tem a ver com o episódio, mas já que falei isso vale outro aviso. Tem gente que fica paranóico demais e faz programação defensiva em excesso e sai colocando tratamento de parâmetros em todas as funções que faz. E está errado. Toda função que é usada internamente não precisa sanitizar tudo toda hora. Só funções que explicitamente recebem dados vindos diretamente de um usuário que precisam disso. Cuidado pra não sair dando copy e paste de sanitização em todo lugar que não precisa e ficar redundante. Ter cuidado não significa atirar pra todo lado. Um bom programador usa sniper, mira e atira com precisão só um tiro e acerta o alvo. Quem atira pra todo lado é claramente amador.&lt;/p&gt;

&lt;p&gt;Eu quis fazer esse episódio porque muita gente usa SSH pra se conectar em servidor mas não sabe o que o SSH realmente consegue fazer. Espero que isso tenha servido pra mostrar que você usa ferramentas sem saber pra que elas realmente servem. Interesse-se mais em explorar o que você usa no seu dia a dia. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e não deixem de compartilhar o video com seus amigos. A gente se vê, até mais!&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5992</id>
    <published>2022-08-04T10:21:00-03:00</published>
    <updated>2022-08-04T10:22:47-03:00</updated>
    <link href="/2022/08/04/akitando-124-como-funciona-sockets-cliente-servidor-e-a-web-introducao-a-redes-parte-4" rel="alternate" type="text/html">
    <title>[Akitando] #124 - Como Funciona Sockets, Cliente, Servidor e a Web? | Introdução a Redes Parte 4</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lc6U93P4Sxw&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Agora que todo mudno sabe como funciona o básico de rede, vamos ver como funciona o básico de aplicações de rede, o que são sockets, pra que eles servem, como disso chegamos em protocolos como HTTP ou FTP e os vários detalhezinhos que complicam a cabeça de um iniciante em programação Web.&lt;/p&gt;

&lt;h2&gt;Conteúdo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;01:36 - Noção de Sockets&lt;/li&gt;
&lt;li&gt;02:26 - Entendendo Comunicação Entre Processos&lt;/li&gt;
&lt;li&gt;03:18 - Memória Protegida/Isolada Impede Comunicação&lt;/li&gt;
&lt;li&gt;05:30 - Comunicação via Pipes&lt;/li&gt;
&lt;li&gt;07:18 - Comunicação via Arquivos&lt;/li&gt;
&lt;li&gt;09:56 - BSD Sockets&lt;/li&gt;
&lt;li&gt;12:43 - Fluxo de conexão&lt;/li&gt;
&lt;li&gt;16:54 - Portas&lt;/li&gt;
&lt;li&gt;19:55 - Recapitulando navegador/DNS&lt;/li&gt;
&lt;li&gt;21:15 - Exemplo de server web com Node&lt;/li&gt;
&lt;li&gt;22:17 - localhost é diferente de 0.0.0.0&lt;/li&gt;
&lt;li&gt;23:43 - Continuando Exemplo de Node&lt;/li&gt;
&lt;li&gt;24:19 - Portas Efêmeras&lt;/li&gt;
&lt;li&gt;25:47 - Continuando Exemplo de Node&lt;/li&gt;
&lt;li&gt;29:00 - Debugando com Telnet e Developer Tools&lt;/li&gt;
&lt;li&gt;31:57 - Camadas de Segurança&lt;/li&gt;
&lt;li&gt;34:07 - Big Loop&lt;/li&gt;
&lt;li&gt;37:24 - OSI, TCP, HTTP&lt;/li&gt;
&lt;li&gt;39:28 - Mostrando FTP&lt;/li&gt;
&lt;li&gt;42:25 - De redes distribuídas a &quot;walled gardens&quot;&lt;/li&gt;
&lt;li&gt;45:06 - Bloopers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;No começo dessa mini-série eu ia só até o episódio passado, mas depois que escrevi, resolvi estender mais três episódios. Estou pulando toda uma parte sobre rede física, MLPS, OLDM, Wimax, 802.11. Vale estudar o livro de redes do Tanenbaum pra esses detalhes, o objetivo é focar mais em assuntos que ajudem programadores e não quem está interessado em seguir na carreira de infraestrutura, por isso eu sei que pra quem é de telecom ou infra tem bastante coisa simplificada. Agora quero ir direto pra camada de transporte e aplicação e nos próximos episódios falar assuntos que o Tanenbaum não chega a explicar no livro.&lt;/p&gt;

&lt;p&gt;Só antes de continuar, tem uma errata que esqueci de falar nos videos anteriores, sobre o primeiro video de redes. Lá eu tava tentando simplificar com metáforas pra falar que ondas como de rádio e internet wireless se parecem com ondas na água, mas do jeito que falei ficou parecendo que wireless precisa de um meio como ar pra se propagar, mas claro que não, ondas na água são diferentes de ondas eletromagnéticas, que se propagam no vácuo, diferente de som. Por isso que no espaço não tem barulho, mas satélites conseguem se comunicar. Já tá na errata do episódio mas achei melhor esclarecer.&lt;/p&gt;

&lt;p&gt;Todo programador vai ter que lidar com coisas como endereços IP, que já expliquei, e também com as tais portas. Toda vez que você roda um &lt;code&gt;npm start&lt;/code&gt; da vida, sobe uma aplicação web nas tais portas 3000 ou 4000. Todo tutorial só menciona isso: magicamente funciona. Mas não explica o que está acontecendo de verdade. É hora de entender a mágica. O episódio de hoje é sobre sockets.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Em resumo bem resumido, pense em sockets literalmente como tomada da sua casa. Diferentes portas tendo diferentes padrões de buracos, como o padrão brasileiro sendo uma porta, padrão europeu sendo outra, padrão americano outra e assim vai. E só plugues corretos encaixam em determinadas tomadas. Por isso seu notebook importado não encaixa na porcaria da tomada do Brasil. +&lt;/p&gt;

&lt;p&gt;Na prática o conjunto de endereço IP e porta de saída, digamos, do seu navegador conectando num site, e o endereço IP e porta de entrada do tal site, essas quatro informações é o mínimo pra definir um socket. É o plug e a tomada conectada. Não se atenha tanto ao nome mas a esse conjunto de informações, é o que se precisa pra formar uma conexão de rede entre dois processos, que é o caminho que os pacotes que falamos antes vão trafegar.&lt;/p&gt;

&lt;p&gt;Sockets, na verdade, é uma forma de comunicação entre processos. Todo sistema operacional existe pra gerenciar processos. Expliquei isso nos videos de Programação pra Iniciantes, em particular nos de concorrência e paralelismo e gerenciamento de memória. Processo é todo programa que tem rodando no seu computador, seja seu spotify ou banco de dados. No nível mais básico, um processo é representado por um número, um process ID ou PID que o sistema operacional dá pro seu programa. Quando do terminal de um Linux se digita &lt;code&gt;ps -ax&lt;/code&gt; dá pra ver os processos e seus PIDs. O sistema operacional aloca uma certa quantidade de memória e internamente, seu processo acha que tem acesso a toda memória teórica de 64-bits, mas na realidade o sistema operacional é quem intermedia o acesso à RAM ou Swap de disco.&lt;/p&gt;

&lt;p&gt;Isso é importante. Um processo é como uma criança rica mimada gastando cartão de crédito dos pais. Ele não tem noção que tem limite, e só vai gastando, e os pais, que é o sistema operacional, vai sempre só pagando, então essa criança tem a ilusão que tem dinheiro infinito. É mais ou menos assim. Seu PC tem só 8 giga de RAM real, mas seu processo acha que tem do endereço 0 até 2 elevado a 64 de memória, que seria o máximo teórico de 16 exabytes. Nenhum computador de verdade tem exabytes de RAM ainda. Isso é importante de entender então vou insistir com outra metáfora.&lt;/p&gt;

&lt;p&gt;Cada processo rodando na máquina enxerga uma memória virtual, de 0 até 16 exabytes. Quando falamos que se consegue endereçar memória, na realidade pense assim: Imagine um livro com uma parte de índice no começo. O índice tem espaço pra 16 exabytes de linhas, mas o livro em si não tem 16 exabytes de páginas, saca? Todos os processos estão compartilhando a mesma memória física de 8 gigabytes reais, mas ele só enxerga o índice.&lt;/p&gt;

&lt;p&gt;O sistema operacional intermedia e garante que um processo não consiga acessar a memória real inteira, as páginas do livro, de outro processo. Por isso falamos que rodam isolados, sem um sobrescrever a memória do outro. Eu explico sobre isso nos episódios sobre gerenciamento de memória. Mas a parte importante é entender que um processo não enxerga nunca a memória de outro processo. Ele acha que está sozinho na máquina.&lt;/p&gt;

&lt;p&gt;Antigamente, nos primeiros computadores sem isolamento de memória, um processo poderia escrever num endereço de memoria real e um outro processo poderia ler essa informação diretamente desse endereço. Um livros sem índices. Na época dos computadores de 8 e 16 bits era mais ou menos assim. Até a Intel inventar memória protegida. Todo processo enxergava todas as páginas e poderia escrever em qualquer lugar. Endereçar memória diretamente era mais rápido mas levava a dezenas de bugs de um processo escrevendo em cima de endereços que outro processo tava usando, levando desde a comportamento não-determinístico até a crashes. Isso era menos ruim quando a gente só rodada um programa de cada vez, na era do MS-DOS. Mas nos primeiros Windows isso já era um problemão.&lt;/p&gt;

&lt;p&gt;Desde que passamos a isolar os processos e demos endereços virtuais em vez de reais, pelo menos essa categoria de bugs foi extinta, já na época do Windows 3.1 e foi melhorando a cada nova versão. Também significa que se um processo quiser se comunicar com outro processo, não tem como, porque não tem como gravar algo na memória que outro processo possa ler diretamente. As únicas duas formas de dois processos se comunicarem ou é via pipes ou via arquivos. Vamos entender.&lt;/p&gt;

&lt;p&gt;Pipes de Linux é simplesmente ligar a saída padrão de um programa na entrada padrão de outro programa, o stdout ligado ao stdin via um pipe, ou cano. Que nem quando fazemos &lt;code&gt;cat algum_arquivo&lt;/code&gt; pipe &lt;code&gt;grep alguma coisa&lt;/code&gt;. Expliquei um pouco disso no episódio de Ubuntu se quiser relembrar. Isso significa que um programa roda primeiro, cospe algum resultado no stdout, e esse resultado é passado pro stdin, pra entrada, do próximo programa. É uma forma de dois programas diferentes se comunicar, mas é uma forma serial: um programa precisa rodar primeiro até o fim, terminar, e só depois começa o outro.&lt;/p&gt;

&lt;p&gt;Portanto é mais complicado se eu quero os dois programas rodando em paralelo, sem um esperar o outro terminar. A única outra solução é um programa ficar cuspindo coisas num arquivo e o outro programa ficar lendo desse arquivo. Esse arquivo serve de ponte, de stream de entrada e saída pra ambos, criando um tipo de conexão. Isso resolve o problema deles conseguirem rodar em paralelo, ao mesmo tempo. Como exemplo, pense num programa que vai gravando coisas num arquivo de log e pense outro programa como o Prometheus, que vai lendo esse log e organizando a informação. Arquivos é uma área compartilhada que dois processos podem usar pra se comunicar, já que memória é isolada.&lt;/p&gt;

&lt;p&gt;Todo iniciante precisa se acostumar a lidar com arquivos. Sem entrar em detalhes, na maioria das linguagens, você manda abrir um arquivo a partir de um path ou caminho, que é a sequência de diretórios e sub-diretórios separados por uma barra e o nome do arquivo com extensão. Isso abre o que chamamos de um stream, pense como se fosse um cano de bits bidirecional, um rio onde flui bits. Agora podemos ou ler ou escrever nesse stream.&lt;/p&gt;

&lt;p&gt;Podemos ler bit a bit, ou pedir pra ir enchendo tipo um balde antes, um buffer, um acumulador, pra pegar linha a linha de cada vez, por exemplo. Mesma coisa pra escrever, podemos jogar bit a bit nesse stream, ou preencher um buffer antes e mandar vários bits de uma só vez, como uma linha inteira. Podemos programar pra deixar encher esse balde, e quando tiver uma certa quantidade de bits ou quando vier algum caracter especial como uma quebra de linha, pedir pra notificar o programa pra fazer alguma coisa com o que veio. De novo, a maioria das bibliotecas de linguagens modernas já cuidam disso e você como programador só lida com coisas como linha a linha. Mas por baixo existem coisas como buffers.&lt;/p&gt;

&lt;p&gt;Vale lembrar que se você programar numa linguagem um pouco mais baixo nível, tipo C, Rust ou até Java, vai lidar mais com esses conceitos de buffers. Java tem classes como Buffered Stream, por exemplo. Numa linguagem mais alto nível como Javascript, ele cuida disso pra gente por baixo dos panos, e você só fala &quot;vai lendo de linha a linha e vai me dando&quot;. Bibliotecas de linguagens de alto nível foram feitas pra oferecer produtividade ao custo de dar menos controle. Por outro lado é o velho 80/20, pra 80% das coisas que precisa, o 20% que oferece por padrão costuma ser suficiente. Pros outros 20%, aí precisa descer pra um C da vida.&lt;/p&gt;

&lt;p&gt;Só pra ter de referência aqui, como que em Javascript com Node dá pra escrever e ler de um arquivo? Primeiro carrega a biblioteca &quot;fs&quot; de filesystem e ele tem várias funções como esse &lt;code&gt;writeFile&lt;/code&gt; onde passo o nome do arquivo e o conteúdo como String. Se não der nenhum erro, podemos usar &lt;code&gt;readFile&lt;/code&gt; pra ler o conteúdo que acabamos de escrever e mostrar na tela com o bom e velho &lt;code&gt;console.log&lt;/code&gt;. Mas o principal é entender que a semântica são funções como essas, read alguma coisa, write alguma coisa.&lt;/p&gt;

&lt;p&gt;`var fs = require(&quot;fs&quot;);
fs.writeFile('hello.txt', 'Hello World', function(err) {
   if (err) {
      return console.error(err);
   }&lt;/p&gt;

&lt;p&gt;   fs.readFile('hello.txt', function (err, data) {
      if (err) {
         return console.error(err);
      }
      console.log(&quot;read: &quot; + data.toString());
   });
});`&lt;/p&gt;

&lt;p&gt;Enfim. Com pipes e arquivos, você tem o mínimo suficiente pra fazer dois processos comunicarem entre si mesmo sem compartilhar memória RAM. Esse tipo de técnica fica embaixo de um guarda chuva de assuntos que chamamos de IPC ou inter process communication. Existem outras formas de comunicação como sinais, mas só isso é suficiente pra hoje. Se você só tem um computador, isso é suficiente. Mas aí lá nos anos 60 inventaram um troço que fodeu esse esquema. Essa tal de rede.&lt;/p&gt;

&lt;p&gt;Com redes, temos mais de um computador rodando em paralelo e seria bacana se um processo rodando em um computador pudesse falar com um processo rodando em outro computador. E seria mais legal ainda se pudéssemos usar uma semântica parecida com lidar com arquivos, sem de fato usar arquivos. Ter comandos parecidos como &quot;read&quot; ou &quot;write&quot;, ler ou escrever num stream de bits. Pensando assim que em 1983 surge a implementação de Berkeley Sockets no UNIX original BSD versão 4.2.&lt;/p&gt;

&lt;p&gt;Por isso são chamados de BSD sockets, de Berkeley Software Distribution ou POSIX sockets. POSIX que é o conjunto de APIs e protocolos que define o que é um UNIX. Pra entender um pouco mais sobre UNIX e Linux e licenças de software livre, não deixe de assistir o episódio de Apple e GPL que fiz faz tempo. BSD Sockets se tornou meio que o padrão de como lidamos com comunicação entre processos de forma mais geral.&lt;/p&gt;

&lt;p&gt;Já vimos nos episódios anteriores dessa mini-série de redes, como que informação é fragmentada em pacotes, e como esses pacotes são transferidos numa rede de um computador até outro, seja numa rede local ou seja na internet em geral. Vamos assumir que tudo isso funciona e que os sistemas operacionais sabem como produzir pacotes e enviar na rede e consumir pacotes que chegam da rede, seja via fibra ótica, cabo de cobre ou rede sem fio.&lt;/p&gt;

&lt;p&gt;Não por acaso venho me repetindo bastante sobre sistemas de armazenamento: como HDs são dispositivos de blocos e não dispositivos de arquivos. No baixo nível do hardware, sistemas operacionais lidam com blocos de bits num HD ou blocos de bits em pacotes de rede. Tudo são blocos, toda informação é fragmentada e transferida num stream. Podemos fazer buffers de blocos. Se blocos de bits forem arquivos num HD, podemos pensar em blocos de bits que vem pela rede meio como arquivos também. As abstrações são parecidas embora no meio físico sejam diferentes. No fim do dia você está lendo bits de algum lugar e escrevendo bits em algum lugar, esse é o conceito principal.&lt;/p&gt;

&lt;p&gt;Não é a definição exata mas a grosso modo poderíamos pensar em sockets como uma generalização das primitivas de arquivos. Um Linux entrega um file descriptor ou file handle pro seu programa, tanto se for um arquivo local no seu HD ou um socket de rede. Pense em um handle ou descriptor como uma caixa preta que representa um arquivo, um intermediário pra quem você pede pra receber bits ou enviar bits. E essa caixa preta poderia ser um arquivo em disco, mas poderia ser uma conexão remota em rede. Tem uma preparação extra que precisa fazer pra se comunicar na rede e em toda linguagem de programação vai ser parecido. O protocolo que os Berkeley Sockets definiram funciona mais ou menos assim.&lt;/p&gt;

&lt;p&gt;`const http = require('http');&lt;/p&gt;

&lt;p&gt;const hostname = '127.0.0.1';
const port = 3000;&lt;/p&gt;

&lt;p&gt;const prepareResponse = (req, res) =&gt; {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World\n');
}&lt;/p&gt;

&lt;p&gt;const server = http.createServer(prepareResponse);&lt;/p&gt;

&lt;p&gt;server.listen(port, hostname, () =&gt; {
  console.log(&lt;code&gt;Server running at http://${hostname}:${port}/&lt;/code&gt;);
});`&lt;/p&gt;

&lt;p&gt;Primeiro seu programa pede um bind pro sistema operacional. Bind é literalmente ligação, a ligação de um endereço IP com um outro número de 16 bits que chamamos de porta. Bind é um pedido de registro. Funciona assim. Todo processo quando inicia, o sistema operacional oferece um número de identificação, o tal PID. Só que esse PID muda o tempo todo. Hoje seu Spotify inicia com PID 10. Mas amanhã pode já ter outro programa com PID 10, então o sistema atribui PID 11 ou qualquer outro número. PIDs identificam programas rodando naquele momento, mas não servem pra identificar um programa individualmente a qualquer momento. Então precisamos de outro número que seja fixo que o programa possa pedir pra se registrar.&lt;/p&gt;

&lt;p&gt;Digamos que seu programa peça pro sistema registrar ele com o número 3000. O sistema operacional mantém uma tabela em memória e poderia associar o PID atual do seu programa, digamos que seja 11, com esse número 3000, que ele chama de porta, ligados ou bound num endereço IP. Porta é um número que identifica um processo que o sistema operacional mantém numa tabela de controle na memória dele. Se ninguém antes pediu pra registrar essa porta, o sistema te associa nela e nenhum outro programa pode pedir pra se ligar na mesma porta. Porta é só isso, um identificador do programa rodando ligado ao endereço IP do computador.&lt;/p&gt;

&lt;p&gt;Quando o sistema consegue te associar nessa porta 3000, devolve um ok pro programa e devolve o equivalente a um stream de bits, o cano aberto, como se tivesse acabado de conseguir abrir um arquivo. E agora seu programa pode entrar em modo de escuta, ou listen. Nesse momento ele fica bloqueado esperando o sistema te mandar alguma coisa por esse cano e não precisa fazer mais nada enquanto isso. Normalmente isso fica numa thread bloqueada ou é usado I/O assíncrono, que não é escopo explicar hoje. Só entenda que depois do bind, seu programa ficaria parado esperando e escutando o que vier através dessa porta 3000. O processo de bind e listen de sockets é similar ao processo de dar open e abrir um arquivo.&lt;/p&gt;

&lt;p&gt;Daí alguém na rede resolve mandar alguma informação, pro endereço IP do computador onde ele tá rodando, e mandando junto que quer falar com seja lá quem está ligado nessa porta 3000. O sistema operacional recebe isso e vê, &quot;hum, quem tá escutando na porta 3000, vamos ver na tabela, ah é o processo de PID 11, vou mandar no cano aberto dele&quot;. Agora seu programa, que estava só escutando, começa a receber bits. Nesse momento ele pode escolher aceitar ou rejeitar, normalmente só se aceita, isso é o passo de accept.&lt;/p&gt;

&lt;p&gt;E é no accept que fecha o que se chama de conexão entre este computador e o outro que pediu a conexão. O socket serve pra estabelecer essa conexão, esse stream de bits, entre dois programas rodando, mesmo se for em máquinas diferentes. E agora os dois programas precisam enviar comandos que ambos entendam. Se seu programa receber bits que formam um comando válido, pode começar ou a receber ou a enviar bits de volta pela mesma conexão. Aqui é a metáfora das tomadas de padrão diferente.&lt;/p&gt;

&lt;p&gt;Pra comunicação acontecer, ambos precisam concordar que comandos aceitam trocar. Esse acordo entre as duas pontas é o tal de protocolo. Pense em protocolo como sendo os pinos nas posições corretas que encaixam certo nos buracos da tomada. Mesmo tendo um processo pendurado numa porta, se ele não for um servidor web, não adianta um navegador web tentar se comunicar nele, eles não vão se entender e só vai dar erro. Por exemplo, se um navegador web tentar se conectar com um servidor de banco de dados, só vai dar erro, porque o banco de dados não entende comandos de HTTP que o navegador manda.&lt;/p&gt;

&lt;p&gt;Falando bem resumido, esse é o básico do básico do básico de programação distribuída em rede. Toda linguagem vai te dar bibliotecas e funções que tem nomes parecidos com esses passos, bind, listen, accept, send. Apesar da sintaxe de cada linguagem ser diferente, podem ver que em todas é o mesmo processo, porque as primitivas no sistema operacional são as mesmas e as tecnologias de rede são as mesmas. Independe da linguagem, o jeito de falar em rede é sempre a mesma. Eu sempre falo que é mais importante saber a fundação do que uma linguagem só, porque se você aprendeu isso em C, facilmente vai conseguir fazer a mesma coisa em Java ou Python.&lt;/p&gt;

&lt;p&gt;Do lado do sistema operacional, entenda que portas é só uma tabelona com identificadores numéricos de 16-bits, onde é possível associar 2 elevado a 16 portas no total, de 0 até 65 mil 536. Por isso você nunca viu número de portas acima disso. Essa tabelona diz que se receber bits endereçados pra um IP e porta, ele sabe pra qual cano de qual processo rodando enviar. Agora vamos entender isso a partir do programa mais comum pra se comunicar em rede hoje, seu navegador web, como Chrome ou Firefox.&lt;/p&gt;

&lt;p&gt;Seu navegador é um exemplo desse tal programa que pede uma conexão, um cliente. Ele não precisa fazer bind, porque não precisa que ninguém se conecte nele. Ele só quer se conectar na porta dos outros, de um servidor web. E é isso que diferencia um cliente de um servidor. Um servidor faz bind e listen numa porta e fica esperando, escutando. Um cliente envia pacotes pro endereço IP do servidor e pede pra se conectar num programa rodando lá que está escutando numa determinada porta. Pra isso funcionar, o cliente precisa saber não só o endereço IP como em qual porta o servidor está escutando. E por isso se estabeleceu que certas portas são exclusivas pra um determinado uso.&lt;/p&gt;

&lt;p&gt;Mas não se enganem, portas são só números numa tabela que o sistema operacional gerencia. Os usos de cada número de porta são completamente arbitrários, determinados por consenso, e pro programa em si não faz nenhuma diferença se ligar numa porta 80 ou 3000 ou 8080. Não existe nenhum tratamento especial diferente. Portas são só números. E por isso você sobe uma aplicação web feita em Node numa porta 3000 e ela funciona igual quando subimos na porta 80 num servidor na Amazon.&lt;/p&gt;

&lt;p&gt;Por convenção, estabeleceu-se que as primeiras 1024 portas, de 0 até 1023 são portas de sistema, e num Linux precisa ter permissão de root pra fazer um programa se registrar numa delas. Essa distinção existe porque vários usuários podem abrir sessões num mesmo servidor UNIX ou Windows Server, mas só o administrador pode subir processos em portas abaixo de 1024. Usuários nesse servidor podem subir processos em outras portas menos nessas. É nesse intervalo que se encontram as duas portas mais famosas de todas, a 80 de HTTP e 443 de HTTPS. Temos outras famosas, como porta 21 de FTP, porta 22 de SSH, porta 25 de SMTP pra envio de emails, porta 37 que usamos pra sincronia de horário via NTP, porta 53 que é DNS, porta 194 é de IRC, 993 pra IMAP de cliente de email e assim por diante.&lt;/p&gt;

&lt;p&gt;Existe uma organização que registra o uso dessas portas baixas, a IANA ou Internet Assigned Number Authority. Dá pra ligar qualquer programa numa porta baixa, mas normalmente essas portas costumam ter serviços específicos como servidor web ou DNS. Por isso que num navegador quando digitamos endereço, podemos omitir a porta, porque o navegador preenche a porta pra gente usando essas convenções. Quando digitamos http://www.google.com primeiro ele precisa resolver esse nome de domínio, pra isso resolve esse nome num endereço IP e pra isso precisa de um name server ou DNS.&lt;/p&gt;

&lt;p&gt;Já mostrei no episódio anterior como seu sistema operacional pergunta quem é esse nome pra um DNS, como o 1.1.1.1 da CloudFlare ou 8.8.8.8 do Google. O DNS vai te devolver um grupo de endereços IP e seu computador escolhe um deles. A partir desse ponto faz um cache disso localmente. Se no navegador pedir de novo o www.google.com, nem precisa pedir pro DNS, porque já tem gravado num cache local. Toda resposta de DNS tem uma coisa chamada TTL ou time to live, que é tipo uma expiração. Quando expirar, aí sim seu computador vai no DNS e pergunta de novo qual é o endereço do nome www.google.com. Em outro video vou explicar DNS com um pouco mais de detalhe mas se acostumem com esse conceito primeiro.&lt;/p&gt;

&lt;p&gt;Enfim, graças ao DNS ou o cache local de DNS, seu navegador agora tem um endereço IP. Ele pode começar a se conectar. Mas como disse antes, precisa saber em que porta nesse endereço vai estar o programa que vai te devolver os HTMLs. Esse programa chamamos de servidor web, e como falei antes, por convenção, vai estar escutando nas portas 80 ou 443. Se no navegador você digitou http:// significa que quer conectar na porta 80. Se digitar https:// quer dizer que quer conectar na porta 443. O que vem antes dos dois pontos é com qual protocolo quer se comunicar com esse servidor, no caso HTTP ou HTTP com TLS.&lt;/p&gt;

&lt;p&gt;Protocolos é quem definem os tais comandos e valores que ambos o navegador e o programa servidor web conseguem entender, os pinos certos pros buracos certos da tomada. No protocolo HTTP temos comandos em texto como GET ou POST pra pedir ou enviar coisas do navegador pro servidor. Vamos fazer um exemplo simples só pra consolidar o conceito. Vou fazer rapidinho um servidor web em Node.js estilo hello world, só pra ter um programa escutando numa porta. Isso você encontra quinhentos tutoriais na internet não vou detalhar muito.&lt;/p&gt;

&lt;p&gt;Olha esse do tutorial do próprio site oficial do Node.js. Primeiro ele carrega a biblioteca &quot;http&quot;. Em seguida declara duas variaveis, uma pra guardar o endereco 127.0.0.1. Esse é o endereço que chamamos de loopback, ele representa o endereço local de rede do seu computador. Todo pacote enviado pra esse endereço não vai sair do seu computador, ele vai devolver pra ele mesmo. Por isso chamamos de loopback. Vale entender isso porque muita gente acha que 127.0.0.1 é a mesma coisa que outro endereço, o 0.0.0.0 e não sabe a diferença.&lt;/p&gt;

&lt;p&gt;Normalmente um endereço IP é associado com uma placa de rede instalado na sua máquina, mas 127.0.0.1 é como se fosse uma placa de rede virtual, de mentira. Ele recebe pacotes e devolve pra ele mesmo. 127.0.0.1 é a mesma coisa que localhost. Quando damos bind de um processo nesse endereço, dá pra abrir o navegador e conectar por ele com http://localhost. Mas se alguém de fora tentar se conectar no mesmo serviço, na mesma porta, não vai conseguir, porque na placa de rede de verdade não tem bind nessa porta. Localhost só é acessível de dentro do próprio computador, não de fora.&lt;/p&gt;

&lt;p&gt;Tem também o endereço 0.0.0.0 que significa “qualquer placa de rede do computador”. Quando queremos pendurar um servidor que vai receber bits independente de por qual placa de rede do seu computador vier, penduramos ele no 0.0.0.0. E no navegador podemos digitar localhost porque 127.0.0.1 é um desses “qualquer placa de rede”, mas também um navegador em outro computador na rede, vai conseguir se conectar no seu programa. Todo computador responde nesses endereços locais. Muita gente pensa que são a mesma coisa, mas só localhost e 127.0.0.1 são a mesma coisa. Pra desenvolvimento a gente usa localhost, porque normalmente não queremos outra pessoa na mesma rede acessando enquanto estamos no meio do desenvolvimento. Mas num servidor de verdade queremos dar bind em 0.0.0.0 pra deixar o programa acessível a qualquer pessoa.&lt;/p&gt;

&lt;p&gt;A segunda variável define que quer se ligar na porta 3000. Eu expliquei que portas de 0 até 1023 são reservados e o sistema operacional exige permissão de administrador pra ligar um programa nessas portas baixas. Por isso que em máquina de desenvolvimento, preferimos usar portas de 1025 até 49 mil 151. Porque com suas permissões de usuário normal, pode pedir pro sistema ligar seu programa numa porta, sem privilégios especiais, sem precisar ficar usando o comando &lt;code&gt;sudo&lt;/code&gt; toda hora. Por isso que pra subir uma aplicação de Node.js ou Django ou Laravel, numa porta como 3000 ou 4000, não precisa executar com &lt;code&gt;sudo&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Pra complementar, de 49 mil 152 até 65 mil 535 são o que chamamos de portas dinâmicas ou privadas. Em particular 49 mil 152, que é um número difícil de decorar em decimal, mas é a mesma coisa que 2 elevado a 15 menos 2 elevado a 14, que é mais fácil de lembrar. Também são chamadas de portas efêmeras. Você até pode mas normalmente não se registra nenhum programa servidor que vai ficar rodando permanentemente nesse segmento. Essas são as portas usadas, por exemplo, por comunicadores como o Zoom ou Skype pra abrir conexão com outra pessoa na internet. Pode ser usado por games pra sessões multiplayer. São portas temporárias. Programas de comunicação tentam diversas dessas portas e usam a primeira livre que acharem. Não é regra, mas eu acho que programas que usam UDP em vez de TCP usam mais essas portas.&lt;/p&gt;

&lt;p&gt;Por isso que nós programadores web, quando fazemos uma aplicação usando Node.js ou Spring ou Laravel ou qualquer coisa assim usamos portas como 3000, 4000, 8000, 8080 como era o Tomcat de Java e assim por diante. Qualquer uma depois de 1024 e antes daqueles 49 mil e tanto servem. Sem interferir com calls de Zoom e sem precisar de privilégios de administrador e ter que ficar digitando &lt;code&gt;sudo&lt;/code&gt;. De novo, portas são só números. Se você achava que porta 8080 ou 3000 tinha algum significado, não tem, foram escolhidas arbitrariamente por alguém e todo mundo só usa a mesma porta por convenção.&lt;/p&gt;

&lt;p&gt;Enfim, agora é específico de cada linguagem e framework, mas no caso do Node criamos uma instância do que ele chama de &quot;server&quot;. Eu modifiquei um pouco o exemplo do tutorial porque quando criamos o servidor já registramos como queremos que ele responda quando vier algum comando de HTTP. Já volto nisso. Na sequência, fazemos essa instância de server pausar e ficar escutando, esperando alguma coisa, que o sistema operacional vai mandar pra gente no cano dessa porta 3000. Se ninguém mais estava pendurado na porta 3000, e ele conseguir se ligar com sucesso, daí escreve essa mensagem de servidor rodando. E pronto, agora está escutando na porta 3000 desse computador.&lt;/p&gt;

&lt;p&gt;Vamos voltar pro navegador. Lá no campo de endereço escrevo que quero usar o protocolo &quot;http&quot;, dois pontos, barra, barra, localhost que é outra forma de dizer &quot;127.0.0.1&quot;, dois pontos, porta 3000. Se eu não colocasse explicitamente esse dois pontos 3000, ele ia assumir que seria porta 80. Tenta aí, em qualquer site como google.com, digita no final dois pontos 80. Vai ver que dá no mesmo.&lt;/p&gt;

&lt;p&gt;E pronto, veio uma resposta Hello World. Vamos voltar pro código do servidor. Lembra que quando criei a instância de server registrei uma função de resposta? O framework do Node esconde os detalhes. O Linux recebeu os pacotes de bits do navegador endereçados pra porta 3000, viu que tinha meu programa pendurado nele e começou a reenviar os pacotes pro cano da porta 3000.&lt;/p&gt;

&lt;p&gt;O meu programa em Node começa a receber pacote e dá accept. E aí vê o que fazer com esses pacotes. Daí tinha essa minha função registrada no server. Ele encapsula esses pacotes numa estrutura chamada Request, ou HTTP Request. É isso que faz essa biblioteca &quot;http&quot; que estamos usando. Ele se encarrega de receber os bits e organizar tudo bonitinho nesse objeto de Request pra gente. A partir desse ponto eu posso fazer lógica de código pra fazer coisas diferentes dependendo do que vier nessa request, como carregar um arquivo HTML. No caso, como é um exemplo besta, eu só mando responder a mensagem de Hello.&lt;/p&gt;

&lt;p&gt;Mas não é só a mensagem. Isso é um protocolo, tem algumas regrinhas. O mínimo do mínimo que se espera de resposta é montar uma segunda estrutura chamada HTTP Response. Nesse response registro um status code 200, que é o código do protocolo HTTP pra dizer que tá tudo ok, daí por boa etiqueta, registro também um cabeçalho com um mime type, no caso pra indicar que não vou responder um HTML e sim só um texto puro. É com esse mime type que o navegador sabe se é pra interpretar como HTML ou se é um binário e é pra fazer download, por exemplo. E finalmente, posso escrever minha mensagem de hello no response como um simples string.&lt;/p&gt;

&lt;p&gt;O Node vai pegar esse pacote de response que preenchi e mandar pro sistema operacional devolver pela rede pro navegador pela mesma conexão. E o sistema operacional por baixo vai fazer tudo que já expliquei antes, vai quebrar esse response em datagrams, vai encontrar a rota pra enviar, etc. E no final o navegador vai receber o pacote de resposta e interpretar isso visualmente na tela, desenhando o hello na tela pro usuário. Esse é o fluxo mais básico de web. Se não entender nem isso, não vai entender nada mais complicado, então garanta que esse fluxo está super claro na sua cabeça.&lt;/p&gt;

&lt;p&gt;Tem outra forma de ver como isso funciona. Antigamente todo sistema operacional vinha com um programa de internet bem simples chamado telnet. É um programa que se conecta em qualquer porta que tenha algum outro programa escutando e com ele posso manualmente digitar comandos pra enviar, caso eles aceitem comandos em texto em vez de binário. No caso do Arch Linux hoje ele fica no pacote opcional chamado de inetutils. Em nenhuma distro se instala mais por padrão porque acho que é considerado um risco de segurança. São ferramentas que precedem o conceito de encriptação. Em vez de telnet hoje se usa SSH pra abrir conexões encriptadas, mas como não quero ter que lidar com criptografia neste episódio, deixa eu mostrar o bom e velho telnet.&lt;/p&gt;

&lt;p&gt;Aquele meu programa de hello de Node continua rodando. Ele tá pendurado na porta 3000 e mesmo quando processa a requisição de algum navegador, continua aceitando novas requisições de outros navegadores. Por isso que um servidor web consegue atender várias pessoas ao mesmo tempo. Vamos abrir um outro terminal e conectar direto nele usando telnet, passando o endereço e a porta.&lt;/p&gt;

&lt;p&gt;Pronto, estou conectado. Agora posso enviar uma requisição HTTP manualmente. O protocolo HTTP diz que posso digitar o comando GET seguido de uma URI que é o barra alguma coisa e no final escrever HTTP/1.0 denotando a versão do protocolo que estou querendo usar. Isso é tudo texto. Nas linhas seguintes posso mandar outros detalhes desse comando, mas se der dois enters, duas quebras de linha, isso indica pro servidor que terminei minha requisição.&lt;/p&gt;

&lt;p&gt;E olha só, quando dei dois enters, o Node entendeu que podia começar a responder, e essa é a resposta que ele envia. Olha só o que tinha mandado ele preencher na tal estrutura de resposta. O status code 200 veio na primeira linha, seguido do content type que é texto puro, mais uma data e horário que o framework preencheu pra mim e o fechamento da conexão seguido do corpo da resposta que é o string da mensagem hello.&lt;/p&gt;

&lt;p&gt;Claro, hoje em dia não preciso de telnet pra ver isso. Todo navegador agora tem o tal Developer Tools, que se  acessa com o atalho control shift J no Windows ou control option J no Mac. Ele tem essa aba Network ou rede, onde podemos ver os pacotes de request e response e os cabeçalhos. Mesma coisa que fizemos no telnet, olha como no request ele mostra que o navegador fez um comando GET e embaixo temos a mesma resposta que recebemos via telnet, status code 200, content type text plain, a data e horário. Antes de existir Dev Tools a gente usava coisas como Telnet pra debugar.&lt;/p&gt;

&lt;p&gt;Em linhas gerais é assim que um navegador, um cliente de APIs e tudo mais que fazemos na web funciona. Com o navegador conectando na porta 80 ou 443 ou numa porta de usuário como a 3000 do meu Node.js. O navegador monta uma estrutura em texto chamada request, um servidor feito em qualquer framework como Node da vida recebe, decide o que fazer baseado no que foi pedido, monta uma outra estrutura chamada response e devolve. Esse é o básico do básico de web.&lt;/p&gt;

&lt;p&gt;A porta 443 e TLS é uma camada a parte. É um processo que acontece entre seu navegador e o servidor web, independente da aplicação por baixo. Esse meu hello em Node não precisa estar ciente que a conexão estava ou não criptografada. O navegador pede um handshake, um aperto de mão, com o servidor que, se estiver devidamente configurado com certificados SSL, vão abrir uma conexão segura usando chaves assimétrica, trocar uma chave simétrica, e a partir daí todo pacote que o servidor web envia vai criptografado e o navegador consegue descriptografar e vice versa. Eu explico isso na parte 2 dos meus videos sobre criptografia.&lt;/p&gt;

&lt;p&gt;Mas eu digo que tanto faz porque o programa Node por baixo vai receber a request já descriptografado direitinho, como se nada tivesse acontecido. E em redes existe isso de cada camada na arquitetura trabalhar os pacotes trafegando e as camadas de cima, como minha aplicação, não precisar estar cientes e funcionar normalmente. Se não sabia o que é uma boa arquitetura, a arquitetura OSI de camadas de rede é uma boa inspiração, onde cada camada tem sua responsabiilidade, cada camada não precisa estar ciente se uma camada embaixo foi adicionada ou trocada, contanto que realizem as mesmas tarefas. Programação de software pra rede costuma ter essa metáfora de camadas de uma cebola. Quem já brincou de Tor e coisas assim já deve ter ouvido falar de Onion, por isso. Outro dia falo de coisas como Tor.&lt;/p&gt;

&lt;p&gt;Um navegador web como Chrome é um dos softwares mais complexos que você tem instalado no seu computador depois do próprio sistema operacional, especialmente pra renderizar a parte gráfica corretamente. Mas pra se conectar num servidor web e mandar comandos simples de GET, um mero telnet como mostrei já serve. Vários outros programas simulam um navegador simples como os programas wget ou curl de Linux, que a gente usa mais pra fazer download, mas tecnicamente ele se comporta como um navegador web. Ou bibliotecas que usamos pra fazer clientes pra APIs como o Axios de Javascript, ou a classe HttpClient de Java ou C#, ou a classe http.client de Python e assim por diante. São todos mini navegadores, sem a parte gráfica.&lt;/p&gt;

&lt;p&gt;Se nunca pensou assim, um servidor é todo programa que pede um bind de uma porta pro sistema operacional e fica em modo listen de escuta, esperando pra dar accept e abrir uma conexão com outro computador remoto. Aliás, vale outro conceito básico. Eu categorizo programas em dois tipos, short lived e long lived. Ou em bom português, programas de tiro curto e programas persistentes. Programa de tiro curto são muitos dos que você roda via linha de comando, comandos como o grep, sed, ou awk de Linux, programas que recebem um input, processam alguma coisa, cospem um resultado e saem fora. Servidores ou programas persistentes são loops.&lt;/p&gt;

&lt;p&gt;Vamos desconsiderar por um segundo que existem threads, I/O assincrono, e que tudo seja como era antigamente, quando só dava pra rodar um comando bloqueante e síncrono por vez. Se eu fosse escrever numa pseudo-linguagem, ou seja, uma linguagem que não existe, a estrutura mais básica de um servidor de rede seria assim:&lt;/p&gt;

&lt;p&gt;`while(true) {
    listen(:3000)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stream = accept()
while(not stream.eof) {
    request += stream.read_line
}
response = processa_alguma_coisa(request)
send(response)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}`&lt;/p&gt;

&lt;p&gt;Todo servidor fica num loop infinito até receber sinal do sistema operacional pra morrer, o tal SIGTERM ou sinal de término, que se você já usou Linux, já enviou com o comando &lt;code&gt;kill&lt;/code&gt;. Ou no Windows via powershell com o comando &lt;code&gt;taskkill&lt;/code&gt;. Enfim, entenda que se executar um programa e ele tiver o comportamento de esperar um evento, reagir, e voltar a esperar, normalmente é um loop, equivalente a um while true.&lt;/p&gt;

&lt;p&gt;Dentro desse while, ele manda comando pro sistema operacional, o &lt;code&gt;listen&lt;/code&gt;, pra pedir pra ficar esperando o que vier numa porta, como 3000. Nesse ponto que eu falei pra ignorar que existe threads e concorrência. Faz de conta que quando ele dá listen, fica parado nessa linha até receber alguma coisa. Uma hora alguém conecta e começa a enviar bits. Daí sai dessa linha e cai no &lt;code&gt;accept&lt;/code&gt; onde recebe o stream de bits, o tal cano de bits. Agora eu fiz um pseudo código arbitrário que é ficar lendo linha a linha até sinalizar &lt;code&gt;eof&lt;/code&gt; que, se nunca viu, costuma ser sigla pra end of file, que terminou o envio ou acabou o arquivo.&lt;/p&gt;

&lt;p&gt;Agora, o programa faz alguma coisa com esses bits que vieram, que chamei de request, e monta alguma resposta, que é a estrutura de response. No final envia de volta ou dá &lt;code&gt;send&lt;/code&gt; desse response, pedindo pro sistema operacional devolver, pela mesma conexão fechada no accept, e o loop reinicia esperando outra requisição. Volta pro estado de listen, até aparecer mais bits. No mundo real existem threads e na realidade uma das coisas que pode acontecer dependendo da linguagem é o accept abrir uma nova thread pra processar o request e response enquanto o loop de listen continua ativo esperando mais conexões. Possibilitando lidar com dezenas ou centenas de conexões simultaneamente.&lt;/p&gt;

&lt;p&gt;Aproveitando o conceito. Todo programa gráfico é um grande loop também. Ele fica esperando você digitar alguma coisa ou clicar alguma coisa com o mouse, é parecido com esse &lt;code&gt;listen&lt;/code&gt;. E fica no loop infinito até receber um sinal pra fechar como o menu de Quit ou alt-f4 no Windows. Um game também é um grande loop, esperando seus comandos e reagindo. Essa é a fundação pro que muitos chamam de programação orientada a eventos original. O programa reage a eventos, como input de mouse e teclado ou algo assim. Não vou entrar em paradigmas, mas só pra lembrar que esse é um dos patterns mais comuns em programação, o que eu pessoalmemnte chamo de Big Loop.&lt;/p&gt;

&lt;p&gt;Normalmente, em web, a gente fica muito preso ao protocolo HTTP, onde as duas peças principais são a tal estrutura texto de HTTP Request e HTTP Response e comandos básicos como GET, POST ou PUT. Todo tutorial de web começa direto neles. E por causa disso muita gente faz o que não devia, tentando embutir tudo que pode dentro dessas estruturas. Mas a internet não é feita só de HTTP, ela só é a mais popular. Muitos falam “internet” pensando em redes, roteadores, pontos de acesso. E chama o resto de Web. Mas Web, que é o nome mais curto pra World Wide Web, que se refere à capacidade de hipertexto de páginas HTML. A idéia que uma página pode ter links pra outras páginas formando uma teia, ou Web.&lt;/p&gt;

&lt;p&gt;Mas internet na realidade é um conjunto de tecnologias, que envolve coisas como ethernet, wifi, fibra, o TCP/IP e todos os outros protocolos de aplicação, incluindo o HTTP de Web. E chamamos assim porque como expliquei antes, o modelo OSI de internet define sete camadas. A camada física que foi o episódio que expliquei sobre ondas, cabos de cobre e wireless. A camada de data link responsavel entre outras coisas como checar erros, como eu expliquei sobre Hamming Code e tudo mais. A terceira camada, de rede é que lida com coisas como roteamento, endereços IP e tudo mais.&lt;/p&gt;

&lt;p&gt;Eu pulei as camadas seguintes, a camada 4 de transporte, a camada 5 de sessão, a camada 6 de apresentação e a camada 7 de aplicação. Isso porque em TCP/IP as funções da camadas acima da 5 acabam sendo acumuladas na camada de aplicação, que é onde mora protocolos como HTTP de Web.&lt;/p&gt;

&lt;p&gt;Em OSI existe uma camada pra lidar com sessões, mas em Web controlamos sessão direto no HTTP usando cabeçalhos como de cookies. Sessões servem pra literalmente identificar uma sessão individual de cada usuário e no modelo OSI esse controle ficaria na camada 5, mas a gente acaba achando mais fácil acumular isso direto na camada de aplicação. Por isso em vez de ter um jeito único de lidar com sessões, protocolos como HTTP, FTP, IMAP e tudo mais, cada um controla sessões do seu jeito. Tem vantagens e desvantagens de se fazer isso.&lt;/p&gt;

&lt;p&gt;Exemplo de outro protocolo era o antigo FTP ou File Transfer Protocol que hoje em dia ninguém mais usa pelo mesmo motivo que não se usa Telnet, ambos precedem conceitos mais modernos de segurança. Muitos antigos servidores de FTP estão expostos atrás de um servidor web, como o FTP da Unicamp que eu usava muito na faculdade e hoje hospeda espelhos de arquivos de Linux. Se digitar ftp.unicamp.br no seu navegador, vai ver que ele coloca http:// na frente, indicando que não está usando o protocolo FTP.&lt;/p&gt;

&lt;p&gt;Num terminal Linux, se instalar o antigo pacote inetutils, além do telnet, vai ganhar o programa ftp. Basta digitar &lt;code&gt;ftp ftp.unicamp.br&lt;/code&gt; no terminal. Ele vai pedir um usuário. Como antigamente, muitos servidores públicos aceitavam o usuário &lt;code&gt;anonymous&lt;/code&gt;. E felizmente esse ainda aceita. A senha pode ser qualquer coisa ou vazia. E pronto, abriu uma conexão de FTP onde ele passa a esperar comandos que são parecidos com os comandos pra navegar no seu HD via terminal como &lt;code&gt;cd&lt;/code&gt; pra mudar de diretórios ou &lt;code&gt;ls&lt;/code&gt; pra listar o conteúdo do diretório. Se digitar &lt;code&gt;help&lt;/code&gt; mostra os comandos que o protocolo FTP suporta. Lembra que HTTP suporta comandos como GET? Em FTP ele suporta comandos diferente. Get em FTP é pra fazer download de algum arquivo.&lt;/p&gt;

&lt;p&gt;Eu até consigo mudar de diretório fazendo &lt;code&gt;cd /pub&lt;/code&gt; pra ir pro diretório público, mas não sei porque, comandos como &lt;code&gt;ls&lt;/code&gt; não devolvem nada e dão timeout. HTTP versão 1.0 era ótimo pra baixar páginas HTML e até mesmo fazer download de arquivos, como agora. Mas era chatinho enviar arquivos pro servidor. Hoje em dia é fácil, mas nos primórdios, a gente ainda não entendia tudo que dava pra fazer, nem lembro se HTML 1.0 tinha form multipart. Pra enviar arquivos pra um servidor, o certo era usar FTP.&lt;/p&gt;

&lt;p&gt;Acumulamos um monte de funções em cima de HTTP que antes eram de outros protocolos. Hoje em dia é muito normal abrir um Gmail e anexar um arquivo. Ou abrir um Dropbox e subir um arquivo. E mesmo se quisermos transferir arquivos num equivalente FTP, não se usa FTP pelo mesmo motivo que hoje usamos HTTPS em vez de HTTP, por segurança. HTTPS é HTTP mais encriptação com certificado TLS. O equivalente seria FTPS. Assim como em vez de Telnet hoje usamos SSH. E SSH é mais versátil porque permite não só abrir um terminal remoto seguro, como também trafegar arquivos, via SFTP que é FTP sobre SSH. Vou falar mais de SSH no próximo episódio.&lt;/p&gt;

&lt;p&gt;Outro protocolo que até existe ainda mas num nicho muito pequeno é o que falei 2 episódios atrás quando expliquei correção de erros e parchives. O protocolo NNTP de rede Usenet. A idéia era ter um servidor de artigos de texto, como fóruns e sincronizar diferentes servidores usando esse protocolo NNTP. De novo, acumulamos essas funções dentro de HTTP e o equivalente seria sites como StackOverflow, HackerNews, Reddit.&lt;/p&gt;

&lt;p&gt;No começo da internet não usávamos navegador web pra tudo. Usávamos programas separados como FTP pra transferir arquivos. News pra ler notícias e discussões da Usenet, literalmente newsgroups. Telnet pra abrir sessões remotas de terminal. E Gopher, que eu quase nunca usei porque meio que tinha o mesmo propósito de HTTP e perdeu espaço quando a Web ficou mais popular. Mas todos são exemplos dos primeiros protocolos de aplicação no começo da internet comercial. Na prática, Usenet ainda é usado até hoje pra facilitar pirataria, algumas coisas que talvez seja mais difícil em BitTorrent ainda se acha na Usenet.&lt;/p&gt;

&lt;p&gt;Todos foram substituídos. HTTPS se tornou o protocolo mais popular. SSH é a única boa alternativa tanto pra telnet quanto ftp. E protocolos específicos como NNTP, IRC de chat, Gopher e outros, tiveram suas funções acumuladas em HTTPS. E o S de segurança se tornou importante nos primórdios do comércio eletrônico, onde encriptação se tornou essencial pra permitir trafegar coisas como número de cartão de crédito. Sem isso seria impossível ter comércio online.&lt;/p&gt;

&lt;p&gt;Tivemos primeiro o SSL da Netscape, e hoje é o TLS ou Transport Layer Security. Isso possibilitou escalar a economia na Web mas também dificultou pra hobistas e amadores que queriam só fuçar a rede com comandos simples no terminal, porque agora temos que lidar com uma camada de encriptação no meio.. Antigamente era só plugar telnet em qualquer porta que já dava pra fuçar, hoje já não é tão simples e como isso torna as coisas mais indiretas e abstratas, imagino que deve dificultar o aprendizado de muita gente.&lt;/p&gt;

&lt;p&gt;Sem contar que se eu quisesse contribuir na rede e participar de sub-redes como servidores de chat IRC, era só montar meu próprio servidor IRC e plugar na internet. Ou se quisesse ter meu próprio servidor de newsgroups era só montar um servidor de NNTP e plugar na Usenet e assim a gente podia fazer parte do mesmo serviço distribuído na rede. No momento onde centralizamos tudo em HTTP, também perdemos a característica de serviços distribuídos e entramos na era de walled gardens ou jardins emparedados. Acho que os únicos exemplos de rede verdadeiramente distribuída onde qualquer um pode entrar e participar é BitTorrent e Bitcoin da vida. Sim, eu sei, como falei antes Usenet e IRC ainda existem, mas são nichos bem pequenos e desconhecidos. Tem muito mais gente usando Discord do que IRC.&lt;/p&gt;

&lt;p&gt;E falando nisso, eu acho que preciso explicar um pouquinho sobre segurança de rede e vou deixar isso pro próximo episódio. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e compartilhem o video com seus amigos. A gente se vê, até mais!&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5958</id>
    <published>2022-07-23T11:54:00-03:00</published>
    <updated>2022-07-23T11:55:29-03:00</updated>
    <link href="/2022/07/23/akitando-123-como-sua-internet-funciona-introducao-a-redes-parte-3" rel="alternate" type="text/html">
    <title>[Akitando] #123 - Como sua Internet Funciona | Introdução a Redes Parte 3</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/gcv5hXyTcIo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Vamos mais na prática como a internet da sua casa funciona. O que vem depois do modem. Como seu PC, seu celular, sua TV, conseguem se comunicar e acessar a internet? E se no ipv4 dizem que já acabou os possíveis endereços de IP, como continuamos usando internet sem migrar pro tal ipv6? Aliás, o que é ipv6? Vamos tirar essas coisas básicas do caminho.&lt;/p&gt;

&lt;h2&gt;Conteúdo:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Introdução&lt;/li&gt;
&lt;li&gt;00:47 - Tanenbaum&lt;/li&gt;
&lt;li&gt;01:43 - OSI Model&lt;/li&gt;
&lt;li&gt;04:39 - Configurando roteador&lt;/li&gt;
&lt;li&gt;06:22 - Endereço ipv4&lt;/li&gt;
&lt;li&gt;07:36 - Máscara de Rede&lt;/li&gt;
&lt;li&gt;09:23 - Telefones e ramais&lt;/li&gt;
&lt;li&gt;11:28 - NAT&lt;/li&gt;
&lt;li&gt;14:25 - ARP&lt;/li&gt;
&lt;li&gt;16:40 - Default Gateway&lt;/li&gt;
&lt;li&gt;17:56 - DHCP&lt;/li&gt;
&lt;li&gt;19:39 - Hubs e Switches&lt;/li&gt;
&lt;li&gt;21:58 - DNS&lt;/li&gt;
&lt;li&gt;26:30 - Pi-Hole (DoH)&lt;/li&gt;
&lt;li&gt;27:57 - Roteamento&lt;/li&gt;
&lt;li&gt;30:20 - CGNAT&lt;/li&gt;
&lt;li&gt;33:20 - IPv6&lt;/li&gt;
&lt;li&gt;40:23 - Conclusão&lt;/li&gt;
&lt;li&gt;40:53 - Bloopers&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Links:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Computer Networks (5th Edition PDF) (https://www.mbit.edu.in/wp-content/uploads/2020/05/Computer-Networks-5th-Edition.pdf)&lt;/li&gt;
&lt;li&gt;What is a DNS record? (https://www.cloudflare.com/learning/dns/dns-records/)&lt;/li&gt;
&lt;li&gt;MAC spoofing (https://en.wikipedia.org/wiki/MAC_spoofing)&lt;/li&gt;
&lt;li&gt;What is Carrier-grade NAT (CGN/CGNAT)? (https://www.a10networks.com/glossary/what-is-carrier-grade-nat-cgn-cgnat/)&lt;/li&gt;
&lt;li&gt;What is IPv6, and why is adoption taking so long? (https://www.networkworld.com/article/3254575/what-is-ipv6-and-why-aren-t-we-there-yet.html)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Finalmente vou poder falar um pouquinho mais sobre a parte que mais interessa pra programadores iniciantes, que é o básico do básico de uma rede caseira. Eu resolvi falar disso porque recentemente estava justamente fuçando minha própria rede de casa, configurando meu NAS novo, e pareceu uma boa hora pra introduzir alguns conceitos de rede. Muito do que quero falar em episódios futuros depende de todo mundo saber pelo menos esse básico.&lt;/p&gt;

&lt;p&gt;Hoje pulamos do modem pro roteador e o vocabulário mais básico sobre endereços IP, rotas, e tudo mais. Pra maioria das pessoas tudo isso é invisível. Você já conecta no seu Wifi e tudo funciona. Mas programadores precisam entender porquê essas coisas funcionam. Então vamos lá.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Sem vocês saberem nos últimos dois episódios eu toquei em dois tópicos específicos de redes. O que se chama de camada física e a camada de dados, physical layer e data link layer. Claro, eu só toquei em pouquíssimos tópicos desses assuntos, se quiserem saber o assunto completo, como sempre, eu recomendo que estudem o famoso livro de redes do professor Tanenbaum. Quem fez ciências da computação provavelmente teve que estudar esse livro, e na minha época da faculdade também era ele.&lt;/p&gt;

&lt;p&gt;Se conseguiram chegar até aqui, espero que já consigam enxergar dados não como arquivos e sim como conjuntos de blocos de bits, ou o que chamamos de pacotes. E espero que já tenham começado a entender a idéia de protocolos como o formato como preenchemos os metadados do cabeçalho desses pacotes. A metáfora de e-commerce onde o produto é o dado e o pacote de fora onde se preenche endereços é o protocolo. Esses pacotes tem coisas como o número deles pra depois a gente receber e montar na ordem certa.&lt;/p&gt;

&lt;p&gt;O hello world do estudo de redes é o modelo de referência OSI, que tecnicamente não é mais usado hoje em dia. A gente tá mais familiarizado direto com o protocolo TCP/IP que se usa na internet. O Tanenbaum diz que os protocolos associados ao modelo OSI não são mais usados, mas o modelo em si é bem útil e geral e ainda válido. Já no TCP/IP é o contrário, o modelo em si não é muito útil mas os protocolos são muito utilizados. Por isso na prática a gente usa TCP/IP mas aprende o modelo OSI.&lt;/p&gt;

&lt;p&gt;O modelo OSI é dividido em 7 camadas e eu disse que já mencionei parte das duas mais baixas, a camada física 1 e a camada de data link 2. A idéia das camadas é que elas poderiam ser substituídas sem alterar as camadas de cima. Por exemplo, posso trocar a camada física de cabos ethernet por wifi e o resto funciona igual. A camada física é isso, a camada responsável por transportar bits por algum canal de comunicação.&lt;/p&gt;

&lt;p&gt;A camada de data layer é responsável por cuidar de coisas como checagens de erros. Checksums e o algoritmo Internet 16-bits que mencionei no episódio anterior. A idéia de detectar um erro e pedir pra repetir o pacote caso tenha erro. Depois vem a camada que não falei, a camada de rede propriamente dita, é quem cuida das rotas e pra onde enviar os pacotes.&lt;/p&gt;

&lt;p&gt;No caso de TCP/IP as camadas física e data link são uma camada só, a camada chamada só de link. E a camada de rede é a responsabilidade do protocolo IP. Quando falamos em IP pensamos em endereço IP, mas IP em si é Internet Protocol, que é a camada de Internet junto com outro protocolo, o ICMP ou Internet Control Message Protocol, que a gente usa quando fazemos ping por exemplo.&lt;/p&gt;

&lt;p&gt;Acima disso, no modelo OSI temos camada de transporte, sessão, apresentação e de aplicação. Tecnicamente a camada de transporte em si é o TCP que é Transport Control Protocol, ou protocolo de controle de transporte. Portanto TCP/IP é justamente protocolo de controle de transporte de pacotes na internet. TCP tem como característica controlar conexão entre duas pontas de forma confiável. Em paralelo temos UDP ou User Datagram Protocol, que é um protocolo que não fecha conexão e não usa o sistema de sequenciamento de pacotes do TCP. Ele tem menos metadados e é mais leve, mas é menos confiável e feito pra situações onde perder um pacote não é um problema. Streaming de audio e video é um exemplo onde UDP pode ser útil.&lt;/p&gt;

&lt;p&gt;A camada de aplicação é onde encapsulamos outros protocolos dentro. É onde nós programadores mais trabalhamos com coisas como HTTP pra web, SMTP pra email, DNS pra domínio e assim por diante. A maioria dos protocolos que usamos no dia a dia de programação são empacotados nessa última camada de aplicação. Pras camadas embaixo, não importa o que foi encapsulado, porque vamos fragmentar tudo em frames e eles só se importam em entregar esses frames de uma máquina pra outra. É como um entregador do mercado livre. Pra ele não interessa o que tem dentro do pacote, ele vai entregar do mesmo jeito seja lá o que tiver lá dentro.&lt;/p&gt;

&lt;p&gt;Mas entregar, precisamos saber pra quem entregar. Eu disse no episódio de modem que a responsabilidade dele é modular e demodular entre sinal digital e sinal analógico. Mas essas caixinhas de uma Vivo ou Claro da vida fazem mais, são também roteador e ponto de acesso. É como um negócio pequeno familiar como a padaria do pai que é padeiro, a mãe recebe pedidos, manda o filho ir entregar e vai funcionando assim. Essa é a beleza dos protocolos de internet, você pode entuchar tudo num PCzinho fraco que vai funcionar ou dividir responsabilidades com PCs mais fortes pra escalar. Pra maioria das pessoas, com 1 TV, 3 ou 4 celulares, 1 ou 2 PCs, usar essa caixinha tá mais que bom e vai funcionar bem o suficiente que poucos vão notar a perda de performance e possível falta de segurança.&lt;/p&gt;

&lt;p&gt;Eu mesmo não tenho usos muitos drásticos, mas talvez por ser mais old school, eu prefiro desligar a função de ponto de acesso e deixar ele conversar só com um dispositivo, o meu router separado. Não é um router robusto nem nada disso, é só um TP-Link Archer C6 normal. Eu uso um cabo cat5e que suporta até 1 gigabit. Como meu plano é de 300 megabits só, mais do que isso sobra. O roteador registra o gateway de saída padrão como sendo o IP do modem que é 192.168.15.1. E o roteador como sendo o dispositivo 192.168.15.2.&lt;/p&gt;

&lt;p&gt;Hoje vamos falar desse tal de endereço IP, esse número de quatro blocos divididos por pontos que todo mundo já viu pelo menos uma vez na vida. Infelizmente pra realmente explicar isso eu precisaria tocar no assunto de roteamento, classless interdomain routing ou CIDR, e eu realmente não tenho nenhuma vontade de explicar isso. Então, de novo, recomendo o livro do Tanenbaum e o capítulo sobre a camada de rede pra estudar. Dessa vez só vou dizer o que é, mas não porque é assim.&lt;/p&gt;

&lt;p&gt;Mesmo cortando o grosso, ainda assim explicar endereços é um saco. Vamos por partes. Um endereço IP é basicamente um número de 32-bits que escolhemos dividir em 4 blocos de 8 bits. Assim como em decimal, quando temos um número grande como 1 milhão, a gente costuma separar por ponto a cada três números pra ficar mais fácil de ler. Sendo blocos de 8 bits, se você é de front-end já tá acostumado com cores RGB sendo 3 blocos de 8 bits cada, indo de 0 até 255. Mesma coisa. Os endereços IP podem ir de 0.0.0.0 até 255.255.255.255.&lt;/p&gt;

&lt;p&gt;Aqui vale falar outra coisa, eu venho falando em blocos ou pacotes ou frames, mas existem várias nomenclaturas usadas de formas confusas e no mundo de IP fala-se em datagrams, é a mesma coisa, pacotes de bits com um cabeçalho de metadados. Assim como antigamente tinha telegrama no mundo analógico, em digital temos datagramas. Mas no fundo é tudo pacote de bits, não se perca nas diferentes nomenclaturas. A parte importante são os campos de metadados definidos pelo protocolo, que descrevem pros diversos componentes da rede, como switches e seus dispositivos o que fazer com esse pacote, pra onde mandar e tudo mais. Mas realmente não tenho nenhuma vontade de detalhar esses campos, novamente, leia Tanenbaum.&lt;/p&gt;

&lt;p&gt;Mas voltando, endereços IP versão 4 ou IPv4 como chamamos são números de 32-bits. Mas só esse número não quer dizer muita coisa sozinho. A gente sempre precisa de uma máscara de sub-rede, subnet mask, que é aquele número 255.255.255.0 ou similar que você também já deve ter visto e como o próprio nome diz é uma máscara pra ser aplicada no endereço IP pra saber qual parte define a sub-rede e qual define o host. Isso é uma coisa que todo programador tem obrigação de saber que são operações bitwise.&lt;/p&gt;

&lt;p&gt;Se nunca viu operação bitwise, vamos ver como que funciona. Vamos pegar o endereço que o modem deu pro roteador, 192.168.15.2 e escrever o número em binário aqui na tela, 32 bits. Eu quero saber qual é o prefixo que identifica a sub-rede. Pra isso usamos a máscara. Lembra, 255 significa tudo 1, então 3 blocos de 8 bits 1 e o resto é zero. Agora fazemos operação bitwise AND, ou seja, fazemos AND bit a bit. 1 and 1 é 1, 1 and 0 é 0, 0 and 0 é 0. Eu expliquei isso no guia mais hardcore de introdução a computadores.&lt;/p&gt;

&lt;p&gt;Fazendo isso, o que sobra no final é o prefixo de rede. 192.168.15.0. A máscara arranca fora o sufixo que identifica o dispositivo individual na rede. Essa máscara na prática me diz que os primeiros 24 bits representam a rede, por isso outra forma de representar isso é escrever 192.168.15.255/24. Isso denota uma rede onde é possível ter até 256 dispositivos. Se fosse 192.168.255.255/16 ou máscara 255.255.0.0 significa que poderíamos ter 2 elevado a 16 ou seja, mais de 65 mil dispositivos. É uma rede grande demais, por isso você não vê rede barra 16 por aí. Num roteador caseiro, uma rede barra 24 é mais que suficiente, porque ninguém tem mais de 200 dispositivos em casa precisando de endereços.&lt;/p&gt;

&lt;p&gt;Por que precisa ter isso? Um roteador é responsável por rotear pacotes na rede interna e também ligar com outro roteador numa rede externa, tipo a rede da Vivo. Se não tivesse essa hierarquia de rede e hosts, todo roteador precisaria ter uma tabela com cada endereço de cada dispositivo dizendo pra onde o pacote de cada um deles precisaria ir. Em vez disso o roteador só precisa saber, se da rede internet vier um pacote cujo prefixo do endereço seja 192.168.15.x ele precisa mandar pra rede externa do modem. Não importa se veio de 192.168.15.2 ou 192.168.15.3. Então o roteador só precisa manter uma linha em memória pra essa rede em vez de uma tabela com todos os endereços dessa rede.&lt;/p&gt;

&lt;p&gt;É mais ou menos como telefones. Se todo mundo tivesse um número de telefone de 11 números aleatórios, toda central em todo estado precisaria ter uma tabelona com 100% de todos os números pra saber pra onde mandar. Mas a gente usa o tal de DDD, os dois primeiros números representam uma região, mais ou menos uma cidade ou conjunto de cidades. Na Grande São Paulo o DDD é 11, Rio de Janeiro, o DDD é 21, em Natal o DDD é 84 e assim por diante. Então se a Central de São Paulo faz uma ligação pra um número com prefixo 84, ele sabe que precisa rotear a chamada pra Central da Natal. A Central de São Paulo não precisa ter o banco de dados com os números de Natal, só mandar pra outra central que vai ter esse sub-conjunto de números de Natal, que é a Central de Natal.&lt;/p&gt;

&lt;p&gt;Endereços IP é a mesma coisa, só que mais simples, ele representa a sub-rede e o host, ou identificador do dispositivo nessa rede. Mas tem mais detalhes pra quem quer estudar redes de verdade que é entender CIDR ou Classless InterDomain Routing, como se usa máscaras pra definir sub-redes independentes dentro de uma mesma organização, por exemplo, blocos de IPs diferentes pra cada departamento. E a forma antiga que se usava até 1993 de classful addressing, onde provedores tinham que lidar com classes A até E. Como falei antes, isso tudo tem a ver sobre como roteadores sabem como rotear quais pacotes pra quais redes. Isso é o capítulo de camada de rede do Tanenbaum.&lt;/p&gt;

&lt;p&gt;O que mais interessa é o problema que vocês já devem ter ouvido falar. Especialmente depois da explosão da internet no fim dos anos 90, seguido da explosão de smartphones nos anos 2010, a gente literalmente tem ordens de grandeza mais dispositivos conectados na internet do que existem endereços disponíveis num número de 32-bits como definido pelo IPv4. É mais ou menos o mesmo problema que expliquei no episódio dos 640 kilobytes de RAM quando se achava que isso era o máximo de RAM que uma pessoa normal jamais ia precisar, e nos anos 90 se provou que isso era muito pouco. 32-bits que dá um máximo de 4 gigabytes também é pouco e hoje estamos em 64-bits.&lt;/p&gt;

&lt;p&gt;IPv4 sendo 32-bits significa que o máximo teórico de endereços seria pouco mais de 4 bilhões. Só de smartphones já temos mais de 3 bilhões. Some a isso todo PC, todo servidor em todo data center. Some a isso um tanto de IPs que já foram desperdiçados também. A gente já passou de 4 bilhões de endereços muitos anos atrás. Mas porque tudo continua funcionando? E pra entender isso eu acho importante todo mundo saber o que é um NAT ou Network Address Translator.&lt;/p&gt;

&lt;p&gt;O conceito é muito simples. Todo provedor como Vivo ou Claro tem muito menos blocos de endereços IP reais do que clientes. Por isso é impossível dar um endereço real pra cada cliente e de fato ninguém faz isso. Em vez disso eles implementam o que chamamos de um CGNAT ou Carrier Grade NAT, ou seja, um NAT tamanho industrial. O grande lance é que podemos ter o intervalo inteiro de IPv4 dentro de cada sub-rede isolada e traduzir o endereço quando ela sai de uma rede e vai pra outra.&lt;/p&gt;

&lt;p&gt;Em redes embaixo de NAT existem 3 blocos de endereços reservados só pra isso. Os com prefixo 10 barra 8, que é um intervalo de 24 bits de mais de 16 milhões de endereços possíveis. Ou com prefixo 172.16 que é barra 12, uma rede de 20 bits ou mais de 1 milhão de endereços e finalmente com prefixo 192.168 que é barra 16 que é uma rede de 16-bits ou mais de 65 mil hosts. Por isso que normalmente seus dispositivos na sua rede local de casa ou do escritório costumam ser 192.168 alguma coisa ou 172 ponto alguma coisa ou 10 ponto alguma coisa. São intervalos reservados pra redes locais embaixo de NAT.&lt;/p&gt;

&lt;p&gt;Meu PC tem endereço 192.168.1.200. Eu quero assistir alguma coisa do YouTube. Faz de conta que o endereço de destino é 142.250.219.14 porta 443 que é a porta de HTTPS. Outro dia explico portas. Então meu PC monta pacotes onde a origem é meu endereço 192.168 com destino 142.250 bla bla porta 443 pedindo a página de video e manda isso na rede local aqui de casa.&lt;/p&gt;

&lt;p&gt;Antes de chegar em NAT vou aproveitar pra adicionar mais detalhes desse processo. Dentro de uma rede local os pacotes não são roteados pelo endereço IP. No protocolo de rede, estamos na camada de rede, mas na rede local o que importa é a camada de data link, da ethernet. E ethernet não entende endereços IP.&lt;/p&gt;

&lt;p&gt;Essa tangente vale a pena fazer. Segura o exemplo do YouTube. Digamos que eu queira falar com um dispositivo aqui dentro na minha rede local. No caso o meu NAS. Eu que configurei então sei que tá com endereço local 192.168.1.161. Posso abrir aqui o command prompt do Windows e dar um ping pra ele. Todo mundo já fez ping pra medir latência e tempo de resposta, ele manda um pacote ICMP e mede o tempo. De novo, como meu PC sabe mandar pacote exatamente pra esse dispositivo e não confunde com, digamos, minha smart TV que tá na mesma rede?&lt;/p&gt;

&lt;p&gt;O jeito mais simples seria toda vez eu ficar mandando pacotes pra todo mundo na rede perguntando &quot;ow, quem aí é IP final 161&quot;, daí o NAS responde &quot;sou eu! manda pra cá&quot;. E ficar fazendo isso toooooda vez, pra todo pacote. Mas conseguem enxergar como isso seria um desperdício? Eu ia ficar toda hora incomodando os vizinhos gritando no corredor. Minha TV ia receber pacotes que não precisa. Meu smartphone conectado via Wifi ia ficar recebendo. Toda vez que preciso enviar pacotes, ia ter que ficar gritando no corredor. Seria um saco.&lt;/p&gt;

&lt;p&gt;Por isso existe um outro tipo de pacote chamado ARP ou Address Resolution Protocol. No command prompt mesmo eu posso digitar &lt;code&gt;arp -a&lt;/code&gt; e vai aparecer uma tabela, que é um cache aqui no meu PC. Quando eu fiz o ping pro meu NAS, na realidade ele pediu pro ARP gritar no corredor &quot;ow, quem aí é o NAS!?&quot;. Aí o NAS respondeu &quot;sou eu, caralho!&quot; e incomodou todo mundo na rede. Mas aí o ARP que não é burro nem nada, registrou a posição do apartamento no andar. É pra isso que serve o tal MAC address. Toda placa de rede, seja ethernet, seja wifi, tem um endereço único de 48 bits que vem de fábrica. 2 elevado a 48 são mais de 281 trilhões de endereços. É endereço suficiente pra todas placas de rede já fabricadas e em operação.&lt;/p&gt;

&lt;p&gt;Olha a tabela que o comando &lt;code&gt;arp -a&lt;/code&gt; me mostra agora. Tem um registro dizendo que 192.168.1.161 é esse MAC address que eu escondi uma parte. O endereço IP é relevante pra roteador de internet, mas na camada de baixo da rede física, IP não quer dizer nada. Ethernet encontra coisas baseado em MAC address. Mas com isso meu PC já sabe que se quiser falar com o NAS de novo, não precisa mais gritar no corredor. É só mandar direto pra placa de rede que responde nesse MAC address.&lt;/p&gt;

&lt;p&gt;Mas isso funciona pra endereços da rede local. Se for endereço IP 192.168 o ARP vai ficar gritando aqui no corredor. Mas e se for o endereço do YouTube, que é no prédio ali na esquina? Agora não adianta ficar gritando no corredor. E é pra isso que serve o tal do default gateway que você também costuma ver na configuração de rede do seu roteador. É como se fosse o porteiro do seu prédio. Todo endereço que não faz parte deste prédio você manda pro default gateway, que por convenção é o endereço IP mais baixo, como 192.168.1.1. E agora pede pra ele ir no outro prédio, pedir pro porteiro de lá, o roteador de lá, gritar no corredor de lá. Rede é mais ou menos isso mesmo, todo mundo gritando pra lá e pra cá e uma correria de entrega de pacotes.&lt;/p&gt;

&lt;p&gt;Mas tudo isso pressupõe que todo apartamento tem um endereço IP único dentro do prédio. Mas digamos que se mudou um vizinho pentelho, e só pra foder, ele resolveu colocar o mesmo número de apartamento do vizinho. O ARP vai gritar no corredor &quot;ow. quem aí é endereço final 161&quot; e dois apartamentos gritam &quot;sou eu!!&quot; e agora? Agora eu não posso entregar o pacote pra nenhum dos dois, porque não sei dizer quem tá dizendo a verdade. Então o pacote é dropado, jogado fora, e nenhum dos dois recebem. Um deles precisa colocar um endereço IP que nenhum outro esteja usando. Essa é a regra de conduta do prédio.&lt;/p&gt;

&lt;p&gt;Pensa, seria uma zona se toda vez que você se muda pra um prédio, você mesmo precisasse definir qual é o número do seu apartamento. O prédio é progressista, ele deixa você escolher o número que mais te agrada. Só que aí metade escolhe número 13, a outra metade escolhe 22. E o ARP ia ficar o dia inteiro gritando nos corredores e não ia poder entregar nada pra ninguém. O prédio seria uma zona. Ainda bem que na vida real, todo apartamento já tem números pré determinados e você não escolhe. Hoje em dia, numa rede local, é a mesma coisa.&lt;/p&gt;

&lt;p&gt;Pra isso serve essa outra coisa que você já deve ter visto no seu roteador chamado DHCP ou Dynamic Host Configuration Protocol. Toda vez que um novo dispositivo se conecta na rede, ele pergunta pro servidor de DHCP, que em cenário caseiro, fica no seu modem ou no seu roteador separado. Em empresas um servidor DHCP pode ser um servidor separado e independente. Mas o funcionamento é o mesmo. O DHCP é o síndico do prédio. O meu PC conecta na rede e checa &quot;ow, tem um síndico ativo por aí?&quot; e o DHCP responde &quot;opa. tem eu aqui! o que manda?&quot;. Aí o PC pergunta &quot;ow, que endereço tem sobrando aí pra mim?&quot; e o DHCP mantém uma tabela parecida com aquela do ARP, e fala &quot;opa, tem final 200, firmeza?&quot; e o PC responde &quot;valeu irmão, vou usar esse então&quot;.&lt;/p&gt;

&lt;p&gt;E quando o NAS conecta, ou a TV ou seu smartphone, todos perguntam pro DHCP qual endereço tá sobrando e ele manda um certinho, de tal forma que ninguém tem um IP colidindo com de outro dispositivo. E assim o ARP funciona e os pacotes são entregues pros lugares certos. No DHCP eu posso também configurar dizendo que um certo MAC address sempre receba o mesmo endereço de IP. É útil pra coisas como meu NAS porque eu monto um drive nele do meu PC e seria um saco se toda hora o DHCP desse um endereço diferente pra ele.&lt;/p&gt;

&lt;p&gt;Tudo parece encaixar bonitinho, mas tem alguns problemas que eram mais graves no começo da internet e foram sendo tratados ao longo do tempo. Por exemplo, antigamente a gente usava hubs em redes, pra compartilhar um cabo entre vários computadores. Lembra antigamente quando tinha extensões de telefone na sua casa? Aí seu namorado liga e toca nos dois telefones, a menina no quarto atende e a mãe na cozinha atende também. Daí a mãe se toca &quot;opa, é o namorado da minha filha, vou desligar pra não ouvir a conversa&quot;. E sendo uma mãe honesta, ela desliga. Mas isso pressupõe que todo participante nessa rede seja honesto.&lt;/p&gt;

&lt;p&gt;É o que placas de rede legítimas numa rede em hub fazem. Se vem pacotes que não é pro MAC address dele, ele dropa, ou rejeita esses pacotes e não fica escutando. Porém, a placa de rede tá aqui fisicamente na minha máquina, eu posso hackear ela. E isso se chama colocar essa placa em modo promíscuo. Quem já brincou com softwares como Wireshark sabe disso. A gente podia ficar vendo todos os pacotes trafegando na rede, mesmo os que não são pra mim. Por isso que hoje em dia se usa switches. Um switch estabelece uma conexão direta entre o dispositivo e o roteador e os outros canais do switch não vêem pacotes que não sejam direto pro MAC address dele. Por isso que hubs são baratinhos, porque eles enviam os pacotes pra todo mundo conectado, sem processar nada.&lt;/p&gt;

&lt;p&gt;Mas ainda tem outro problema. De novo, MAC address é um endereço que vem de fábrica na placa de rede. Mas eu posso modificar essa placa e fazer ele responder qualquer outro MAC address. Daí o switch é obrigado a mandar pacotes pra mim que não eram pra mim. Não só isso, mesmo meu endereço IP mudando por causa do DHCP, meu MAC address é sempre o mesmo, então o provedor consegue saber o tempo todo com quem estou me conectando e registrar isso. Por isso empresas como a Apple vem planejando gerar MAC address aleatórios em iPhones por exemplo, pra dificultar saber quem é quem.&lt;/p&gt;

&lt;p&gt;Na prática, se você compartilha a internet numa casa, dormitório ou sei lá, com outras pessoas, não use hubs, use switches. Não importa se você confia nas pessoas da sua casa, você não sabe se ela sem querer instalou um malware que colocou o PC dela em modo promíscuo e tá escutando os pacotes da sua rede. Em computação você não deve nunca confiar em ninguém, sob nenhuma hipótese, essa é a regra número um. Nada nunca é 100% confiável, então assuma sempre 100% suspeito.&lt;/p&gt;

&lt;p&gt;Enfim, com esses componentes básicos você já deve entender que seus dispositivos conseguem endereços IP usando um servidor de DHCP e por isso muitos de vocês nunca precisaram configurar endereços na mão. Também já sabem que tem esse ARP que descobre qual dispositivo na rede responde pra qual endereço e mantém um cache que faz o de-para de endereço IP pra MAC address. Sabe que todo mundo está conectado no roteador seja diretamente via cabo de rede, via wifi, e se for cabo e tiver múltiplos dispositivos, coloca um switch pra conectar todo mundo de forma mais segura.&lt;/p&gt;

&lt;p&gt;E sabe que internamente os pacotes são roteados pelo MAC address, mas se eu quiser acessar servidores fora da minha rede, peço pro default gateway, que é tipo o porteiro do prédio, pra ir falar com o porteiro do outro prédio. E esse é o básico do básico de redes que você precisa saber.&lt;/p&gt;

&lt;p&gt;Mas vamos voltar pro exemplo de acessar o YouTube, acho que dá tempo de explicar como de fato me comunico com o mundo externo, fora do meu prédio. Eu falei que meu PC tem o endereço IP interno de 192.168.1.200 e quero assistir um video no youtube. Eu magicamente falei que ele pode se conectar talvez no prédio vizinho cujo endereço é 142.250.219.14.&lt;/p&gt;

&lt;p&gt;Deixa eu explicar como essa mágica funciona e ela começa na configuração do seu servidor de DHCP no roteador, onde configurei esse tal de endereço DNS. Isso é um servidor externo e normalmente vem o endereço do servidor de DNS do seu provedor, como da Vivo. Eu recomendo como primeira coisa mudar isso ou pro servidor do Google que é 8.8.8.8 ou da CloudFlare que é 1.1.1.1.&lt;/p&gt;

&lt;p&gt;Explicando de forma mais simples, um servidor DNS tem como única função traduzir um endereço em forma de texto como www.youtube.com em um endereço IP. A rede não entende nomes humanos, só entende endereços. Quando seu PC pediu um endereço IP pro DHCP ele ganhou também essa configuração de DNS. Quando no seu PC você digitou www.youtube.com no navegador, o PC checa pra qual DNS deve perguntar e vai ser pro endereço que recebeu do seu DHCP. Todos os outros dispositivos da sua casa vão ter essa mesma configuração, seja seu smartphone, seja sua smart TV.&lt;/p&gt;

&lt;p&gt;Se eu abrir um terminal de Linux temos o comando &lt;code&gt;dig&lt;/code&gt; e eu posso fazer &lt;code&gt;dig www.youtube.com&lt;/code&gt; e volta essa tabela aqui. O que interessa pra gente é esse &quot;Answer Section&quot;. E eu vejo que a primeira linha é o &lt;code&gt;www.youtube.com&lt;/code&gt; que eu pedi. E ele aponta pra um registro CNAME ou canonical name. Um registro CNAME diz pro DNS fazer uma nova pesquisa pra esse nome &lt;code&gt;youtube-ui.l.google.com&lt;/code&gt;. Daí volta o resto da tabela que diz que esse domínio aponta pra múltiplos registros A com vários endereços. Registro A é um endereço IPv4, registro AAAA seria endereço IPv6 e já falo disso. Mas e agora, o que eu faço com esse tanto de IPs?&lt;/p&gt;

&lt;p&gt;Toda vez que você perguntar pro DNS sobe o youtube, ele vai devolver conjuntos de endereços IP diferentes. Essa é uma estratégia do Google. Ele nunca te dá 100% de todos os registros, porque eles tem literalmente centenas de máquinas pro YouTube, em dezenas de data centers espalhados pelo mundo. Ele vai te devolver alguns endereços de máquinas do data center que é geograficamente mais próximo de você. Daí seu PC recebe esse conjunto e faz um round-robin load balancing. Traduzindo, ele faz balanceamento de carga. Ou seja, numa aba ele vai usar o primeiro endereço. Numa segunda aba, ele talvez vai usar o segundo endereço e assim por diante. Assim você vai sempre caindo em servidores diferentes, pra balancear sua carga.&lt;/p&gt;

&lt;p&gt;Cada servidor recebe no pacote algumas informações sobre você, como os cookies do seu navegador. Daí ele sabe que você já tinha feito login num outro servidor e te mantém logado no servidor seguinte. Pra você, é como se estivesse sempre no mesmo lugar, sem saber que por baixo seu PC tá te mandando toda hora pra um desses outros servidores da lista. Um DNS é isso, um serviço que contém uma tabelona apontando um nome de servidor pra um ou mais endereços IP.&lt;/p&gt;

&lt;p&gt;Lembram quando falei sobre modo promíscuo que alguém pode estar escutando seus pacotes na rede? Então, um provedor sempre está, entre aspas, em modo promíscuo, porque ele recebe os pacotes de todos os clientes assinantes pra poder mandar pra internet. Então ele é obrigado a receber todos os pacotes pra passar pra frente. Claro, se ele for honesto, assim como sua mãe na extensão, deveria só passar pra frente e não escutar a conversa. Mas, seguro morreu de velho e o certo é assumir que não só eles estão escutando, mas registrando e até revendendo esses dados. Você não tem como saber.&lt;/p&gt;

&lt;p&gt;Por isso se fala pra nunca acessar sites que não sejam HTTPS, ou seja, cuja conexão é encriptada. Daí mesmo se alguém no meio escutar seus pacotes não tem como saber o que tá sendo transmitido. Porém, DNS normalmente não é criptografado e antes do HTTPS ativar, o navegador precisa converter www.youtube.com pra um dos endereços IP do Google. E essa pergunta pro DNS é feita em forma não encriptada. Claro, o provedor ou qualquer um no meio do caminho pode só ver o endereço IP e deduzir que é endereço do Google. Mas eu já estou dando a informação que é www.youtube.com de mão beijada. E por isso existem novas tecnologias como DNS over HTTPS ou DNS over TLS que, como o nome diz é o equivalente HTTPS pra DNS.&lt;/p&gt;

&lt;p&gt;Isso é simples de ativar, basta colocar na configuração do seu router e seu celular o endereço de um servidor de DNS que suporte DNS over HTTPS, no caso o mais famoso é o 1.1.1.1 do CloudFlare. Mesmo assim, o CloudFlare é outra entidade, o Google também. Eu estaria confiando que eles não estão fazendo nada de errado, o que também é impossível de saber. Mas entre confiar na Vivo ou na Claro, hoje em dia nossa escolha é escolher o menor de dois maus. É por isso que eu pessoalmente uso um DNS caseiro, o Pi-Hole. Como ele cacheia a resolução de endereços, mesmo o CloudFlare pra onde ele conecta não sabe exatamente quando eu pedi esses endereços. Mas isso é tema pra outro dia. Só entenda que estamos meio expostos no setor de DNS ainda.&lt;/p&gt;

&lt;p&gt;Enfim, eu fiz uma longa tangente só pra voltar pro assunto de NAT. Agora meu PC tem um endereço IP local graças ao DHCP do meu roteador e eu tenho o endereço IP do youtube, graças a algum DNS como do CloudFlare. Pronto, agora eu posso fazer um pacote com origem sendo 192.168.1.200 e destino sendo 142.250.218.14. Mando o pacote pro default gateway do meu roteador TP-Link. Ele por sua vez roteia o pacote pro Modem na rede 192.168.15.1. E o Modem manda esse pacote pra rede da Vivo.&lt;/p&gt;

&lt;p&gt;A rede da Vivo vai enviar pra Internet e aqui vem a parte de roteamento na internet que eu não estou a fim de explicar. Mas pelo menos posso mostrar. No Linux costuma ter um comando chamado &lt;code&gt;traceroute&lt;/code&gt; e no Windows tem o comando &lt;code&gt;tracert&lt;/code&gt;. Eu chamo então &lt;code&gt;tracert 142.250.218.14&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;e ele vai me mostrar o caminho do pacote da minha máquina até a máquina do Google. Olha só, na primeira linha, como falei, vai pro roteador que é 192.168.1.1. Dele vai pro modem 192.168.15.1 que ele chamou de meuvivofibra.&lt;/p&gt;

&lt;p&gt;Lá dentro da rede da Vivo ele passa por outras sub-redes como esse da telesp. Pra quem não sabe a Vivo lá por 2002 ou 2003 foi criada como a junção de várias telecoms pelo Brasil. Em São Paulo, a empresa original era a Telesp. Enfim, dessa rede ele passa pra outra chamada vivozap. E aí vai passando por outros hops até no final chegar no destino. Existem vários serviços com tabelas que associam endereços IP com coordenadas geográficas de latitude e longitude. É uma estimativa. Dá pra chegar perto, mas eu posso mudar o endereço IP de uma máquina numa região pra de outra região. Ainda mais do tamanho do Google que eles podem remanejar grandes grupos de endereços de um data center nos Estados Unidos pro Brasil, por exemplo.&lt;/p&gt;

&lt;p&gt;Mas segundo o serviço IP2Location ele diz que esse IP 142.250.218.14&lt;/p&gt;

&lt;p&gt;que eu tentei acessar está em Mountain View, na Califórnia, sede do Google. Mas se eu usar outro serviço como o IPapi.co, ele parece ser um pouco mais atualizado e me diz que esse endereço na realidade está em São Paulo. E isso faz mais sentido. O DNS do Google me mandou pra um endereço mais próximo geograficamente de mim, pra ter menos hops ao longo da rota. Quanto mais perto, menor a latência, maior a velocidade. Por isso grandes serviços como Google tem servidores espalhados ao redor do mundo pra garantir que a maioria das pessoas tenham a melhor velocidade possível.&lt;/p&gt;

&lt;p&gt;Tá ótimo. Mas e agora? Como que o Google sabe pra quem devolver os pacotes de video? Ué, é só devolver pra origem que tá registrado no pacote, não? Só que lembra? Meu pacote tá com origem 192.168.1.200.&lt;/p&gt;

&lt;p&gt;Dentro da rede da Vivo não tem só você. Todo cliente da Vivo vai ter IPs como 192.168 alguma coisa. Vira aquela situação que eu falei do vizinho pentelho que copiou o mesmo número do seu apartamento.&lt;/p&gt;

&lt;p&gt;O Google não tem como devolver a resposta pra pessoa certa só com isso. E ele nem precisa, porque ele nunca recebeu esse endereço 192.168.1.200.&lt;/p&gt;

&lt;p&gt;E finalmente eu volto pro assunto do tal NAT e como não temos endereços IPv4 suficientes pra todos os dispositivos do mundo. Pra visualizar é a mesma coisa que um número de telefone comercial e ramais. Já pensou que dificuldade que ia ser se uma grande empresa tivesse um número de telefone diferente pra cada pessoa que trabalha lá? Uma empresa com 10 mil funcionários precisando ter 10 mil números de telefone diferentes? Eles não tem. Na realidade normalmente tem um número de telefone comercial que liga num PBX. Esse PBX roteia pra mesa da pessoa certa baseada num número de ramal.&lt;/p&gt;

&lt;p&gt;É parecido na Vivo, o NAT é o PBX. Ela tem alguns poucos IPv4 de verdade. Mas internamente dá tipo ramais pra cada cliente. É isso que um NAT faz, no caso um CGNAT, carrier grade, porque é um NAT gigante. Faz um teste, vai no Google e pesquise por &quot;what is my ip&quot;, qual é meu ip. Qualquer primeiro link serve. No meu caso vai aparecer esse aqui que estou escondendo os últimos dígitos. Esse é o endereço IP da Vivo de saída e é esse endereço que ele carimbou por cima do meu pacote antes de enviar pro Google, e é pra esse endereço que o Google vai devolver os pacotes de video.&lt;/p&gt;

&lt;p&gt;Daí o CGNAT da Vivo mantém uma tabelona que mapeia que quando voltar resposta do Google, é pra devolver pro modem certo que fez o pedido. E é por isso que você consegue conectar na internet e se comunicar mesmo não tendo um endereço IPv4 válido na internet. O CGNAT faz o trabalho de enviar e devolver os pacotes pras sub-redes corretas. Literalmente Network Address Translator ou tradutor de endereço de redes. O problema disso é que você consegue se conectar nos outros, mas outros de fora não conseguem achar você diretamente. Pra alguém de fora se conectar em você, primeiro você precisa se conectar nele pra abrir uma conexão.&lt;/p&gt;

&lt;p&gt;Vou deixar como lição de casa. No mundo de hoje que fazemos chamadas de Zoom, ligação via Whatsapp e outras formas de Voz sobre IP ou VoIP, o requerimento é que duas pontas só conseguem se conectar diretamente se ambos os lados conseguem se encontrar na internet. Mas normalmente você e a pessoa que quer falar estão escondidas numa sub-rede privada atrás de um NAT de provedor. Portanto nenhum dos dois lados está acessível. Como que se cria uma conexão ponto a ponto, sem precisar de intermediários? Pra isso existem formas como ICE que é STUN/TURN e protocolos como RTP. Dêem uns googles aí de como isso funciona. Não vou tentar explicar hoje.&lt;/p&gt;

&lt;p&gt;Voltando ao problema de endereços IP serem 32-bits e permitirem no máximo 4 bilhões de endereços. A essa altura já era pra ter acabado esses endereços. Se uma Vivo tiver, sei lá, 100 milhões de clientes, ia precisar de 100 milhões de endereços IPv4, pelo menos, sem contar que cada cliente tem hoje um PC, uma smart TV, alguns smartphones, então fácil ia precisar de meio bilhão de endereços. Mas graças ao CGNAT, faz de conta, com 1 único endereço IPv4 ela pode manter esse meio bilhão de dispositivos em redes privadas escondidas com endereços 192.168 ou 10 ponto da vida por trás. E é assim que a gente evitou acabar os IPv4. Mas isso é um paliativo.&lt;/p&gt;

&lt;p&gt;NAT ajudou a evitar o colapso da internet por falta de endereços mas ao mesmo tempo dificultou e complicou muita coisa, como o que falei de Voz sobre IP e todo tipo de conexão ponto a ponto, como você conseguir jogar online com seus amigos. Como na prática todo mundo tá escondido em redes privadas atrás dos provedores, estamos à mercê da boa vontade deles pra muita coisa. Esse problema não é novo, já se falava do problema de acabar endereços desde o fim dos anos 80. Desde o começo dos anos 90 já se começou pesquisas sobre como resolver esse problema. E já temos a solução na forma do IPv6 desde 1998.&lt;/p&gt;

&lt;p&gt;Você já deve ter visto IPv6. É um número de 128 bits. Eu venho dizendo que 64 bits é um número absurdo. Computadores modernos de 64 bits não tem barramentos de 64 bits ainda, o máximo acho que é 48 bits, que já é endereço de memória como se não houvesse amanhã mesmo pra um supercomputador. Pra ter uma idéia, a maioria das pessoas hoje já acha 128 gigabytes de RAM algo absurdo. Pra endereçar 128 giga um barramento de 37 bits seria suficiente.&lt;/p&gt;

&lt;p&gt;2 elevado a 128 é um número tão absurdo que um ser humano normal não consegue imaginar. Imagina a superfície inteira do planeta Terra, incluindo a superfície dos oceanos, inteiramente coberta por PCs. Eu poderia ter 7 x 10 elevado a 23 endereços IP por metro quadrado. Conseguem entender? 10 seguido de 23 zeros de quantidade de PCs, por metro quadrado. Estudantes de química vão notar que esse número é maior que o número de Avogadro. Literalmente seria possível dar um endereço IP pra cada molécula da superfície da Terra e sobrar. É astronomicamente grande.&lt;/p&gt;

&lt;p&gt;Na prática isso significa que todo dispositivo em existência hoje, todo PC, todo servidor, cada smartphone, cada dispositivo inteligente da sua casa como sua TV ou sua geladeira, cada uma poderia ter um endereço IP único com facilidade, por gerações a fio. E fazendo isso não precisaríamos mais estar atrás de um CGNAT. Todo mundo poderia estar diretamente na internet sem intermediários, o que facilitaria muito todo tipo de serviço ponto a ponto como voz sobre IP, e nunca teríamos que nos preocupar de novo com acabar endereços e ter que fazer gambiarras como NAT.&lt;/p&gt;

&lt;p&gt;O problema é que o sucesso da internet evitou que pudéssemos migrar pra melhor solução a tempo. IPv6 surgiu no calor da febre da bolha da Internet do fim dos anos 90. Pra maioria da população, nem se sabia se essa tal de internet ia mesmo dar certo. Muita gente achava que era só um troço passageiro que ia morrer logo. Coisa de hipster. Usar internet no fim dos anos 90 era opcional, e muita gente não tava interessada. Pra dar certo, investiu-se tudo no que já existia e se sabia que funcionava, que era IPv4. Por que a gente ia se preocupar em acabar endereços se ninguém nem sabia se ia entrar gente suficiente pra acabar os endereços? Migrar naquele momento pra IPv6 parecia uma otimização prematura.&lt;/p&gt;

&lt;p&gt;E pra piorar, tivemos o crash da bolha da internet em 2001. Aí sim, ninguém ia investir milhões de dólares pra migrar toda a infraestrutura de tudo pra IPv6. Pra tudo funcionar perfeito, o certo, seria o mundo inteiro migrar quase que tudo de uma vez pra IPv6. Senão como alguém com IPv4 vai conectar num servidor IPv6? Pra isso foram sendo inventadas formas de migração menos agressivas como NAT de IPv6 pra IPv4. Temos NAT64 por exemplo, que permite que PCs que já usem endereço IPv6 consigam conectar em servidores que ainda são IPv4. Mas a moral da história foi essa: o inimigo do IPv6 foi o sucesso do IPv4.&lt;/p&gt;

&lt;p&gt;E vocês sabem, se alguma é opcional, a maioria sempre vai esperar até a última hora. Por que eu vou gastar meu tempo pra ter todo esse trabalho se todo mundo ainda vai estar usando o esquema antigo? Vou ficar no antigo também. É a lei do menor esforço. Migração pra IPv6 só vai acontecer o dia que for mandatório, tipo os governos decidirem que ninguém mais acessa nenhum serviço do governo se não for IPv6. Mas governos normalmente são os últimos a adotar qualquer coisa nova. Eu não esperaria muito deles. Enquanto isso a gente vive num mundo em eterna transição entre IPv4 e IPv6. Alguns provedores já suportam nativamente, alguns você pode ligar e pedir pra habilitar roteamento por IPv6, mas não tem um padrão. Cada caso é um caso.&lt;/p&gt;

&lt;p&gt;Tem uma questão de privacidade também. Se todo dispositivo tiver um IPv6 estático, seria como um MAC address que nunca muda. Hoje em dia ninguém do lado de fora pode dizer com 100% de certeza quem é você porque só enxerga o IP público do provedor e não seu IP privado da rede atrás do NAT. Mas se for direto com IPv6 dá pra dizer com certeza que veio do seu dispositivo. Por isso sistemas operacionais como Windows usam números IPv6 temporários pra se comunicar e garantir algum nível de privacidade. Não recomendo desligar isso. Mas veja, se eu der o comando &lt;code&gt;ipconfig&lt;/code&gt; no meu Windows eu vejo que eu tenho um IPv6 estático na minha placa de rede, que obviamente estou obfuscando uma parte, e embaixo tenho o IPv6 temporário, que é o que um servidor iria ver se eu conectar direto nele sem NAT.&lt;/p&gt;

&lt;p&gt;De qualquer forma, se IPv4 até dava pra você decorar. IPv6 é mais difícil. Primeiro o número é longo, então a notação pra ficar menos difícil digitar foi separar oito blocos de 16 bits por dois pontos. Mas diferente de IPv4 onde escrevemos cada bloco no formato decimal mesmo, de 0 até 255, 16 bits é um número que vai até mais de 65 mil então ficaria longo. Por isso escrevemos em hexadecimal, onde cada dígito vai de 0 até F, assim o bloco precisa de no máximo 4 dígitos hexadecimais. São 8 blocos de 4 dígitos hexadecimais separados por dois pontos.&lt;/p&gt;

&lt;p&gt;Se escolher direito, pelo menos no começo, vai ter muito endereço com vários blocos de zero no meio. Pra facilitar, o protocolo permite omitir esses blocos. E finalmente, endereços IPv4 podem ser escritos como IPv6 colocando dois dois pontos antes do endereço escrito no mesmo formato de antes. Enfim, pra maioria tudo isso é automático, porque o modem do provedor vem com servidor de DHCP e ponto de acesso wifi, então ele automaticamente te dá endereço IP, configura seu DNS, e basta você se conectar no Wifi que já tá navegando. Mas programadores precisam saber que esses detalhes existem. Em breve muitos de vocês vão precisar lidar com esse problemas, como conectar pessoas num app de comunicação ponto a ponto tendo que furar NAT ou implementar IPv6.&lt;/p&gt;

&lt;p&gt;E com isso terminamos essa parte da mini-série sobre o básico de redes. Isso resume, super resumido, os capítulos de camada física, camada de data link, camada de rede e um pouco da camada de transporte do modelo OSI, com um pezinho na camada de aplicação que é a parte mais prática de verdade e que devo tentar falar mais em outros episódios. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e compartilhem o video com seus amigos. A gente se vê, até mais!&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5956</id>
    <published>2022-07-13T10:50:00-03:00</published>
    <updated>2022-07-13T10:14:13-03:00</updated>
    <link href="/2022/07/13/akitando-122-detecao-e-correcao-de-erros-introducao-a-redes-parte-2" rel="alternate" type="text/html">
    <title>[Akitando] #122 - Detecção e Correção de Erros | Introdução a Redes Parte 2</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/3W-8TQJwuWY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Apesar de ser Parte 2 da mini-série de redes, vou falar pouco sobre redes propriamente ditas e focar em um aspecto que tem a ver com mais coisas que só rede: detecção e correção de erros.&lt;/p&gt;

&lt;p&gt;Já se fez essa pergunta? Dados são transmitidos como ondas eletromagnéticas por cabos de cobre ou sem fio. Bits são gravados magneticamente em mídias como HDs ou Blu-Rays. Como que nenhum bit nunca é gravado ou transmitido errado? E se é, como poderíamos saber se 1 único bit é entendido errado?&lt;/p&gt;

&lt;p&gt;Mas não se preocupem, apesar de não ser diretamente &quot;redes&quot; é importante saber o episódio de hoje, mas no próximo episódio vou realmente mostrar mais de &quot;redes&quot;.&lt;/p&gt;

&lt;h2&gt;Conteúdo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;01:20 - Recapitulando RAID-0 e RAID-1&lt;/li&gt;
&lt;li&gt;05:37 - Bit Rot, Bit Flip e Raios Cósmicos&lt;/li&gt;
&lt;li&gt;10:37 - Hamming Code&lt;/li&gt;
&lt;li&gt;15:11 - Hamming e Cartões Perfurados&lt;/li&gt;
&lt;li&gt;15:53 - Eficiência de bits de paridade&lt;/li&gt;
&lt;li&gt;19:37 - Checagem de erros em rede&lt;/li&gt;
&lt;li&gt;20:51 - Usernet e Par2&lt;/li&gt;
&lt;li&gt;24:23 - M-Discs&lt;/li&gt;
&lt;li&gt;27:27 - Data Scrubbing&lt;/li&gt;
&lt;li&gt;28:40 - Conclusão&lt;/li&gt;
&lt;li&gt;29:43 - Bloopers&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;WHAT CONNECTS AN INCIDENT IN 2003 WITH SPACE, ASSEMBLY ELECTIONS, INTEL, AEROSPACE AND ELECTRIC CARS? (https://www.linkedin.com/pulse/what-connects-incident-2003-space-assembly-elections-intel-jogani/)&lt;/li&gt;
&lt;li&gt;How An Ionizing Particle From Outer Space Helped A Mario Speedrunner Save Time (https://www.thegamer.com/how-ionizing-particle-outer-space-helped-super-mario-64-speedrunner-save-time/)&lt;/li&gt;
&lt;li&gt;COSMIC RAY FLIPS BIT, ASSISTS MARIO 64 SPEEDRUNNER (https://hackaday.com/2021/02/17/cosmic-ray-flips-bit-assists-mario-64-speedrunner/)&lt;/li&gt;
&lt;li&gt;Bit Rot: What It Is and How To Stop It From Destroying Your Data (https://getprostorage.com/blog/bit-rot-stop-destroying-your-data/)&lt;/li&gt;
&lt;li&gt;How to send a self-correcting message (Hamming codes) (https://www.youtube.com/watch?v=X8jsijhllIA)&lt;/li&gt;
&lt;li&gt;Hamming codes part 2, the elegance of it all (https://www.youtube.com/watch?v=b3NxrZOu_CE)&lt;/li&gt;
&lt;li&gt;What is error correction? Hamming codes in hardware (https://www.youtube.com/watch?v=h0jloehRKas)&lt;/li&gt;
&lt;li&gt;Reed–Solomon error correction (https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;No episódio anterior eu quis trazer algumas noções de como dados trafegam por redes cabeadas ou wireless. Isso porque ultimamente andei brincando com coisas como meu novo NAS. Mas antes de poder falar dele e coisas que andei prometendo como Docker, achei que finalmente chegou a hora de falar um pouquinho sobre redes. Como já tinha falado antes, estou longe de ser especialista de redes, então quero explicar as coisas do ponto de vista mais de alguém que é programador e usa a rede mais do que monta redes.&lt;/p&gt;

&lt;p&gt;No episódio de hoje quero explorar o motivo de porque eu montei um NAS: evitar perder dados. A maioria das pessoas hoje tá acostumado a um mundo plug and play, sempre conectado, serviços de cloud e tudo mais e tem muito menos preocupação com seus dados. Na real, a maioria não tem dados tão importantes que se perder vai ser um grande problema mesmo. Por isso acho que não tem problema. Se perder, perdeu. Se doeu, deveria ter se precavido. Mas programadores eu acho que deveriam ter um pouco mais de cuidado, especialmente porque lidam com dados dos outros.&lt;/p&gt;

&lt;p&gt;Sobre isso tem dezenas de assuntos que poderíamos falar, como segurança de dados, mas hoje quero falar só sobre os blocos mais básicos de como erros podem acontecer e como a ciência da computação vem resolvendo esses problemas desde pelo menos os anos 60.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Veja meu NAS. Ele está construído numa configuração RAID-6. Mas vamos falar de novo do formato mais simples, só 2 HDs. Se ainda não assistiu minha playlist de armazenamento, vou resumir só essa parte de novo. Com dois HDs eu posso fazer 2 coisas: escolher segurança ou escolher performance. Se eu for um gamer que posso baixar tudo de novo da Steam caso perca os HDs, talvez faça sentido colocar em modo RAID-0. Nesse modo o sistema operacional vai enxergar os dois HDs num único volume lógico. Se os HDs tem 1 terabyte cada, em RAID-0 o volume lógico, o meu &quot;C:&quot; no Windows, vai ter 2 terabytes inteiros e vou ter quase o dobro de performance, porque posso escrever nos dois ao mesmo tempo.&lt;/p&gt;

&lt;p&gt;Lembram o que já expliquei antes sobre arquivos e transmissão de dados em geral? A maioria das pessoas olha um Windows Explorer e assume que a menor unidade são arquivos. Os sistemas operacionais escondem os detalhes e mesmo programadores só lidam no máximo com arquivos na maior parte do tempo. Mas isso é ineficiente. HDs não são dispositivos de arquivos, são dispositivos de blocos. Por isso o serviço da Amazon onde posso instalar um HD virtual na minha máquina virtual do EC2 se chama EBS ou Elastic Block Store. Ele nos dá um dispositivo de blocos. Eu explico isso nos videos sobre armazenamento, assistam lá depois.&lt;/p&gt;

&lt;p&gt;No episódio passado eu falei que comunicação de redes usa hoje em dia comutação de pacotes. Pacotes são essencialmente blocos de bits. Por isso tenha na cabeça que tanto arquivos no seu sistema como transfência desses arquivos pela rede, acontecem de forma fragmentada, um pacote de cada vez e não um arquivo inteiro de uma vez. Por isso tenho o dobro de performance num RAID-0. Se o sistema só enxergasse arquivos, quando fosse gravar, gravaria o arquivo inteiro só em um HD, daí a velocidade máxima seria só desse HD.&lt;/p&gt;

&lt;p&gt;Mas como na realidade a partição é uma tabela que tem um registro dizendo &quot;arquivo hello.mp3 tem 2000 blocos de 4 kilobytes cada, e os blocos de ID 1 até 1000 estão no HD 1 e os blocos de 1001 até 2000 estão no HD 2&quot;, daí ele pode mandar gravar os blocos nos dois HDs ao mesmo tempo, e por isso eu tenho mais velocidade tanto pra ler quanto pra gravar, porque o arquivo pode ser segmentado nos dois HDs. Mas também por isso esse sistema de RAID-0 tem um grande calcanhar de aquiles: e se um dos HDs sofrer um defeito e morrer?&lt;/p&gt;

&lt;p&gt;Se um HD morrer, você perdeu tudo dos dois HDs. Entenderam? Por que alguns blocos dos arquivos no HD que está vivo estavam no HD que morreu, então você ficou com um monte de arquivos quebrados que não servem pra nada. Por isso que você nunca deve usar RAID-0 se os dados que tiver sejam importantes. Se era só pra games que você pode recuperar tudo depois, beleza. Compra outro HD, remonta o RAID-0, formata tudo, reinstala o Windows do zero e baixa tudo de novo da Steam. A importância aqui era a performance e os dados tinha backup fora, então ok.&lt;/p&gt;

&lt;p&gt;Mas se vai misturar com trabalho, e coisas que se perder vão te dar prejuízo, e só puder comprar dois HDs, então coloque eles em RAID-1. Esse é o sistema de espelhamento. O seu sistema operacional vai enxergar os dois HDs de 1 terabyte como se fosse um único HD de 1 terabyte. Toda vez que mandar gravar um arquivo, o sistema vai mandar todos os blocos pros dois HDs e fazer uma cópia idêntica, bloco a bloco. Então a performance vai ser a de 1 único HD, um pouco menos até porque agora vai ter o trabalho de gravar no outro HD também. Quando ler, não vai ter quase diferença. Em termos de performance, vai perder um pouco, então, qual a vantagem?&lt;/p&gt;

&lt;p&gt;A vantagem é que se um dos HDs morrer, o sistema automaticamente elege o outro como HD primário e na prática você nem vai notar. Seu sistema não vai crashear e vai poder continuar usando tudo como antes. Zero perda de dados. Agora é só comprar outro HD, substituir o que falhou, e no próximo boot o sistema vai copiar tudo do HD que sobrou pro HD novo, criando outra cópia espelho. Aqui trocamos um pouco de performance e metade do espaço por segurança.&lt;/p&gt;

&lt;p&gt;Pra entender os outros níveis de RAID com mais HDs, de novo, veja minha playlist sobre armazenamento pra entender mais sobre isso de blocos e redundância de HDs, mas pra hoje eu precisava que vocês tivessem na cabeça esse modelo de tradeoff de performance, segurança e redundância. Isso porque poderia parecer que um RAID-1 é perfeito e vai deixar seu sistema protegido. Isso é verdade até certo ponto. É muito raro de se ver, mas um evento que acontece é bit rot ou degradação de bit ou simplesmente bit flip.&lt;/p&gt;

&lt;p&gt;Entenda, qualquer sistema de armazenamento, seja um HD mecânico, seja um SSD que usa chips NAND flash como também SD cards ou pendrives, seja memória RAM, tudo está sujeito a uma hora sofrer algum tipo de degradação. O melhor é quando a falha é bem aparente e ele pára de funcionar totalmente. O problema é quando ele continua funcionando mas os bits armazenados não são mais confiáveis. Tudo é gravado magneticamente. Informação é um linguição de bits. Bits são cargas eletromagnéticas, armazenadas ou transportadas num fio de cobre ou em ondas pelo ar. Qualquer tipo de interferência eletromagnética poderia flipar bits aleatoriamente.&lt;/p&gt;

&lt;p&gt;Por isso existe blindagem e todo um processo de fabricação pra evitar ao máximo qualquer tipo de interferência externa, mas nada é perfeito. Escrevendo sobre isso me lembrou de duas historinhas que seria legal vocês saberem. Em 18 de maio de 2003 aconteceu a primeira eleição com voto eletrônico da cidade de Schaerbeek em Bruxelas. Milhões votaram eletronicamente pela primeira vez. Tudo correu conforme esperado até o dia de contar e checar os votos.&lt;/p&gt;

&lt;p&gt;Aí a comissão da eleição notou duas coisas fora do comum. Primeiro porque uma candidata razoavelmente desconhecida ganhou milhares de votos a mais do que seria esperado. Mas a segunda coisa que foi mais absurda é que o total de votos foi maior do que o total de pessoas que votaram! Claramente isso é um problema e obviamente vocês imaginam o tanto de gente apontando o dedo um pro outro atrás de fraude. Teve diversas investigações pra entender o que aconteceu, se foi um bug ou manipulação, mas uma coisa chama a atenção. O excesso de votos contados dava exatamente 4096 votos.&lt;/p&gt;

&lt;p&gt;Pra todo mundo, 4096 é só um número, mas pra gente de computação, 4096 é um número muito específico. Se vocês não sentiram alguma coisa quando ouviram o número, precisam praticar mais, porque é exatamente 2 elevado a 12. Lembra quando eu falo que vocês precisam se acostumar a converter números decimais e pensar em binário? Um número é uma cadeia de bits, digamos de 32 bits. Se você flipar o bit na posição 13, vai dar esses 4096 a mais se lá estivesse zero. Não quer dizer que não houve fraude, mas é muita coincidência alguém que fosse fraudar pensar num número exatamente assim.&lt;/p&gt;

&lt;p&gt;O objetivo não é detalhar o caso, então depois Googlem pra saber como foram as investigações, de fato encontraram bugs de segurança e coisas assim no software. Todo software de votação, ainda mais na primeira vez, ainda mais se não foi visto e revisto, testado e retestado e auditado por diferentes grupos de programadores independentes, certamente tem bugs importantes. Não é talvez, é certeza absoluta. Mas mesmo assim, nenhum dos bugs encontrados conseguia explicar esse acidente em particular. E uma hipótese que não dá pra provar, mas tem chances de acontecer, é que esse bit foi flipado por raios cósmicos.&lt;/p&gt;

&lt;p&gt;Pois é, caso você não saiba, estamos o tempo todo sendo bombardeados por milhares de partículas subatômicas que vieram de explosões de Supernovas, que caem na Terra como raios cósmicos. Muitos experimentos delicados são feitos abaixo da terra, com muita blindagem, pra tentar minimizar o efeito desses raios, onde qualquer mínima variação da ordem de um elétron, pode mudar os resultados. Dependendo do estudo que achar pode encontrar que é possível ter 1 bit flip por gigabyte de memória por ano. Sabe aquela tela azul de Windows que só aconteceu uma vez e nunca mais e não teve motivo nenhum? Talvez tenha sido um bit flip de raios cósmicos.&lt;/p&gt;

&lt;p&gt;Tem muito mais histórias do que essa das eleições e só mais uma que eu achei interessante porque é um assunto que eu gosto, foi num speedrun de 2013 de Super Mario 64, do usuário DOTA_Teabag. Speedrun pra quem não sabe é competição pra ver quem consegue acabar um jogo no menor tempo possível. Durante esse speedrun, ele esbarrou num glitch considerado impossível sem manipular o jogo. Num determinado pedaço da fase, ele tava simplesmente pousando numa plataforma e fez um warp pra cima do teto sem nenhuma razão.&lt;/p&gt;

&lt;p&gt;Todo mundo da comunidade tentou replicar isso, usando emuladores, replicando exatamente os passos do Teabag frame a frame com scripts, mas ninguém conseguiu replicar esse efeito. Isso porque pra realizar esse warp requer o que se chama de um single event upset que é fora do controle do jogador. A única forma de replicar o warp é fazendo um bit flip no número que representa o peso do Mario. Se você flipar 1 bit mudando de C5 pra C4 hexadecimal isso dá o peso exato pra fazer o warp pra cima naquele exato momento.&lt;/p&gt;

&lt;p&gt;De novo, é impossível dizer que foi isso, mas a hipótese de novo é que raios cósmicos fliparam esse 1 bit naquela hora, causando o warp. Mas o ponto é que bit flips acontecem, e raios cósmicos é só mais um fator numa enorme lista de fatores que podem levar à corrupção de dados. Em particular quando usamos mídias magnéticas pra armazenar e transportar dados. E se tudo isso ainda não te convenceu, não precisa ir muito longe: quantos de vocês lembram da época de CDs piratas de games e como vocês gostavam de esfregar o disco na camiseta e mesmo o disco tendo vários riscos, ele ainda funcionava. Como pode essas coisas?&lt;/p&gt;

&lt;p&gt;Isso é tudo pra vocês entenderem que erros e corrupção de dados são inevitáveis e se vocês nunca notaram isso é um testamento de como a ciência da computação já conseguiu resolver a grande maioria desses problemas e pra todo mundo, tudo simplesmente só funciona. Vamos recapitular. Imagine um arquivo de imagens como esse gato. Obviamente é sempre um gato. Mas o que a gente vê é a representação visual de um linguição de bits. Eu expliquei isso no episódio sobre compressão. Só por conveniência pra mostrar aqui na tela posso representar essa linha contínua de bits quebrando em várias linhas, digamos, em linhas de 11 bits. E eu posso organizar mais ainda cada linha de 11 bits como um bloco.&lt;/p&gt;

&lt;p&gt;Em particular, vou organizar os 11 bits da primeira linha num bloco de 16 posições, numerado de zero a quinze. As posições 0, 1, 2, 4, e 8 vamos reservar e eu preencho as 11 posições seguintes com meus bits de dados. Agora vamos olhar metade dos bits e checar o que chamamos de paridade. Paridade é literalmente se temos um número par ou número ímpar de bits 1. Nesse caso temos 2 bits 1, então é par. E assim preenchemos o bit de paridade na posição 1 com zero.&lt;/p&gt;

&lt;p&gt;Agora olhamos o próximo grupo e checamos a paridade. Temos 3 bits 1, é ímpar então na posição 2 preenchemos esse outro bit de paridade com 1. Agora que vimos os grupos de colunas na vertical, vamos checar grupos na horizontal. Esse próximo grupo também tem número ímpar de bits 1. Portanto no bit de paridade da posição 4 preenchemos com 1. E finalmente checamos o último grupo, que também tem número ímpar de bits 1, portanto preenchemos o bit de paridade da posição 8 com 1.&lt;/p&gt;

&lt;p&gt;Finalmente, checamos a paridade do bloco inteiro. Temos 8 bits 1, é par, então o bit de paridade da posição 0 vai ser zero. Pode parecer complicado mas aguentem até o final do exemplo pra eu explicar o que estamos montando aqui. Nesse momento criamos um bloco de 16 bits pra carregar 11 bits de dados e 5 bits de checagem de paridade. Agora faz de conta que estamos transmitindo esse bloco pela internet, sujeito a todo tipo de interferência eletromagnética, erros de burst, degradação do material do fio e tudo mais, e um ou mais bits desse bloco vai ser flipado no meio do caminho. Mas na outra ponta a gente não sabe disso.&lt;/p&gt;

&lt;p&gt;Agora a outra ponta recebeu esse bloco. Duvido que vocês lembrem de cabeça como era o bloco original, então vamos checar a paridade. Checamos o primeiro grupo e vemos número par de bits 1, então o bit de paridade na posição 1 está correto com zero e sabemos que a 2a e 4a colunas parecem ok. O próximo grupo tem um número par de bits 1 mas no bit de paridade na posição 2 está 1, o que é um problema. Sabemos que tem um erro em algum lugar nessas últimas duas colunas. Mas como já sabemos que a quarta coluna tava ok, o problema tá em algum lugar na 3a coluna.&lt;/p&gt;

&lt;p&gt;Checando o próximo grupo, temos um número ímpar de bits 1 então o bit de paridade na posição 4 está correto sendo 1, portanto sabemos que a 2a e 4a linha estão ok. E no último grupo temos um número par de bits 1 mas o bit de paridade na posição 8 está 1 em vez de zero. Então temos algum problema nessa duas últimas linhas. Mas já sabemos que a 4a linha tava ok pela checagem anterior, portanto o erro tem que estar na 3a linha. E como já sabemos que tem algum erro na 3a coluna, cruzando as duas sabemos que deve ter um erro na posição 10. Esse bit foi flipado.&lt;/p&gt;

&lt;p&gt;Mais do que isso, olhando o bloco inteiro, vemos que a paridade é impar, então no bit de paridade do bloco deveria estar 1 em vez de zero. Isso nos dá confiança de saber que só houve 1 bit flip e não dois, portanto só o bit da posição 10 foi flipado. E se mudarmos de 1 pra zero, jogarmos fora os 5 bits de checagem, temos o bloco correto original de 11 bits que foi enviado.&lt;/p&gt;

&lt;p&gt;O que fizemos aqui foram duas coisas: nós tivemos a capacidade de checar a existência de um bit flip, e ainda tivemos a capacidade de recuperar o bit que foi danificado e recuperar o bloco inteiro. O que vocês acabaram de ver foi o algoritmo de correção de erros Hamming Code, nomeado depois de seu criador Richard Hamming. E essa é uma das formas que temos hoje pra correção de erros.&lt;/p&gt;

&lt;p&gt;Essas animações devem ser familiares, porque peguei emprestado do excelente canal 3Blue1Brown, que fez dois videos excelentes dois anos atrás explicando exatamente como funciona o Hamming Code. E ele fez isso em conjunto com outro canal que vocês já viram eu mostrar aqui, o do Ben Eater, que implementou Hamming Code em protoboards, em hardware, pra demonstrar como é possível colocar correção de erros em hardware. Eu altamente recomendo que assistam os videos deles pra saberem em detalhes como que isso funciona. Eu assisti esses videos dois anos atrás e tava esperando uma oportunidade pra encaixar eles nos meus videos. Quem seguiu minha recomendação de seguir os canais deles já deve ter visto.&lt;/p&gt;

&lt;p&gt;Richard Hamming é um pesquisador da época que se programava em cartões perfurados. Imagina você levar dias escrevendo um programa nesses cartões daí leva pra uma máquina mecânica que vai ler esses cartões através dos furos. Mas vira a mexe ele erra um furo e registra a informação errada. Se até hoje aquelas porcarias de máquinas de refrigerante não conseguem identificar seu dinheiro, imagina uma máquina mecânica velha que tem que ler centenas de cartões sem errar nenhum furo. Isso acontecia tanto que deixou o Hamming putaço. E não existe melhor forma de invenção do que não se acostumar com problemas e sim coçar a própria coceira. E assim ele fez, e inventou todo esse sistema de paridade do Hamming Code pra checar e corrigir erros.&lt;/p&gt;

&lt;p&gt;A gente usa Hamming Code até hoje em coisas como memória ECC ou Error Correction Code em RAM de workstations e datacenters. A memória que você tem no seu PC provavelmente não é ECC, mas o servidor de cloud que armazena seus dados certamente tem ECC. E é por isso que coisas como raios cósmicos não são aparentes no nosso dia a dia. Outra forma de correção de erros é o Reed-Solomon Code, que é usado em CDs e Blu-Rays e é por isso que mesmo você riscando o disco, esfregando na sua camiseta, ainda é possível ler os dados.&lt;/p&gt;

&lt;p&gt;Mas o mais astuto entre vocês assistindo poderia pensar. Peraí, você tá me dizendo que pra um bloco de 11 bits eu precisei reservar mais 5 bits pra redundância. Ou seja, 45% do espaço e banda de transferência foi desperdiçado pra isso? Então num Blu-Ray de 40 gigabytes eu precisaria ter quase 20 gigabytes a mais só pra redundância?&lt;/p&gt;

&lt;p&gt;Mas o mais astuto do mais astudo entre vocês talvez tenha notado que eu fui fazendo checagens naquele bloco de metade em metade. Toda vez que em ciência da computação você vê procuras feitas de metades de metades deveria vir à cabeça procura binária. Lembra os episódios onde falei de árvores e porque árvores são uma das coisas mais importantes que todo programador deveria ter em mente?&lt;/p&gt;

&lt;p&gt;Checamos 2 paridades de 2 grupos de colunas verticais, depois 2 paridades de 2 grupos de linhas horizontais. Num bloco pequeno de 16 bits precisamos fazer 4 perguntas de paridade pra localizar e corrigir o bit flipado. Mas e num bloco bem maior, de 256 bits? Vamos precisar fazer só 8 perguntas de colunas e linhas, e pra isso precisamos reservar só 8 bits de redundância. Ou seja, só 3% do tamanho dos dados e não 45% como era no bloco de 16 bits. Quanto maior for o bloco, menor vai sendo a quantidade de bits de redundância necessário. Esse é o poder da procura binária. E eis porque vocês precisam estudar algoritmos e estruturas de dados.&lt;/p&gt;

&lt;p&gt;Estamos usando blocos de 16-bits só porque é mais fácil de colocar aqui na tela ou pra você ver no seu celular. Mas um bloco de sistema de arquivos costuma ter pelo menos uns 4 kilobytes de dados. Um bloco de rede costuma ter pelo menos 1500 bits. Mas em rede não se usa nem Hamming e nem Reed Solomon. Só pra coisas como programa espacial da Nasa, onde o Voyager por exemplo usa Reed Solomon Code pras transmissões. Hoje em dia se usa variações de Reed Solomon pra fitas magnéticas, CDs, Blu-rays e toda mídia de armazenamento, incluindo HDs. Você nem sabe disso, mas toda vez que um bloco de dados é gravado no disco, o sistema calcula esses bits de redundância e grava no lugar apropriado no HD.&lt;/p&gt;

&lt;p&gt;Então tá resolvido tudo então? Só usar Hamming ou Reed Solomon em tudo que tá tudo certo? Mais ou menos. Por mais que eles sejam altamente eficientes e não gastem tantos bits assim, ainda assim são bits extras, pra cada bloco de bits. A indústria de HDs por exemplo vem pesquisando outras formas que sejam quase tão eficientes quanto Reed Solomon, mas perdem em 0.1% dos casos, como o LDPC code. Reed Solomon tem complexidade O(n&lt;sup&gt;2&lt;/sup&gt;) o que significa que pra discos gigantes de dezenas de terabytes leva muito tempo pra corrigir erros. Particularmente ruim em data centers que tem arrays com centenas de HDs. Um LDPC atinge 99.9% de recuperação comparado com Reed Solomon mas com complexidade O(n). É uma área ainda com pesquisas ativas.&lt;/p&gt;

&lt;p&gt;Recuperar erros não só gasta bits extras de armazenamento como exige processamento pra fazer essa correção. Imaginem agora switches de rede em escala de internet. Por isso eu acho que em rede não se fala tanto em correção de erros e sim em checagem de erro. É muito menos trabalho identificar que tem um erro, mas não recuperar esse erro. Se você tá fazendo aquele download longo e vê a velocidade cair porque a internet da sua região é bem instável, não é tentativa de correção de erros que tá acontecendo e sim repetição de pacotes.&lt;/p&gt;

&lt;p&gt;Em rede, funciona mais ou menos assim. Seu arquivão de download é quebrado em vários pacotes, enviados um atrás do outro. Seu PC vai recebendo esses pacotes e remontando o arquivo. Mas ele faz uma checagem antes. Essa checagem é parecida com a checagem de bits de paridade do Hamming Code, mas um pouquinho só mais sofisticado. No caso de internet falamos de checksum, em particular, Internet 16-bits. Esse checksum é uma soma dos bits do bloco dividido por palavras de 16-bits. Existem formas melhores como Fletcher's checksum ou CRC, que é Cyclic Redundancy Check. Mas eu acho que ainda se usa Internet 16-bits mesmo.&lt;/p&gt;

&lt;p&gt;De qualquer forma, entenda que existe essa checagem via checksum pra cada pacote. Se for detectado que houve um bit flip ou coisas como burst, que é comum, o pacote é rejeitado e seu PC manda aviso de volta pro servidor, &quot;olha, o pacote ID xpto veio cagado, manda de novo&quot;. Esse é NAK que falei antes. Daí o servidor manda o mesmo pacote outra vez. É um dos motivos de porque você vê a velocidade de download caindo e flutuando. No caso de rede é mais barato pedir o pacote de novo do que tentar corrigir ele. Correção é mais importante quando você não tem pra quem pedir uma cópia original, como arquivos no seu HD, que só você tem.&lt;/p&gt;

&lt;p&gt;No começo da internet a gente usava um serviço chamado Usenet, que usava o protocolo NNTP. Pensa tipo um sistema de fóruns. Assim como o protocolo SMTP de email ou HTTP de web, era outro protocolo baseado em texto puro de 8 bits. Ele foi feito pra transmissão de texto e não arquivos binários. Pense em usenet como Wordpress que povo tenta entuchar mais coisa do que ele foi planejado pra fazer. Usenet foi feito pra sincronizar artigos de texto entre diferentes servidores Usenet. Mas obviamente, a gente achou jeitos de entuchar arquivos binários pra fazer pirataria. Estamos falando de uma era anos antes de bittorrent.&lt;/p&gt;

&lt;p&gt;Eu mesmo fiz muito isso na faculdade. A gente baixava imagens, software e tudo mais da Usenet. O problema é que por diversas razões, os arquivos podiam acabar corrompidos. Erros de conversão. Erros nos discos rudimentares dos servidores da época e tudo mais. Mas o problema é que invariavelmente eles acabavam corrompidos. Primeiro, a gente precisa resolver o problema de trafegar arquivos binários num protocolo de textos.&lt;/p&gt;

&lt;p&gt;Os mais astutos já tem a resposta na ponta da língua. Base64. É uma forma de transformar binários em textos de 7-bits. Base64 não é encriptação de jeito nenhum. O problema é que se você tentar abrir um arquivo binário num notepad, não tem como dar copy e paste em outro notepad e salvar o arquivo, vai corromper tudo, porque um editor de textos não reconhece todos os símbolos binários. A solução é converter o binário num sub-conjunto de ASCII que um editor de textos entenda e depois converter de volta num binário. O custo disso é gastar um pouco mais de espaço.&lt;/p&gt;

&lt;p&gt;Cada dígito de Base64 representa exatamente 6 bits de dados. Então 3 bytes de 8 bits cada do arquivo, ou seja, 3 x 8 ou 24 bits, podem ser representados por 4 dígitos de 6 bits, que dá 24 bits também. Isso significa que a versão Base64 do arquivo vai ser 133% maior do que o original. Um arquivo binário de 100 megabytes vai virar 133 megabytes em Base64.&lt;/p&gt;

&lt;p&gt;Como arquivos podem ter tamanhos variados, como um filme longo por exemplo, ou o instalador do Photoshop ou coisas assim, pra Usenet não reclamar, a gente quebrava esse binários primeiro em múltiplos binários usando o bom e velho formato rar ou WinRar, porque assim como Zip ele compactava o arquivo, mas diferente de zip, ele permitia quebrar em múltiplos arquivos com final .rar2, .rar3 e assim por diante. E depois que baixava todos os rars, o WinRar conseguia remontar e descompactar o original.&lt;/p&gt;

&lt;p&gt;Então o processo era simples: faz vários arquivos rar, converte em texto Base64, na época usando a ferramenta uuencode do UNIX, que significa unix to unix encode. Postava cada pedaço em um artigo da Usenet. Do outro lado a gente baixava o base64 de cada pedaço, usava uudecode pra conseguir os binários de volta, e usava um equivalente WinRar pra descompatar os arquivos originais. Era assim que a gente fazia antes de Google Drive, Mega ou BitTorrent.&lt;/p&gt;

&lt;p&gt;Como disse antes, a Usenet não foi feita pra transportar arquivos, e por inúmeras razões, incluindo defeitos nesse monte de passos que a gente tinha que fazer, alguns bits acabavam corrompidos. Então a gente precisava de uma forma de garantir que dava pra recuperar de alguma forma após o download, caso notássemos que o arquivo veio corrompido. E aí surgiu a idéia do Parchive ou arquivos .par ou a versão melhor que foi o Par2. Agora você podia baixar os arquivos rar e também os arquivos par2, que era bem menores. E se o rar estivesse corrompido na hora de descompactar, podia experimentar recuperar usando os arquivos .par2.&lt;/p&gt;

&lt;p&gt;E o que é Par2? É basicamente aqueles bits de redundância como no exemplo do Hamming Code, só que são os bits pra Reed Solomon Code, como eu disse que funciona pra CDs. Os primórdios da Internet era muito legal porque a gente era obrigado a aprender essas coisas se quisesse baixar pirataria. Eu me lembrei disso porque recentemente decidi fazer backup dos videos originais aqui do canal em M-Discs ou Millennial Disc, a evolução dos discos Blu-Ray. É tecnologia nova, de 2010 pra cá. E me perguntaram se não era bom eu fazer parchives pra gravar junto, eu até fiz, mas acho que não precisava.&lt;/p&gt;

&lt;p&gt;De curiosidade pra quem não sabia, CDs, DVDs e Blu-rays comerciais comuns, vem com uma camada de proteção pra proteger contra riscos leves, aquela esfregada leve na camiseta. Porém, essa camadinha é feita de material orgânico que reage com oxigênio e umidade do ar. Significa que mesmo que você guarde com todo cuidado possível, eventualmente essa camada vai degradar e você pode acabar com um disco que não dá mais pra ser lido. Pode demorar 10 anos, demorar um pouco mais, mas é questão de tempo. Sua coleção de CDs não vai durar pra sempre a menos que esteja numa câmara a vácuo com zero umidade e temperatura controlada. E mesmo com os bits de redundância de Reed Solomon, você vai perder dados se essa camada danificar o suficiente.&lt;/p&gt;

&lt;p&gt;Por isso escolhi M-Discs. Eles são basicamente Blu-rays só que a camada de proteção é feita de material inorgânico, não reagente, não oxidante. Só pra ter uma imagem, pensa uma camada de proteção feita de pedra. Em testes de mais de 250 horas sob temperaturas de 90 graus Celsius, 85% de umidade, mesmo assim os discos permanecem intactos. Por isso se chamam millennial discs, são discos feitos pra durar mil anos, sob condições adversas. Perfeitos pros meus backups. Alguns vieram sugerir fitas magnéticas, mas nesse mesmo nível de durabilidade, custam ordens de grandeza mais caro por gigabyte.&lt;/p&gt;

&lt;p&gt;E eu disse que não precisava ter manualmente criado arquivos par2 com os bits de Reed Solomon Code porque um Blu-ray por padrão já tem esses bits. Hoje em dia tem poucos motivos pra se preocupar com bits de redundância porque a maioria dos sistemas de armazenamento modernos tem esse cuidado, incluindo seu SSD e seu HD. Só sua RAM que ainda não porque memória ECC é bem mais cara. Mas como no mundo caseiro a gente dá refresh nessa RAM porque bootamos toda hora, a chance de um bit flip causar problemas é muito baixa. De vez em quando, uma tela azul de Windows sem motivo nenhum, é a rara consequência que você vai encontrar.&lt;/p&gt;

&lt;p&gt;Hoje em dia não precisamos baixar arquivos par pra recuperar downloads corrompidos, mas muitos sites de downloads você já deve ter visto um código md5 ou sha1 junto pra checar a integridade do binário. Eu já expliquei sobre isso no episódio de criptografia, mas um sha1 é um algoritmo de hashing, um tipo de checksum. Se baixarmos a ISO de uma distro de Linux, e tiver sequer um bit flip, passando por sha1 o hash resultante vai ser completamente diferente do hash original que deveria ser. Por isso é um mecanismo importante pra sabermos se não houve bit rot ou mesmo manipulação no meio do caminho. Sha1 é muito superior a uma mera checagem de paridade, mas também custa mais caro pra processar.&lt;/p&gt;

&lt;p&gt;Menciono isso porque já que comecei falando de NAS, em sistemas de RAID como o Linux Raid ou sistemas de arquivos realmente modernos como Btrfs ou ZFS que expliquei no último episódio da série de armazenamento, eles oferecem proteção contra erros e exigem rodar uma tarefa em background com alguma periodicidade chamada Data Scrubbing. No ZFS tem um comando justamente chamado &lt;code&gt;scrub&lt;/code&gt; pra isso, detectar e corrigir data rot.&lt;/p&gt;

&lt;p&gt;No meu NAS também tem essa opção de agendar um data scrubbing a cada 3 meses ou o período que eu quiser. Não precisa rodar sempre. Mesmo o HD tendo suporte a Reed Solomon Code, nem isso garante que vai estar tudo ok. Checar e corrigir erros é uma tarefa que custa processamento e tempo, por isso não se roda toda hora. Ele vai usar coisas como checksum de cada arquivo, equivalente ao que falei de checar o hash depois de um download, pra ver se o arquivo está intacto. Isso exige ler e calcular em cima de todos os bits do arquivo.&lt;/p&gt;

&lt;p&gt;Por isso custa processamento. Se coisas como o checksum ou paridade não baterem, ele usa a cópia redundante do RAID pra reconstruir o pedaço com danificado. E vai fazendo isso arquivo a arquivo. Se tiver poucos erros é fácil de consertar, mas se deixar acumular por chegar num estado que é impossível recuperar, por isso se roda data scrubbing periodicamente.&lt;/p&gt;

&lt;p&gt;Como disse antes, pra maioria de vocês, tudo isso é invisível. Se um pacote na rede é corrompido, seu sistema pede pro servidor reenviar os pacotes. Se um arquivo é levemente corrompido na hora de gravar no seu HD, ele tem sistemas pra se auto corrigir. Com exceção de desastres graves, você raramente tem problemas de ver corrupção de dados. Seu streaming sempre funciona, as páginas que carrega sempre vem inteiros, e tudo isso porque pesquisadores como Hamming, Reed, Solomon, Shannon e muitos outros criaram as bases pra que tudo isso funcione. Mas de qualquer forma é bom saber porque as coisas funcionam num ambiente hostil onde corrupção de dados deveria ser o normal.&lt;/p&gt;

&lt;p&gt;O objetivo desse episódio era complementar o que falei nos episódios de armazenamento, no episódio sobre compressão, e o anterior onde comecei a falar sobre transmissão de dados e redes. Viemos muito longe desde um simples telefone de copinho e barbante e agora você sabe porque. Se ficaram com dúvidas mandem nos comentários abaixo. Se curtiram o video deixem um joinha, assinem o canal e compartilhem o video pra ajudar o canal. A gente se vê, até mais.&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5955</id>
    <published>2022-07-01T18:09:00-03:00</published>
    <updated>2022-07-02T13:46:15-03:00</updated>
    <link href="/2022/07/01/akitando-121-entendendo-transferencia-de-sinais-digitais-introducao-a-redes-parte-1" rel="alternate" type="text/html">
    <title>[Akitando] #121 - Introdução a Redes: Como Dados viram Ondas? | Parte 1</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/0TndL-Nh6Ok&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;h2&gt;DESCRIÇÃO&lt;/h2&gt;

&lt;p&gt;Finalmente vou começar a explicar o básico do básico de redes pra programadores iniciantes finalmente entender como de fato a Internet funciona, pra no futuro poder explicar melhor como aplicações web são arquitetados e como coisas como Docker conseguem funcionar.&lt;/p&gt;

&lt;p&gt;Esta é a Parte 1 de uma mini-série inicial de 3 episódios. Assinem o canal pra não perder o resto!&lt;/p&gt;

&lt;h2&gt;Conteúdo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;00:53 - Modem da Vivo&lt;/li&gt;
&lt;li&gt;01:52 - Bits vs Bytes&lt;/li&gt;
&lt;li&gt;02:59 - Primórdios da Internet discada&lt;/li&gt;
&lt;li&gt;05:10 - Fibra ótica e cabo de cobre&lt;/li&gt;
&lt;li&gt;08:04 - Wifi&lt;/li&gt;
&lt;li&gt;10:11 - 5G&lt;/li&gt;
&lt;li&gt;14:31 - Preciso de quanta velocidade?&lt;/li&gt;
&lt;li&gt;16:24 - Primórdios da comunicação: telefone de copinho&lt;/li&gt;
&lt;li&gt;19:19 - Telecom e computadores&lt;/li&gt;
&lt;li&gt;22:27 - teclados de novo??&lt;/li&gt;
&lt;li&gt;23:18 - Par trançado, multiplexing e modulação&lt;/li&gt;
&lt;li&gt;27:10 - Modem discado e ADSL&lt;/li&gt;
&lt;li&gt;31:23 - Datagramas&lt;/li&gt;
&lt;li&gt;33:08 - De comutação de circuito a pacote&lt;/li&gt;
&lt;li&gt;36:30 - Conclusão&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Cheat sheet: which 4G LTE bands do AT&amp;amp;T, Verizon, T-Mobile and Sprint use in the USA? (https://www.phonearena.com/news/Cheat-sheet-which-4G-LTE-bands-do-AT-T-Verizon-T-Mobile-and-Sprint-use-in-the-USA_id77933)&lt;/li&gt;
&lt;li&gt;What are the differences between 2G, 3G, 4G LTE, and 5G networks? (https://rantcell.com/comparison-of-2g-3g-4g-5g.html)&lt;/li&gt;
&lt;li&gt;How DSL Works (https://computer.howstuffworks.com/dsl.htm)&lt;/li&gt;
&lt;li&gt;Why are Wires Twisted in Twisted Pair Cables? (https://www.systoncable.com/why-are-wires-twisted-in-twisted-pair-cables/)&lt;/li&gt;
&lt;li&gt;Ethernet cable speed categories explained (https://www.allconnect.com/blog/what-ethernet-cord-do-you-need)&lt;/li&gt;
&lt;li&gt;Coaxial vs Ethernet cable - What's the Difference? (https://www.truecable.com/blogs/cable-academy/coaxial-vs-ethernet-cable-whats-the-difference)&lt;/li&gt;
&lt;li&gt;[Akitando #79] Teclados Mecânicos Exóticos | Minha coleção - Parte 2 (https://www.akitaonrails.com/2020/05/12/akitando-79-teclados-mecanicos-exoticos-minha-colecao-parte-2)&lt;/li&gt;
&lt;li&gt;V.90 Dial-up Modem Handshake - Transactional Analysis (https://www.youtube.com/watch?v=VaWpi9o_hHI)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Quem me acompanha no instagram viu minha saga atualizando meu armazenamento pra um NAS maior e mais rápido, inclusive os problemas que tive e como resolvi. Mas durante as semanas que postei stories, recebi dezenas de mensagens diretas com dúvidas elementares. Mesmo programadores tem buracos de conhecimentos importantes quando se fala de redes.&lt;/p&gt;

&lt;p&gt;Eu não sou nenhum especialista em redes, bem longe disso, e é um dos motivos de porque evitei falar do assunto por tanto tempo. Minha intenção não é falar sobre tudo que existe do assunto, precisaria de um canal inteiro pra isso. O que quero falar neste e nos próximos dois episódios são só alguns conceitos que podem ser úteis pra iniciantes. Sem entender nem o que vou explicar nesta mini-série, não tem como entender coisas como Docker, que eu quero falar no futuro.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Vamos começar com coisas que todo mundo tem em casa. Esse aqui é meu modem da Vivo, estou com um plano de fibra ótica de 300 megabits por segundo. Olha aqui, esse fininho é a fibra ótica. Vamos checar? Só digitar &quot;speedtest&quot; no Google e você pode usar esse aqui do próprio Google, ou esse aqui embaixo da Ookla que é bem famoso, ou o Fast do Netflix, que por alguma razão dá números um pouco mais inflados. Cada um vai dar medidas um pouco diferentes mas no geral é próximo da velocidade de verdade. Vamos ver os três rodando um pouco.&lt;/p&gt;

&lt;p&gt;Olhem só como é diferente, mas uma coisa é certa, o download tá ok dentro do plano de 300 megabits por segundo. Nos três fica até um pouco acima disso, mas o upload é sempre lá pela metade dessa velocidade. Provedores sempre fazem isso porque você nota mais lentidão em download do que upload. Mas tudo bem, um episódio aqui do canal fica na faixa de 10 a 15 gigabytes. Nessa velocidade eu consigo subir pro YouTube em aproximadamente 15 minutos. Não é ruim. Agora, você consegue saber como chega nessa estimativa?&lt;/p&gt;

&lt;p&gt;Muita gente vê os planos escritos 100 mega, 300 mega, e assume que é 100 megabytes ou 300 megabytes, mas não, são megabits. E isso faz muita diferença. Lembram? 1 byte são 8 bits. Portanto 300 megabits você divide por 8 pra ter em megabytes. 300 dividido por 8 é 37 megabytes por segundo, esse é o máximo que vai conseguir em downloads. Se for um plano de 100 megabits, divide por 8, não é muito mais que 12 megabytes por segundo de download. Essa é a velocidade que interessa.&lt;/p&gt;

&lt;p&gt;Quando se fala em velocidades de rede, por alguma razão se fala em bits em vez de bytes. Eu acho que tem o lance de pro marketing parecer melhor pra colocar números grandes em propagandas, mas acho que tem a ver com o histórico de que antigamente, estou falando de década de 60, transmitir 300 bits por segundo era considerado bastante. Eu mesmo cheguei a usar modem de linha discada de 9600 bits por segundo. Em 1991, a velocidade média era de 14 kilobits por segundo. Se dividir por 8 pra ter em bytes, é nada menos que 1.75 kilobytes por segundo. Pouco mais de mil e setecentas letras de 1 byte cada, por segundo.&lt;/p&gt;

&lt;p&gt;No fim dos anos 90, começo dos anos 2000, a maioria das pessoas estava em modems de 56 kilobits por segundo. Só quando começamos a ter ADSL com serviços como o Speedy, que chegamos perto do elusivo 1 megabit por segundo, que ainda assim, dividido por 8 são só 128 kilobytes por segundo. Lembro quando eu vi um demo da Ajato lá por 1993, e download de 1 megabit parecia a coisa mais rápida do mundo.&lt;/p&gt;

&lt;p&gt;Deixou o eu no colegial daquela época realmente imaginando que chegamos no futuro. Agora pensa, um arquivo de música mp3 de qualidade média é alguma coisa na faixa de 5 a 10 megabytes. Digamos que seja 10 megabytes. Quanto tempo leva pra baixar esse arquivo com um modem do fim dos anos 90 de 56 kilobits por segundo?&lt;/p&gt;

&lt;p&gt;A conta é 10 megabytes vezes 1024 pra ter em kilobytes. 1024 porque estamos falando em binário e isso é 2 elevado a 10. Tem toda uma controvérsia de usar múltiplos de 1000, mas isso é só marketing. O certo é sempre potências de 2. Daí divide isso por 56 kilobits, dividido por 8 pra ser bytes. E isso vai dar uns 1462 segundos, dividido por 60 e vai dar nada menos que 24 minutos.&lt;/p&gt;

&lt;p&gt;Quase meia hora pra baixar uma música que não tem muito mais que 5 minutos de duração. Essa é a era antes do streaming ser possível. Lembram do episódio que expliquei compressão? Pra streaming de áudio ser possível, eu precisaria no mínimo ser capaz de baixar essa música nos mesmos 5 minutos de duração. Por isso que não existia streaming nos anos 90 e começo dos anos 2000.&lt;/p&gt;

&lt;p&gt;A velocidade mínima seria esses 10 megabytes do mp3 divido por 5 minutos, ou seja, 2 megabytes por minuto. Isso vai dar 40 kilobytes por segundo, ou 320 kilobits por segundo. Nem é muita coisa, mas o modem da época de 56 kilobits era 6 vezes mais devagar. Os anos 2000 não foram fáceis não, por isso a gente deixava um monte de downloads enfileirados a noite inteira pra conseguir baixar um álbum de música. Em pouco mais de 20 anos fomos de 1 megabit pros 300 megabits que tenho em casa agora. 300 vezes mais rápido, e isso nem é o máximo que dá pra chegar. Nos Estados Unidos, Canadá, Japão e outros lugares existem alguns planos de 1 gigabit por segundo.&lt;/p&gt;

&lt;p&gt;Quando falamos em velocidades assim, pensamos em fibra ótica, que é o que tem nesse meu modem. Pense numa fibra ótica como um tubinho muito fino de fibra de vidro por onde passa luz. Literalmente se eu acender luz numa ponta, vai acender na outra ponta. Vocês já devem ter visto em algumas baladas.&lt;/p&gt;

&lt;p&gt;Se não me engano um bom cabo de fibra ótica, e tem tipos diferentes, tem capacidade de transmitir até 10 gigabits por segundo por uma faixa de 5 a 10 quilômetros. Isso é máximo teórico. Distância afeta banda, porque mesmo numa fibra ótica existe perda de sinal. Em todo cabo, quanto maior a distância, menor a velocidade porque maior vai ser a perda de sinal e interferência no meio do caminho.&lt;/p&gt;

&lt;p&gt;Abaixo de fibra ótica estão cabos de rede como este, cabos ethernet, com conector RJ-45. Eles são divididos em categorias. Na prática à medida que conseguimos ir melhorando os processos de fabricação e os processadores de switches e roteadores, fomos subindo de categoria. E a gente classifica esses cabos pela categoria ou cat. Esse na minha mão acho que é o mais comum, Cat 5e. Na dúvida hoje em dia, assuma que deve ser cat 5e ou até cat 6.&lt;/p&gt;

&lt;p&gt;Cat 1 até 3 acho não se usa mais, não lembro se teve cat 4. Era pra redes inferiores a 10Base-T, ou seja, menos que 10 megabits por segundo pra baixo. Lembra que nos anos 2000 a gente male male tava falando de 1 megabit pra internet? 10 megabits era 10 vezes isso, tava ótimo pra época. Um CD-ROM ocupava 650 megabytes, seria menos de 10 minutos pra transferir numa rede de 10 megabits. No cat 5 subimos pra rede de 100 megabits. Alguns lugares com cabeamento antigo ainda devem ser cat 5. Mas tudo isso já é considerado obsoleto.&lt;/p&gt;

&lt;p&gt;Com cat 5e fomos pra rede de 1000 megabits ou 1 gigabit por segundo. Eu acho que a maioria dos computadores, pelo menos dos últimos 10 anos pra cá, se tem placa de rede, é 1 gigabit. A maioria das placas mãe de boas marcas como Gigabyte ou MSI já tem com porta de ethernet 1 gigabit. A minha placa mãe mais moderna que é o X570s vem com porta de 2.5 gigabits. Cabos mais modernos são categoria 6 e 7, que tem especificações de construção mais exigentes e conseguem suportar sinais na faixa dos 500 Mhz e conseguem ir até os 10 gigabits por segundo de redes modernas. Mas até pra redes 1 gigabit vão ser melhores que cat 5e. Na dúvida, só compre cabos cat 6. Cat 7 e cat 8 a maioria não vai precisar.&lt;/p&gt;

&lt;p&gt;1 gigabit por segundo numa rede local tá ótimo. A internet que a maioria das pessoas tem em casa é 10 vezes mais lento que isso. Então dá e sobra pra conectar múltiplos dispositivos na rede, compartilhar a internet e ainda sobra banda pra transferir arquivos localmente e tudo mais. Pra fazer streaming de video, na prática, acima de 3 megabits já é possível. Ou seja, a maioria das pessoas não lida com nada muito maior que 10 megabits. Rede 1 gigabit é 100 vezes isso.&lt;/p&gt;

&lt;p&gt;Considere também as redes sem fio como do seu celular. Se estiver numa boa região metropolitana com 4G LTE pelo menos, as velocidades são mais ou menos. No meu celular eu consigo a faixa de 60 a 70 megabits por segundo pra download e menos de 5 megabits pra upload. Eu pessoalmente acho horrível, mas pra média da população isso é rápido o suficiente. De novo, acima de 3 megabits dá pra fazer streaming, receber mensagem de áudio e tudo mais, então tá ótimo.&lt;/p&gt;

&lt;p&gt;Falando nisso, saiu faz pouco tempo o novo padrão Wifi 6 que promete ser umas 3x mais rápido que Wifi 5, que é o que todo mundo tem. Mas Wifi 5 não é grande coisa também. Vamos fazer outro teste rápido. Esse é o Librespeed, uma versão open source do Speedtest do Google que mostrei lá no começo. É um teste de download e upload do meu PC pro meu NAS usando rede cabeada. Tá ótimo, faixa de 6 gigabits de download e de upload.&lt;/p&gt;

&lt;p&gt;Minha rede é de 10 gigabits, mas depois falo disso. Agora vamos ver meu smartphone conectado via wifi na mesma rede acessando o mesmo serviço. É essa coisa ridícula aqui de faixa de 200 megabits por segundo. Wifi é umas 3x mais lento que uma rede 1 gigabit que todo mundo tem. Mas é mais rápido que a internet do seu provedor então, de novo, tá ótimo.&lt;/p&gt;

&lt;p&gt;Wifi e outras formas de comunicação sem fio usam o ar como o meio de transporte das ondas em vez de cabos de cobre. Mesmo com paredes no meio do caminho, a onda consegue passar e continuar em frente, embora perdendo força. Você sabe disso porque se gritar de um quarto, sua mãe vai te xingar lá da cozinha. A onda da sua voz atravessa paredes e chega até lá. Tudo que oferece resistência vai diminuir o sinal e introduzir barulho. Por isso que você não consegue ir tão longe do seu roteador sem notar que a velocidade caiu bastante.&lt;/p&gt;

&lt;p&gt;Sua voz tem baixa frequência, por isso atravessa paredes fácil. Quanto maior a frequência, mais difícil é atravessar paredes, por isso que o Wifi mais comum é o que opera na faixa de 2.4Ghz e você consegue ir até uns 100 metros de distância e ainda navega, mesmo que mais lento. Mas na rede mais rápida de 5Ghz, uma ou duas paredes e você já nota que fica bem ruim de usar.&lt;/p&gt;

&lt;p&gt;Isso é outro assunto que daria uma tangente enorme, mas basta saber que essas bandas de frequência são convenções definidas por política. Várias organizações estabeleceram quais bandas podem ser usadas pra quais coisas. A faixa de 900 Mhz e 2.4 Ghz, por exemplo, são chamadas de ISM ou Industrial, Scientific, Medical, que são bandas não licenciadas que podem ser livremente usadas sem interferir com aplicações militares, por exemplo. Mas por isso também os canais dessas bandas costumam ser super congestionadas porque coisas como seu telefone sem fio, controle remoto de garagem e tudo mais operam nas mesmas bandas que seu Wifi e tudo isso causa interferências.&lt;/p&gt;

&lt;p&gt;Por causa disso, a partir de 2001 a organização FCC abriu a faixa de 5Ghz até 64Ghz pro público. É nela que trafega coisas como TV aberta em alta definição, que sua TV capta por antena. É na parte mais baixa dessa banda, nos 5Ghz que opera a maioria dos pontos de acesso de Wifi. Na prática, quanto maior a frequência, maior a velocidade, mas menos área o sinal cobre e mais dificuldade tem de passar obstáculos, como paredes. É o problema do 5G, que tem várias versões. Se você não sabia, 5G não é só uma coisa, são diferentes bandas de frequência.&lt;/p&gt;

&lt;p&gt;No começo dos anos 2000 a gente experimentou redes sem fio 2G, que ainda era comutação de circuito e velocidades de pico na faixa de 64 kilobytes por segundo. E cuidado que wireless a gente fala em velocidade de pico, que é em situação ideal, com sinal limpo, pouca ou nenhuma interferência. Na vida real a velocidade vai ser menor e variar bastante dependendo de onde você está. Mas em seguida tivemos 3G, na época do iPhone 3G lá por 2008. Agora já era packet switch indo até uns 2 megabits por segundo, e uma revisão que veio depois, o 3G HSPA+ que podia ir de uns 8 megabits até 21 megabits por segundos, usando a banda de de 15 a 20 megahertz.&lt;/p&gt;

&lt;p&gt;Eu tô simplificando bastante porque 3G, 4G, 5G são famílias de tecnologias. Cada uma usa diversas bandas diferentes de frequências. Por isso o 4G de um provedor pode ser diferente do 4G de outro provedor. Por exemplo, nos Estados Unidos a T-Mobile usava bandas de 800 Mhz até 1900 megahertz. Mas a Sprint usava banda de 850 até 2500 megahertz. Velocidade máxima teórica de 4G seria equivalente a uma rede de 1 gigabit, mas na prática não ia muito mais que 50 megabits por segundo.&lt;/p&gt;

&lt;p&gt;A gente sempre operou wireless em faixas abaixo de 2 gigahertz. A tal quinta geração ou 5G entra numa categoria de ondas chamado Extreme High Frequency, literalmente frequências extremamente altas, que vai de 30 até 300 gigahertz. Isso é o que se chama de millimiter wave. Pra comparação, o que a gente chama de microondas ou microwaves é de 3 a 30 gigahertz. Millimiter wave começa nos 30 gigahertz. Eu posso estar falando bobagem, mas acho que o que se chama de 5G comercial hoje em dia é bem abaixo disso, um pouco mais que a faixa de 4G LTE. É mais rápido, mas não tanto quanto se espera.&lt;/p&gt;

&lt;p&gt;Millimiter wave é muito mais rápido, subindo do máximo teórico de 4G de 1 gigabit e indo direto até 10 gigabits. De novo, teórico, porque apesar da alta velocidade, a altíssima frequência significa que tem alcance muito menor, muito mais direcional, e é muito fácil de interferir, por isso tem dificuldade de atravessar obstáculos como prédios entre o celular e as antenas. Logo, precisa de muito mais antenas pra cobertura e por isso demora tanto pra ter em todo lugar. Não espere ver essa banda perto de você tão cedo.&lt;/p&gt;

&lt;p&gt;Na prática é só entender que transmitimos dados como ondas usando o ar como meio. Existem várias faixas de frequências úteis que vão de 3 kilohertz até o topo de 300 gigahertz. Politicamente várias dessas faixas foram reservadas pra usos específicos, por exemplo, rádio AM dentro da banda que vai de 300 kilohertz até 3 megahertz, rádio FM dentro da banda que vai de 30 megahertz até 300 megahertz, 4G que tá banda seguinte até 3 gigahertz.&lt;/p&gt;

&lt;p&gt;E agora explorando millimiter wave que começa em 30 gigahertz e antes era usado mais pra comunicação via satélite, astronomia e coisas assim. Tudo isso porque aprendemos a modular e demodular sinal digital em analógico, e graças a pesquisadores como Claude Shannon sabemos como lidar com interferência e garantir que os pacotes que recebemos estão intactos.&lt;/p&gt;

&lt;p&gt;Estou dando todos esses números pra vocês terem um pouco de ordem de grandeza das coisas. No mundo de data centers, os engenheiros estão lidando com redes acima de 10 gigabits, Fibra ótica e tudo mais. No mundo caseiro, a média da população ainda tá na era da rede 100 megabits, tecnologia de uns 20 anos atrás. Mas como eu disse, a média da população não lida com petabytes de dados. A maioria dos HDs de vocês male male tem meio terabyte, um quarto de terabyte. O maior volume de transferência é streaming de video. Realmente não precisa de mais que 100 megabits. 1 gigabit já tem muita sobra.&lt;/p&gt;

&lt;p&gt;Eu acho 300 mbps de download e 150 mbps de upload da Vivo muito lento. Mas por outro lado, eu também não uso mais que isso no dia a dia, só de vez em quando queria ter internet gigabit. Mas o custo não compensa, por isso aguento com velocidades baixas assim. Um exemplo de eventos raros onde isso faz diferença. Estou fazendo backup do meu backup no serviço AWS Glacier. É tipo um S3 mas feito pra backups e coisas que não se acessa o tempo todo, então é mais barato pra armazenar. Estou transferindo uns 15 terabytes de dados pra lá. Quanto tempo acha que leva, nesse plano da Vivo de meros 150 megabits de upload?&lt;/p&gt;

&lt;p&gt;Vamos fazer as contas. 150 megabits, dividido por 8, é menos de 19 megabytes por segundo. 15 terabytes, multiplicado por 1024 dá 15.360 gigabytes, multiplicado de novo por 1024, dá mais de 15 milhões de megabytes. Agora dividindo pela velocidade de 19 megabytes por segundo, vai dar mais de 800 mil segundos.&lt;/p&gt;

&lt;p&gt;Dividido por 3600 dá mais de 229 horas, dividido por 24 horas, dá pouco mais de 9 dias. Isso sem contar horas do dia que a velocidade fica instável ou cai. Fácil isso vai dar mais de 10 dias pra eu conseguir transferir tudo pra lá. Por isso eu disse que é uma velocidade horrendamente baixa. Só que só retardados como eu faz uploads de 15 terabytes de coisas. Não é o normal.&lt;/p&gt;

&lt;p&gt;Já que estamos falando de modem, eu queria fazer uma tangente pra dar outra noção que muitos não devem ter. Como que os dados são transportados na fibra ou em cabos como coaxial ou ethernet? Pra explicar isso precisaria de mais física do que estou confortável pra explicar então vou usar um exemplo beeeem simplificado só pra ter uma imagem na cabeça. Lembra quando vocês eram criança e brincavam de telefone de copinho? Como que fazia? Prendia cada ponta de um fio bem longo no fundo de dois copos e ia longe até esticar o cabo sem arrebentar.&lt;/p&gt;

&lt;p&gt;Quando você gera voz, tá empurrando o ar com uma certa frequência e amplitude pra cada fonema. Já viram como a voz é representada num programa de edição como esse aqui? Tudo que eu vou falando é representado como ondas. Olha só, aaaaa .... bbbbb .... ccccc .... No mundo físico isso são minhas cordas vocais vibrando e empurrando o ar. Isso vai bater no fundo do copo e vibrar o fio conectado nele.&lt;/p&gt;

&lt;p&gt;Se o fio estiver tensionado o suficiente, ele vai transferir essas ondas até a outra ponta. E o copo no final vai servir pra amplificar um pouquinho o que recebeu e empurrar o ar até seus ouvidos. Na real &quot;empurrar&quot; não é o  termo adequado porque não está movendo nada, é como numa bacia de água em repouso quando vc toca numa ponta, gera uma onda, que vai até outra ponta, mas se tiver algo boiando no meio, ela continua no mesmo lugar. A água, o ar, ou o fio, servem de meio por onde as ondas vão trafegando. E a voz e outros sons são formados pelas diferenças em amplitude e frequência dessas ondas.&lt;/p&gt;

&lt;p&gt;É mais ou menos assim mesmo que os telefones antigos também funcionavam, com materiais um pouco menos rudimentares que copinhos de plástico e barbante. Mas nesse modelo, só dá pra ligar uma pessoa até outra pessoa. E se eu quisesse falar com pessoas diferentes? Já viram filmes antigos que tinha telefonistas? Você estava sempre conectado com uma central onde atendia uma telefonista. Daí dizia pra ela, &quot;me conecta com fulano de tal&quot; e aí ela pegava um fio e ligava num painel. Era ela ligando o fio do seu telefone, via esse painel, com o fio do outro telefone, fechando um circuito.&lt;/p&gt;

&lt;p&gt;Isso é no começo da telefonia comercial, um século atrás quase, claro, depois eliminou-se essa posição de pessoa ligando cabos e passou a se usar sistemas que conectavam números de telefone automaticamente, os switches. Nem eu antigamente nunca falei com uma telefonista, era só discar o número e esperar a outra ponta atender. Mas imagine que é o mesmo cenário, onde se faz switching, ou comutação, e fecha um circuito entre as duas pontas, como no telefone de copinhos.&lt;/p&gt;

&lt;p&gt;Isso se chama circuit switching ou comutação de circuito. E nesse caso a linha entre eu e a outra pessoa fica dedicada a essa sessão até uma das pontas desconectar. Por muitos anos foi assim que funcionou, toda casa tinha uma linha telefônica ligada numa central, e a gente pagava por minuto que a sessão ficava aberta entre dois telefones. O sistema automático que substituiu a telefonista fazia o switching e a bilhetagem. E foi assim que o mundo se conectava.&lt;/p&gt;

&lt;p&gt;Em paralelo, estamos falando dos anos 60 a 70, no mundo empresarial estavam experimentando com esse tal de computador, que como todo mundo já viu em fotos antigas, eram equipamentos gigantes que ocupavam um quarto inteiro. Super quentes, e pros padrões de hoje, extremamente fracos. Se falarmos de começo dos anos 70, um PDP-11 tinha CPU de 16-bits e poderosos 15 megahertz e na faixa de pouco mais de 2 kilobytes de RAM, acho que podia ir até 32 kilobytes.&lt;/p&gt;

&lt;p&gt;Se você não consegue entender essa ordem de grandeza pense no seu celular onde está assistindo esse video, ou seu notebook. Ele tem uma CPU de múltiplos cores onde cada um roda na faixa de 2 gigaherz ou mais. Giga é mil vezes mais que mega. Estamos na ordem de processamento de mais de mil vezes um processador dos anos 70. E você tem mais de 1, talvez uns 8, rodando em paralelo. No PDP-11 era um só de 15 megahertz.&lt;/p&gt;

&lt;p&gt;Memória então, nem se fala. Seu celular talvez tenha o quê, uns 4 gigabytes? Gigabyte é 1024 vezes megabyte que é 1024 vezes kilobyte. Você está numa ordem de grandeza de um milhão de vezes mais memória que se tinha nos anos 70. Não é dobro, não é 100 vezes, é um milhão de vezes.&lt;/p&gt;

&lt;p&gt;Até aproveitando, muita gente me pergunta qual computador é ideal pra aprender a programar e a resposta é óbvia: qualquer um que você conseguir comprar. Não existe &quot;computador melhor pra aprender&quot;. O computador mais furreca que você comprar hoje é mil vezes melhor do que eu tinha pra aprender nos anos 80. Se eu aprendi a programador num PC na faixa de 6 megahertz com 1 mega de RAM, não vejo porque você não possa aprender a programar num Raspberry Pi de 1 gigahert com 1 gigabyte de RAM. Quem realmente quer aprender, aprende com qualquer equipamento que tiver acesso e aprende a espremer o máximo dele. Se você não consegue extrair o máximo de um computador fraco, um computador mais forte é um desperdício pra você.&lt;/p&gt;

&lt;p&gt;Só que nos anos 70, 15 megahertz de processamento e 2 kilobytes de RAM era bastante coisa e super caro. Coisa da faixa de 150 mil dólares, em dólares dos anos 70. Ajustado pela inflação seria mais de 1 milhão de dólares de 2022. É um puta investimento. E custando caro assim, obviamente eram recursos que precisavam ser divididos por mais pesquisadores e engenheiros ao mesmo tempo. Desses esforços que surge coisas como o sistema operacional UNIX cuja função era não só facilitar escrever aplicativos, mas compartilhar a máquina com mais de um usuário. É o conceito de time sharing, ou compartilhamento de tempo.&lt;/p&gt;

&lt;p&gt;Pra conseguir ter várias pessoas usando o mesmo computador ao mesmo tempo, primeiro o sistema operacional precisava dividir o tempo de processamento e a memória pra vários aplicativos rodando em paralelo. Eu fiz dois vídeos explicando como se faz isso, mas em resumo ele deixa um aplicativo rodar por um tempo, daí pausa ele, e dá tempo pra outro aplicativo de outra pessoa rodar um pouco. Daí pausa esse, e resume o anterior que tava pausado. No total cada um vai levar mais tempo pra rodar, mas pelo menos os dois puderam iniciar os trabalhos ao mesmo tempo e não ter que ficar esperando na fila de braços cruzados esperando o outro acabar.&lt;/p&gt;

&lt;p&gt;Quando esse tipo de sistema começou a ficar possível, agora seria mais conveniente que cada pessoa tivesse seu próprio monitor e teclado, pra realmente trabalhar simultaneamente no mesmo computador. E assim começou a surgir os tais terminais burros. Na parte 2 dos meus videos de teclado eu explico um pouco como isso funcionava. E parte das tecnologias do mundo de telecomunicações passaram a entrar nos computadores.&lt;/p&gt;

&lt;p&gt;Como um exemplo pequeno, eu tenho aqui esse teclado IBM Model M dos anos 80 e ele não tem um cabo USB, óbvio, mas também não é o conector PS2 que se tinha nos anos 90. É um cabo telefônico de par trançado com conector RJ-11, que é o mesmo cabo e conector que se usava em telefones nos anos 80. Eu não cheguei a usar esses terminais, mas acho que eles literalmente &quot;discavam&quot; pro computador pra se conectar, mais ou menos como se fazia com modems discados.&lt;/p&gt;

&lt;p&gt;Na época já existia cabos coaxiais, que é parecido com da sua TV a cabo ou cabo de áudio, aquele mais grosso, que tem um canal físico no meio insulado por uma camada que é uma blindagem. A camada do meio transporta o sinal e é envolto por outro canal físico que é criado pelo plástico. O canal de fora é o ground, ou terra. São bons cabos mas caros.&lt;/p&gt;

&lt;p&gt;A alternativa surge com o que chamamos de twisted pair ou par trançado, que contém literalmente um par de fios de cobre, normalmente com cores diferentes, trançados um no outro. Lembram das aulas de física elétrica no colegial? Se você tem um cabo por onde passa uma corrente, o que acontece? Ele gera um campo eletromagnético. Se fez cursinho até deve lembrar do truque do dedão, onde colocamos o dedão na direção da corrente e os outros dedos representam o campo eletromagnético.&lt;/p&gt;

&lt;p&gt;O problema de cabos não trançados é que quando eles são expostos a um campo magnético, formam um loop e indução de voltagem. Os circuitos no final do cabo ficam mais suscetíveis a barulho induzido se não estiverem trançados. Um circuito é formado por dois cabinhos, daqueles coloridos, com voltagens opostas. Se um fio carrega voltagem positiva, a outra é igual mas negativa. A voltagem das duas é a mesma mas com polaridades opostas. Sem entrar em detalhes, ao ter dois cabos passando a mesma amplitude e frequencia mas de polaridades opostas trançados entre si, isso cancela a interferência eletromagnética, garantindo uma taxa de sinal melhor e menos barulho.&lt;/p&gt;

&lt;p&gt;Agora começa a parte importante. Você pode carregar múltiplos canais num mesmo meio físico, como os fios de cobre. Isso se chama multiplexing e é um tópico beeeem complicado. Na prática pense em várias conversas de voz que podem ser combinadas num único sinal, esse é o processo de multiplexing ou mux e tem vários jeitos de fazer isso. Lembrem voz, música é tudo onda, frequências e amplitudes que são transmitidos como sinais elétricos.&lt;/p&gt;

&lt;p&gt;Mux pode ser feito com divisão de espaço ou de frequência ou de tempo ou polarização e por aí vai. O importante é saber que é possível combinar múltiplos canas num único sinal e transferir tudo junto num mesmo meio físico, no caso os cabos. E do outro lado, na central, é possível fazer demultiplexing ou demux e separar os canais. É assim que rede telefônica funciona e esse processo não é novo, foi inventado no fim do século 19.&lt;/p&gt;

&lt;p&gt;Quando falamos do termo bandwidth ou banda, estamos falando do máximo teórico de faixas de frequência que o meio físico permite passar. Isso determina quantos canais podem passar pelo mesmo cabo. Muito abaixo ou muito acima da frequência que o meio aguenta e o sinal é degradado. Daí você cai em coisas como cálculo de capacidade de canal de Shannon-Hartley. Claude Shannon, que todo mundo de ciências da computação, conhece como o pai da Teoria da Informação, que justamente criou a matemática pra calcular esse tipo de coisa.&lt;/p&gt;

&lt;p&gt;Enfim, tem muita matemática e física envolvida em telecomunicações e eu só passei muito rápido por cima, mas eu queria chegar no ponto do telefone de copinho de novo. Com tudo isso, no final o que temos é um modelo mais sofisticado pro mesmo conceito de telefone de copinho. Onde uma vez estabelecido uma sessão entre dois participantes, você trava o canal de comunicação de ponta a ponta só pra vocês. Por isso o modelo de negócio era tarifar por minuto onde esse canal ficava preso. Isso é comutação de circuito.&lt;/p&gt;

&lt;p&gt;Agora, ondas são super versáteis. Não é só conversa de voz ou música. Podemos converter dados binários em ondas. E de novo, é um tanto de física e matemática, mas falando de forma super simplificada, podemos usar a amplitude pra representar quando é bit 1 ou bit 0 e a frequência pra determinar quantos bits podem trafegar por segundo. O processo de transformar bits digitais em ondas é o que se chama de modulação, e o processo inverso pra recuperar esses bits é a demodulação e por isso os equipamentos que fazem isso são chamados de &quot;modem&quot;.&lt;/p&gt;

&lt;p&gt;Com os primeiros modems comerciais, a gente conectava a linha telefônica de casa num modem e discava pra um provedor. No provedor ele ia ter dezenas de linhas telefônicas com modems também. Quando discava pra lá um modem atendia e travava tanto minha linha telefônica quanto a do provedor, fechando o circuito.&lt;/p&gt;

&lt;p&gt;Imagina como isso era caro. Antes da privatização das telecomunicações, ter uma linha telefônica era caríssimo, se bobear mais caro que um carro, era literalmente um investimento. Acho que foi logo depois que privatizaram e linhas telefônicas começaram a ficar acessíveis que possibilitou ter provedores com dezenas ou centenas de linhas pra atender as pessoas, mas mesmo assim, se quisesse ter 100 pessoas simultaneamente conectadas precisava ter 100 linhas, era bem difícil de escalar e era super comum ligar pro provedor e dar ocupado.&lt;/p&gt;

&lt;p&gt;Sem contar que era inconveniente pras pessoas. Toda vez que discava e conectava a linha ficava ocupada, então não dava pra receber chamadas nem ligar pra ninguém. Um dos motivos de porque a gente só entrava na internet de noite e de madrugada. Fora que tinha aquele esquema acho que era da noite de sábado e domingo só cobrava um pulso então não saia tão caro. Pois é, era difícil isso de acessar a internet nos anos 90. Você tinha que se planejar. Mas felizmente a tecnologia avançou rápido.&lt;/p&gt;

&lt;p&gt;Uma característica da voz falada humana é que a gente não usa todas as frequências que um cabo de telefone suporta. A gente pensa em cabos e fala em bits por segundo. Mas simplificando, lembra que bits são modulados em ondas por um modem. O cabo na verdade tem uma capacidade de frequências e por isso falamos em cabos digamos de 1.5 megahertz e male male arredondamos pra 1.5 megabits.
A conversão não é direta assim, mas pra hoje pensa que é relacionado. Enfim, digamos que o cabo telefônico tenha uma banda de 1.5 megahertz. A voz humana não precisa de uma faixa muito maior que de 0 até 4 kilohertz, então poderíamos reservar só essa faixa pra voz e vai sobrar muitos outros canais até 1.5 megahertz.&lt;/p&gt;

&lt;p&gt;Significa que quando travamos o circuito pra uma ligação, estamos desperdiçando toda essa banda numa conversa. Daí nasce a era dos modems ADSL como da Speedy que funcionava na mesma rede telefônica e permitia deixar o canal de voz livre pra ligações e conectar na internet com o mesmo cabo. No antigo sistema CAP, de 0 até 4 kilohertz era reservado pro canal de voz, daí de 25 a 160 kilohertz era o canal de upstream, pra upload de dados. O canal downstream pra download começava em 240 kilohertz e ia até a banda máxima do cabo que podia ser 1.5 kilohertz. Tem um espaço vazio de frequências entre cada canal pra diminuir interferência.&lt;/p&gt;

&lt;p&gt;O sistema CAP mudou rápido pra DMT que é discrete multitone e em resumo ela divide as frequências em vários pequenos canais de 4 kilohertz, um total de 247 canais separados. Lembra quando falei que antes um provedor precisava ter uma linha telefônica pra atender cada um usuário? Pensa agora uma única linha podendo atender um máximo de 247 canais na mesma linha. Se um canal estivesse com muito ruído, o switch mudava pra outro canal. Não quero entrar em detalhes nisso porque eu nunca trabalhei numa telecom nessa área então vai ter muito detalhe que não vou saber. Mas imagina que isso evoluiu pro modem da TV a cabo ou da fibra ótica da Vivo que eu uso.&lt;/p&gt;

&lt;p&gt;Mais importante é que mudamos do conceito de circuit switching pra packet switching. Agora vocês precisam entender pelo menos o conceito de frames e protocolos. O jeito antigo é o que podemos chamar de ponto a ponto, inclusive o protocolo de provedor discado era usando um protocolo chamado PPP que é Point to Point Protocol, tinha um outro mais antigo chamado SLIP. Quem usou internet no começo dos anos 90 vai lembrar disso.&lt;/p&gt;

&lt;p&gt;O modem faz o handshake, aquele barulhinho nostálgico que povo da minha idade conhece, o aperto de mão com o outro modem do outro lado e eles abrem uma conexão stateful, uma sessão exclusiva fechada. A partir daí está aberto um stream de dados, que pode ser half duplex ou full duplex. Uma via de mão única ou uma via com duas faixas, uma pra download e outra pra upload.&lt;/p&gt;

&lt;p&gt;Uma vez aberto um stream, se está fazendo download, inicia o recebimento e espera até acabar tudo e não faz mais nada enquanto isso. Se está enviando, começa a enviar, e espera até acabar. Que nem numa chamada de voz se não tem tráfego nenhum, mesmo assim a sessão fica aberta e sendo desperdiçada até um dos lados desligar. Mas isso é um desperdício. Em vez disso podemos pegar qualquer informação, seja um email, seja um arquivo, seja um comando ou o que for e fracionar essa informação em blocos de tamanhos fixo. Lembra o linguição de bits que eu expliquei em outros episódios, e lembra o conceito de que a menor unidade na partição do seu HD não são arquivos e sim blocos? É parecido.&lt;/p&gt;

&lt;p&gt;Muita gente que só usou interface gráfica em sistema operacional tem a noção que a menor unidade é sempre um arquivo. E que uma partição é basicamente como o &quot;C:&quot; que você vê no Explorer. Por isso fiz uma playlist inteira pra explicar sobre armazenamento e como dados são realmente organizados num disco. Todo HD ou pendrive é o que chamamos de dispositivos de bloco. E em telecomunicações é parecido, mas a menor unidade chamamos de pacote ou datagram. E datagram pra não esquecer é só pensar que no mundo analógico temos telegrams ou telegramas. Datagram é o telegrama de sinais digitais.&lt;/p&gt;

&lt;p&gt;Se tava achando complicado, a partir daqui que realmente vai complicar. Então antes de continuar eu realmente preciso que vocês tenham na cabeça essa idéia de fragmentar informação em pacotes. Pensa assim. Todo dado, seja um arquivo ou um email, é um linguição de bits. Expliquei bastante sobre isso no episódio sobre a diferença de arquivo texto e binário. Se temos aquele arquivo mp3 de 10 megabytes, são 80 megabits ou mais ou menos 80 milhões de zeros de uns um atrás do outro que formam o conteúdo desse arquivo.&lt;/p&gt;

&lt;p&gt;Na mesma analogia do telefone de copinho, se formos transferir esse arquivo tudo de uma só vez, os dois lados precisam esperar essa operação acabar pra conseguir enviar ou receber outros arquivos. E no modelo de circuito fechado, se vocês ficarem conectados o dia todo, mesmo se não estiver passando nenhum dado, a conexão vai ser desperdiçada. Por isso os sistemas de telecomunicação mudaram do modelo de circuit switch pra packet switch e de sessões stateful pra stateless. É isso que permite a escalabilidade que temos agora. Estou resumindo de forma bem grosseira, mas vamos entender o que muda.&lt;/p&gt;

&lt;p&gt;A partir de agora cada canal não fica mais travado entre dois pontos. Está todo mundo conectado sem travar uma sessão, por isso chamamos de stateless, sem estado. Agora eu quero fazer download daquele mp3, eu mando o comando pra algum servidor na outra ponta e ele vai começar a me mandar esse arquivo. Só que não vai mandar tudo de uma só vez. Em vez disso ele vai quebrar esse arquivo em pacotes. Digamos, pedaços de tamanho fixo de 1500 bytes. Isso vai dar um total de mais ou menos 7 mil pedaços. Mas não é só isso.&lt;/p&gt;

&lt;p&gt;Imagine um sistema de correios ou DHL e esses pedaços precisam ser entregues. Como faz? Você coloca cada pedaço de informação numa caixa e preenche os dados do destinatário. No caso da internet vamos dizer que é o endereço de IP da pessoa que pediu. Além disso o servidor coloca outros metadados no pacote, como o número do pedaço, porque quando eu for recebendo, vou precisar remontar o arquivo pedaço a pedaço mas precisa ser na ordem certa, senão o arquivo seria corrompido.&lt;/p&gt;

&lt;p&gt;Agora o servidor começa a enviar esses pacotes, um a um. Toda vez que eu receber, eu devolvo uma resposta, um acknowledgement ou ACK, dizendo &quot;beleza, manda o próximo&quot;. Se durante a transmissão teve alguma interferência no cabo ou algo assim, e o pacote for danificado, quando eu recebo consigo checar. Existe toda uma forma de reconhecimento de erros que vou explicar no próximo episódio, só assuma que eu sei como reconhecer se o pacote veio amassado e aí eu jogo fora e respondo &quot;não veio direito, pode mandar outro&quot;, esse é o NAK, o oposto de ACK, daí o servidor repete o mesmo pacote e envia, e vai fazendo isso pacote a pacote.&lt;/p&gt;

&lt;p&gt;Agora digamos que um segundo usuário aqui em casa na mesma rede resolva assistir episódio novo de série na Disney+. Se fosse que nem antigamente, não ia conseguir, ia precisar esperar eu terminar meu download. Mas o lance é que no intervalo entre um pacote e outro do meu mp3, ela pode mandar outros pacotes pra outros lugares no mesmo canal. O grande lance é que um cabo de comunicação era uma rodovia de várias faixas, ou canais como a gente chama, que só transitava um carro de cada vez. Quando fechava o circuito eu tava fechando e monopolizando essa rodovia inteira só pra mim. Mas agora a rodovia fica aberta e a gente vai usando as faixas que estão livres pra transitar mais carros.&lt;/p&gt;

&lt;p&gt;De novo, é uma explicação grosseira, mas o conceito é que em vez de monopolizar um circuito, agora a gente compartilha, fragmentando a carga em vários pedaços. Daí mesmo que todas as faixas estejam ocupadas, na mesma faixa pode ir vários pedaços de vários arquivos. Como pacotes sendo enviados pelo Mercado Livre, cada pacote com seu endereço e destinatário pra não misturar e cada um recebendo seu pacote no final direitinho. É como se eu quisesse comprar um PC na Amazon. Eu compro cada componente separado. Eles são entregues fora de ordem, mas com o manual correto, eu consigo remontar o PC direitinho no final. E cada pacote veio de um lugar diferente por rotas diferentes até. A internet funciona mais ou menos assim.&lt;/p&gt;

&lt;p&gt;Mas eu estou me adiantando. Até aqui eu quis descrever em linhas gerais pra que serve esse cabo de fibra ótica ou o cabo de TV a cabo que chega até meu modem e porque é possível ter várias pessoas no meu prédio usando o mesmo cabo, em diferentes canais. Esse é o papel do modem, modular e demodular o sinal digital em sinal elétrico, e trafegar em canais de frequência por cabos de cobre ou fibra até o switch de alguma central, no caso do provedor, e de lá ir pra internet. Agora que começa a parte interessante.&lt;/p&gt;

&lt;p&gt;Por acaso, esse modem da Vivo, assim como modems de outros provedores como a Claro, tem múltiplas funções. É ao mesmo tempo um modem, mas acumula funções de um roteador e ponto de acesso Wifi. E eles costumam ser ruins em todos eles. Pra poucos dispositivos e pouco uso, realmente não faz nenhuma diferença usar. Mas no meu caso eu prefiro desligar essas funções e conectar num roteador separado, no caso esse meu TP-Link aqui do lado.&lt;/p&gt;

&lt;p&gt;Mas acho que por hoje já é suficiente e vou pausar a história por aqui. No próximo episódio quero resumir pra que serve um roteador e como ele distribui a conexão pro meu PC, TV, e tudo mais. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e compartilhem o video com seus amigos. A gente se vê, até mais.&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5954</id>
    <published>2022-06-13T10:47:00-03:00</published>
    <updated>2022-06-14T09:48:45-03:00</updated>
    <link href="/2022/06/13/akitando-120-discutindo-gestao" rel="alternate" type="text/html">
    <title>[Akitando] #120 - Discutindo Gestão</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/2tpshOTtleM&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;h2&gt;DESCRIÇÃO&lt;/h2&gt;

&lt;p&gt;Gestão é um mundo enorme e complicado e por isso muito iniciante tem muita dificuldade de entender o que realmente precisa fazer. E no video de hoje resolvi falar um pouco sobre a arte de gerenciar e alguns pontos que eu pessoalmente considero importante depois de 30 anos gerenciando projetos de uma forma ou de outra. Não é o guia definitivo nem nada disso, só um pequeno empurrão que pode trazer alguns insights, seja você gerente ou não.&lt;/p&gt;

&lt;h2&gt;Conteúdo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00:00 - Intro&lt;/li&gt;
&lt;li&gt;00:01:47 - O problema com histórias&lt;/li&gt;
&lt;li&gt;00:02:39 - PMI&lt;/li&gt;
&lt;li&gt;00:04:52 - Onde aprender?&lt;/li&gt;
&lt;li&gt;00:06:30 - O que é gestão?&lt;/li&gt;
&lt;li&gt;00:08:47 - O mais difícil em gestão&lt;/li&gt;
&lt;li&gt;00:10:54 - A trindade de projetos&lt;/li&gt;
&lt;li&gt;00:14:02 - PO e stakeholders&lt;/li&gt;
&lt;li&gt;00:17:50 - Jovem demais&lt;/li&gt;
&lt;li&gt;00:20:36 - Cicatrizes de guerra&lt;/li&gt;
&lt;li&gt;00:23:26 - Filosofia Peaky Blinders&lt;/li&gt;
&lt;li&gt;00:26:13 - Hardware vs Software&lt;/li&gt;
&lt;li&gt;00:30:09 - Tipos diferentes de software&lt;/li&gt;
&lt;li&gt;00:32:39 - Tomando decisões&lt;/li&gt;
&lt;li&gt;00:35:01 - Motivação&lt;/li&gt;
&lt;li&gt;00:36:20 - Alguns livros&lt;/li&gt;
&lt;li&gt;00:37:10 - Métricas&lt;/li&gt;
&lt;li&gt;00:40:19 - Comunicação&lt;/li&gt;
&lt;li&gt;00:42:54 - Cortando na carne&lt;/li&gt;
&lt;li&gt;00:44:12 - Knowledge Workers&lt;/li&gt;
&lt;li&gt;00:48:10 - Evoluindo as pessoas&lt;/li&gt;
&lt;li&gt;00:52:28 - Conclusão&lt;/li&gt;
&lt;li&gt;00:54:02 - Bloopers&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A Arte da guerra: Os treze capítulos originais (https://www.amazon.com.br/Arte-guerra-treze-cap%C3%ADtulos-originais-ebook/dp/B00BBFSD3O/ref=sr_1_1?__mk_pt_BR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&amp;amp;crid=GEA04AJZ22PV&amp;amp;keywords=sun+tzu&amp;amp;qid=1655163041&amp;amp;s=digital-text&amp;amp;sprefix=sun+tzu%2Cdigital-text%2C186&amp;amp;sr=1-1)&lt;/li&gt;
&lt;li&gt;Managers Not MBAs: A Hard Look at the Soft Practice of Managing and Management Development (https://www.amazon.com.br/Managers-Not-MBAs-Management-Development-ebook/dp/B0064R22Q8/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;amp;qid=1655153736&amp;amp;sr=1-1)&lt;/li&gt;
&lt;li&gt;The Psychology of Computer Programming: Silver Anniversary eBook Edition (https://www.amazon.com.br/Psychology-Computer-Programming-Anniversary-English-ebook/dp/B004R9QACC/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;amp;qid=1655125738&amp;amp;sr=1-4)&lt;/li&gt;
&lt;li&gt;Peopleware: Productive Projects and Teams (https://www.amazon.com.br/Peopleware-Productive-Projects-Teams-English-ebook/dp/B00DY5A8X2/ref=sr_1_1?__mk_pt_BR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&amp;amp;crid=XU8MUZWUAH9M&amp;amp;keywords=peopleware&amp;amp;qid=1655159919&amp;amp;s=digital-text&amp;amp;sprefix=peoplewa%2Cdigital-text%2C215&amp;amp;sr=1-1)&lt;/li&gt;
&lt;li&gt;The Balanced Scorecard: Translating Strategy into Action (https://www.amazon.com.br/Balanced-Scorecard-Translating-Strategy-English-ebook/dp/B000SEIJPG/ref=sr_1_2?__mk_pt_BR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&amp;amp;crid=2BBLX7HO86ELX&amp;amp;keywords=balanced+scorecard+kaplan+norton&amp;amp;qid=1655161125&amp;amp;s=digital-text&amp;amp;sprefix=balanced+scorecard+kaplan+norto%2Cdigital-text%2C173&amp;amp;sr=1-2)&lt;/li&gt;
&lt;li&gt;The New New Product Development Game (https://hbr.org/1986/01/the-new-new-product-development-game)&lt;/li&gt;
&lt;li&gt;Soldier To soldier || Thomas Shelby to Mr. Churchill || Peaky Blinders | @JARAEditz (https://www.youtube.com/watch?v=H-DF8nzcjlM)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal Fabio Akita&lt;/p&gt;

&lt;p&gt;Se tem um papel que é difícil de definir e também de evitar em qualquer empresa de tecnologia é o do famigerado &quot;gerente&quot;, seja um product owner, seja um scrummaster, seja um chapter leader ou seja lá que nome andam inventando agora. Todo mundo reclama quando eu fico cagando regra, mas já aviso que esse episódio vai realmente ser uma grande cagação de regra. Esse episódio é pra você que é gerente, ou quer ser gerente ou se viu obrigado a atuar como gerente. Todo mundo, em algum momento, vai passar por isso. Depende de você decidir se vai abraçar a oportunidade ou fugir. Porque sempre é mais fácil ser o reclamão do grupo, mas ninguém quer ser o cara que vai realmente resolver o problema.&lt;/p&gt;

&lt;p&gt;O objetivo deste episódio não é ser um guia definitivo de tudo que existe sobre gestão mas só apresentar alguns pontos que eu pessoalmente considero importantes quando se discute esse assunto. Pontos que se você já não gerenciou diversos tipos diferentes de projetos, talvez não seja tão óbvio.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Tenho que deixar claro que tudo que eu disser hoje não são verdades absolutas. O problema de projetos e administração em geral é que a gente vê resultados ao longo do tempo. E durante esse tempo prestamos atenção em alguns poucos pontos que parecem importantes pra gente e ignoramos dezenas de pequenos pontos que pra mim pode não parecer importante porque são coisas que eu faço sem pensar, mas pra outras pessoas não são óbvias porque elas não estão acostumadas a fazer. Daí eu conto uma história que vai ser incompleta e por isso é difícil de replicar. O mesmo vale pra versão de outras pessoas. Por isso que ninguém vira gerente só assistindo videos ou lendo meia dúzia de histórias.&lt;/p&gt;

&lt;p&gt;Se duas pessoas contarem a mesma história sobre um mesmo projeto, as histórias vão ser diferentes e ambas vão ser incompletas. É impossível resumir tudo que realmente foi importante num projeto que durou 12 meses em 12 minutos. Por isso valorizamos a tal da experiência. Parte da experiência são items que são fáceis de explicar, outra parte é implícito e difícil de colocar em palavras. São as várias pequenas decisões invisíveis que podem fazer a diferença no final. Portanto não, não tem nenhum livro que você possa ler que vai te ensinar essas coisas, só prática real, várias vezes, em cenários diferentes, prestando atenção em tudo que está fazendo e quais as consequências. Algumas vezes foram ações específicas que levaram ao sucesso, outras vezes foi literalmente a sorte de estar no lugar certo na hora certa. Não dá pra prever.&lt;/p&gt;

&lt;p&gt;Falando em livros e teoria, uma cena que ficou famosa do meu video sobre o Guia de Aprendendo a Aprender foi quando rasguei meu certificado de PMP. Eu acho que expliquei isso lá mas vale repetir: eu quis dizer que certificados não significam nada. Colecionar papel pode ser um hobby, mas na prática é inútil. Porém não significa que eu não respeite o PMBOK.&lt;/p&gt;

&lt;p&gt;Pra quem não sabe, existe um corpo de conhecimento documentado no Project Management Body of Knowledge, que é o PMBOK. É o material ensinado pelo PMI pra quem quiser se certificar como gerente PMP. É como se fosse uma enciclopédia e dicionário dos principais temas e termos de gestão como custos, escopo, comunicação. É o básico de conhecimento pra quem tem intenções sérias de se tornar um bom gestor. Mas como todos os cursos, ele não ensina como se tornar um bom gestor.&lt;/p&gt;

&lt;p&gt;Pense no PMBOK como uma caixa de ferramentas. Só um bom carpinteiro sabe como usar quais ferramentas pra fazer bons móveis, mas só o manual de cada ferramenta nunca vai te tornar um bom carpinteiro. O PMBOK é a mesma coisa, tem chave de fenda, tem martelo, tem alicate, e como usar cada uma. Mas só isso é inútil pra você se considerar um gestor de verdade. Mas, vale a pena estudar pra saber o que todo mundo já sabe e não ficar tentando reinventar a roda.&lt;/p&gt;

&lt;p&gt;No mundo de agilidade existem até alguns frankensteins como Agile PMI. Pelo mesmo motivo, isso não quer dizer muita coisa. Só misturar vocabulário de PMI com de Agile. E só mais um produto de marketing que consultorias tentam empurrar goela abaixo dos desavisados. O famoso “se der certo, é porque a metodologia é boa, mas se der errado, é porque você não seguiu exatamente como a gente falou, então paga a gente de novo pra corrigir”.&lt;/p&gt;

&lt;p&gt;Eu falo disso no episódio de &quot;Esqueça Metodologias Ágeis&quot; onde explico como o termo Agile foi sequestrado e diluído por consultorias tentando vender a próxima grande moda da vez. E no episódio &quot;O Guia Definitivo de Organizações&quot; explico porque se você usa o tal modelo do Spotify, você não entendeu o modelo do Spotify. Em ambos os episódios explico o básico sobre a evolução e filosofia de organizações, então recomendo que assistam depois de terminar aqui porque não vou repetir o que já expliquei lá.&lt;/p&gt;

&lt;p&gt;As maneiras formais de aprender sobre gestão é fazer faculdades como de engenharia de produção ou administração ou um MBA. E antes que comecem a me perguntar qual deveriam fazer, já respondo: tanto faz. Varia de pessoa pra pessoa e eu não te conheço pra dizer o que é melhor pra você, só você sabe disso e já digo que se você sequer consegue decidir o que é melhor pra você, como acha que vai decidir o que é melhor pro projeto dos outros? Já começa aqui o indicativo que você talvez não tenha inclinação pra ser um gerente.&lt;/p&gt;

&lt;p&gt;Falando em MBAs, muita gente faz pelo prestígio do papel. Poder dizer que é um MBA. Tem quem coloque “virgula MBA” depois do nome no LinkedIn. Não posso criticar demais porque eu também colocava “virgula PMP” no meu, mas a vergonha alheia me fez tirar rápido. Também era 2003. Em 2022 já fica ridículo. Aproveite que ninguém viu e tira o seu porque é cringe. Nos anos 90 até 2000 alguns consideravam grande coisa. Hoje nem tanto. E se alguém se exibe por ser MBA, é um sinal vermelho. Que nem foto de Tinder com carrão atrás.&lt;/p&gt;

&lt;p&gt;Tem quem confunda ser MBA com ser um bom gestor, e não existe correlação. Fazer MBA em instituições renomadas tem o objetivo maior de criar networking, não muito mais que isso. Se você está nesse caminho recomendo ler livros como do professor Henry Mintzberg e seu clássico “Managers, not MBAs”. Se certificar como PMP ou se formar como MBA é só mais uma forma de colecionar papel, e não garantia de se tornar um bom gestor. Eu ainda acho que se você puder estudar o material deles, é super válido. Só não pode achar que já sabe tudo e sim que pelo menos sabe o básico do básico.&lt;/p&gt;

&lt;p&gt;Se eu fosse resumir o que é gestão em poucas palavras diria que é a capacidade de tomar decisões na hora que precisa, mesmo tendo informações incompletas. Alguém que não consegue tomar decisões e assumir responsabilidades nunca deveria gerenciar nada. Ser gerente não é obedecer ordens e preencher planilhas e relatórios sem fim. Qualquer idiota pode fazer isso. Isso não é um talvez, se seu dia a dia é só checar o backlog, checar as métricas, preencher nas suas planilhas e reportar, isso não é ser um gerente, é só ser um burocrata. E sim, existe a necessidade de burocratas em empresas grandes, mas não se iluda achando que está gerenciando porcaria nenhuma.&lt;/p&gt;

&lt;p&gt;Vamos voltar ao conceito mais básico. Qual é o objetivo de uma empresa? Você pode inventar que é inovação, que é fazer diferença social e todo bla bla, mas isso é marketing. Não há nada de errado nisso porque toda empresa precisa construir sua marca em torno de algum tipo de marketing. Mas vamos tirar o marketing do caminho e ir na realidade nua e crua. O objetivo de toda empresa é dar lucro de forma sustentável. Nada mais, nada menos. Empresas que não dão lucro e ficam vivas ao custo de investimento é o que chamamos de “empresas zumbi”.&lt;/p&gt;

&lt;p&gt;Mas não é “lucro a qualquer custo” como amadores pensam. Se o objetivo for dar lucro rápido sem nenhuma preocupação com futuro é simples: venda drogas. Dá muito mais dinheiro com muito menos trabalho do que tentar fazer uma tech startup. Só que o risco de você acabar preso ou morto é muito alto, por isso eu enfatizo o &quot;sustentável&quot;. Não é difícil ter lucro de vez em quando, é muito difícil sustentar esse lucro por anos a fio. Crescer 10%, 20%, todo ano, anos seguidos.&lt;/p&gt;

&lt;p&gt;Outra definição importante é que projetos tem começo, meio e fim. Muita gente esquece disso, mas se alguma coisa não tem objetivo bem definido e não tem uma data final, não é projeto, é operação. Não existe projeto que não tem fim. E mais importante: na prática nem todo projeto precisa ir até o fim. Saber quando cancelar um projeto e não insistir numa coisa que não vai dar certo é uma arte. Não existe nada de errado em descobrir que as premissas iniciais estavam erradas e reajustar. Mas tem muito problema em insistir num erro depois de descobrir que estava errado.&lt;/p&gt;

&lt;p&gt;Na verdade, em cursos e em guias sobre gestão se dá muita ênfase em como documentar as coisas, como fazer métricas, e muita coisa que, francamente, é bem inútil. Eu quero começar falando sobre o que quase nunca vejo ninguém discutindo fora de happy hour e conversa de bebedouro. Como dizer &quot;não&quot;. Ser um yes man ou yes woman é muito fácil. O cachorrinho abanando o rabo quando o superior manda fazer alguma coisa. Um yes man nunca vai ser um bom gestor porque um bom gestor é aquele que sabe dizer não quando precisa.&lt;/p&gt;

&lt;p&gt;Por outro lado, alguém que só diz não e nunca entrega nada, é igualmente inútil. Um bom gestor é como um bom general, ele sabe escolher quais batalhas vai investir. Porque batalhas custam seu batalhão, custam tempo. Só lute batalhas que sabe que vai ganhar, e espere ter condições mais pra frente de lutar as batalhas que hoje iria perder. E só assim, escolhendo certo uma batalha de cada vez que se vence uma guerra. Sabedoria militar é especialmente interessante em empresas. Não a toa todo mundo fala sobre Sun Tzu e sua arte da guerra, mas ninguém sabe o que ele quer dizer, porque seu livro não pode ser lido literalmente. São alegorias inúteis pra qualquer um que já não tenha lutado uma guerra. E de fato, eu mesmo lá no começo era estilo mais kamikaze, ficava pegando briga que não era minha, e perdendo tempo com batalhas inúteis. Demorei anos pra aprender isso.&lt;/p&gt;

&lt;p&gt;Um gestor ou um general só precisam existir porque lidamos com recursos escassos. Não existe dinheiro infinito, não existe tempo infinito, porque se existisse, ninguém precisaria gerenciar nada, era só gastar eternamente. É por isso que eu sempre acho que toda tech startup unicórnio e essa idéia maluca de só crescer fatia de mercado sem se preocupar em dar lucro tem como consequência gerar pessoas fracas. Porque muito pouca decisão precisa ser tomada, o problema sempre pode rolar pra frente, até o próximo round de investimentos. É viver a ilusão que se tem dinheiro infinito. E se tem dinheiro infinito, pra que escolher ser eficiente? Basta sair contratando mais gente, infinitamente. Até a bolha estourar. E podem acreditar, ela sempre estoura.&lt;/p&gt;

&lt;p&gt;Um projeto costuma ser definido pela trindade escopo, tempo e custo. É impossível ter o maior escopo no menor custo e tempo. Se aumentar o escopo ou o custo aumenta ou o tempo aumenta. Não existe outra forma. Como custo e tempo costumam ser os fatores limitantes o que sempre precisa variar, pra menos, é o escopo. E sempre o escopo é mau definido e os objetivos do projeto que esse escopo quer cumprir são mau entendidos. Só durante o projeto que esse entendimento vai melhorando e isso sempre tem que levar à revisão do projeto.&lt;/p&gt;

&lt;p&gt;Quase 10 anos atrás tive um cliente muito burro, o cara era tipo herdeiro. Quem construiu tudo foi a família dele, agora ele achava que só porque fez seu MBA ou sei lá, sabia alguma coisa. Herdeiros acham tudo fácil, lógico, sempre ganhou tudo de mão beijada. É um saco. Na época ele veio com o clássico “Akita, a gente podia encaixar aqui uma funcionalidade de chat, igual o Facebook Messenger, agora que já existe deve ser fácil né? Não dá pra fazer em um mês?” Minha resposta é sempre a mesma “igual? Claro que dá. Se você pagar a mesma coisa que o Facebook pagou, a gente faz”. Sério, lidar com gente que acha que tudo é fácil é foda. Tenho pena de quem trabalhava embaixo dele.&lt;/p&gt;

&lt;p&gt;O que um mau gestor sempre vai preferir fazer primeiro, é tentar limitar o problema do tempo fazendo a equipe trabalhar mais horas no dia. E isso é outra burrice sem tamanho. Isso pode funcionar numa emergência de poucos dias. Mas transforme isso numa rotina e automaticamente a qualidade geral de tudo vai cair, e isso se torna uma bola de neve muito rápido, porque as coisas mau feitas dessa semana vão atrapalhar a semana que vem, e agora todo mundo vai ficar correndo atrás do próprio rabo pelo resto do projeto. Como um projeto atrasa 1 ano? Simples, um dia de cada vez.&lt;/p&gt;

&lt;p&gt;A trindade de projetos é escopo, custo e tempo, mas existe a quarta variável invisível que todos esquecem. Tente entuchar o máximo de escopo, num tempo pequeno, cortando custos e naturalmente a variável que vai decair drasticamente é a qualidade. Essa é a quarta variável invisível. Falei no episódio anterior que qualidade foi um fator que passou a aparecer entre os anos 80 e 90, mas até hoje muita gente ainda ignora esse fator, porque é mais fácil falar em dólares e horas, mas é difícil de descrever qualidade. Os problemas gerados por um projeto mau feito tendem a ser empurrados pra operação depois.&lt;/p&gt;

&lt;p&gt;Comece estabelecendo uma data final. Estabeleça um orçamento. Saiba quais são os objetivos que precisam ser atingidos, em ordem de prioridade, e estabeleça qualidade como fator importante. Daí acomode um escopo que caiba nessas restrições. Normalmente não cabe. E agora vem a inteligência de toda a equipe envolvida em decidir as melhores formas de executar cada tarefa de forma eficiente. Sem fazer horas extras. Garantindo que o que foi feito até agora não seja uma bomba relógio que vai explodir lá na frente. Tudo que é feito às pressas e sem cuidado vai virar um problema muito rápido.&lt;/p&gt;

&lt;p&gt;Hoje em dia existe a figura de um PO ou Product Owner. Que é um nome mais bonito pra um gerente de projetos e gerente de operações. Dependendo de pra quem perguntar, vai ter definições diferentes. Mas eu gosto de pensar num PO como sendo um gerente com uma responsabilidade a mais, a de ser responsável pelo ROI do produto, o retorno do investimento. Numa empresa grande, abaixo de um PO vão existir várias equipes, cada uma com projetos próprios, que contribuem pro produto principal. As decisões do PO devem sempre levar em consideração o retorno de investimento de todos. Essa é uma das formas de limitar suas decisões e priorizar seus recursos nos lugares certos. Dê prioridade nos projetos que trazem o maior retorno de investimento. Parece fácil falar, mas muitos se esquecem disso.&lt;/p&gt;

&lt;p&gt;Toda empresa e todo projeto tem diversos stakeholders, que são as partes mais interessadas. O financeiro está interessado em manter as contas sob controle, de preferência faturando mais do que gasta. O marketing está interessado em atingir novos clientes, aumentar a fatia de mercado, aumentar as vendas. O marketing só se preocupa com clientes que já existem quando o churn começa a ficar preocupante, ou seja, quando o cliente já existente desiste de você. O suporte ao cliente é o oposto, ele está mais preocupado com o cliente que já existe do que com os que ainda não pagam nada.&lt;/p&gt;

&lt;p&gt;Os gerentes de projeto estão no meio desse fogo cruzado. Eles precisam saber colaborar entre si, mas seu foco é no projeto que está executando nesse instante. É nesse fogo cruzado que deveria atuar a camada C, o CEO, CFO, CMO, CTO, etc. A camada C existe pra tomar as decisões que afetam a empresa como um todo. As decisões que um gerente isolado não pode tomar. Facilitar a resolução de conflitos. Por isso eu digo que não existe nada pior que um C-level que não toma decisões e fica empurrando problemas com a barriga, normalmente jogando o osso e deixando os gerentes abaixo brigando entre si pra ver quem ganha ou fala mais alto. É o sintoma de um péssimo C-level e, por consequência, uma péssima empresa.&lt;/p&gt;

&lt;p&gt;Da mesma forma, especialmente em empresas grandes, existe uma camada do meio muito gorda, os gerentes midlevel. Eles nem são C-level que podem tomar as decisões mais cruciais e nem estão na linha de frente vendo a guerra de verdade sendo travada nos fronts, onde estão os coordenadores e funcionários em geral. Eles acabam sendo mais burocratas, os famigerados pilotos de Excel e Powerpoint. É um mau necessário em muitas empresas, e eu realmente não consigo ver uma forma de eliminar isso totalmente, só minimizar. Não importa se você chama departamentos de chapters, é tudo a mesma coisa, estrutura hierárquica profunda e pior, estática.&lt;/p&gt;

&lt;p&gt;Onde esse problema já existe, que é na maioria das multinacionais e empresas realmente grandes, só vejo isso mudando em épocas de mudança de liderança. Numa mudança de CEO, por exemplo. Onde é forçado a tal &quot;reestruturação&quot; de cima pra baixo. Não é garantia de melhoria, mas pelo menos o caos força mudanças, algumas podendo ser boas. Mas isso é arriscado e custa caro, por isso não se vê todo dia. Em empresas pequenas e médias, o problema é evitar chegar nesse ponto cedo demais.&lt;/p&gt;

&lt;p&gt;No começo de uma empresa, ela vai necessariamente ter a cara dos sócios fundadores, que são a primeira geração de C levels dessa empresa. Se os sócios são ruins, a empresa vai ser ruim. As exceções são quando esses sócios tem conexões e investidores que forçam um C level diferente dos fundadores. Mas pensando no caso geral, é por isso que eu sempre digo que um jovem nos seus 20 e poucos anos não deveria pensar em abrir empresa se já não tiver sido muito bem criado e educado com herdeiro de um negócio pré-existente.&lt;/p&gt;

&lt;p&gt;Eu disse isso em episódios anteriores, porque muito jovem ingênuo até hoje ainda fala das histórias lendárias dos jovens Bill Gates, Steve Jobs ou Mark Zuckerbergs e a lendária história da startup de garagem, como se todo mundo pudesse fazer a mesma coisa. Existe uma razão de porque se menciona Bill Gates, que abriu a Microsoft no fim dos anos 70. Tem muito pouco exemplo na história. Todos os nomes que eu falei largaram faculdades renomadas. Harvard e Stanford. Experimente entrar nelas. O nome do Bill Gates é William Henry Gates Terceiro. A família já era reconhecida. O famoso contrato da Microsoft com a IBM pra licenciar o MS-DOS só saiu porque o a mãe do Bill Gates era amiga do CEO da IBM daquela época. É assim que se faz. Nem eu, nem você temos como ter contatos assim aos 20 anos.&lt;/p&gt;

&lt;p&gt;Quem não é um bom herdeiro vai precisar gastar pelo menos a primeira década de trabalho criando conexões. Conhecendo outras pessoas em começo de carreira que vai confiar em você porque viu em primeira mão que você é competente e entrega o que promete. Daqui alguns anos, ele vai ser dono de empresa, investidor, ou diretor, agora você tem networking de verdade. Isso se constrói e se conquista demonstrando competência de verdade e não se conectando no LinkedIn. Vou repetir: eu não aceito nenhum convite no LinkedIn de quem eu ja não tenha conhecido antes na vida real. Quando aceito desconhecido foi acidente. E outras pessoas que aceitam também não dão nenhum peso. Convite de LinkedIn, cartão de visita, e-mail de spam, vai tudo pro lixo. Não perca seu tempo. Quem eu confio tá no speed dial do meu celular, o resto é resto.&lt;/p&gt;

&lt;p&gt;Enfim, o ponto que estava falando é que a empresa vai sempre ter a cara dos fundadores. Se os fundadores forem inexperientes e incompetentes, é assim que a empresa vai ser. Daí vai precisar ter sorte pra conseguir contratar alguém bom que aceite trabalhar pra você. É bem loteria. Por isso eu acho que o certo é primeiro você se tornar bom, um profissional nível A, porque só assim, vai conseguir contratar outro profissional nível A. Um profissional nível B só contrata caras nível C. E caras nível C só conseguem contratar caras nível E, e assim por diante. Só A contrata A.&lt;/p&gt;

&lt;p&gt;E isso é importante porque quando possível, parte do trabalho de um gestor é contratar pessoas. Só que se ele próprio não for um cara reconhecido como nível A, vai continuamente contratar as pessoas erradas e colocar nos lugares errados. Das duas uma, ou vai contratar cara nível C quando precisava de um A. Ou vai dar tarefas nível C pra um cara nível A. Em ambos os casos vai ser uma equipe ruim.&lt;/p&gt;

&lt;p&gt;Muitos gestores em começo de carreira ficam lendo os cases e metodologias aplicadas em grandes empresas como Netflix ou Google e enchem a boca pra falar “vamos fazer do jeito X porque a Netflix faz assim”. Mas reconheça como isso é idiota. Uma grande empresa como Netflix pode pagar profissionais nível A que ganham meio milhão de dólares por ano ou mais. Primeiro se pergunte: você pode pagar isso? O budget dos projetos que eles executam são da ordem de milhões de dólares. Seu projeto tem esse tipo de orçamento? Se não, por que diabos você está se iludindo achando que pode fazer o que eles fazem?&lt;/p&gt;

&lt;p&gt;Sinceramente eu não acredito num gestor que não foi ele próprio um profissional igual aos membros de sua equipe. Um time de futebol onde o treinador não foi ele próprio jogador de futebol. Um chef de cozinha que não foi ele próprio descascador de batatas. Um general que não foi ele próprio soldado raso. E um gerente de equipe de desenvolvimento que não foi ele próprio um desenvolvedor de software. Nem todo programador tem capacidade de virar um gestor, mas certamente um gestor não técnico, nunca vai saber o que é ser um programador num seminário de poucas semanas. Nenhum soldado jamais vai respeitar um general que nunca esteve numa batalha de verdade.&lt;/p&gt;

&lt;p&gt;Daí caímos no caso onde temos uma equipe que não respeita seu gestor e o gestor que não confia na própria equipe, que eu chutaria que descreve a maioria das equipes por aí. Respeito e confiança é diferente de ser o cara engraçado no happy hour. Foda-se ser engraçado. Bons profissionais tem respeito mútuo entre si, independente do ranking e classificação. Eles nem precisam ser amigos e gostar um do outro. Mas cada um sabe que na hora H pode contar com o outro. Acaba sendo natural. E esse é o terror das equipes de RH e recrutamento porque eles também não tem a mínima idéia de como contratar certo, porque são só burocratas, nunca estiveram na linha de frente.&lt;/p&gt;

&lt;p&gt;E como eu sei que eu sou bom, nível A? Tem um checklist? Tem pré-requerimentos? Tem curso ou certificados que preciso ter? Não, não, não e não. Eu gosto de dizer que ser bom é que nem paixão. Como eu sei que estou apaixonado? Bom, quando estiver apaixonado, você vai saber. E todo mundo ao seu redor também vai saber. É a mesma coisa de ser bom. Se você não sabe se é bom, provavelmente não é. Quando for, vai saber, e todo mundo ao seu redor - que não for sua família e amigos - vão saber. E eu faço essa distinção porque quem automaticamente diz pra você que é bom, mesmo não sendo, tem boas intenções, mas está sendo desonesto. Um amigo de verdade diz quando você está errado, caso contrário está sendo um péssimo amigo, porque está roubando sua oportunidade de melhorar às custas dele se sentir bem consigo mesmo.&lt;/p&gt;

&lt;p&gt;Quando você for bom, naturalmente, vai conseguir identificar outras pessoas boas. É assim que funciona. Pra ilustrar, eu gosto da série Peaky Blinders. E me identifico com um aspecto que o protagonista Tommy Shelby tende a no mínimo respeitar quem lutou na guerra como ele. Ele tem vários traumas das trincheiras da França na Primeira Grande Guerra Mundial, que foi uma das mais carniceiras de todos os tempos. Quando você encontra outro ex-soldado condecorado que esteve em batalhões que lutaram juntos, você meio que sabe do que ele é feito. Todo filme de guerra tem esse aspecto, e no mundo fora das guerras de verdade, é similar, a gente que tem cicatrizes sabe reconhecer as cicatrizes dos outros. Só que um recém formado ou iniciante não tem cicatrizes ainda. Esse é o ponto.&lt;/p&gt;

&lt;p&gt;Mas não é só executar o trabalho? Não é só preencher planilhas e preencher relatórios e meu trabalho como gestor tá acabado? Esse é o pensamento que muito iniciante tem. Das duas uma, você vira o yes man, que vai só executando o que pede. Ou você se torna o maverick, o imprudente que acha que precisa revolucionar as coisas do seu jeito. Mas como eu disse, nenhum dos dois tem cicatrizes ainda pra saber até onde pode ir e nem como ir. E ambos estão fazendo bobagens. O problema é que o termo “gerenciar projetos” é o jeito errado de dizer. O objetivo é entregar projetos mas você sempre gerencia pessoas. Sua taxa de sucesso em projetos é diretamente proporcional ao seu bom relacionamento com suas equipes.&lt;/p&gt;

&lt;p&gt;E pessoas não são máquinas. Especialmente em programação. Aliás, eu sempre falo que essa é a diferença entre software e hardware. Quando falei que é importante saber PMI mas não seguir nada deles ao pé da letra é porque muito do que existe documentado sobre gerenciamento de projetos deriva da engenharia de produção, da gestão de fábricas, relíquias da revolução industrial, o legado de modelos como da Ford. Aquele estereótipo de Charles Chaplin onde pessoas são secundárias e a linha de produção é o principal.&lt;/p&gt;

&lt;p&gt;Nesse ambiente, tudo é minuciosamente planejado, você faz blueprints, plantas baixas, esquematizações e daí é possível calcular até que com bastante precisão quanto vai custar e quanto tempo vai levar pra construir a fábrica. E a partir daí a produção dos items é bem uniforme, com pouca variabilidade. Muita coisa é automatizada e no mundo ideal, nem é pra ter pessoas envolvidas, só robôs, como numa fábrica de carros mais moderna. A maior parte do conhecimento de administração e gerenciamento deriva desse cenário de produção em massa.&lt;/p&gt;

&lt;p&gt;No jeito antiquado de pensar, se em hardware funciona, em software também deveria funcionar. Afinal, o programador é como se fosse o operário da linha de produção. Basta desenhar o projeto da forma mais detalhada possível, fazer a planta baixa, que seria função de um arquiteto, daí é só o programador executar como um operário numa linha de fabricar carros. Outra metáfora que se usa quando povo tenta falar de software como arte é equiparar com uma orquestra de música clássica, onde temos um maestro ou condutor, que é o gerente de projetos, a composição que é o trabalho do equivalente arquiteto, daí cada um na orquestra com seu instrumento só precisa executar sua parte com precisão pra termos uma peça de sucesso.&lt;/p&gt;

&lt;p&gt;Só que ambos os pontos de vista estão errados. O trabalho de operário ou do músico da orquestra, o trabalho repetitivo, braçal, já é totalmente automatizado. Quem cumpre esse papel é o compilador ou interpretador. É o compilador que recebe a tal planta baixa e cospe o binário que o computador vai executar, mecanicamente. Agora o que é essa planta baixa? Esse é o código-fonte, o programa propriamente dito. O programador não é o operário. Todo programador é tecnicamente arquiteto. E na realidade não se trata de uma orquestra de música clássica e sim um grupo de jazz, onde intuição, improvisação, trabalho em equipe, fazem toda a diferença.&lt;/p&gt;

&lt;p&gt;Eu não sou músico, mas do pouco que entendo, jazz seria o mais próximo de programação. Cada integrante toca improvisando, complementando e competindo com os demais no mesmo grupo, mas trabalhando de forma a criar uma música que faça sentido. Programação é a mesma coisa. Cada programador tem seu estilo, seu ritmo, e apesar de precisarem ter alguma base mínima pra não virar só barulho, a contribuição individual de cada um conta.&lt;/p&gt;

&lt;p&gt;Um programador não é um operário. E software é ordens de grandeza mais complicado do que hardware porque hardware obedece as mesmas leis da natureza que são não-ambíguos pra todo mundo. Duas empresas de construção que recebem a mesma planta baixa, no final vão construir basicamente o mesmo prédio. Com quantidades similares de cimento, tijolos e tudo mais. A especificação é precisa o suficiente pra isso. E tem que ser assim porque uma vez construído, não tem como refatorar uma ponte. Não tem como adicionar ou tirar um andar no meio de um prédio.&lt;/p&gt;

&lt;p&gt;Software é diferente. Dois programadores que receberem qualquer tipo de especificação de software, vão sair com dois conjuntos de código completamente diferentes. E quanto mais específico você tentar ser, mais próximo do código final você vai chegar, então no final é como se estivesse você mesmo fazendo o código. O código é a especificação. E o erro é justamente esse: achar que é possível especificar um software com precisão antes de comecar a codificar. Isso é impossivel.&lt;/p&gt;

&lt;p&gt;Claro, se você está num ramo de atividade onde sempre faz o mesmo software o tempo todo, aí é mais fácil. Por exemplo, uma agenciazinha cujo trabalho é customizar Wordpress pra pequenos clientes. Nesse caso o grosso do software já está pronto, é o Wordpress. Você muda tema, muda logotipo, faz uma ou outra customização. Mas agora sim, você está muito mais próximo de uma linha de montagem de fábrica. Porque o grosso do trabalho não é codificar, é copiar e colar mesmo.&lt;/p&gt;

&lt;p&gt;Mesma coisa um ERP como os de uma Totus da vida. O trabalho de programação mesmo é fazer e atualizar esse ERP, adicionar novas funcionalidades. Mas o grosso do trabalho de instalar nos clientes é customização, e não totalmente programação. A maior parte do software é padronizado e já está pronto. Daí o que tem de programação é mais a integração dele com outros sistemas, por exemplo. Daí é um ambiente um pouco mais controlado. Nesse tipo de ambiente que funciona as tais ferramentas low code ou no code, que é pra grudar peças que já existem e foram codificadas por um programador. Daí um operador só precisa customizar a integração desses peças. Mas o grosso do software em si, que é o ERP, já está pronto.&lt;/p&gt;

&lt;p&gt;Na prática, em graus diferentes, ninguém mais faz 100% de todo software do zero. O sistema operacional é padronizado. Componentes como bancos de dados são padronizados. Pouco a pouco plataformas como AWS da Amazon ou Azure da Microsoft foram comoditizando diversas coisas que antes fazíamos do zero, até machine learning e inteligência artifical hoje são serviços que a gente só integra. A maioria dos projetos hoje tem uma parte grande que é basicamente só integração. O que muitos programadores fazem o dia inteiro é na realidade o trabalho de um integrador.&lt;/p&gt;

&lt;p&gt;E mesmo assim o trabalho ainda não é linear de linha de produção porque temos diversos aspectos que mudam. Requerimentos de performance, requerimentos de escalabilidade, requerimentos de segurança, requerimentos de usabilidade. Nenhum deles permanece estático pra sempre. Mudanças nesses requerimentos às vezes exigem reescrever uma parte do software que não atende mais o que precisa. Às vezes é jogar fora um legado por um produto mais moderno. E é aí que entram as equipes de desenvolvimento pra decidir qual o melhor caminho: um band-aid em cima do que já existe, reescrever um pedaço, integrar com um produto de terceiros, ou uma combinação de tudo isso.&lt;/p&gt;

&lt;p&gt;E de novo voltamos à formação da equipe. Uma vez definido a direção e objetivos. Quem decide como as coisas vão ser feitas? Um gerente não técnico tem zero condições de decidir essas coisas. Então, ou ele precisou receber essa ordem de alguém, ou precisou confiar que a equipe sabe quais as melhores decisões técnicas. E agora ele se ferrou, porque se não teve a oportunidade de criar essa equipe, se for o caso que falei antes de um gestor que não conquistou o respeito da equipe, tenha certeza que parte dessas decisões vai ser tomada no automático, e os resultados nunca vão ser os melhores.&lt;/p&gt;

&lt;p&gt;Quando um gestor não tem a capacidade de decidir essas coisas, ele precisa de suporte. É onde normalmente deveria entrar o tal do CTO, o Chief Technology Officer. Dependendo do tamanho da empresa ele vai ter pares como um CSO ou Chief Security Officer que se preocupa com coisas como governança. E um bom CTO se for realmente um nível A, deveria já ter uma equipe de líderes técnicos que ele confia, e juntos conseguem prototipar, criar provas de conceito, e decidir a melhor solução técnica e orientar as demais equipes dos caminhos que decidiram tomar.&lt;/p&gt;

&lt;p&gt;Agora cabe aos gestores de cada equipe coordenar com suas equipes e ver se é possível executar o que foi demandado. Às vezes a resposta vai ser não, e aqui um bom gestor tem que ser o firewall da equipe. Ele tem que saber decidir quando precisa forçar a solução pra equipe ou quando a equipe sabe que não vai dar certo, e porquê, e rediscutir o problema pra cima. E isso é sempre um problema quando o gestor não confia na própria equipe ou a equipe não respeita o gestor. Alguém sempre vai estar vendido nessa situação, e por isso eu disse que gestão de projetos é na realidade gestão de pessoas.&lt;/p&gt;

&lt;p&gt;Por princípio, se não for do tipo kamikaze como eu, a maioria das pessoas detesta conflitos, e vai fazer o possível pra evitar, inclusive prejudicando os projetos, empurrando o problema com a barriga até ser tarde demais. Esse é o pior tipo de gestor, na minha opinião, porque ele vai dar burn out na equipe, executando uma coisa que já se sabia que não ia dar certo, só pra postergar a discussão do problema. O famoso jogar embaixo do tapete e torcer pra ninguém ver. Dentro da equipe, o único que tem obrigação de não fugir de conflitos, é o gestor. A equipe deve ser blindada de conflitos desnecessários, e o gestor é quem tem que aguentar a úlcera e blindar a equipe.&lt;/p&gt;

&lt;p&gt;E falando na equipe, agora temos o problema de composição e comunicação. A realidade é que na maior parte do tempo, nenhum trabalho é realmente interessante. Você tem períodos de coisas legais e períodos de coisas chatas. Infelizmente as duas coisas são importantes. Sabendo disso, muito gestor tenta o caminho de tentar ser o motivador, cheerleader ou algo assim. Isso não funciona. Nenhum gestor motiva equipes, no máximo eles evitam desmotivar. Motivação é intrínseco, cada pessoa só consegue motivar a si mesma. Motivação externa é no máximo um rush temporário. Cada pessoa precisa encontrar propósito nela mesma, e isso é um trabalho individual. Psicólogos podem ajudar, mas gestores só precisam ajudar a não atrapalhar.&lt;/p&gt;

&lt;p&gt;Se o gestor enxerga as coisas como fábrica, projetos como linha de produção, programadores como operários, a vida vai ser difícil. Porque ele vai sempre passar só o mínimo de informação, normalmente na forma de ordens, vai microgerenciar como se não houvesse amanhã, e raramente vai ter a via inversa que é de conversa e comunicação. Dizer uma informação é diferente de se comunicar. Comunicação é diálogo. Um diálogo não precisa ter uma resolução imediata, mas no mínimo precisa existir duas mãos na conversa. Isso é o mínimo.&lt;/p&gt;

&lt;p&gt;Não dá pra aprender relacionamento de pessoas lendo livros, mas não tem nenhum mau em complementar seu conhecimento com alguns bons livros. E eu recomendo dois. O primeiro é o lendário “Psychology of Computer Programming” do grande Gerald Weinberg. E ele tem vários outros livros interessantes como o “Becoming a Technical Leader”. Mas se quiserem só um resuminho, acho que no mínimo todo mundo já deveria ter lido outro livro famoso, o “Peopleware”, do Tom DeMarco. Esse eu acho que tá um pouco defasado. Várias coisas que eram novidade nele, valiam pro começo dos anos 2000 e hoje parte já é meio considerado comum, mas continua sendo válido ler. Eu vou bater no ponto que a parte mais importante de gerir projetos é entender pessoas, então todo gestor deveria fazer um esforço nessa direção, nem que seja ler alguns livros.&lt;/p&gt;

&lt;p&gt;O pior caso é quando o gerenciamento passa a ser em sua maior parte orientado a métricas. Métricas são boas e necessárias. Mas nem toda métrica é boa. As piores métricas são as de produtividade. E isso sempre cai no problema das famigeradas estimativas. O primeiro grande erro é quem confunde estimar com prever. Sabem porque existem duas palavras diferentes? Porque elas significam coisas diferentes. Muita gente ainda pede estimativa mas na realidade estão esperando previsão. Por isso sempre dá discussão &quot;ah, mas você disse que 2 semanas estaria pronto&quot;.&lt;/p&gt;

&lt;p&gt;Estimativa é sempre aquela brincadeira de &quot;você finge que fala a verdade e eu finjo que acredito&quot;. Na prática não importa muito com que instrumentos você quer estimar, seja story points, seja horas-homem. O problema é dividir as tarefas ou storys em tamanhos que não sejam muito diferentes. Por exemplo, se o menor tipo de story é de meio dia, o maior não deveria ser mais que uns 2 ou 3 dias. Mais do que isso é um épico e deveria ser dividido mais. Acho importante no mínimo se ter um backlog com stories e um acompanhamento de stories por sprint, tipo um burn out chart. Se não sabe do que estou falando leia os papers originais de Scrum do Schwaber e Sutherland.&lt;/p&gt;

&lt;p&gt;Agora, muito mais do que isso eu acho desnecessário. Isso porque essas medidas já são grosseiras, é mais pra ter ordem de grandeza e não exatidão. Sprints também deveriam ser semanais, duas semanas no máximo, mais do que isso e eu posso adivinhar que tem outros problemas graves sendo escondidos. Por exemplo, suas reuniões duram tempo demais e certamente são desnecessárias. Lógico que não dá pra ter um sprint de 1 semana se a segunda feira inteira é gasta só em planejamento e a sexta feira inteira é gasta com review e retrospectiva, e de terça a quinta todo mundo da equipe é toda hora interrompido.&lt;/p&gt;

&lt;p&gt;Não adianta querer medir produtividade, no sentido de código entregue por semana. Seu maior problema de produtividade é desperdício de tempo. De novo, um gestor que não tem muita noção do que está fazendo acha que discutir por horas, vai chegar em algum lugar. No máximo é um gestor que sente que está trabalhando muito por gastar muitas horas fazendo reunião. Fazer reunião é o oposto de trabalhar. Tudo que pode ser resolvido por email, deve ser resolvido por email. Você se junta cara a cara quando a discussão claramente não está evoluindo. Aí faz uma reunião pra bater o martelo, e não pra gerar mais discussões.&lt;/p&gt;

&lt;p&gt;Isso sem contar métricas aleatórias que obviamente são prejudiciais. Por exemplo, já vi lugares que mediam quantidade de bugs corrigidos. Ou seja, parecia uma coisa boa, quem mais corrigisse bugs, era o melhor. Não precisa ser nenhum gênio pra saber o que isso vai gerar: ninguém vai se preocupar em não gerar bugs. Já que a métrica é corrigir bugs, então o incentivo vai ser em criar mais bugs. Métricas ruins podem afundar sua empresa, porque se você encheu o lugar de pessoas ruins e deu incentivos pra elas serem ruins, é isso que elas vão ser: ruins. Nunca se esqueçam que no geral, as pessoas sempre vão escolher o caminho de menor esforço, é praticamente uma lei econômica isso.&lt;/p&gt;

&lt;p&gt;Não tem nada em comunicação que me irrita mais do que quem não sabe o objetivo da comunicação. Ninguém gosta de ouvir sua voz por mais tempo do que o necessário. Ninguem está interessado no seu discurso. A gente só quer saber: qual o objetivo? Qual a direção? Se for um navio é &quot;virar 15 graus a bombordo&quot;. Pronto, essa é a informação. Tudo que você falar antes ou falar depois é desnecessário. O bom dia pode vir na mesma mensagem e não antes de um &quot;digitando ...&quot; que fica nos três pontinhos por 5 minutos e me faz ficar esperando que nem um idiota. E você tem que estar disponível pra todo mundo da equipe o tempo todo, porque o papel de um bom gestor é ser um facilitador. Gestor que é difícil de encontrar, não presta. Ah, mas é porque ele tá sempre em reunião. Não tenho dúvidas, e isso é um problema que ele precisa se virar e resolver.&lt;/p&gt;

&lt;p&gt;Toda interrupção é desperdício. Toda tarefa de 1 hora que foi interrompida por 5 minutos no meio vira 1h e meia ou mais. Porém, toda informação relevante precisa chegar na hora certa. Nem antecipado que não altera o fluxo de nada, e nem atrasado que agora vai custar o dobro pra agir em cima. Qual é a hora certa? É pra isso que existe um gestor. Interrompa todo mundo o tempo todo e sua produtividade vai pro buraco. Evite interromper quando realmente precisa e vai cometer erros que vai custar mais caro lá na frente. Se fosse fácil decidir isso, não precisaríamos de gestores.&lt;/p&gt;

&lt;p&gt;Chats como Slack não foram feitos pra serem usados o tempo todo. Se algo realmente urgente surgiu que não pode esperar, sim, acione o alerta vermelho, interrompa quem precisar pra resolver. Porém, se tudo é alerta vermelho, em breve ninguém mais vai dar atenção. Se a informação não for urgente, anote, comunique quem precisa por email no fim do dia. É o tipo de etiqueta que não deveria precisar ser dito.&lt;/p&gt;

&lt;p&gt;Falando nisso, aprenda de uma vez por todas: se tudo é uma prioridade, por definição, nada é prioridade. Eu fiz um video só sobre priorizar. E saber priorizar vale pra todo mundo, todo integrante de qualquer equipe, até chegar no dono da empresa. Não existe querer fazer tudo, porque não temos recursos infinitos. Gerenciar é a eterna tarefa de eleger prioridades e cortar coisas que não são prioridade. E como define prioridades? Essa é a pergunta de um milhão de dólares, se existisse forma fácil eu estaria vendendo. Agora, se você não consegue definir prioridades, certamente não deveria estar liderando. Foi o que eu falei antes: tomar decisões quando elas precisam ser tomadas, e não empurrar problemas com a barriga.&lt;/p&gt;

&lt;p&gt;Estimativas e prioridades são coisas que só quem está ativamente participando do projeto deveria se preocupar. Burocratas que aparecem de vez em quando, não tem que dar pitaco em nenhuma das duas. E esse é outro problema frequente, especialmente em empresas grandes demais onde um diretor não tem como fisicamente estar presente em todos os lugares. E aí voltamos ao problema do nível A que contrata outro nível A. Todo mundo precisa poder confiar em outras pessoas. Não cegamente, mas com o mínimo de informações que te dão segurança o suficiente pra poder focar em outros lugares que precisam mais da sua atenção. Não existe metodologia nem fórmula mágica pra isso. Ou você tem pessoas que confia ou não tem, e se não tem, não dá pra substituir por métricas mágicas. Elas só vão te iludir por mais tempo do que deveria.&lt;/p&gt;

&lt;p&gt;Eu disse antes que um soldado não confia num general que não foi ele mesmo um soldado antes. No mundo ágil temos a história do porco e da galinha. O gerente que não precisamos ter são os galinhas. Por que galinha bota o ovo e vai embora, mas o porco corta na carne. Quem toma decisões precisa cortar na própria carne e não largar o problema pra equipe e ir embora, e só aparecer quando é pra tomar o crédito. Faça isso muitas vezes, e daqui a pouco vai estar se perguntando porque a produtividade da sua equipe só cai.&lt;/p&gt;

&lt;p&gt;Houve uma época que toda decisão era mais baseada em métricas financeiras. É de onde vem a má reputação no meme popular de que empresas são entidades do mau que só pensam em dinheiro. E muitas realmente são assim. Daí teve a época de qualidade total, onde finalmente caiu a ficha que qualidade faz tanta diferença quanto financeiro. Foi a partir do fim dos anos 60 que passamos a diferenciar trabalhadores entre aqueles que executam tarefas rotineiras e padronizadas, como numa fábrica, versus uma categoria que começou a crescer cada vez mais, o que Peter Drucker cunhou como knowledge workers ou trabalhadores do conhecimento, que executam tarefas não-rotineiras, como nós programadores.&lt;/p&gt;

&lt;p&gt;Muitos conceitos de gestão ainda vem da época da Revolução Industrial. A idéia de hora-homem por exemplo. Mas esses conceitos não funcionam pro dia a dia moderno porque fábricas deixaram de ser o principal componente das indústrias. Então medir só com base nos números financeiros já não dava mais indicativos da saúde geral da empresa. Foi quando surgiram outras formas de enxergar empresas como o famoso Balanced Scorecard do Kaplan e Norton e suas variações a partir do fim dos anos 80, com métricas financeiras e não-financeiras.&lt;/p&gt;

&lt;p&gt;Se você for de admistração certamente já viu Balanced Scorecards, se não for, os detalhes não interessam tanto, mais a noção que hoje as empresas não são todas tão burras de só olhar métricas financeiras como muitos ingenuamente assumem. Elas podem ser incompetentes em alguns aspectos, mas a maioria tem que preocupar com múltiplos aspectos da empresa, hoje em dia incluindo coisas como reputação em redes sociais. Mas tudo bem, eu sei que tem empresas burras porque vira e mexe ouço falar dos idiotas que querem monitorar produtividade mandando o funcionário ligar a câmera e instalando software pra ver se ele tá mesmo digitando ou coisas assim. São empresas assim que mancham nosso mercado e eu espero que eles entrem em falência logo. Ou você confia em quem contrata, ou então não contrata. Se alguém quebra sua confiança nesse nível, você não fica brincando de monitorar, você manda embora. É simples asim.&lt;/p&gt;

&lt;p&gt;Pra complementar, o maior problema de uma empresa é quando ela trata todo mundo como criança. Sabe, piscina de bolinha e afins? Quando é uma creche, sim, consigo ver a necessidade de monitoramento ativo. Porque crianças são difíceis de prever, e alguém pode acabar se machucando. Mas uma empresa precisa tratar todo mundo como adulto. Se você tem adultos, não precisa ficar microgerenciando nem monitorando. Por outro lado, se você trata todo mundo como criança, de novo é culpa sua. Empresas não são creches. Não acumule crianças birrentas. Monitorar crianças só funciona quando elas mijam nas próprias calças e brincam de ficar fuçando onde achar faca da cozinha pra se cortar. Num trabalho sério, monitoramento desse tipo não serve pra nada, só pra comprovar que a liderança é completamente incompetente.&lt;/p&gt;

&lt;p&gt;Enfim, Peter Drucker, pelo menos na minha época, era o be-a-ba da administração. E naquela época também tivemos trabalhos como de Ikujiro Nonaka e Hirotaka Takeuchi, como eu falei no episódio anterior os japoneses aceleraram seus processos de qualidade e gestão. E Nonaka e Takeuchi escreveram vários artigos, incluindo o famoso The New New Product Development Game, que é de onde surgiu o termo &quot;scrum&quot; que depois o Ken Schwaber usou pra formular a metodologia Scrum e se tornou uma das raízes do movimento Ágil.&lt;/p&gt;

&lt;p&gt;Qualquer um que tenha minimamente estudado agilidade já leu esse paper. E eu recomendo que dêem uma boa estudada caso ainda não tenham feito isso. É o resumo do resumo sobre organizações e termos como auto organização. Não é de hoje que se fala em coisas como autonomia. Esse paper é dos anos 80 e até hoje a maioria das empresas ignora o que já sabemos que funciona. Uma boa parte das idéias dessa época fala em elevar os funcionários a pensar em vez de só obedecer e executar. Mas sem entrar em caos. Eu explico isso no meu video sobre Organizações.&lt;/p&gt;

&lt;p&gt;Parte disso tem a ver com evoluir as pessoas e passar esse conhecimento pra novos projetos. Por exemplo, um dos erros que até hoje ainda vejo muitas empresas cometendo é quando querem iniciar um novo projeto, mas todo mundo está ocupado já. O que eles fazem? Contratam uma empresa terceirizada pra fazer o novo projeto. Isso é um grande erro.&lt;/p&gt;

&lt;p&gt;Às vezes não existe outra forma a não ser aumentar o head count, ou seja, quantidade de pessoas. Um bom gestor sempre deve designar novos projetos pros membros mais seniors das suas equipes e colocar terceiros embaixo deles. Se por acaso for uma especialização que ninguem das suas equipes tem, mesmo assim você coloca seus seniors junto com os terceiros pra eles aprenderem seja lá qual especialização que não tem.&lt;/p&gt;

&lt;p&gt;Terceiros devem ser tratados como juniors internos. Ambos os casos embaixo dos seniors internos. Os seniors se tornam seniors quando eles aprendem a mentorar os membros menos experientes. Esse é o ciclo sustentável. Depois que o projeto que eles entregaram passa pra fase de operação, que é realmente ser usado, receber correções, ajustes, manutenção, eles precisam aprender a criar sucessores pras suas posições, pra continuar o trabalho enquanto eles assumem novos projetos.&lt;/p&gt;

&lt;p&gt;Autonomia só existe se todos os membros conseguem confiar nos demais membros, não cegamente, mas porque os mais experientes sabem que conseguem mentorar os mais novos, e porque os mais novos sabem que tem quem consultar quando tem dúvidas ou problemas. Uma equipe só de seniors é um desperdício e uma equipe só de juniors é um desastre esperando pra acontecer. Toda equipe deve ter capacidades mistas e serem rotativas. Um junior bem mentorado vai se tornar o novo senior no futuro, que vai passar a mentorar os novos juniors. Os antigos seniors vão continuar em novos projetos, que passa a aceitar novos juniors pra mentorar e assim por diante.&lt;/p&gt;

&lt;p&gt;Bons gestores sabem reconhecer os pontos fortes e fracos de cada membro e a ajustar as equipes de acordo com isso. Se em todo jogo você só manda o dream team jogar, quem está de reserva nunca vai ganhar experiência, e quando um dos membros estrela tiver problemas, ninguém vai conseguir jogar no seu lugar, a equipe não sabe como lidar com a mudança. Todo jogo não decisivo precisa misturar reservas. Todo treinador sabe disso. É assim que começam a surgir novas estrelas. Todo gestor precisa saber disso também. Nada que fica estático tempo demais é bom, sempre vai deteriorar. Pequenas mudanças, periódicas, é bom. É a beira do caos que falei no episódio passado.&lt;/p&gt;

&lt;p&gt;Confiança não é amizade. Entendam isso de uma vez por todas. Uma coisa é o happy hour. Outra coisa é o trabalho. Quem confunde essas coisas, acha que esconder os erros do amigo é uma coisa boa, porque ele não quer se o cagoeta, o x9. Mas isso prejudica ele, prejudica a equipe e prejudica quem está cometendo os erros. Se você desse o feedback honesto, estaria dando a chance dele corrigir o erro e melhorar. E quando a pessoa se recusa a corrigir o erro, ele é quem está quebrando a confiança com você. Ninguém precisa de pessoas desonestas do lado, especialmente aquele que te chama de amigo só quando interessa. Corte essas pessoas, o quanto antes. Seja realista. Incompetência é algo que podemos tentar consertar. Porém, maucaratismo se corta na raíz. Eu sempre digo que se a mãe dele não conseguiu criar caráter nele em 20 anos, não sou eu que em 2 meses vai conseguir. Corta.&lt;/p&gt;

&lt;p&gt;Mas o trabalho é assim mesmo: quando você faz algo certo, não é nada demais, afinal você é pago pra fazer certo. Por outro lado, quando comete erros, eles sempre se sobressaem. É a vida, se fosse fácil, qualquer um fazia e ninguém precisaria de você. Senioridade é aprender a lidar com isso e continuar caminhando em frente. E o papel do gestor não é fazer tudo sozinho, é conseguir construir uma equipe onde todos tem essa mentalidade, e naturalmente os projetos tendem a alcançar sucesso. Não porque todo mundo previu logo no começo tudo que precisava fazer, mas sim porque entendem que erros vão acontecer no caminho e as expectativas vão sendo realinhadas de acordo. Os stakeholders não ficam sabendo de desastres só quando já é tarde demais, esse é o principal. Todo erro identificado o quanto antes, tem chances de ser arrumado.&lt;/p&gt;

&lt;p&gt;Gerenciar projetos é gerenciar pessoas. E gerenciar pessoas é gerenciar expectativas. Ninguém precisa de alguém que só faz promessas. Todo mundo precisa de pessoas que assumem seus erros e fazem o possível pra corrigir e, claro, evitar repetir os mesmos erros. É só isso. Promessas e discursos são inúteis e desnecessários. A camada de gestão é responsável pela estratégia, a direção e objetivos. As equipes são responsáveis pela tática, a execução. Todo soldado na frente precisa confiar que o soldado de trás vai cobrir sua retaguarda, e assim todos avançam coordenados em busca do mesmo objetivo, que é ganhar mais uma batalha.&lt;/p&gt;

&lt;p&gt;Como falei no começo, esses foram só alguns pontos que eu considero importante lembrar. Mas cada parágrafo que eu falei não é mais que um pequeno resumo de um assunto gigante. O tema completo de gerenciamento engloba décadas de experiência e técnicas que vieram sendo aprimoradas, modificadas, e continuam em evolução. Toda técnica moderna de gerenciamento de software nasceu nos últimos 15 a 20 anos. Muita literatura ainda fala de coisas de 3 ou mais décadas no passado. Algumas coisas continuam valendo, outras não. E a função de todo bom gestor é se manter atualizado, experimentar coisas novas, e não perder o foco sobre sua missão, que é entregar projetos de forma sustentável e ajudar a evoluir suas equipes. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal, e compartilhem o video com seus amigos. A gente se vê, até mais.&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5953</id>
    <published>2022-05-30T10:33:00-03:00</published>
    <updated>2022-05-30T10:45:04-03:00</updated>
    <link href="/2022/05/30/akitando-119-rant-aprendizado-na-beira-do-caos-rated-r" rel="alternate" type="text/html">
    <title>[Akitando] #119 - Rant: Aprendizado na Beira do Caos | Rated R</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/am-FQ86mKV0&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;h2&gt;DESCRIÇÃO&lt;/h2&gt;

&lt;p&gt;Finalmente chegamos na marca de 300 mil assinantes no canal! E hoje vai ser mais um video de Rant, Rated R, onde quero só falar livremente sobre minhas pesquisas e hipóteses sobre uma teoria geral sobre os princípios de aprendizado, gestão, empreendedorismo e qualidade. Vai ser uma longa conversa metafísica que costuma ser assunto de minhas conversas de bar. Vamos tentar falar sobre isso sem o efeito de álcool. Vai ser meio bizarro, mas aguentem firme.&lt;/p&gt;

&lt;h2&gt;Conteúdo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00:00 Intro&lt;/li&gt;
&lt;li&gt;00:01:55 Determinismo&lt;/li&gt;
&lt;li&gt;00:06:54 Atualizando Newton&lt;/li&gt;
&lt;li&gt;00:10:41 Caos&lt;/li&gt;
&lt;li&gt;00:13:10 Lavagem Cerebral&lt;/li&gt;
&lt;li&gt;00:15:02 História da Qualidade&lt;/li&gt;
&lt;li&gt;00:21:18 Deming a Six Sigma&lt;/li&gt;
&lt;li&gt;00:25:14 Origens de Agile&lt;/li&gt;
&lt;li&gt;00:31:34 Resolvendo Problemas Desconhecidos&lt;/li&gt;
&lt;li&gt;00:33:28 Priorização&lt;/li&gt;
&lt;li&gt;00:34:44 Mito da Ordem e Progresso&lt;/li&gt;
&lt;li&gt;00:36:01 Beira do Caos&lt;/li&gt;
&lt;li&gt;00:39:00 Ordem e Caos&lt;/li&gt;
&lt;li&gt;00:45:02 Jogando Tudo Fora&lt;/li&gt;
&lt;li&gt;00:47:21 Medo do Erro&lt;/li&gt;
&lt;li&gt;00:52:08 Gestão de Risco&lt;/li&gt;
&lt;li&gt;00:55:44 Saindo da sua Área&lt;/li&gt;
&lt;li&gt;00:58:31 Conclusão&lt;/li&gt;
&lt;li&gt;01:01:00 Bloopers&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;DR. DEMING’S 14 POINTS FOR MANAGEMENT (https://deming.org/explore/fourteen-points/)&lt;/li&gt;
&lt;li&gt;Edge of Chaos (https://www.systemsinnovation.io/post/edge-of-chaos-1)&lt;/li&gt;
&lt;li&gt;Education at the edge of chaos (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3207078/)&lt;/li&gt;
&lt;li&gt;Your Brain Operates at the Edge of Chaos. Why That's Actually a Good Thing (https://www.cnet.com/science/biology/features/your-brain-operates-at-the-edge-of-chaos-why-thats-actually-a-good-thing/)&lt;/li&gt;
&lt;li&gt;Edge of chaos as a guiding principle for modern neural network training (https://arxiv.org/abs/2107.09437)&lt;/li&gt;
&lt;li&gt;Optimal Machine Intelligence at the Edge of Chaos (https://arxiv.org/abs/1909.05176)&lt;/li&gt;
&lt;li&gt;cowsay and cowthink (https://web.archive.org/web/20120225123719/http://www.nog.net/~tony/warez/cowsay.shtml)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Minha voz anda ruim esses dias, mas decidi gravar assim mesmo, espero que não incomode. Este video é pra comemorar os 300 mil assinantes do canal. Obrigado a todos que acompanham e quem é recém chegado, os videos deste canal foram feitos pra serem assistidos desde o começo. Não tem videos opcionais aqui. Se você tem uma pergunta, eu provavelmente já respondi antes. Enfim, já faz muitos episódios que eu só tenho abordado temas bem técnicos, e sendo honesto até pra mim é cansativo editar tanto detalhe de conceito técnico então resolvi dar uma pausa hoje pra falar de alguns assuntos que estudei por muitos anos mas ainda não discuti no canal. O assunto vai ser sobre como eu penso sobre aprendizado. Uma teoria mais geral sobre aprendizado.&lt;/p&gt;

&lt;p&gt;Este video é pra todos vocês que já me perguntaram, “Akita, como eu devo estudar?” ou “Akita, como devo gerenciar um projeto?” ou “Akita, como devo administrar uma empresa?” Impressionante, que apesar de eu já ter falado diversas vezes que vocês não deveriam estar fazendo essas perguntas pra mim nem pra nenhum outro influencer, elas continuam aparecendo. Mas hoje eu quero tentar ensinar a fundação de como vocês mesmos podem tentar responder essas perguntas.&lt;/p&gt;

&lt;p&gt;Já deixo avisado logo de cara que quase tudo que vou contar vai soar como auto-ajuda, coaching e tudo mais. A diferença é que não estou vendendo nada. O problema é que assuntos como aprendizado, gestão, empreendedorismo, são abstratos por natureza. Não existem definições exatas e objetivas pra nenhuma delas. Por isso cada um precisa ter uma imagem na cabeça, um modelo pra encaixar na realidade. O que vou contar agora não é a resposta definitiva nem nada disso. É só a forma como eu penso sobre esses assuntos. Alguns de vocês talvez já tenham imagens diferentes na cabeça, e não tem nenhum problema. Vocês queriam saber como eu penso, bem, eis como eu penso.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Lógico, sei que não importa quantas vezes eu disser que é perda de tempo fazer perguntas como “Akita, o que eu devo fazer da minha vida”, é da natureza humana querer uma resposta fácil, resultados fáceis. Consigo simpatizar com a idéia de imaginar “cara, esse sujeito parece ter sucesso, vou perguntar pra ele como que ele fez, daí é só copiar”. Existem milhões de coisas simples que conseguimos achar respostas fáceis. Como faz pra fazer um ovo cozido com gema meio mole? Procura no Google, e descobre que é só deixar exatamente 7 minutos em água fervente.&lt;/p&gt;

&lt;p&gt;Se pra achar como cozinhar um ovo é tão fácil, por que tudo não pode ser assim fácil? Aliás, dá pra extrapolar bastante. Se quero viajar, basta colocar o destino no Waze ou Google Maps, e ele consegue me estimar com bastante precisão o tempo que vai levar pra chegar. De novo, parece fácil. E posso ficar o resto do dia só elencando exemplos de coisas que você não sabe, mas com uma rápida pesquisa no Google, eu acho a resposta e vai ser até bem exata.&lt;/p&gt;

&lt;p&gt;Se eu consigo achar respostas tão fáceis pra coisas como achar um trajeto no Waze, por que não consigo achar qual o melhor caminho pra estudar? Ou qual a melhor forma de gerir um projeto que vai ser barato e acabar rápido? Ou qual a forma mais eficiente de administrar uma empresa? Ou simplesmente, como ter sucesso na vida? Por que diabos o Google não consegue me dar essa resposta? Será que é só questão de tempo até a tal da inteligência artificial e machine learning melhorarem um pouco mais?&lt;/p&gt;

&lt;p&gt;No fim das contas, por trás de todas essas perguntas que me fazem, elas podem ser reduzidas só a essa mais simples: “Akita, como eu faço pra ter sucesso na vida?” Cara, todo coaching vai te vender que sabe como. E se você for bem trouxa ou muito desesperado, vai cair que nem um pato. Mas como eu não tenho paciência pra isso já digo logo de cara: nem eu, nem ninguém sabe essa resposta e nem nunca vai saber. Nem se juntar todas as cem pessoas consideradas de maior sucesso do século XXI na mesma sala e mandar eles escreverem uma lista de procedimentos. Ninguém vai conseguir seguir e ter o resultado esperado. E intuitivamente a maioria de vocês assistindo já sabem disso, mas por que exatamente?&lt;/p&gt;

&lt;p&gt;Foi fazendo essa pergunta que eu me interessei em estudar mais um pouco sobre conceitos de outras áreas. E se você ler demais vai começar a entrar em filosofia e em diversos outros campos. Apesar de achar filosofia clássica chata pra caralho, obviamente tem coisas interessantes a considerar. É fazer as perguntas difíceis, aquelas que todo mundo vai responder “ah, isso é óbvio”, mas se pedir pra detalhar, ninguém consegue colocar em palavras. Deixa eu pegar um exemplo super simples. Lembra as aulas de física básica que você teve no colégio? Lembra daquela formulinha besta de S = v x t? Ou espaço é igual a velocidade vezes o tempo?&lt;/p&gt;

&lt;p&gt;Com essa fórmula, se eu estiver num carro a uma velocidade média de 100 quilômetros por hora, e dirigir por exatamente 2 horas, significa que no final vou ter percorrido 200 quilômetros. Você já parou para pensar a implicação disso? Não só dessa mas de todas as fórmulas que aprendeu no colegial? Com tudo que Newton descobriu e documentou no que chamamos de “física clássica” eu não consigo só prever onde meu carro vai estar daqui 2 horas, mas até a órbita de planetas do sistema solar. Dá pra prever onde Marte vai estar em relação à Terra daqui 20 anos, e dá pra saber onde estava 20 anos atrás. Essas fórmulas descrevem o presente, confirmam com os dados vistos no passado e conseguem prever o futuro! Já parou pra pensar nisso?&lt;/p&gt;

&lt;p&gt;Um século atrás isso levantou várias questões filosóficas e morais. Aqui eu posso estar sendo simplista porque realmente não sou estudante de filosofia, mas se começar a pensar no que acabei de explicar vai cair no ramo da filosofia conhecida como Determinismo, que na real teve sua origem na Grécia antiga, 7 ou 8 séculos antes de Cristo, época dos primeiros e verdadeiros “influencers”, como Aristóteles. Mas no Ocidente ela é associada com a física clássica de Newton e a busca pelas leis e fórmulas que descrevem a realidade como algo estático e previsível. Vocês entendem a implicação das fórmulas? Se eu tenho na fórmula todas as variáveis que causam um efeito, como velocidade e aceleração, e se eu consigo extrapolar a variável de tempo pro futuro, e se o resultado tem que bater com a realidade, então em essência, estou prevendo o futuro.&lt;/p&gt;

&lt;p&gt;Por um lado isso parece extremamente agradável, se a pesquisa continuar e conseguirmos desvendar todas as leis da natureza, quem sabe, não existe uma fórmula mágica, determinista, de como ter sucesso? Por outro lado, se parar pra pensar no lado moral, se essas leis realmente existem, o que isso diz sobre coisas como nosso livre arbítrio? Na realidade a gente não tem agência sobre nós mesmos? Se é possível existir leis que prevêem o futuro, então tudo que estamos fazendo agora, neste minuto, eu gravando este video, ou vocês aí assistindo, não são escolhas que fizemos mas sim passos já pré-destinados que não temos nenhum controle? Será que existe uma fórmula pra isso que a gente só não descobriu ainda?&lt;/p&gt;

&lt;p&gt;Na realidade, não. No século XX as chamadas Leis de Newton foram quebradas, caso vocês não saibam. Elas não eram bem “leis”. É por isso que as pessoas tem dificuldade de confiar na ciência. Por um lado mantemos a “Teoria da Evolução de Darwin” ainda como “Teoria” sendo que na verdade já tem bases mais que suficientes pra ser promovida a Lei. E eu sei que alguém já tá indo nos comentários pra discordar, mas esquece, só aceita que dói menos. Mas por outro lado, as “Leis” de Newton não são leis. Deixa eu explicar.&lt;/p&gt;

&lt;p&gt;A física clássica de Newton funciona super bem pra fenômenos com dimensões que vão desde o átomo até o tamanho de algumas estrelas e planetas. Parece um escopo bem grande, mas não é o total, nem de longe. Essa mesma física que consegue prever com boa precisão onde meu carro vai estar depois de dirigir por um determinado tempo ou onde um planeta vai estar em relação à outra daqui alguns anos, falha miseravelmente em tamanhos ou densidades muito maiores que estrelas, particularmente quando falamos de coisas como buracos negros. Ela não consegue explicar da onde vem a gravidade. E falha miseravelmente em tamanhos muito menores que o átomo, o mundo sub-atômico da física quântica.&lt;/p&gt;

&lt;p&gt;No campo cosmológico, Einstein nos trouxe a Teoria Geral da Relatividade e a Teoria Especial da Relatividade. Na prática, a física clássica de Newton é um caso especial, simplificado, da Relatividade. Ou seja, das teorias de Einstein você consegue derivar física newtoniana pra dimensões “pequenas” onde o espaço-tempo parece estático e linear. Quando falamos em escalas de quilômetros, Newton funciona, por isso ainda aprendemos na escola. Mas quando falamos de escalas de anos-luz, velocidade da luz, aí precisa de Einstein. Mesma coisa quando falamos de efeitos quânticos, não dá pra prever onde uma sub-particula vai estar usando Newton.&lt;/p&gt;

&lt;p&gt;Literalmente não dá realmente pra prever onde uma sub-particula vai estar porque no mundo quântico falamos em espaços de probabilidades. Ela pode estar na direita, mas também pode estar na esquerda. As diferentes respostas existem ao mesmo tempo em superposição e só vamos realmente saber exatamente onde depois que colapsarmos o sistema. O mundo quântico não é determinista. A gente sempre acha que toda causa tem um efeito. Mas no mundo quântico temos uma gama de probabilidades. Incerteza e aleatoriedade passam a fazer a diferença. E descobrimos que Deus joga dados. Nada é sagrado, nem as Leis de Newton.&lt;/p&gt;

&lt;p&gt;E como eu sempre aviso: cuidado pra não cair em papo de coach quântico. Como o nome diz, física quântica só se aplica no mundo quântico, sub-atômico. Mas o ponto é que até o colegial, você só aprendeu a encarar a natureza como se fosse algo determinista, causa e efeito, segundo as Leis de Newton, que por coincidência corroboram visões antiquadas de destino ou algum tipo de força maior te guiando num caminho pré-definido. Por intuição, na cabeça de muitos, ou existe uma fórmula mágica determinista pra alcançar algum tipo de sucesso, ou na verdade, você está pré-destinado a não ter sucesso e só aceita isso. E a resposta é bem mais complicada do que isso.&lt;/p&gt;

&lt;p&gt;Onde eu quero chegar é primeiro fazer vocês entenderam o seguinte: atividades curtas, em dimensões de seres humanos como um ingrediente de cozinha, ou em tempos curtos com poucas variáveis, como a duração de uma viagem pra uma cidade vizinha, existem sim, fórmulas Newtonianas, que conseguem prever como partir de um ovo cru e chegar em um ovo com gema cremosa em 7 minutos. E se o destino da sua viagem fica a 200 quilômetros daqui, se conseguir manter uma velocidade média de 100 quilômetros por hora, vai chegar em aproximadamente 2 horas. Ordens de grandeza muito diferente disso, nem machine learning, nem inteligência artificial, vão conseguir ajudar. Tudo passa a ser uma probabilidade.&lt;/p&gt;

&lt;p&gt;Aleatoriedade é um campo inteiro de pesquisas que levam a física do Caos. Pequenas variações iniciais que pareciam insignificantes e podem levar a grandes efeitos difíceis de prever. É uma das razões de porque apesar de todo o poder computacional que nós temos, até hoje é impossível prever a meteorologia com precisão, todos os dias do ano, sem errar. Conseguimos sim, ter uma noção geral, mas nunca vamos acertar como exatamente vai estar o tempo daqui exatamente 1 ano. Não digo no mesmo mês de daqui um ano, mas se vai estar chovendo ou não no dia 1o de junho do ano 2033, volume exato de precipitação entre quais horários. É impossível, e se acertarmos não vai ser porque previmos, vai ser sorte ou coincidência.&lt;/p&gt;

&lt;p&gt;Tudo isso que falei foi pra concluir o seguinte: sim, pra coisas com tempo de duração relativamente curto, é possível ignorar muitas dessas variáveis e aleatoriedade, e conseguir seguir um conjunto de passos que te leva de algo simples como um ovo cru pra um ovo cozido decente. É um período de tempo e dimensões onde conseguimos reduzir objetivos a passos deterministas. Porém, estique pra um período de tempo maior, e de repente variáveis que estávamos ignorando começam a aparecer e a dar efeito, até perdermos o controle. Pra todos os efeitos e propósitos, toda receita funciona pra coisas pequenas na dimensão de horas ou dias, mas nunca anos ou décadas, e pra todos os efeitos e propósitos, considere que é impossível ter uma receita determinista, repetível, de como ter sucesso na vida ou mesmo num projeto curto. Quanto mais complexo o objetivo e mais tempo for necessário, mais a aleatoriedade vai quebrar toda fórmula e todo planejamento.&lt;/p&gt;

&lt;p&gt;Entenderam? É impossível prever o futuro. Os deterministas newtonianos perderam faz mais de um século já, desde quando descobrimos Relatividade, Física Quântica e Teoria do Caos. Por um lado, não perdemos nosso livre arbítrio. O futuro é incerto e suas decisões afetam o futuro, portanto você é responsável por elas. Mas por outro lado, é seguro dizer que é impossível ter uma receita de sucesso. Mas, cuidado, não é a mesma coisa dizer que estamos totalmente à mercê da aleatoriedade. Isso também nos tira livre arbítrio. O futuro não depende de só ter sorte ou ter azar. Na realidade ela depende das suas decisões somado a ter sorte ou azar. Se tudo fosse só aleatório, nós estaríamos vivendo em caos total. Mas você sabe que não é verdade. Então deixa eu explicar o que é a realidade.&lt;/p&gt;

&lt;p&gt;Imagino que estejam confusos. Lembra em episódios passados do Guia de Aprendendo a Aprender ou o que Cursos não tem ensinam? Eu disse que todos vocês sofreram lavagem cerebral a vida inteira. Até se formar no colégio sua vida toda foi pré-determinada. Você sabia passo a passo o que vinha pela frente. Toda matéria era explícita. Você podia escolher não estudar, mas só. De repente você cai no mundo real, e descobre que não tem matéria pré-definida, não tem prova bimestral, não adianta tirar 5. Você não repete mais de ano, mas mesmo assim parece que todo ano é uma repetição do mesmo fracasso anterior. E por isso fica desesperado tentando achar de novo uma receita, como tinha no colégio. Você é um junkie, viciado em passar de ano artificialmente como se isso representasse algum tipo de progresso.&lt;/p&gt;

&lt;p&gt;Toda vez que vem à sua cabeça “vou perguntar a receita do sucesso” pra alguém, você está com a noção que existe uma fórmula determinista, uma fórmula newtoniana, pra ter uma carreira de sucesso, ou um projeto de sucesso ou uma empresa de sucesso ou sucesso em geral. Você acha que existe uma forma de seguir passos pré-definidos, algum segredo que ninguém te contou, mas que se conseguir achar, seguir, trabalhar duro todos os dias, garantidamente vai atingir esse sucesso.&lt;/p&gt;

&lt;p&gt;E eu contei toda essa história pra te confirmar o que você já sabia mas não queria admitir: esse segredo não existe. E só trabalhar duro não te garante absolutamente nada. Vou repetir: só trabalhar duro, sem método, não adianta nada. Na verdade, até tem um tipo de “segredo” sim, mas você provavelmente não vai acreditar que funciona. Na verdade é irônico que vocês preferem acreditar numa mentira bonita do que numa verdade inconveniente. Antes de chegar nela vou precisar contar outra história pra vocês que começa com um outro tema relacionado, chamado “qualidade”.&lt;/p&gt;

&lt;p&gt;A primeira vez que parei pra pensar no termo qualidade, foi numa matéria eletiva na faculdade, lá no meio dos anos 90, alguma coisa sobre Qualidade Total. E eu quero que vocês parem dois segundos pra pensar: vocês conseguem definir a palavra &quot;qualidade&quot;? O que significa alguma coisa ter qualidade? Pra alguns, qualidade significa &quot;ter muitas funcionalidades&quot;. Pra outros significa &quot;ter a menor quantidade de defeitos&quot;. Também pode significar &quot;o que deixa o cliente mais satisfeito&quot;. Você consegue pensar numa definição não-ambígua, objetiva, que teria consenso de todo mundo? Acha que é possível?&lt;/p&gt;

&lt;p&gt;Qualidade, assim como aprendizado, empreendedorismo, gestão, sucesso, é outro termo abstrato e subjetivo. O que pra mim parece ser de qualidade, pra outra pessoa pode ser o oposto. Um exemplo bobo é uma coisa que quem me segue no insta já me viu reclamando. Pra grande maioria das pessoas aqui no Ocidente, o que você encontra de sushi num buffet de churrascaria é de excelente qualidade. Inclusive, quanto mais cream cheese, mais qualidade parece ter. Não é isso? Agora, pra mim, e pra maioria dos Orientais, um sushi com cream cheese faz todos os melhores sushiman que já existiram no Japão virar nos seus caixões de desgosto. E o ponto é que no contexto de Ocidente, eu provavelmente estou errado: se existe demanda, provavelmente a qualidade não é zero, especialmente na definição que falei antes de &quot;o que deixa o cliente mais satisfeito&quot;. Se o cliente tem mau gosto, quem sou eu pra julgar.&lt;/p&gt;

&lt;p&gt;Mas deixa eu voltar um pouco na história. No fim dos anos 70, começo dos anos 80, o mundo Ocidental se viu numa encruzilhada estranha. Assistam este trechinho do filme De Volta pro Futuro 3, que saiu no fim dos anos 80.&lt;/p&gt;

&lt;p&gt;(... trecho)&lt;/p&gt;

&lt;p&gt;Esse trecho é da hora, porque o Doc versão de 1955 fica chocado quando o Marty de 1985 fala que componentes eletrônicos japoneses são os melhores, mas na visão dos anos 50, se um componente dava problema, obviamente era por ser japonês. Lembrem-se, em 1945 o Japão perdeu a 2a Guerra Mundial, foi tomada pelos americanos, proibida de se armar de novo, e teve que reconstruir o país, depois de ter tomado duas bombas nucleares no rabo. Fazer o que, meio que mereceu. Mas o resultado é que o país inteiro ficou totalmente quebrado. Imagina reconstruir um país não do zero, mas do negativo.&lt;/p&gt;

&lt;p&gt;Em pouco mais de 30 anos, só 3 décadas, ela foi de um país devastado que não conseguia produzir nada de qualidade, pra chegar nos anos 80 com os melhores produtos eletrônicos do mundo. Pra você que é novo, quanto é 30 anos? Se contar de hoje, é mais ou menos a partir dos anos 90 pra cá. Alguns podem achar que é pouco tempo, outros que é muito tempo, mas considere que essa é mais ou menos a idade da internet comercial. Nesse período nós evoluímos de todo mundo fazer ligações telefônicas de orelhão, pra quase todo mundo ter um computador ligado na internet no bolso. 30 anos é o tempo da minha carreira inteira.&lt;/p&gt;

&lt;p&gt;Mas o ponto é que quando chegamos nos anos 80, pela primeira vez os Estados Unidos realmente ficaram com medo da invasão de produtos japoneses. Outro fator, foi na Inglaterra que começou a Revolução Industrial no fim do século 19. E nos anos 80 foi quando pela primeira vez ela tava importando mais produtos manufaturados do que exportando. E o Oriente, liderado pelo Japão, tava causando uma invasão inesperada, mas em vez de uma invasão de mísseis e explosivos, era uma invasão no sistema econômico, via produtos de consumo de alta qualidade e mais baratos que da concorrência.&lt;/p&gt;

&lt;p&gt;E não só do Japão, mas de outros países orientais que ficaram conhecidos como os Tigres Asiáticos, incluindo Cingapura, Coréia do Sul, Hong Kong, Taiwan. Enquanto os Estados Unidos ficaram perdendo tempo com a Guerra do Vietnam e com a recessão até os anos 70, os asiáticos cresceram monstruosamente. Tudo bem que logo depois, o Japão meio que se fodeu com o estouro da bolha imobiliária e a grande recessão nos anos 90 que se segue até hoje. Essa foi a primeira década perdida deles. E nós notamos isso quando as marcas mais reconhecidas de produtos asiáticos eram Sony, Mitsubishi, Toyota, Honda e agora são Samsung, LG, Xiaomi, Huawei.&lt;/p&gt;

&lt;p&gt;Mesmo assim, no começo dos anos 80 tava claro que o Ocidente precisava se modernizar. E por isso todos os olhos se voltaram pra Ásia, em particular, pro Japão. Que diabos eles estavam fazendo que conseguiam produzir produtos de alta qualidade em larga escala a preços baixos? Os governos e empresas ocidentais estavam realmente determinados a quebrar esse código. Eles mandaram pesquisadores e delegações pro Japão pra entender esse fenômeno. Nunca é um único fator. Tem o contexto histórico, tem a diferença cultural, tem até fator sorte. Por isso nunca acredite numa única resposta mágica que seja bala de prata.&lt;/p&gt;

&lt;p&gt;Mas uma das coisas que eles descobriram, mais próximo de uma receita, foi o tal Modelo Toyota de Produção ou TPS. Esse modelo descreve o funcionamento e filosofia de uma fábrica inteira, não só a linha de produção, mas desde a contratação até a evolução das pessoas. O Ocidente se focou mais em técnicas, como o sistema de pull em vez de push, pra evitar desperdícios, que muitos de vocês já devem ter ouvido falar, o tal do Kanban. E o conceito que é o coração de praticamente toda metodologia que surgiu depois disso, o ciclo de melhoria contínua conhecido como Kaizen.&lt;/p&gt;

&lt;p&gt;No começo dos anos 80 foram publicados vários livros relacionados mas tem 2 que preciso mencionar. O primeiro foi o famoso livro Out of Crisis, ou Saindo da Crise, do Dr. Edward Deming, famoso pelo ciclo PDCA que é similar ao modelo Kaizen da Toyota. PDCA pra quem nunca viu, é o ciclo básico de Planejar, Fazer, Checar, Ajustar e repetir. Já falo mais sobre isso mas esse livro tornou famoso os 14 pontos de gerenciamento de Deming como: criar constância de propósito, ou parar de depender de inspeção pra ter qualidade. Todo mundo que tem posição de gerência precisa pelo menos saber o que são esses pontos. Alguns são óbvios e ninguém faz direito até hoje, justamente porque ignora a história. Vou deixar um link com o resumo dos 14 pontos na descrição abaixo.&lt;/p&gt;

&lt;p&gt;O segundo livro eu vejo coaches porcaria falando dele até hoje. É sobre o TOC, ou Teoria das Restrições do Dr. Goldratt. O livro onde ele apresenta essa teoria é o &quot;The Goal&quot;, publicado em 1984, no formato de uma novela, uma historinha mesmo, de um cara trabalhando numa fábrica e otimizando processos. Em resumo muito resumido ele fala sobre a idéia de resolver um problema de cada vez, a analogia de &quot;qual é a força total de uma corrente?&quot; E a resposta correta é &quot;a força do elo mais fraco&quot;. Portanto conserte o elo mais fraco primeiro. Mas agora outro anel vai se tornar o elo mais fraco, então conserte ele. E vai indo um anel de cada vez. É outra forma de explicar melhoria contínua.&lt;/p&gt;

&lt;p&gt;Mas mais influente do que todos eles, acho que foi a Motorola por volta de 1986. Ela também foi afetada pela invasão japonesa e conseguiu dar a volta por cima criando uma nova metodologia que chamou a atenção de diversas outras empresas como a poderosa General Electric, de Jack Welsh. Foi a metodologia Six Sigma, que tem como objetivo fazer o processo de fabricação não produzir mais que 3.4 DPMOs ou 3.4 defeitos por milhão de oportunidades. Essa metodologia é bem mais detalhada, densa e tratada praticamente como uma doutrina que deve ser seguida à risca. Se você acha engraçado uma posição como scrummaster hoje em dia, Six Sigma tem faixas pretas e faixas verdes. E eles levam a sério saporra.&lt;/p&gt;

&lt;p&gt;Se pra você um CEO influente é alguém como Elon Musk ou Jeff Bezos, nos anos 90 só se falava de Jack Welsh da GE. A Motorola inventou isso de Six Sigma mas foi a GE que tornou ela famosa. O objetivo era esse mesmo: como aumentar qualidade e diminuir custos ao mesmo tempo? E os resultados da Motorola foram impressionantes na época. O objetivo deles era melhorar 10X ao longo de 5 anos e eles ainda aumentaram a aposta de melhorar pra ser 10X a “cada” dois anos ou 100X em quatro anos! E em 10 anos de 1987 até 1997 eles cresceram as vendas em 5 vezes, lucros de 20 porcento ao ano, economia de uns USD 14 bilhões, e os preços das ações da Motorola subiram pra uma taxa de mais de 21 porcento ao ano.&lt;/p&gt;

&lt;p&gt;Era isso que a gente lia nos anos 90. Mas olhando agora de 2022, obviamente você pode ver que nem mesmo os criadores do Six Sigma tiveram sucesso por tempo indeterminado. Quantos de vocês tem algum produto da Motorola hoje em dia? Não só isso, em paralelo, eu falei do tal Modelo Toyota, que todo mundo de gestão de desenvolvimento de software adotou também, o tal de Lean. Povo startupeiro ficou doido com o Lean Startup do Eric Ries em 2009, inspirado no mesmo Lean da Toyota. Mas se era tão bom assim, era pra gente estar vendo muito mais produtos Toyota ou Motorola hoje em dia, não? Cadê?&lt;/p&gt;

&lt;p&gt;Vários autores já tentaram responder a essa pergunta como Clayton Christensen e seu famoso livro The Innovator’s Dillemma, ou O Dilema do Inovador. Ou Chan e Renée no livro The Blue Ocean Strategy. De novo, variáveis imprevisíveis que a fórmula não tem como prever, somado a um comportamento de sentar em berço esplêndido, em bom português, achar que já ganhou e desacelerar até concorrentes menores e mais motivados passarem na sua frente. Ou condições de mercado imprevisíveis como crises aparecerem quando você não tava preparado. Ninguém consegue prever o futuro e muitas vezes você fracassa mesmo, mesmo achando que sabe tudo.&lt;/p&gt;

&lt;p&gt;O Ocidente se assustou com o Japão nos anos 80 mas ao longo dos anos 90 e 2000 eles aceleraram e alcançaram patamares semelhantes, daí tivemos a recessão do mercado japonês, e o surgimento do milagre Chinês no século XXI. O mundo não é estático e ninguém está de braços cruzados esperando. Todo mundo já já superou esses métodos. Mas isso não quer dizer que o Modelo Lean ou o Six Sigma são ruins, só quer dizer que a geração de administradores depois dos criadores dos modelos originais fracassaram em continuar evoluindo o método, pelo menos fracassaram em andar rápido o suficiente pra não ficarem pra trás.&lt;/p&gt;

&lt;p&gt;Eu trago esses casos porque hoje em dia todo mundo vai encontrar um monte de jargões e conceitos que nasceram nos anos 80 mas todos repetem e usam sem entender porque. Enquanto o mundo de fábricas, manufatura e gestão empresarial em geral estava passando por essa revolução de qualidade ao longo dos anos 90, o mundo de desenvolvimento de software também estava passando por outras revoluções. A transição do mundo de mainframes e computadores inacessíveis para os microcomputadores nos anos 70 e 80 e depois a internet comercial dos anos 90 aos anos 2000. E não por acaso, nós pegamos inspirações no mundo de manufatura.&lt;/p&gt;

&lt;p&gt;De onde veio essa idéia de Ágil que todo mundo fala hoje em dia? Da onde saiu esses rituais de planejamento, retrospectiva, sprints e Scrum? De onde veio essa idéia de desenvolvimento baseado em testes? Depois de estudar tudo que saiu dos anos 80 até hoje em gestão, eu penso que todo mundo chegou na mesma conclusão mais ou menos ao mesmo tempo. Alguns foram inspirados por autores como Deming, alguns copiaram uns aos outros mesmo, e alguns realmente pararam pra pensar no problema e naturalmente chegaram a soluções parecidas. Os princípios de todos eles são simples e qualquer um conseguiria chegar em conclusões semelhantes. E chegaram, na real.&lt;/p&gt;

&lt;p&gt;No coração do mundo Lean da Toyota está um princípio chamado Kaizen, ou melhoria contínua. Foi Deming quem definiu esse princípio em sua forma mais geral no chamado ciclo PDCA, que é sigla pra Planejar, Definir, Checar e Agir. No mundo da Toyota o conceito é que nenhum problema é resolvido permanentemente na primeira tentativa. Todo problema identificado deve dar origem a uma contramedida. E essas contramedidas podem ser de curto prazo, tipo um band-aid pra corrigir rápido, mas seguido de contramedidas de longo prazo, que buscam resolver o problema na raíz. Daí que vem técnicas como os famosos 5 Ps ou 5 porques. Deixa eu pegar um exemplo do livro.&lt;/p&gt;

&lt;p&gt;Digamos que o problema identificado seja “as unidades de fabricação por hora estão abaixo da meta”. Por que? “Não conseguimos produzir peças suficientes por hora”. Por que? “Estamos perdendo oportunidades de produção”. Por que? “Perda de tempo”. Por que? “Perdas de tempo de ciclo”. Por que? “A carga da máquina leva muito tempo”. Por que? “O operador precisa caminhar um metro e meio pra ir pegar o material”. Opa. Esse parece ser a raíz do problema, e o que podemos fazer?&lt;/p&gt;

&lt;p&gt;Dar um jeito de reduzir essa caminhada. Portanto, isso vai diminuir o tempo pra carga da máquina. Portanto, isso vai reduzir perdas de tempo de ciclo. Portanto, isso vai aumentar o tempo disponível. Portanto, isso vai aumentar as oportunidades de produção. Portanto, isso vai produzir peças suficientes por hora. Portanto, talvez consigamos atingir as metas de unidades de fabricação por hora.&lt;/p&gt;

&lt;p&gt;Ou seja, temos o problema que estamos fabricando poucas peças por hora. A hipótese da raíz do problema é que o operador precisa caminhar demais toda hora pra pegar material e recarregar a máquina. Uma possível solução é trazer mais material pra perto do operador. Mas isso vai resolver de verdade? Só nos restar testar. Depois de um certo período de tempo, que poderíamos chamar de “sprint”, vamos verificar a quantidade de peças produzidas por hora e verificar se realmente voltamos a atingir a meta por hora. Se sim, podemos definir uma contramedida de longo prazo talvez colocando um armazém mais próximo com os materiais pra todos os operários. Se não, analisamos de novo e buscamos uma nova hipótese pra testar.&lt;/p&gt;

&lt;p&gt;O que eu falei pareceu meio óbvio? Esse exemplo é da página 334 do manual de aplicação do Modelo Toyota. Um livro de 423 páginas. Isso não é o primeiro capítulo, é uma das partes finais do manual. O que eu acabei de explicar é o processo PDCA. Planejar uma contramedida que é diminuir o tempo de carga do operador. Realizar a contramedida que é estocar mais material próximo dele. Daí checar, medir o tempo médio de produção e validar que a meta foi atingida ou não. Finalmente, agir em estabelecer uma contramedida de longo prazo ou procurar outra possível raíz do problema. Resolveu? Então o elo mais fraco mudou pra outro elo, repete o processo e assim por diante.&lt;/p&gt;

&lt;p&gt;No mundo Six Sigma não se fala de PDCA e sim de DMAIC, que é sigla pra Defina, Meça, Analise, Melhore, Controle. É um pouco mais específico no sentido que uma vez resolvido o problema, controle e garanta que o problema não se repita. Além disso as métricas também são estatisticamente mais exigentes, a começar pelo objetivo do nome: alcançar o nível Six Sigma que significa não mais que 3,4 defeitos por milhão de oportunidades ou 99,9997% de qualidade. Mas em princípio, PDCA e DMAIC são a mesma coisa.&lt;/p&gt;

&lt;p&gt;Sabe o que mais é a mesma coisa? Um sprint de Scrum. Planeje as stories, trabalhe e programe, no final faça retrospectiva, revise as métricas, ajuste o processo, recomece outro sprint. Isso não é por acaso. Existe outro ciclo parecido que é a base da civilização moderna. Sim, o tal método científico que todo pseudo-cientista de Twitter esquece que existe. O método começa pela observação, que deve ser sistemática e controlada, a fim de que se obtenham os fatos científicos.&lt;/p&gt;

&lt;p&gt;O método é cíclico, girando em torno do que se denomina teoria científica, a união do conjunto de todos os fatos conhecidos e de um conjunto de hipóteses testáveis. Os fatos, embora não necessariamente reprodutíveis, têm que ser necessariamente verificáveis. As hipóteses têm que ser testáveis frente aos fatos, e por tal, falseáveis. As teorias nunca são provadas e sim corroboradas. Se você está focado em tentar provar alguma coisa via algumas poucas evidências, você não está praticando o método científico, está só se inventando uma história.&lt;/p&gt;

&lt;p&gt;Mais importante, o método científico é cíclico. Se a hipótese falhou, checam-se os novos fatos, ajusta-se a teoria, e verificamos as previsões de novo. A cada nova tentativa, aprendemos novos fatos. Melhoramos a teoria, continuamente. De novo, melhoria contínua, ou Kaizen. Todos os modelos ciclicos, PDCA de Deming, DMAIC de Six Sigma, Kaizen de Lean, Sprints de Scrum, MVPs de Lean Startup, tem em comum o fato de serem métodos de resolução de problemas desconhecidos. Eu iria um passo além e dizer que esse é o único método que conhecemos hoje pra resolução de qualquer problema desconhecido.&lt;/p&gt;

&lt;p&gt;É meio anticlimático, mas eu falei que vocês talvez tenham dificuldade de aceitar que a única solução pra todos os seus problemas se chama literalmente “tentativa e erro”. Qual a melhor forma pra estudar? Tentativa e erro. Qual a melhor forma de gerir um projeto? Tentativa e erro. Qual a melhor forma de administrar uma empresa? Tentativa e erro. Qual a receita pra ter sucesso? Tentativa e erro.&lt;/p&gt;

&lt;p&gt;Todos que tentaram seguir receitas, fracassaram. O mundo não é determinista nem newtoniano. A fórmula que você está procurando não existe. A coisa mais frustrante que existe no mundo corporativo até hoje, em particular com empresários amadores, é justamente essa idéia de trazer consultores com balas de prata, que ficam ali um, dois anos tentando implementar um monte de ferramentas e métricas, e no final o resultado é uma droga. É o mesmo problema do monte de gente confiando em cursos mágicos que garantem resultados irreais e achar que certificados garantem alguma porcaria. Todos vocês estão cometendo o mesmo erro, que foi o que a Toyota dos anos 80, a GE dos anos 90, a Apple dos anos 2000 conseguiram melhor que os outros: resolver um problema de cada vez e melhorando um passo de cada vez de maneira sistemática.&lt;/p&gt;

&lt;p&gt;Óbvio, quando eu falo “tentativa e erro” não é só sair tentando qualquer coisa e se contentar com todos os erros. Tentativa e erro não é isso. Primeiro, é não errar a mesma coisa mais de uma vez. Isso tem que ser sistemático, é o ítem de controle do DMAIC de Six Sigma. Segundo, é testar logo, é a contrapartida de curto prazo de Lean da Toyota. Terceiro, é ter obcessão por evitar todo e qualquer tipo de desperdício, que é a raíz de toda metodologia de qualidade. Quarto, é não copiar ninguém, e sim identificar e resolver um problema de cada vez que você tenha, sistematicamente, disciplinadamente, obcessivamente, num nível de psicopata mesmo.&lt;/p&gt;

&lt;p&gt;Isso me lembra um meme de desenvolvimento de software que alguns de vocês já devem ter visto alguma versão. Lembra os 5 Ps que falei antes? Por que vocês não escrevem testes pras novas funcionalidades? Porque não temos tempo. E por que não tem tempo? Porque tem muita coisa pra fazer e o sistema tem muitos bugs inesperados que nos atrasam. E por que tem tantos bugs? Porque o sistema não tem testes. E por que vocês não escrevem testes? Porque não temos tempo! - É impressionante que toda desculpa se resume a “não tenho tempo”. Quer uma novidade? Ninguém nunca tem tempo. A diferença entre eu e você é que eu não uso tempo como desculpa. Se tem uma coisa que todo mundo tem igual é tempo, a diferença é que alguns priorizam as coisas melhores do que outros. Por que vocês acham que um dos primeiros videos que fiz neste canal é justamente sobre priorização?&lt;/p&gt;

&lt;p&gt;Se voltar pra você enquanto estudante. Tem milhões de coisas que você não sabe. E isso deixa você explodindo em ansiedade. Você gasta um tempo fazendo cursos ou seguindo tutoriais. Mas parece que não está evoluindo. O tempo passa, a quantidade de coisas que você não sabia parece que só aumenta. E no desespero começa a ouvir coaches e gurus vendendo cursos mágicos que prometem que vão te acelerar. E você vai na conversa.&lt;/p&gt;

&lt;p&gt;Seja tentando descobrir como estudar, ou como gerir um projeto, ou como administrar uma empresa, você sempre entra num momento de desespero e tenta achar alguma fórmula mágica. Você tenta sair do caos e encontrar alguma ordem, seja lá qual ordem, pra tentar conseguir algum progresso. Ordem e Progresso. Eu pessoalmente sempre achei essa frase muito estranha. Ordem não leva ao progresso. Ordem no máximo mantém as coisas no mesmo lugar, o que não é progresso, mas o mais natural é que só ordem eventualmente deteriora e retorna à desordem. É inevitável. Até se falarmos de novo em termos de física, é a 2a lei da termodinâmica. Todo sistema fechado tende a aumentar a entropia, que é o grau de desordem. É literalmente uma lei da física.&lt;/p&gt;

&lt;p&gt;Ordem e progresso é o lema dos positivistas de Auguste Comte, ironicamente ela é pró-ciência, mas derivada da época da Revolução Industrial e realmente se acreditava que o pré-requisito pro progresso seria criar ordem. Obviamente é um pensamento já ultrapassado, assim como o determinismo newtoniano. Mas o que eu quero argumentar é que ordem não leva ao progresso. E alguns poderiam concordar dizendo que caos leva ao progresso. Mas anarquia absoluta também não leva a progresso. Então qual a saída?&lt;/p&gt;

&lt;p&gt;Já que falei da física que você aprendeu no colégio, lembra outra coisa que aprendeu naquela época? O que acontece quando você ferve água e ele chega no ponto de ebulição de 100 graus celsius no nível do mar? Ele começa a virar vapor certo? Por outro lado se congelar essa água pra baixo de zero graus celsius, ela muda de líquido pra sólido. São as mudanças de fases de um material. Isso é transição de fase de um sistema em equilíbrio. Não vou tentar explicar termodinâmica, nem sou especialista e é muito fácil fazer metáforas erradas, mas deixa eu só arriscar um pouco. A parte importante é só pra lembrar que existe esse fenômeno de transição de fases de um estado de baixa entropia pra um estado de mais entropia, da ordem do gelo pro caos do vapor e de volta pra líquido.&lt;/p&gt;

&lt;p&gt;Eu gostaria de ter um pouco mais de bases pra conseguir explicar Einstein, Podolski, Rosen, condensado Bose-Einstein, transição de fase de não-equilíbrio. Quem for físico assistindo, vejam se conseguem explicar em português nos comentários abaixo. Mas vamos nos contentar só com um conceito. A entre-fase entre uma fase e outra, a fronteira que separa uma fase de outra durante sua transição, que é conhecido como Edge of Chaos ou beira do caos, como foi cunhado nos anos 80 na Teoria do Caos por Norman Packard. Mas é na biologia que esse conceito me chamou mais a atenção, em particular no fenômeno de adaptabilidade de sistemas complexos não-lineares. Eu falei um pouco sobre isso no episódio sobre Organizações, depois assistam.&lt;/p&gt;

&lt;p&gt;Eu sei que tá ficando bem meta isso, mas aguentem firme. A observação interessante é que todos sabemos que ao longo de milhões de anos, todos os seres biológicos sofrem mutações e via seleção natural temos a evolução das espécies. Mas interessante como apesar de tantas mutações, apesar da segunda lei da termodinâmica que falei antes, os ecossistemas não decaem em caos. Fala-se em adaptabilidade na beira do caos. E falando em termos mais gerais, beira do caos é usado como metáfora pra descrever outros fenômenos difíceis de explicar em sistemas complexos, como uma sociedade por exemplo. Se tudo tendesse ao caos e aleatoriedade, como explicar que todos nós, conseguimos conviver numa sociedade organizada e estruturada que, apesar de altos e baixos, não regride pra puro caos, mas parece avançar. Devagar e sempre?&lt;/p&gt;

&lt;p&gt;Beira do caos é o que temos na transição de fase entre ordem e aleatoriedade ou caos. Não temos nem ordem e nem caos, mas ficamos indo e voltando entre ordem e caos. E se isso tá ficando esquisito, existem pesquisas que argumentam que nosso cérebro, nossa rede neutral, funciona mais ou menos na beira do caos. E em particular no campo de Deep Learning existem pesquisas de aplicação do conceito de beira do caos pra melhorar inteligência artificial. Ainda é um campo aberto e em estudo, mas onde eu quero chegar é que nem ordem e nem caos levam ao progresso, nem à adaptabilidade e evolução, mas sim um estado transitório entre ordem e caos, que é a beira do caos.&lt;/p&gt;

&lt;p&gt;E aqui vem uma hipótese minha. Pra mim, beira do caos, é o que todo mundo realmente quer dizer quando falamos em “encontrar balanço”, ou “ser equilibrado”. Quando falamos isso, todo mundo pensa automaticamente em ordem, num sistema estático, fixo. Mas eu acho que isso tá errado. Balanço é justamente balançar entre ordem e caos. Por um lado você entende balanço como ordem, como equilíbrio estático, que significa uma balança com os dois lados sempre imóveis e fixos. Pra mim balanço é beira do caos, pendendo entre ordem e caos de tempos em tempos. É um equilíbrio, só que é dinâmico. Hora é +10 pontos, hora é -10 pontos, na média dá zero, saca. Esse é o “segredo”. E como fazemos isso? Disciplinadamente evoluindo um passo de cada vez num ciclo entre ordem e caos.&lt;/p&gt;

&lt;p&gt;Se não ficou claro ainda: todos os ciclos que eu expliquei antes: o PDCA de Deming, o DMAIC de Six Sigma, o Kaizen de Lean, o Sprint de Scrum. Durante um sprint de Scrum exercitamos a ordem: trabalhamos linearmente, uma story de cada vez, ao longo da semana. No final fazemos retrospectiva e no review adicionamos um pouco de caos, uma mudança com objetivo de testar uma nova hipótese de resolver um problema. Iniciamos um novo sprint, ordem de novo. Terminamos com um novo review e adicionamos mais um pouco de caos, que é sair um pouco da ordem anterior. Assim como uma mutação genética esperando uma seleção natural no mundo biológico. Ou como nossos neurônios funcionam pra promever aprendizado.&lt;/p&gt;

&lt;p&gt;É por isso que você está constantemente ansioso e inseguro quando está estudando. Você tem um programa pra seguir, aulas pra assistir, trabalhos pra fazer, materiais pra ler, mas não sente que tá evoluindo. E é claro. Provavelmente não está mesmo, só tá fazendo o tempo passar. Quantas vezes você pára e testa o que sabe? Realmente testar. No caso de programação, parar e fazer um programinha simples do zero diferente de tudo que já foi pedido nos exercícios? Fora da zona de conforto, tirando as rodinhas da bicicleta? Você nunca faz isso. E quando tenta, não consegue e desiste rápido. Mesmo quando um professor pára pra dar uma prova, isso não testa nada, no máximo que você é bom de decorar as coisas. Mas decorar coisas não torna ninguém um bom profissional em nada.&lt;/p&gt;

&lt;p&gt;O ideal seria que os professores dessem provas surpresa o tempo todo. E que as aulas fossem ajustadas e modificadas depois dessas provas. E as provas não tivessem nada de decoreba e sim de raciocínio lógico, que desse pra deduzir a maioria das respostas só por indução e bom senso. Todo mundo aqui ia odiar isso, e eis o motivo: seria caótico, seria fora de ordem, poderia até parecer injusto. Mas eis a realidade: o mundo real é caótico, fora de ordem e injusto, mas você nunca treinou pra isso. Sua reação instintiva é tentar encontrar ordem onde ela não existe. E aí vocês ficam que nem idiotas perguntando pra influencers o que eles acham que você deveria estudar, que cursos deveria fazer, que livros deveria ler. Não importa. Nada disso importa se você não está preparado pra viver à beira do caos. Abandonar esse conceito que existe uma ordem, e abraçar momentos de caos e incerteza como coisas boas.&lt;/p&gt;

&lt;p&gt;Se ainda não ficou claro vou repetir. É impossível prever o futuro. Tentar prever é um exercício em futilidade. Tentar planejar tudo que vamos fazer anos pra frente é inútil. Em vez disso planejamos só o que é possível: uma semana pra frente, um mês pra frente, não muito mais. Agora executamos. Seja estudo, seja trabalho num projeto, seja administração de uma empresa. O importante é parar depois de um período curto de tempo e checar a hipótese. Conseguimos atingir essa pequena meta? O que deu errado? O que dá pra melhorar? Adicionamos um pouco de caos, reformulamos a hipótese, e agora continuamos executando por mais um período de tempo. Paramos de novo. E repetimos esse ciclo, resolvendo um pequeno problema de cada vez, melhorando um pequeno aspecto de cada vez, mas garantindo que não estamos cometendo os mesmos erros, e que a melhoria que fizemos vai se acumular.&lt;/p&gt;

&lt;p&gt;Você tem dúvida se vai gostar de um determinado curso de 10 meses? Nunca, jamais, sob hipótese nenhuma, compre os 10 meses adiantado, isso é estupidez, é tentar prever o futuro - que já estabelecemos que é impossível. Pare de insistir no impossível. Pague 1 mês e teste. Se gostar e ver resultados, pague mais um mês. Ah, mas se pagar os 10 meses de uma vez vai ser mais barato. Não, não vai. Porque se você não gostar, vai jogar 10 meses fora. Por outro lado, se pagar só um mês e não gostar, vai jogar fora só esse 1 mês. Mas e se o curso não der opção de pagar só um mês? Isso é sinal que eles não acreditam no que estão vendendo e precisam já te forçar a pagar tudo de uma vez pra não poder desistir. Isso é um sinal claro que você pode descartar essa opção porque sem nem experimentar já sabemos que se trata de uma grande fraude. Tá vendo, é assim que evitamos ser feitos de idiotas.&lt;/p&gt;

&lt;p&gt;E isso vale pra tudo. Pegue um projeto longo, como garanto que consigo entregar? Eu não garanto. Sempre use a boa e velha estratégia de Jack o estripador: vá por partes. E sempre se lembre do bom e velho Napoleão: dividir pra conquistar. É daí que nasce o conceito de Minimal Viable Product ou MVP de Lean Startup. Em vez de tentar planejar um sistema gigante que parece que vai levar 2 anos pra uma equipe de 10 pessoas entregar, primeiro escolha 1 funcionalidade, em versão simplificada, que é a mais importante do sistema e que 2 pessoas consigam fazer em 2 meses.&lt;/p&gt;

&lt;p&gt;Você vai aprender mais sobre a equipe, sobre o produto, e no final de 2 meses, se não gostar do resultado, só perdeu 2 meses e não 2 anos. E se não ficou claro, todo projeto você define o custo e o tempo primeiro, daí encaixa só o que couber nesse limite. O que não couber, não coube. O errado é definir todo o escopo primeiro e tentar fazer o custo e o tempo encaixar. Isso é impossível e quem faz isso parece um idiota tentando encaixar uma peça redonda num buraco quadrado.&lt;/p&gt;

&lt;p&gt;A mesma coisa vale do ponto de vista de um programador como eu ou você. Na dúvida, comece fazendo um protótipo. Um código que talvez você pode jogar fora. Talvez não, um código que certamente vai jogar fora. Programar não é só digitar código que nem um idiota. É jogar fora a maior parte do que digitar. Escreva e jogue fora. Escreva e jogue fora. Acostume-se a jogar fora o que faz. Nem tudo que você faz é especial, a maior parte nunca é. Nunca vai ser especial se guardar tudo.&lt;/p&gt;

&lt;p&gt;Você precisa jogar fora e produzir mais. Programe um dia inteiro, qualquer coisa, sem nenhum propósito real. E no final do dia, jogue tudo fora. Quantas vezes ouviu alguém dizer isso? Se nunca ouviu falar isso, você só teve péssimos professores. Jogue, tudo, fora. Respira. Começa tudo de novo. Se você economiza e escreve só o que parece que precisa, só pra uma tarefa em especial, e raramente joga alguma coisa fora, eu tenho certeza que você é medíocre.&lt;/p&gt;

&lt;p&gt;Só quando tiver uma versão mínima que se sinta realmente confortável, só aí comece a se comprometer com esse código. Desde o começo você vai cometer erros e, pior, começar a repetir os mesmos erros. Adicione testes unitários. Isso garante que no mínimo o que já foi feito, vai continuar funcionando, mesmo que você continue experimentando adicionando mais coisas. Pare, reflita o que fez, adicione caos, refatore o código, pague parte da dívida técnica, e continue. E repita esse ciclo. É como desenvolvimento de software moderno funciona, e não é à toa.&lt;/p&gt;

&lt;p&gt;Tudo que você, ou faz sem pensar e só vai fazendo no caos, ou tudo que planeja demais e segue cegamente um plano sem parar pra revisar, vai dar errado. Você precisa balancear ordem e caos. O que significa que precisa intercalar períodos de ordem com períodos de caos, é isso que significa achar balanço. E não é aleatório. Em todo intervalo você mede e critica a hipótese original. Modifica a hipótese e tenta de novo. Nem só ordem, nem só caos. Dar um passo pra trás pra conseguir dar dois passos pra frente. Isso é o que eu quis dizer no começo com “tentativa e erro”, uma forma cíclica de sistematicamente melhorar um passo de cada vez. Melhoria contínua ou Kaizen. Sempre balanceando na beira do caos.&lt;/p&gt;

&lt;p&gt;O que estou tentando fazer vocês refletirem é que provavelmente sua forma de pensar ainda é newtoniana determinista positivista, buscando ordem absoluta, evitando caos o máximo possível, com a promessa de progresso. A razão disso é muito simples: vocês tem medo de errar. E nesse medo acaba acreditando que algum guru ou alguém mais especial que você vai dar o camiho das pedras. O segredo é simples: assuma que vai errar, só garanta que vai errar pouco e errar cedo pra não sair tão caro. Daí é só ajustar e repetir.&lt;/p&gt;

&lt;p&gt;A raíz do problema de tudo na sua vida é medo. E é um grande clichê mas ainda bem que você tem medo. Parece que tem gente que não tem medo de nada e sai enfiando a cabeça em tudo, até literalmente rachar a cabeça num poste, ou pior, atropelar alguém. Isso não é coragem, isso é imprudência, ou melhor, pura burrice mesmo. Coragem é dar um passo pra frente, mesmo com medo. Eu falei no video sobre como eu aprendi inglês que o que falta pra maioria dos adultos é voltar a ser criança um pouco. A diferença de porque crianças parecem ter mais facilidade de aprender as coisas que adultos é porque elas não tem medo de serem julgadas, de alguém ridicularizar os resultados ruins que ela vai ter no começo. Elas simplesmente ignoram os comentários dos outros e seguem em frente até conseguir o que querem.&lt;/p&gt;

&lt;p&gt;Elas viram adultos quando a prioridade passa a ser imaginar o que essa entidade abstrata e fantasiosa chamada “outros” pode pensar dela. Será que vão achar que sou lento? Será que vão achar que sou incompetente? Será que vão achar que eu não levo jeito. Será que vão achar que sou velho demais pra começar? Esse monte de “será” inútil se torna sua muleta e sua rotina de desculpas pra não fazer nada e você se torna um adulto realmente inútil, absolutamente chato e totalmente desinteressante. Todo dia que você não faz nada porque acha que é tarde demais só garante uma coisa, que amanhã vai ser tarde demais menos um dia.&lt;/p&gt;

&lt;p&gt;O que você precisa fazer? Ligar o foda-se pra opinião dos outros. Opinião é lixo, opinião não vale nada, e principalmente, a opinião dos outros não paga minhas contas. Felizmente eu pago minhas próprias contas, não dependo de ninguém e por isso eu tô literalmente cagando e andando pro mundo. Se o mundo resolver criticar o que eu estou fazendo falando que é infantil, ou não combina ou qualquer merda dessas, minha resposta é sempre a mesma: vem pegar minhas bolas pra ver se tá gelado.&lt;/p&gt;

&lt;p&gt;Programação é a mesma coisa: você não precisa começar programando nada que seja útil. Só precisa começar a programar qualquer coisa, qualquer lixo, pra perder o medo da janela vazia do editor de textos. Programe qualquer coisa e jogue fora. E se você não consegue imagina o que significa algo inútil, esses dias me lembrei que toda distribuição Linux dá pra instalar um programinha super inútil chamado “cowsay”. Em alguns já vem instalado, mas num Arch linux dá pra instalar com &lt;code&gt;pacman -S cowsay&lt;/code&gt; ou num Ubuntu acho que um &lt;code&gt;apt install cowsay&lt;/code&gt; também funciona. Daí abre um terminal e digita &lt;code&gt;cowsay&lt;/code&gt; seguido de uma mensagem qualquer e olha o que aparece.&lt;/p&gt;

&lt;p&gt;Sim, esse programa desenha uma vaca com caracteres ASCII na tela. Só isso. Você já viu um programa mais inútil que isso? E mesmo assim é um programa que todo mundo que usa Linux faz algum tempo sabe que existe. Pode sair perguntando pros seus amigos mais veteranos e todos eles vão saber disso. Esse programa existe faz anos, foi escrito em Perl e eu acho que é um excelente exercício pra um iniciante de qualquer linguagem. Experimente fazer um cowsay em Javascript, ou em C# ou em Go, não importa. E pra que? Pra absolutamente nenhuma razão, só pra fazer. Jogue fora no final, faça de novo em outra linguagem, faça outra imagem diferente da vaca, como do pinguim Tux que é outra opção desse programa. Só faz. Programar não tem que ter outro objetivo além de programar em si mesmo. O código no final não é importante, só saber que você é capaz. No final pode jogar fora esse código.&lt;/p&gt;

&lt;p&gt;Aliás, eu fico pasmo e chocado quando vejo gente com medinho de começar a escrever código sem objetivo, sem utilidade, com a mentalidade que se ele não fizer algo que funcione perfeito, nem vale a pena começar a fazer. Mas ao mesmo tempo perde horas vendo TikTok, Instagram, jogando Candy Crush ou LOL ou qualquer outra porcaria desses. O que você produziu depois de ficar 1 hora jogando LOL? Absolutamente nada. Ficou mais rico? Não. Ficou mais inteligente? Não. Avançou sua carreira? Não. Foi uma total e completa perda de tempo. Mas você fez mesmo assim. Encare fazer código da mesma forma. Só faça, sem objetivo. Garanto que vai ser mais útil fazer um cowsay pra jogar fora do que jogar meia hora de Candy Crush.&lt;/p&gt;

&lt;p&gt;Aprendizado, gestão, empreendedorismo, qualidade, todas essas palavras subjetivas pra mim se resumem a uma coisa: gerenciamento de risco. Gerenciar risco não significa ter zero risco. Muita gente medrosa acha que o objetivo é zerar os riscos. Zero risco é você nunca sair da sua cama, porque só de se levantar você corre o risco de tropeçar, bater a cabeça e morrer. Todos os dias você aceita diversos riscos. Risco de morrer num acidente de carro. Risco de ser morto num assalto na rua. Risco de um fucking meteoro cair na sua casa. Não existe vida sem risco. Se você tentar só minimizar risco, é a mesma coisa que não viver. Morre de uma vez então. Em vez disso assuma que o risco vai existir e você vai avançar de forma prudente, só isso.&lt;/p&gt;

&lt;p&gt;Pra finalizar esse motivacional, relembrei de um video motivacional que eu mostrava em palestras que eu fazia lá por 2010 e 2011. É super clichê. Mas nem por isso é menos verdade.&lt;/p&gt;

&lt;p&gt;(life == risk)&lt;/p&gt;

&lt;p&gt;De novo, nada disso significa ser imprudente. Significa ter coragem. Pra ter coragem você não precisa quebrar leis nem migrar pra esportes radicais. Significa a coragem de simplesmente fazer uma página de código inútil que vai jogar fora no final. Significa testar uma idéia num projeto, num ambiente controlado, e assumir que se não der certo, tudo bem porque o experimento no mínimo vai te informar que aquela direção não ia dar certo. Saia da ordem tempo suficiente pra testar um caos controlado, aprender algo com isso, e reajustar pra uma direção melhor.&lt;/p&gt;

&lt;p&gt;Em vez de tentar prever o futuro e errar completamente, você vai ajustando a direção um pedaço de cada vez até chegar no objetivo que queria, com menos desperdício do que se tivesse tentado planejar tudo em detalhes desde o começo. Tentar planejar todos os passos antes de começar a achar que vai conseguir adivinhar todos os passos até o fim é o maior desperdício de todos. Em gerenciamento de risco a máxima é que se sentar ter zero riscos vai acabar tendo 100% de perda. Gerenciar riscos é que nem um seguro de carros. Todo ano você paga e parece um desperdício porque nunca tem um acidente. Imprudência é decidir que isso é um desperdício e cancelar o seguro. É quando você vai ter um acidente feio e não vai ter dinheiro pra cobrir o prejuízo. Isso sim é um desperdício de verdade. Aceite um prejuízo pequeno pagando o seguro. Economia porca não é a mesma coisa que não desperdiçar.&lt;/p&gt;

&lt;p&gt;Isso tudo dito, agora eu quero que vocês pensem em outra coisa. Tudo isso que eu falei aqui, eu não li num post de blog essa semana e vim repetir pra vocês. Quem acompanha meu blog desde pelo menos 2008 já leu algumas dessas coisas. Eu sempre tive as mesmas dúvidas que todos vocês. Será que eu estou estudando do jeito certo? Será que estou gerenciando projetos da melhor forma? Será que administro empresa do jeito certo? E se não ficou claro, todas essas perguntas sempre são a mesma coisa. Administrar uma empresa, gerenciar um projeto ou aprender algo novo, tem os mesmos princípios. Você é uma empresa de uma pessoa só, sua carreira é um projeto, e tudo é um aprendizado. Não são assuntos distintos.&lt;/p&gt;

&lt;p&gt;Como você vai aprender se não sabe o que significa aprendizado? Como vai entregar um projeto se não sabe definir o que é gerenciar? Como quer empreender se sequer sabe definir o que significa a palavra empreender? A maioria de vocês tem um entendimento superficial dessas palavras e os diversos posts de Linkedin, palestras de gurus e videos de influencers, cada um vai vomitando definições erradas, sem dar fontes, dizendo só o suficiente pra você comprar os cursos deles. Fazendo frases bonitas de efeito com tom afirmativo e confiante. É tática de vendedor e você cai que nem um pato, porque tem preguiça de pesquisar.&lt;/p&gt;

&lt;p&gt;Ao mesmo tempo que sempre teve um monte de coisas de programação e tecnologia que eu queria aprender e nunca parei de estudar, eu me cansei de não ter certeza e em paralelo gastei uma década estudando todas as coisas que mencionei neste video e muito mais. Essas coisas não se ensina na escola. Quem é de engenharia de produção deve ter reconhecido quando falei sobre coisas como Lean e Six Sigma. Quem é de física e biologia reconheceu quando falei de transição de fases e adaptabilidade. Quem é de economia já deve ter ouvido conceitos similares também.&lt;/p&gt;

&lt;p&gt;Parte da sua carreira, se quiser evoluir pra uma etapa superior, é sair da sua própria área e investigar um pouco da área dos outros, em particular das ciências de verdade. Vai notar que muitos assuntos que discutimos em engenharia de software como se fosse uma grande novidade na verdade já existe em economia, física, biologia, outras engenharias, faz décadas. Pare de ler posts motivacionais de LinkedIn, que contribuem zero pra sua carreira, e busque a literatura original. Eu sempre falo que os livros que realmente compensam ler precisam passar pelo teste do tempo, e a maioria é do século passado. No meu caso, eu tive a sorte de esbarrar em alguns deles lá atrás, nos anos 90 e começo dos anos 2000. E já adianto que de lá pra cá não teve tanta novidade não, só os mesmos assuntos que ganharam outros nomes e por isso parece que tem tanta coisa nova. De novo, técnica de venda, marketing, e você continua caindo. Os últimos 10 anos foram muito tediosos nesse sentido.&lt;/p&gt;

&lt;p&gt;Em resumo, por mais estranho que os conceitos apresentados pareçam, é basicamente assim que todo ecossistema da natureza funciona. Sem um sistema centralizado de autoridade. Sem um planejamento centralizado. Sem uma ordem definitiva que todos os animais e seres vivos sigam. À primeira vista parece um sistema fechado que vai deteriorar com o tempo, aumentar a entropia, como na segunda lei da termodinâmica. Mas em vez disso sabemos que não só esse sistema não deteriorou como evoluiu, desde microorganismos nos oceanos milhões de anos atrás, até a diversidade de espécies que temos hoje. Esse é o resultado de um sistema de beira do caos.&lt;/p&gt;

&lt;p&gt;Mais importante, descentralização, livre competição, adaptação, são características de um sistema natural. Autoridade centralizada, planejamento ordenado, são características de um sistema artificial que sempre deteriora. É inevitável. Portanto tirem da cabeça que qualquer sistema assim possa jamais funcionar, seja sistema educacional, sistema empresarial, sistema econômico ou sistema político. Mesmo que por um curto período de tempo parece que funciona, eventualmente começa a deteriorar.&lt;/p&gt;

&lt;p&gt;Enfim, como disse no começo, a intenção desse episódio foi só apresentar as fontes de onde tirei meu modelo mental de como fazer as coisas. A idéia não é vender nada e nem dizer que essa é a única maneira de pensar. Só uma introdução pra desmistificar o que alguns gurus possam estar tentando te vender de forma incompleta como se fosse grande novidade. Eu tenho um pensamento quase obcessivo nessa idéia de beira do caos e melhoria contínua. Toda vez que estou fazendo a mesma coisa do mesmo jeito por muito tempo, eu preciso chacoalhar e fazer alguma coisa diferente.&lt;/p&gt;

&lt;p&gt;E pra quem chegou faz pouco tempo no canal. Eu venho contando aspectos disso em diversos videos, em particular na playlist de carreira, onde falo sobre faculdade, mercado, carreira e como encarar os desafios de se comprometer, assumir suas responsabilidades e parar de terceirizar suas decisões pra um influencer bosta qualquer. O medo é sempre o mesmo: e se eu errar. Ué, se errar errou. Levanta, e faz de novo, diferente, pra não errar a mesma coisa. Louco é aquele que fazendo sempre a mesma coisa, espera ter resultados diferentes. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e não deixem de compartilhar o video com seus amigos. A gente se vê, até mais.&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5952</id>
    <published>2022-05-09T11:20:00-03:00</published>
    <updated>2022-05-09T10:23:02-03:00</updated>
    <link href="/2022/05/09/akitando-118-fiz-um-servidor-de-sql-entendendo-banco-de-dados" rel="alternate" type="text/html">
    <title>[Akitando] #118 - Fiz um servidor de &quot;SQL&quot;?? | Entendendo Banco de Dados</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/_7nISfpofec&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;h2&gt;DESCRIÇÃO&lt;/h2&gt;

&lt;p&gt;O que tecnologias de compiladores e estruturas de dados tem a ver com banco de dados? Resolvi brincar de fazer um pequeno banco &quot;stupid&quot; pra vocês terem uma visão diferente sobre o que é um banco de dados por baixo dos panos. Disclaimer: isso não vai ser um tutorial de SQL, mas acho que pode mudar a visão de vocês sobre bancos.&lt;/p&gt;

&lt;h2&gt;CONTEÚDO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00:00 Intro&lt;/li&gt;
&lt;li&gt;00:00:35 Problema de Tutoriais/Cursos&lt;/li&gt;
&lt;li&gt;00:02:51 Banco de Dados de Usuários&lt;/li&gt;
&lt;li&gt;00:03:29 &quot;Simulando&quot; banco de dados&lt;/li&gt;
&lt;li&gt;00:04:58 &quot;Fake&quot; SQL?&lt;/li&gt;
&lt;li&gt;00:07:16 Construindo o Fake SQL&lt;/li&gt;
&lt;li&gt;00:10:59 Experimente com Código!&lt;/li&gt;
&lt;li&gt;00:15:39 Testando Fake SQL com Jest&lt;/li&gt;
&lt;li&gt;00:19:38 Adicionando &quot;SQL&quot; no &quot;Fake SQL&quot;&lt;/li&gt;
&lt;li&gt;00:22:01 Conhecendo Antlr4&lt;/li&gt;
&lt;li&gt;00:24:39 Copiando do SQLite3&lt;/li&gt;
&lt;li&gt;00:30:34 Criando meu Listener de Parse Tree&lt;/li&gt;
&lt;li&gt;00:39:41 O que é um Índice?&lt;/li&gt;
&lt;li&gt;00:49:10 Bancos de Dados tem Interpretadores!&lt;/li&gt;
&lt;li&gt;00:49:51 Transaction Log!&lt;/li&gt;
&lt;li&gt;00:55:33 Como &quot;grava&quot; um banco de dados em disco??&lt;/li&gt;
&lt;li&gt;01:05:57 B-Trees&lt;/li&gt;
&lt;li&gt;01:08:30 Fazendo Cliente-Servidor!&lt;/li&gt;
&lt;li&gt;01:09:55 Conclusão&lt;/li&gt;
&lt;li&gt;01:12:26 Bloopers!&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;GitHub do Stupid Database - código do episódio (https://github.com/akitaonrails/akitando_episode_0118)&lt;/li&gt;
&lt;li&gt;GitHub do repositório de gramáticas do Antlr (https://github.com/antlr/grammars-v4)&lt;/li&gt;
&lt;li&gt;SQLite Amalgamation (https://www.sqlite.org/amalgamation.html)&lt;/li&gt;
&lt;li&gt;Visualização de AVL Tree (https://www.cs.usfca.edu/~galles/visualization/AVLtree.html)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Finalmente resolvi falar um pouco sobre bancos de dados. Esse é provavelmente o primeiro grande desafio pra todo mundo que começa a carreira de programação e parece uma parede muito alta pra escalar, muitos acham intimidante, e muitos começam a usar, decoram alguns comandos SQL, mas não tem muita certeza do que diabos estão fazendo. O objetivo de hoje é tentar dar um modelo mental que vocês consigam enxergar servidores de bancos de dados de uma forma menos complicada.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Alguns avisos antes de começar. O episódio de hoje não vai explicar nenhum banco de verdade, seja Postgres ou Mongo. Quem está esperando um tutorial de SQL, não vai ser este video. O começo vai parecer estranho se estava esperando isso, mas tenha paciência e continue acompanhando, eu acho que vocês vão ficar surpresos. Até pra quem já entende de banco de dados, no mínimo vai ter material aqui pra ajudar vocês a explicarem pros seus colegas iniciantes.&lt;/p&gt;

&lt;p&gt;Outra coisa, todos os exemplos de código que vou mostrar serão em Javascript. Eu pessoalmente teria feito tudo em Ruby ou Crystal que são linguagens que sou mais confortável, mas acho que a maioria das pessoas assistindo é mais Javascript, então vamos de Javascript. Quem for de outras linguagens deve conseguir acompanhar porque não vou usar nada exótico. Tudo que eu fizer aqui dá pra fazer em Python, PHP, C# ou o que quiserem. Desta vez todo o código está disponível num repositório no GitHub que vou deixar o link nas descrições abaixo. Inclusive fica de lição de casa refazer o que eu mostrar aqui na linguagem favorita de vocês. O código desse repositório é ingênuo de propósito porque foi um exercício que eu decidi fazer durante o feriado de Tiradentes e eu me coloquei um limite de 2 a 3 dias pra fazer tudo.&lt;/p&gt;

&lt;p&gt;A motivação desse episódio é porque a menos que eu esteja muito enganado, a maioria dos tutoriais e cursos sobre banco de dados começa mais ou menos assim. Faça &quot;apt install mariadb&quot;, que é um fork do MySQL. Daí termina de instalar, inicia o serviço, abre o console que conecta no servidor e começa: agora vamos fazer &quot;create table users&quot; bla bla e depois &quot;insert into users&quot; bla bla e finalmente &quot;select name from users&quot; e já vai direto pra explicar SQL. Pior ainda se começa num tutorial de framework, que aí você já cai num Hibernate ou Sequelize e sequer sabe o que tá acontecendo por baixo.&lt;/p&gt;

&lt;p&gt;Só que isso eu acho que é ir do zero a 100 muito rápido. Onde que os dados ficam gravados? Onde é esse tal de &quot;banco&quot;? Como que uma linguagem como SQL atua em cima desses dados? Pra que serve um índice? Qual a diferença disso pra eu simplesmente gravar um JSON num arquivo? Vamos dar uma pausa e começar do começo, bem do começo. E no começo tudo que vocês sabem é lidar com variáveis e collections como arrays ou sets ou, no caso de Javascript, objects.&lt;/p&gt;

&lt;p&gt;Pois bem, pra começar resolvi escrever o banco de dados mais simples que dá pra fazer. Chamei ele de Stupid Database, porque não é pra ninguém achar que isso é um banco de verdade. Ele só se comporta como um. Tenho uma tabela de usuários, num array simples, onde cada linha é um objeto de Javascript, como podemos ver aqui. Na 1a linha, com ID 1 temos ninguém menos que nosso querido Peter Parker. Não esse, esse outro aqui. Com uns 25 anos, da cidade de Nova Iorque. Na 2a linha, com ID 2, temos o Clark Kent com 35 anos, de Metropolis, e assim por diante. Um total de 37 linhas nesse array, com várias pessoas desconhecidas, de nomes comuns.&lt;/p&gt;

&lt;p&gt;O que é isso? É um array de objetos. Também podemos chamar isso de &quot;tabela&quot;. Olha só. Se eu abrir um console de Node.js, carregar esse arquivo com &lt;code&gt;db = await import('./lib/users-database.mjs')&lt;/code&gt; e fizer &lt;code&gt;console.table(db.users)&lt;/code&gt;, ele vai printar exatamente como qualquer um esperaria ver uma tabela, certo? Pra todos os efeitos e propósitos, essa é uma tabela mínima.&lt;/p&gt;

&lt;p&gt;Ok, mas se é uma tabela, é possível inserir novas linhas, certo? Claro. Podemos criar um novo usuário, vamos dar o nome de outra pessoa desconhecida de nome comum, Levi Ackerman, uns 37 anos, de Paradis. E pra inserir num array de Javascript usamos a função &lt;code&gt;.push&lt;/code&gt;. E no nosso caso, inventei que o id de cada linha é um número crescente que começa em 1, então essa nova linha vai ser o tamanho da tabela mais um. E pronto, esse seria o equivalente em SQL a fazer um &lt;code&gt;insert into users&lt;/code&gt; da vida.&lt;/p&gt;

&lt;p&gt;Beleza, mas então qual é o equivalente a um &lt;code&gt;select&lt;/code&gt; de SQL? Simples, uma das formas é a função &lt;code&gt;filter&lt;/code&gt;. Eu passo pra função outra função que ele vai executar em cada objeto, um por um em cada linha desse array, no caso, procurando o objeto que tem o nome &quot;Levi Ackerman&quot;. O resultado disso é outro array só com os objetos encontrados. Podemos passar de novo pelo comando &lt;code&gt;console.table&lt;/code&gt; e voilá, esse é o resultado correto, que seria o equivalente em SQL a fazer &lt;code&gt;select * from users where name = 'Levi Ackerman'&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Aí nesse ponto você deve estar cético, pensando aí. Porra, Akita tá de sacanagem. Isso aí é só uma porcaria de um array, não é um banco de dados. Eu quero ver comando de SQL, porra! Ok, talvez você tenha razão. Então deixa eu ir pro terminal e executar um programinha que eu escrevi, um REPL. Lembram o que é um REPL? O comando &lt;code&gt;node&lt;/code&gt; onde eu estou é um REPL, um console que interpreta os comandos que eu digito imediatamente e já me dá o resultado. O REPL &lt;code&gt;node&lt;/code&gt; interpreta javascript e executa. Pra quem é de Python, se digitar &lt;code&gt;python&lt;/code&gt; vai abrir o REPL de Python, que executa imediatamente os comandos em Python que digitar, e assim por diante. Várias linguagens tem REPLs que já vem na instalação ou que você pode instalar separadamente, mas só lembrar que REPL é um console interativo da linguagem. É uma ferramenta comum em linguagens dinâmicas.&lt;/p&gt;

&lt;p&gt;Enfim, vou executar o REPL que eu escrevi e chamei de &lt;code&gt;repl.mjs&lt;/code&gt;. Olha só, abriu uma linha de comando. Vocês queriam ver SQL? Tá bom, vamos começar digitando &lt;code&gt;insert into users (name, age, city) values ('Levi Ackerman', 37, 'Paradis')&lt;/code&gt;. Opa, retornou o número 38. Agora vamos ver se inseriu mesmo, vou digitar a query &lt;code&gt;select * from users where users.city = 'Paradis'&lt;/code&gt;. E, olha só, achou esse senhor desconhecido de nome Levi, como deveria.&lt;/p&gt;

&lt;p&gt;Pronto, não era SQL que vocês queriam ver? Tá aqui. Mas isso não é MySQL, não é Postgres, nada nessa manga, nada nessa outra, é só Javascript puro. Vamos fazer outra query, quero todos os indivíduos nessa tabela com idades entre 60 e 2000 anos, porque alguns indivíduos tem expectativa de vida bem alta ultimamente. Como que faz? &lt;code&gt;select * from users where users.age between 60 and 2000&lt;/code&gt;, é assim que vocês aprenderam né? E olha só, obviamente, achou certinho. Olha lá o Steve Rogers com 100 anos, acho que é 100 né? Ou nosso amigo Thor com 1500 anos. Só não comer açúcar, fazer dieta intermitente, e vive saudável muitos anos.&lt;/p&gt;

&lt;p&gt;Eu espero que eu tenha conseguido deixar vocês confusos. O que diabos tá acontecendo aqui? Eu abri uma linha de comando bizarra e comecei a digitar SQL como se fosse um Postgres da vida, e ele claramente tá pesquisando em cima daquele array de objetos em Javascript. Como isso é possível? E esse é o real objetivo desse episódio, eu quero mostrar como eu fiz isso e em paralelo vocês entenderem mais ou menos como um banco de dados de verdade é feito. Então vamos lá.&lt;/p&gt;

&lt;p&gt;Primeiro de tudo vamos deixar super claro: isto é meramente um exercício educacional, um código que eu me limitei em uns 2 a 3 dias pra fazer. Todo código que vou mostrar a partir de agora não tem condições de ser usado em produção de verdade. Pra isso ele precisaria ser bem mais complicado pra levar em consideração coisas como autenticação e autorização, locks e concorrência, otimizações de performance, otimizações de redundância, etc e tudo isso só ia dificultar a explicação. Por isso eu fiz só o mínimo do mínimo necessário pra ficar fácil de explicar, que é o objetivo de todo código educacional.&lt;/p&gt;

&lt;p&gt;O segredo de tudo começa na biblioteca que eu chamei de &lt;code&gt;lib/fake-sql.mjs&lt;/code&gt;. Essa é uma coleção de funções que eu escrevi com nomes estratégicos. Deixa eu mostrar. A primeira é a função &lt;code&gt;from&lt;/code&gt; que recebe como argumentos as variáveis &lt;code&gt;table&lt;/code&gt; e &lt;code&gt;conditions&lt;/code&gt;. Lá na frente eu vou carregar o array &lt;code&gt;users&lt;/code&gt; e outro array chamado &lt;code&gt;heroes&lt;/code&gt;, tudo dentro de um hash chamado &lt;code&gt;database&lt;/code&gt;, daí se fizer &lt;code&gt;database['users']&lt;/code&gt; eu acesso o mesmo array que tava mostrando até agora.&lt;/p&gt;

&lt;p&gt;E aqui uma das vantagens de usar uma linguagens dinâmica interpretada como Javascript. Já que performance e segurança não são preocupações pra hoje, eu posso fazer um protótipo simplesmente montando aquele comando de &lt;code&gt;filter&lt;/code&gt; como fiz antes concatenado com a variável &lt;code&gt;conditions.where&lt;/code&gt;. Se não ficou claro, deixa eu mostrar o que essa função faz. Vamos abrir o console do Node e deixa eu dar uns copy e paste só pra importar esse arquivo 'fake-sql.mjs' e o mesmo 'users-database.mjs' de antes. Não se preocupem com essa parte que é só carga.&lt;/p&gt;

&lt;p&gt;Pronto, agora vamos fazer a mesma pesquisa de antes onde eu quero todo mundo com idades entre 60 e 2000 anos. É só fazer &lt;code&gt;from('users', { where: 'users.age &amp;gt;= 60 &amp;amp;&amp;amp; users.age &amp;lt;= 2000'})&lt;/code&gt; que é a mesma coisa que o &lt;code&gt;between 60 and 2000&lt;/code&gt; que fiz em SQL. De propósito chamei essa função de &lt;code&gt;from&lt;/code&gt; e fiz um hash com uma chave &lt;code&gt;where&lt;/code&gt; pra ficar meio parecido com SQL. E olha só, voltou o mesmo resultado de antes. Tô vendo o Steve Rogers, o Thor, a Diana e o Hank Pym. Essa função é o grosso do meu simulador de SQL, todo o resto agora é perfumaria.&lt;/p&gt;

&lt;p&gt;Pra parecer mesmo SQL falta começar com &lt;code&gt;select&lt;/code&gt; né? Pra que serve o select? Serve pra eu pegar só as colunas, ou chaves, que eu quiser, em vez de todas. Vamos criar a função &lt;code&gt;select&lt;/code&gt; lá na biblioteca então. E olha só, parece mais complicadinho. Essa função recebe como argumentos uma lista de colunas e o resultado do &lt;code&gt;from&lt;/code&gt;, ou seja, aquele array de resultados.&lt;/p&gt;

&lt;p&gt;A primeira metade é outra trapaça. Se eu pedir &quot;*&quot; ele vai substituir por uma lista com todas as chaves dos objetos. Aqui estou assumindo que todos os objetos do resultado sempre tem as mesmas chaves. Não estou checando nada e não vou introduzir o conceito de Schemas, por isso falei que tô trapaceando. Enfim, daí ele vê se o argumento foi a palavra &quot;count&quot;, daí devolve direto só o total de objetos no array de resultados. E caso contrário, pega a string que passei e dá split por vírgula, pra transformar num array.&lt;/p&gt;

&lt;p&gt;O resultado dessa função é um &lt;code&gt;map&lt;/code&gt;. Lembra o que faz um map? Ele pega uma coleção, transforma cada elemento e devolve tudo em outra coleção, convertendo cada objeto da coleção original usando essa função que passo de parâmetro. Em resumo, vai transformar meu objeto original num outro objeto só com as chaves que pedi pra selecionar. Vamos ver na prática lá no mesmo console de Node.&lt;/p&gt;

&lt;p&gt;Estou de novo no console e faço &lt;code&gt;select('name,age')&lt;/code&gt;. Aqui é uma escolha de design, eu poderia passar como parâmetro um array com os strings 'name' e 'age' mas só pra ficar mais &quot;limpo&quot; nessa sintaxe que tenta imitar SQL, decidi passar tudo grudado num único string. Por isso, do lado de dentro, preciso fazer um split. Obviamente esse é o jeito menos otimizado de passar parâmetros, mas como eu disse antes, é só um exercício.&lt;/p&gt;

&lt;p&gt;Aproveitando a deixa, é isso que eu falo em vários videos: experimente com código. Faça experimentos como esse na sua máquina, código que não é de nenhum projeto do trabalho, é só um treino sem compromisso. E aí experimente designs que normalmente ninguém deixaria você fazer no trabalho. Só quando se experimenta coisas que ninguém faria é que às vezes pode dar inspiração pra coisas que podem ser úteis.&lt;/p&gt;

&lt;p&gt;Pra quem curte anime, isso me lembra o Food Wars, já viram, Shokugegi no Souma? É uma história de competição numa escola culinária onde o protagonista, por acaso, adora fazer comidas bizarras que nunca dão certo. Esse é o motivo. Se você nunca tentou fazer os códigos mais impensáveis, nunca vai chegar num design inovador de verdade. Abra um diretório vazio, começa a escrever, apaga, joga fora, vai treinando. Ninguém precisa nem saber.&lt;/p&gt;

&lt;p&gt;Enfim, voltando ao console, depois de passar a lista de colunas que queremos, como segundo parâmetro colocamos a mesma função &lt;code&gt;from&lt;/code&gt; de antes. Olha o resultado: uma lista filtrada só com as colunas que queremos, sem a coluna de cidade ou id, no caso. E veja de novo a implementação. Eu pego o resultado e faço um map. A função de transformação faz um array só com as colunas que eu escolhi. E no final faço um &lt;code&gt;reduce&lt;/code&gt; pra recriar objetos com o nome das colunas como chave.&lt;/p&gt;

&lt;p&gt;Parece um pouco complicado se você nunca usou &quot;map&quot; e &quot;reduce&quot;, que existem em todas as linguagens e servem pra transformar listas e reduzir o resultado num acumulador. Teste só esse trecho num script com vários tipos de listas pra entender o que isso tá fazendo. É bem básico e você precisa saber. De novo, exercite até conseguir enxergar esse trecho e saber exatamente o que está acontecendo. Tem outras formas de escrever a mesma coisa pra ter o mesmo resultado, eu poderia não ter usado map e reduce e feito só com &lt;code&gt;for&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Enfim. Olha o que estou fazendo. Só de escrever duas funções, esse &quot;select&quot; e &quot;from&quot; que recebe um hash de condições com uma chave &quot;where&quot;, já consigo simular o básico do básico de um select de SQL. Só pra ver mais um exemplo, num SQL você sabe que poderia fazer &lt;code&gt;select name from users where bla bla order by name desc&lt;/code&gt; pra ordenar o resultado em ordem decrescente de nome. Como daria pra fazer nesse meu fake SQL? Vamos criar uma nova função chamada &lt;code&gt;orderBy&lt;/code&gt; que recebe como parâmetro o nome da coluna por onde queremos ordenar, daí um segundo parâmetro pra dizer se é ordem ascendente ou descendente e como terceiro parâmetro, de novo o resultado da função &lt;code&gt;from&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Essa implementação é simples, é basicamente o que qualquer iniciante de Javascript que já estudou algoritmos simples de ordenação já exercitou: implementar a função de comparação que se passa pra função &lt;code&gt;sort&lt;/code&gt; que todo array tem. Novamente, se nunca tentou ordenar um array procure no google &quot;como ordena um array em javascript&quot; e vai achar a explicação dessa função. Não tem nada de especial aqui.&lt;/p&gt;

&lt;p&gt;Vamos voltar pro console de Node. Como que fica pra usar isso? Aqui é que vai ficar um pouco diferente de um SQL normal porque o &quot;ORDER BY&quot; costuma ir no final do comando, mas na minha versão em Javascript, vai antes. E fica assim &lt;code&gt;select('name', orderBy('name', 'desc', from('users', bla bla bla)))&lt;/code&gt;. E olha só, funcionou direitinho, retornou ordenado em ordem decrescente, Lois Lane na frente, Barbara Gordon no final. Pára e pensa, com a função &lt;code&gt;from&lt;/code&gt; filtramos a tabela inteira pra pegar só quem é de Gotham ou Metropolis. Agora ordenamos o resultado, e finalmente selecionamos só as colunas que queremos. E por que essa ordem? Porque poderíamos escolher ordenar por idade mas no final só selecionar a coluna de nome. Por isso a ordenação vai antes de selecionar as colunas pro resultado final.&lt;/p&gt;

&lt;p&gt;Só pra terminar essa parte, vamos ver rapidamente só mais duas funções. Como faz o equivalente a um &quot;insert&quot; de SQL? Essa é fácil. é só usar a função &lt;code&gt;push&lt;/code&gt; que todo array tem como já mostrei antes. No caso, eu estabeleci que todo objeto vai ter um ID que é numérico e cresce incrementalmente, como muita primary key padrão que todo mundo cria, só pra ficar mais fácil. Vamos no console do Node e testar isso. Mesma coisa do primeiro exemplo &lt;code&gt;insert('users', { name: 'Levi Ackerman', age: 37, city: 'Paradis' })&lt;/code&gt;. E devolve 38 que é o ID que ele calculou pra esse objeto.&lt;/p&gt;

&lt;p&gt;Agora pra simular delete como no SQL é a mesma coisa que o filter da função &lt;code&gt;from&lt;/code&gt; pra encontrar os objetos que queremos, daí só usamos a função &lt;code&gt;pop&lt;/code&gt; que todo array de Javascript tem, e isso vai arrancar os objetos da lista. Nesse código-fonte eu implemento outras funções como o equivalente a &quot;update&quot; e tem uma versão inacabada de como seria um 'inner join' e 'outer join' se tiverem curiosidade de tentar terminar da forma correta. E é assim que vou fazendo um fake-sql pra simular os diversos aspectos que tem num banco de dados SQL comum.&lt;/p&gt;

&lt;p&gt;Mas claro, mesmo sendo um exercício, já tem código suficiente aqui pra confundir a gente. Então esse é um bom momento pra fazer alguns testes unitários pelo menos né? Lembram como faz? Primeiro é só fazer um &lt;code&gt;npm install --save-dev jest&lt;/code&gt; pra instalar o framework de testes Jest. Agora abrimos o arquivo &lt;code&gt;package.json&lt;/code&gt; e colocamos esse comando grandão em scripts, test. Eu precisei usar essa flag experimental pra conseguir carregar os módulos ES certinho. Não sei se é o melhor jeito, mas foi como funcionou aqui.&lt;/p&gt;

&lt;p&gt;Agora é só rodar &lt;code&gt;npm test&lt;/code&gt; no terminal e ele vai executar os testes que eu já tinha feito no arquivo &lt;code&gt;stupid.test.js&lt;/code&gt;. O comando do Jest vai vasculhar por arquivos terminados em &quot;.test.js&quot; e rodar os testes que tem lá dentro. E como é esse arquivo? Vamos abrir rapidinho só pra ver alguns exemplos. As primeiras linhas são pra carregar as funções do fake-sql e as tabelas de users e heroes.&lt;/p&gt;

&lt;p&gt;Criamos um bloco de describe pra função &quot;select&quot; que é de longe quem tem mais casos de testes. Sabe todos aqueles testes que fizemos manualmente no console do Node? Pois é, é o que queremos automatizar em forma de testes unitários. Por exemplo, o primeiro teste é que deve achar o nome, então executa aquela mesma linha de select name from users where users.name igual Peter Parker. Pega o array de resultados, escolhe o primeiro objeto e espera que a propriedade “name” desse objeto seja Peter Parker, “to be” Peter Parker, se for, esse teste passa. E vai fazendo isso pra outros exemplos.&lt;/p&gt;

&lt;p&gt;Vocês podem olhar esse arquivo com calma depois que tá no repositório. Link na descrição abaixo como falei, mas veja o último que é mais um teste integrado. Eu começo fazendo dois novos inserts, um pra tabela users com o objeto do Katar Hol, e outro insert na tabela heroes dizendo que na verdade ele é o Hawkman! Quem diria? E ligando com o que seria equivalente a uma chave estrangeira numa tabela de SQL, fazemos a propriedade user_id desse objeto ser igual ao valor que voltou no insert anterior. E é assim que seria um sisteminha CRUD da Liga da Justiça, pensa o estag do Bruce Wayne fazendo isso.&lt;/p&gt;

&lt;p&gt;Só pra comparar depois, pedimos a contagem das duas tabelas e guardamos nessas duas variáveis de total. Agora fazemos um select com o innerJoin que eu não mostrei o código, mas se você já aprendeu SQL, inner join é uma forma de fazer a intersecção de duas tabelas de resultado. Ele vai ligar o resultado desse &lt;code&gt;from&lt;/code&gt; feito na tabela users com a tabela heroes via a chave &lt;code&gt;user_id&lt;/code&gt;. E esperamos receber uma tabela de resultado com as colunas name e alterego. Alter ego só tem na tabela heroes.&lt;/p&gt;

&lt;p&gt;Então esperamos que na primeira linha do resultado, a propriedade name seja Katar Hol e o alterego seja Hawkman. Continuando, digamos que eu errei a idade do Katar, não era 35 e sim 37. Então eu posso usar essa outra função &lt;code&gt;updateFrom&lt;/code&gt; que eu não mostrei o código. Quero que o age, a idade, mude pra 37, e isso nos objetos que voltar da mesma função &lt;code&gt;from&lt;/code&gt; onde a cidade é Thanagar.&lt;/p&gt;

&lt;p&gt;Eu faço de novo um select pela mesma condição e checo que a idade que voltou mudou pra 37 mesmo. E estamos quase no fim. Agora quero testar a função &lt;code&gt;deleteId&lt;/code&gt; que também não mostrei o código. Vou apagar os objetos com os ids user_id e hero_id que tinha guardado lá em cima. E pra saber se apagou mesmo, peço um select count pra trazer a contagem das duas tabelas e tem que ter um objeto a menos do que tinha antes de quando eu gravei os totais depois do insert lá em cima. Vamos pro terminal, se rodar &lt;code&gt;npm test&lt;/code&gt;, olha só, tudo passa corretamente. E agora eu tenho um mínimo de testes caso eu queira brincar de mexer mais nesse código.&lt;/p&gt;

&lt;p&gt;Testes é outro assunto super longo, mas mesmo num exercício de fim de semana como esse, ter testes me ajudou muito. Se não tivesse eu teria levado o dobro do tempo pra fazer as coisas. Mas eu não segui nenhum tipo de TDD, eu escrevia um pedaço e testava depois, mudava o design, mudava o teste, e fui fazendo assim. Não pensa tanto em procedimento, só faz a porra do teste.&lt;/p&gt;

&lt;p&gt;Até esse ponto implementei um monte de funções de javascript com nomes como select, insert, updateFrom, from, innerJoin, orderBy e assim por diante pra imitar os nomes dos comandos mais comuns que todo banco de dados SQL tem. E como exercício, eu poderia parar por aqui. Entenderam mais ou menos como é possível simular um fake SQL em cima de arrays de javascript? Mas claro, não é isso que você queria ver. Por mais parecido que seja, isso ainda não é SQL. Funções de Javascript tem um monte de parênteses, o orderBy tá na ordem errada, tá parecido mas tá longe de estar igual a SQL.&lt;/p&gt;

&lt;p&gt;O que eu quero é poder ter um script como eu fiz nesse arquivo &lt;code&gt;test.sql&lt;/code&gt; que também tá no repositório. Olha só, vários comandos SQL igualzinho o que você conseguiria executar no MySQL, SQL Server. Pra executar esses SQL eu fiz outro script chamado &lt;code&gt;stupid.mjs&lt;/code&gt;. Vamos ver o finalzinho dele. Olha só, ele abre o arquivo &lt;code&gt;test.sql&lt;/code&gt;, passa o script nessa função esquisita &lt;code&gt;parseScriptWithSplitQueries&lt;/code&gt; e pra cada linha devolvida dá um eval. O que acontece se executar isso? Vamos voltar pro terminal e executar.&lt;/p&gt;

&lt;p&gt;E voilá, ele executa todos os comandos SQL! Eu dei &lt;code&gt;console.log&lt;/code&gt; do que tá rodando, e são minhas funções Javascript e não o SQL. Como que faz isso? Vamos voltar praquele script e o segredo começa naquela função esquisita &lt;code&gt;parseScriptWithSplitQueries&lt;/code&gt;. Ela passa o conteúdo do arquivo &lt;code&gt;test.sql&lt;/code&gt;, que são aqueles montes de comandos SQL, pra essa classe SplitQueries. Se olhamos lá em cima, ela é importada desse outro arquivo &lt;code&gt;parser/SplitQueries.mjs&lt;/code&gt;. Vamos abrir ela.&lt;/p&gt;

&lt;p&gt;Pronto, a primeira coisa que ele faz é importar essa biblioteca chamada Antlr4. E vamos pausar aqui pra eu explicar. Como expliquei no começo, no feriado de Tiradentes eu tava pensando em como ia explicar banco de dados pra vocês, aí tive a idéia de fazer um do zero. Se eu terminasse o episódio aqui, acho que já dava pra ter o primeiro gostinho de como um banco funciona por baixo. Esse código que mostrei até agora é super trivial de fazer, não foi mais que algumas poucas horas brincando. Mas a cereja do bolo seria mostrar como que comandos SQL de verdade funcionam e apesar das funções javascript que fiz terem nomes parecidos com SQL, ainda não é SQL. E aí fiquei coçando a cabeça, porque eu queria mostrar SQL, mas eu não queria gastar muito mais que um dia pra isso.&lt;/p&gt;

&lt;p&gt;O episódio que lancei antes desse justamente explica o que é uma linguagem compilada e interpretada, e mencionei que tanto um compilador quanto um interpretador começam com um parser. E uma das formas de se fazer um parser é usar um software chamado Antlr4, que é feito em Java. Naquele episódio eu disse que não ia explicar ele. Pois bem, vou explicar rapidinho agora. O antl4 é um gerador de parsers. Lembram o que é um parser? É quando pegamos uma gramática, passamos o código fonte por um lexer pra conseguir uma coleção de tokens e passamos esses tokens pra um parser pra conseguir uma ParseTree.&lt;/p&gt;

&lt;p&gt;Num interpretador ou compilador, essa ParseTree é transformada numa Abstract Syntax Tree ou AST, que daí passa por uma série de otimizadores pra reescrever o código da forma mais eficiente possível, e aí finalmente gerar o código de máquina tanto pra executar na hora, caso seja um Just in Time Compiler ou gerar um binário num arquivo, caso seja um Ahead of Time Compiler ou AOT. No meu caso, eu não preciso fazer tudo isso, só preciso ir até o ponto onde gera uma Parse Tree, pra a partir dela, fazer um transpiler, que vai converter SQL em Javascript. Entenderam onde eu quero chegar?&lt;/p&gt;

&lt;p&gt;Escrever uma gramática do zero é extremamente tedioso e trabalhoso. Mas minha vantagem é que não estou inventando uma nova linguagem, pelo contrário, eu quero um SQL que seja o mais compatível possível com o SQL de verdade. E aqui vem outra vantagem do Antlr4. Ele tem uma comunidade muito esforçada que já se deu ao trabalho de escrever a gramática de praticamente todas as linguagens que você já ouviu falar e um monte que vocês nem sabiam que existiam. Vou deixar o link do repositório nas descrições abaixo, mas olha essa página do GitHub que coisa linda.&lt;/p&gt;

&lt;p&gt;Lembram que eu falei da linguagem BCPL? Quer saber como é a gramática? Basta entrar aqui nessa pasta e abrir o arquivo &lt;code&gt;bcpl.g4&lt;/code&gt; que é a extensão de gramática do antlr4. Olha o que eu expliquei no episódio anterior. Começa definindo o que ele entende por &quot;letras&quot;, são todas as maiúsculas e minúsculas. E essa distinção é importante porque tinha linguagens antigas que só entendiam comandos em maiúsculo. Continua definindo, o que é um dígito em octal, um dígito em hexadecimal, o que são dígitos numéricos.&lt;/p&gt;

&lt;p&gt;E qual é o tamanho da gramática pra entender BCPL inteiro? Que é a linguagem que inspirou o C? Nada mais que 131 linhas, contando linhas em branco. E se a gente voltar pra primeira página, olha lá embaixo, uma pasta chamada &quot;python&quot;, dentro tem python2, e no python3 temos de novo arquivos .g4 que ele separou pro lexer e pra gramática do parser. E mesmo esse arquivo tem menos de 200 linhas.&lt;/p&gt;

&lt;p&gt;Mas de volta à primeira página, o que nos interessa mesmo é essa pasta sql lá embaixo. Tem as gramáticas do mysql, postgres mas o que eu queria, e ainda bem que tinha, é do sqlite3, que é o banco de dados sql mais simples e provavelmente o mais usado do mundo. Isso porque enquanto um postgres da vida é usado em servidores, todo dispositivo IoT, internet das coisas, que é qualquer dispositivo com um mínimo de processamento com acesso a internet, precisa de um banco de dados local. Pense um roteador de wifi da sua casa. 99% de certeza que dentro usa um sqlite3. Pensa vários apps no seu celular Android, vários sqlite3. Pensa uma geladeira inteligente, certamente tem sqlite3 dentro.&lt;/p&gt;

&lt;p&gt;Aliás, o que estou construindo neste episódio estou chamando de stupid database, porque é o banco mais besta e ao mesmo tempo mais simples com objetivo educacional, mas se quiser ver o menor banco de dados SQL realmente completo e funcional, tem que estudar o código C do Sqlite3. Existe um subprojeto chamado Sqlite Amalgamation que concatena os mais de 100 arquivos de código fonte num único arquivo &quot;sqlite3.c&quot; e por isso se chama amalgamation.&lt;/p&gt;

&lt;p&gt;O sqlite3 inteiro consiste de 75% de código C e o resto é arquivos .h de cabeçalhos. Esse arquivão dá um total de quase 240 mil linhas ou 145 mil se tirar linhas em branco, totalizando 8.4 megabytes de código fonte. Se um dia realmente quiser saber em detalhes como um banco de verdade funciona, precisa entender como o sqlite3 funciona, O código fonte tá disponível pra ler e ir testando pedaço a pedaço. Bom trabalho de iniciação científica ou TCC pra quem tá na faculdade de ciências da computação.&lt;/p&gt;

&lt;p&gt;Mas, explicar 145 mil linhas de código em um único episódio seria impossível então espero que se contentem com essa minha versão de stupid database. Enfim, falei tudo isso pra dizer que foi nesse respositório de gramáticas do Antlr4 que peguei emprestado os arquivos &quot;SqliteLexer.g4&quot; e &quot;SqliteParser.g4&quot;. E como eu não ia ter comandos como &quot;create table&quot;, &quot;alter table&quot; e coisas assim, saí apagando da gramática tudo que não ia suportar até ficar com uma versão mais reduzida. Ainda sobrou muita coisa que eu não implemento, mas apaguei o suficiente pra facilitar meu trabalho.&lt;/p&gt;

&lt;p&gt;Daí eu precisei instalar o antl4, e pra isso é só ir no site deles e ver como instala, não vou detalhar aqui. Assumindo que já tá instalado, no terminal é só rodar o comando &lt;code&gt;antl4 -Djavascript&lt;/code&gt; pros dois arquivos SqliteLexer e SqliteParser.g4. Esse flag &quot;-D&quot; diz em que linguagem ele vai gerar o lexer e parser. E dá pra gerar em praticamente todas as linguagens que quiser, seja python, java e no nosso caso, javascript. Isso gera esses arquivos &quot;.interp&quot; e olha só SqliteLexer.js e SqliteParser.js. Esses arquivos a gente não precisa mexer. Nem vou abrir porque pode assustar os mais fracos de coração.&lt;/p&gt;

&lt;p&gt;O mais importante é que ele gerou esse SqliteParserListener.js. Esse podemos abrir e vamos descobrir um monte de funções vazias. Que diabos que são essas funções? E eles estão em pares, veja só aqui no começo temos uma função chamada &quot;enterParse&quot; e depois &quot;exitParse&quot;. Se abrirmos aquele arquivo que pegamos emprestado, o SqliteParser.g4 podemos ver aqui no começo que &quot;parse&quot; tem esse &quot;sql statement list&quot; e EOF que é End of File ou final do arquivo. Daí vemos que sql statement list tem vários sql statements e sql statement pode ser vários tipos como delete statement, insert statement, select ou update. Entenderam? É assim que ele começa definindo as regras do que esse parser consegue entender, e são justamente as operações que queremos. Então já sabemos que o ponto de entrada no parser vai ser por essa regra “parse” que engloba o arquivo inteiro de comandos SQL que passarmos pra ele.&lt;/p&gt;

&lt;p&gt;A gente não mexe nesse arquivo de Listener também, porque se amanhã eu mexer no arquivo .g4 e mandar regerar com o comando &lt;code&gt;antl4&lt;/code&gt;, daí vai sobrescrever nesses arquivos todos. Por isso criei esse outro chamado CustomListener.js. Ele é basicamente uma classe que extende da classe SqliteParserListener e lá embaixo eu copio todas aquelas funções vazias pra sobrescrever elas. Agora sim posso começar a brincar.&lt;/p&gt;

&lt;p&gt;Eu não vou mostrar tudo em detalhes, depois vocês podem olhar o código fonte no meu repositório. Mas só pra entender vamos voltar pro arquivo principal desse diretório, o SplitQueries.mjs que é pra onde eu tava mandando os comandos SQL do arquivo &lt;code&gt;test.sql&lt;/code&gt; lembram? Agora que expliquei o que são os principais arquivos, veja que nesse script eu importo todos eles, o Lexer, o Parser e o meu CustomListener. Ele começa recebendo um input, que pode ser uma linha de SQL ou várias linhas de SQL como o conteúdo do arquivo &lt;code&gt;test.sql&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Uso a função InputStream da biblioteca antlr4 pra quebrar num stream de caracteres. Lembra o que são streams? Lembro que já expliquei isso em algum episódio, mas pra quem não sabe, iniciantes estão acostumados a carregar o conteúdo de um arquivo tudo de uma vez em memória, mas e se o arquivo não couber na memória? O jeito mais eficiente é ir lendo pedaços do arquivo de cada vez e ir processando aos poucos, pra isso abrimos um stream. Enfim, passamos esse stream pro Lexer, que criar uma lista de tokens a partir desse stream. Ou seja, ele recebe a linguiçona de strings e vai quebrando numa lista de tokens.&lt;/p&gt;

&lt;p&gt;Agora passamos esses tokens pro Parser e em seguida pedimos pra construir a Parse Tree, a árvore, como expliquei no episódio anterior. Pra você que é de front-end, é como quando o navegador carrega uma página HTML, que é um stream de caracteres, e gera o DOM, a árvore que representa aquele HTML. Agora criamos uma instância do nosso CustomListener passando a referência pra esse array “result” que é onde vamos escrever o javascript final. Pra gerar essa parse tree, começamos da primeira regra que tem no nosso arquivo .g4 do parser, que é a regra chamada &quot;parse&quot;, por isso chamamos &lt;code&gt;parser.parse&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finalmente, fazemos o antl4 passear pela parse tree, nó a nó, por isso essa função chama &lt;code&gt;ParseTreeWalker&lt;/code&gt; e damos como parâmetro pra ele o Listener, que tem as regras de conversão, e a árvore propriamente dita. E é aqui que entra meu CustomListener. Vamos entrar lá de novo. Aqui eu vou dar uma trapaceada porque quero gerar o javascript baseado no meu fake sql, lembra? As funções em javascript que eu criei chamada &quot;select&quot;, &quot;from&quot;, &quot;orderBy&quot;. Pra isso eu vou pegar os pedaços que a ParseTree vai me dando e vou preenchendo nesse objeto construído pela função &lt;code&gt;buildSqlStruct&lt;/code&gt; que eu inventei. Ela tem os pedaços que formam um comando SQL. Olha só como tem um array pra colunas, outro array pra condições, e flags pra saber se tem &quot;order by&quot; e coisas assim.&lt;/p&gt;

&lt;p&gt;Logo embaixo tem a função &lt;code&gt;enterParse&lt;/code&gt;, mas nesse momento eu não preciso fazer nada. Deixa ele ir navegando pela árvore até achar um comando como &quot;insert&quot; ou &quot;select&quot;. Digamos que eu tenha um comando como &lt;code&gt;insert into users (name, age, city) value ('Katar Hol', 37, 'Thanagar')&lt;/code&gt; como mostrei lá no começo. Um comando sql normal de insert. Olha o nosso arquivo de parser .g4, a regra do insert se chama &lt;code&gt;insert_stmt&lt;/code&gt; de statement. Então provavelmente quando o parser achar o insert, vai chamar a função &lt;code&gt;enterInsert_stmt&lt;/code&gt; passando um contexto nessa variável &lt;code&gt;ctx&lt;/code&gt; que é a sub-árvore do comando de insert.&lt;/p&gt;

&lt;p&gt;Essa parte foi a mais trabalhosa que é decifrar o que vem nesse contexto. E já aviso que a documentação que achei online não foi muito útil não. Na prática, o que eu aprendi a fazer foi executar o parser com um comando SQL de teste e vasculhar o que vem no contexto usando o bom e velho &lt;code&gt;console.log&lt;/code&gt; e eu entendi que o que importa é saber quem são os nós filhos, ou seja &lt;code&gt;ctx.children&lt;/code&gt;. Então, vamos lá na função de entrada do insert e deixa eu colocar esse &lt;code&gt;console.log&lt;/code&gt; aqui. Se eu printar o ctx inteiro vai ser super longo, então só imprimo o nome da classe a o &lt;code&gt;getText()&lt;/code&gt; que é representação em texto desse nó. Vamos abrir o REPL e mandar o comando insert de teste.&lt;/p&gt;

&lt;p&gt;Olha só o que ele imprime: os nós, que são instâncias de TerminalNode costumam ser tokens como o comando &quot;insert&quot; e as outras partes do comando como &quot;into&quot;. Daí quando tem um nome de tabela, com outra super classe, no caso Table_nameContext, daí vem outro TerminalNode que é a abertura de parênteses. Daí temos um Column_nameContext que é o campo &quot;name&quot; e assim por diante. Foi isso que o Lexer fez, ele quebrou o comando e o Parser organizou nesse sub-árvore de nós que podemos navegar. Vamos voltar pro código no Listener.&lt;/p&gt;

&lt;p&gt;Já sabemos que é só navegar por cada nó dos filhos do contexto, então pra cada &lt;code&gt;child&lt;/code&gt; de &lt;code&gt;ctx.children&lt;/code&gt; eu faço um switch case. Se a super classe for Table_nameContext eu preencho o nome da tabela naquele objeto sqlStruct. Próximo nó, se for um TerminalNode, não faço nada. Próximo nó, se for um Column_nameContext vou acumulando numa lista de colunas no objeto sqlStruct e mesma coisa se forem ExprContext, são os valores, então vou acumulando na lista &lt;code&gt;values&lt;/code&gt; no sqlStruct. Quando acabar a lista de nós filhos, sai dessa função. Daí o Walker vai chamar a função de saída que, por convenção, vai se chamar &lt;code&gt;exitInsert_stmt&lt;/code&gt;, por acaso é a função logo embaixo nesse Listener.&lt;/p&gt;

&lt;p&gt;Pronto, quando entra nessa função sabemos que navegamos por todo o comando insert. Agora é só pegar o que cadastramos no objeto sqlStruct e literalmente montar uma string com a versão do comando em javascript. Primeiro eu monto uma string combinando o nome de cada coluna como chave, dois pontos, o valor, que é a sintaxe de javascript pra montar um objeto. Depois eu começo a montar a função &quot;insert&quot;, concateno o nome da tabela e depois junto os valores com vírgula entre chaves. Isso vai gerar esse comando em javascript que eu mostro no REPL antes de executar.&lt;/p&gt;

&lt;p&gt;Expliquei meio rápido mas deu pra entender a idéia? Todo comando que eu passar praquela função SplitQueries vai passar pelo Lexer e Parser, e vai gerar uma árvore, a Parse Tree. Daí eu ando nessa árvore, nó a nó, usando as funções do Listener. Toda vez que quero implementar um comando SQL, pego a função de entrada, como a &lt;code&gt;enterInsert_stmt&lt;/code&gt;, pego o contexto que é a sub-árvore e vou dividindo os pedaços que preciso e acumulando na minha estrutura sqlStruct. E na função de saída monto a string com a versão em Javascript, que é o que a função SplitQueries vai retornar, no caso pro REPL.&lt;/p&gt;

&lt;p&gt;Vamos abrir o script do tal REPL, a linha de comando que vim mostrando até agora. Eu uso uma biblioteca chamada &quot;prompt-sync&quot; que faz exatamente o que o nome diz: abre um prompt que fica esperando eu escrever algum comando. Tá vendo ele escrevendo &quot;stupid query&quot;? Daí eu digito o comando insert e ele captura nessa variável &lt;code&gt;query&lt;/code&gt;, que vai passar pra função &lt;code&gt;parseScriptWithSplitQueries&lt;/code&gt; que tá definido lá em cima. Igual eu tava fazendo no script &lt;code&gt;stupid.mjs&lt;/code&gt; que lia comandos do arquivo &lt;code&gt;test.sql&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Essa função passa o script pro SplitQueries, que é o parser, que vai navegar e transformar a árvore e cuspir um string com o comando javascript do nosso fake-sql. Pra cada linha que ele cuspir dou um &lt;code&gt;eval&lt;/code&gt; e executo, imprimo o resultado na tela com o &lt;code&gt;console.table&lt;/code&gt; e no final abro o prompt de novo, e fica nesse loop, que é o que um repl tem que fazer mesmo. E pronto, é assim que se constrói um interpretador simples de SQL em javascript.&lt;/p&gt;

&lt;p&gt;O que eu fiz no Listener pra converter o insert em javascript agora preciso fazer a mesma coisa pros comandos delete, update, e select, que é o mais complicado de todos. Vamos só bater o olho no select. Começa pela função &lt;code&gt;enterSelect_stmt&lt;/code&gt;. Diferente do insert que forcei olhar pros nós filhos direto, aqui só crio o objeto sqlStruct que vou precisar preencher e deixo o Walker passear pelos próximos nós pra ir coletando o que eu preciso. Por isso falei que é mais complicadinho, porque precisa implementar várias regras diferentes do Listener que não vou mostrar em detalhes aqui.&lt;/p&gt;

&lt;p&gt;Em particular um select é complicado porque nas condições que começa depois do Where eu posso ter uma série de expressões, e expressões dentro de expressões. Começo a capturar lá em cima na função &lt;code&gt;enterExpr&lt;/code&gt;ession. E aqui eu chamo uma função que vai ser recursiva, justamente porque pode ter expressões dentro de expressões. Falei que ia ser complicado. Não tenta entender linha a linha num primeiro momento, começa tentando entender só o conceito.&lt;/p&gt;

&lt;p&gt;Não vou nem tentar ir passo a passo, de novo, fica de lição de casa, mas só pra mostrar rapidinho, essa função &lt;code&gt;recursiveChildren&lt;/code&gt; vai tentando interpretar uma expressão de cada vez. Por exemplo, em SQL podemos fazer algo como &quot;age between 60 and 2000&quot; e em javascript isso tem que virar &quot;age &gt;= 60 &amp;amp;&amp;amp; age &amp;lt;= 2000&quot; e esse tratamento começa nesse if que checa se esbarrou num &quot;between&quot; e cria uma estrutura pra isso.&lt;/p&gt;

&lt;p&gt;Ou pior, em SQL podemos ter algo como &quot;city in ('Gotham', 'Metropolis')&quot; e em javascript isso tem que virar &quot;['Gotham', 'Metropolis'].includes(city)&quot; e de novo olha aqui no código o if que ativa se esbarrar no token &quot;IN&quot; pra criar uma estrutura de &lt;code&gt;range&lt;/code&gt;. Continuando, em SQL o &quot;NOT&quot; vem antes da expressão, então quando falo &quot;NOT age &gt; 40&quot; tem que virar &quot;age &amp;lt;= 40&quot; que é o inverso. Lá em cima eu fiz uma função chamada &lt;code&gt;invertOp&lt;/code&gt; pra inverter todos os operadores. Se for igual vira diferente, se for maior, vira menor ou igual e assim por diante.&lt;/p&gt;

&lt;p&gt;Enfim, pra conseguir pegar a maioria dos casos mais comuns de select, eu tive um bom trabalhinho. Por isso que agora consigo ir no meu repl e digitar vários tipos de SELECT e ele vai conseguir montar. Quando terminar de transformar todos os nós que preciso da sub-árvore do SELECT, vai cair na função de saída, o &lt;code&gt;exitSelect_stmt&lt;/code&gt;. Só deem uma olhada. No final vai ter preenchido estruturas como sqlStruct.orderby, sqlStruct.table, sqlStruct.conditions e assim por diante. Daí é só montar a string com minha sintaxe de javascript do fake sql e retornar.&lt;/p&gt;

&lt;p&gt;Eu sei que parece complicado mas realmente não é. Se você já teve que usar jQuery pra manipular nós de HTML do DOM num navegadow, é basicamente o mesmo conceito. Quando a gente tem um HTML como &quot;&lt;ul&gt;&lt;li&gt;1&lt;/li&gt;&lt;li&gt;2&lt;/li&gt;&lt;/ul&gt;&quot; que é uma lista, o contexto começa com o nó &quot;ul&quot;, ele vai ter 2 children que são nós &quot;li&quot;, o primeiro &quot;li&quot; vai ter valor 1, o segundo &quot;li&quot; vai ter valor 2. E se tivermos isso aberto no DOM que podemos manipular com javascript, podemos transformar isso em dois parágrafos, por exemplo. E é algo semelhante que fui fazendo nesse Listener, convertendo nós de SQL em string Javascript, pra todos os comandos que eu quis suportar no banco stupid.&lt;/p&gt;

&lt;p&gt;Esse projetinho ainda tá incompleto, como falei antes, coisas como inner join, outer join, sub-selects e coisas assim eu não implementei. Fica de exercício pra vocês se quiserem. Mas vamos recapitular: até aqui fiz um banco de dados em memória bem stupid, que são só arrays de objetos. Fiz funções javascript com nomes que se assemelham aos comandos SQL e chamei isso de Fake SQL. E de bônus usei o Antlr4 pra pegar a gramática do Sqlite3, gerar lexer e parser em javascript e preencher o listener com código pra transpilar SQL de volta em javascript que chama minhas funções de Fake SQL.&lt;/p&gt;

&lt;p&gt;Mas até aqui só expliquei como que de comandos SQL dá pra executar alguma coisa por baixo. Mas pra ser um banco de dados de verdade, ainda faltam funcionalidades que vou explicar só em conceito. O mais importante é vocês entenderem índices. Todo iniciante esquece que existem índices porque não entendem a importância. Alguns sabem que é pra aumentar a performance, mas como em máquina de desenvolvimento, que você dá insert em poucas linhas na tabela, não faz quase nenhuma diferença, todo mundo esquece de usar. Então deixa eu explicar o problema.&lt;/p&gt;

&lt;p&gt;Pra ilustrar criei esse script de benchmark chamado &lt;code&gt;bench_index.mjs&lt;/code&gt;. Agora, esquece o parser de SQL, vamos usar só o fake sql. Notem que lá em cima eu carrego uma nova biblioteca chamada &lt;code&gt;fake-sql-index.mjs&lt;/code&gt;. Pula por um segundo. A primeira função aqui é um timer genérico, passo um comando e ele grava a hora logo antes, a hora logo depois, subtrai os dois pra saber quantos milissegundos levou pra processar. Depois faço uma rotina de criação de índices. Vou explicar o que isso significa já já, mas na prática estou criando um índice pra cada coluna da nossa tabelinha.&lt;/p&gt;

&lt;p&gt;Em seguida crio uma nova função de insert decorada em cima do insert anterior. Ignorem por enquanto, já explico. Agora vem a parte da carga, vou duplicar aquela tabela de 37 usuários 18 vezes. Só que é acumulativo, então da primeira vez, 37 vira 74. Daí duplica de novo e vira 148, daí duplica de novo e vira 296 e faz isso 18 vezes, aumentando a quantidade de objetos exponencialmente até dar quase 10 milhões de objetos em memória.&lt;/p&gt;

&lt;p&gt;Agora olhem o primeiro teste do benchmark, é usando minha função &lt;code&gt;from&lt;/code&gt; lembra? Que vai fazer um eval chamando a função &lt;code&gt;.filter&lt;/code&gt; desse nosso array de quase 10 milhões de objetos e checando um a um se a propriedade &lt;code&gt;city&lt;/code&gt; é igual a &quot;Wakanda&quot;. Ele vai checar isso quase 10 milhões de vezes, objeto a objeto. Porque não tem outra forma nessa versão simples e no nosso teste vai levar algo na faixa de 150 milissegundos, o que é bem rápido porque já tá tudo na memória e fuçar RAM é muito rápido. Se fosse pesquisar num HD seria 100 ou 200 vezes mais devagar que isso, levando vários segundos inteiros pra fazer a mesma pesquisa.&lt;/p&gt;

&lt;p&gt;O segundo teste é diferente, ele usa outras funções que ainda não mostrei e que está naquele arquivo &quot;fake-sql-index.mjs&quot;. Logo no começo chamei a função &lt;code&gt;createIndex&lt;/code&gt; dessa biblioteca. Vamos ver o que ela faz. Olha só, já tinha uma lista chamada &lt;code&gt;databases&lt;/code&gt; pra representar o banco de dados e agora criei outra estrutura global chamada &lt;code&gt;indexes&lt;/code&gt; pra representar índices. Daí crio um índice pra cada chave da tabela. Então a chave pra idade da tabela users seria &lt;code&gt;indexes['users']['age']&lt;/code&gt; e cada uma delas vai ter uma instância desse tal de &lt;code&gt;AVLTree&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;AVLTree é parecida com uma Red-Black Tree. Expliquei isso no vídeo de Árvores, lembra? Deixa recapitular o conceito rapidinho aqui. Eu tenho aquela lista de usuários com nome, idade e cidade, certo? Se eu quiser achar todo mundo da cidade de Wakanda como faz? Do jeito que tá agora preciso olhar o 1o elemento e vai ser o Peter Parker em Nova Iorque, não é. Pula pro próximo, vai ser Clark Kent, em Metropolis. Só quando chegar na linha 12 que vamos achar o T'Challa em Wakanda. Mas eu não sei se é só ele, vamos checando um a um de novo até chegar na linha 25 e achamos a Okoye. Mas ainda não sabemos se acabou, então tem que ir até a última linha 37 que é a Jubilee em Los Angeles e agora sabemos que só tem 2 usuários de Wakanda, mas pra saber isso tivemos que checar 37 vezes. Tem como melhorar isso? Tem.&lt;/p&gt;

&lt;p&gt;O jeito mais ingênuo é jogar numa árvore de procura binária. O processo é simples. Primeiro de tudo ordemos a lista de cidades. Vou simular aqui. Depois, não começamos do começo, da linha 0. Começamos do meio. Metade de 36 é 18. E na linha 18 temos New York. Agora, Wakanda é maior ou menor que New York? Por ordem alfabética é maior, então ignoramos tudo abaixo da linha 18 e vamos pra metade entre 18 e 36, que é a linha 27. E 27 continua sendo outro New York. Wakanda ainda é maior, então pegamos o meio entre 27 e 36 que é 31. De novo Wakanta é maior. O meio entre 31 e 36 é 33. E nessa linha temos San Francisco. Ainda Wakanda é maior. Meio de 33 e 36 é 34. Agora é Themyscira. Wakanda ainda é maior. E finalmente, entre 34 e 36 temos 35, onde encontramos o primeiro Wakanda.&lt;/p&gt;

&lt;p&gt;Esse foi por acaso, o pior caso, onde procurei Wakanda que fica no final da lista. Mas olhem só, no pior caso anterior tinha que vascular 37 vezes. Mas desse jeito binário de ir jogando metade fora a cada checagem, tivemos que fazer só 6 comparações. Ou seja, 6 vezes menos comparações no pior caso. Se em vez de eu procurar na tabela principal, procurar no índice ordenado de cidade, vou descobrir as duas últimas linhas depois de só 7 comparações e isso me resulta nos ids 12 e 25. Agora pego esses ids na tabela principal e acho o T'Challa e a Okoye, totalizando 7 comparações e 2 operações pra pegar os objetos na tabela de usuários. 9 operações, que é quase 4 vezes menos operações.&lt;/p&gt;

&lt;p&gt;Vamos ver o benchmark das 10 milhões de linhas. Vasculhando, que em mundo de banco de dados é o que chamamos de fazer um &quot;table scan&quot; porque estamos escaneando linha a linha da tabela inteira, 10 milhões de vezes, levou na casa de 150 milissegundos. Mas agora buscando no índice, que é o que faz essa função getFromIndex, vai custar na faixa de 10 milissegundos. 15 vezes mais rápido. Mas aí vai levar uns 35 milissegundos pra pegar os ids e buscar os objetos na tabela, que é o que faz essa função &lt;code&gt;getFromIds&lt;/code&gt;, um total de uns 40 milissegundos, que continua sendo pelo menos 3 vezes mais rápido do que um table scan. Lembrando que esses tempos só são em millisegundos porque estamos em RAM, se fosse em disco seria muito mais lento.&lt;/p&gt;

&lt;p&gt;No caso, por causa do javascript, essa função &lt;code&gt;getFromIds&lt;/code&gt; tá prejudicando nosso benchmark. Mas se fosse em C estaríamos falando de uma diferença ainda maior, na casa de tudo 10 vezes mais rápido mais ou menos. Antes que alguém comece a chiar nos comentários, Javascript não é sempre X vezes mais lento que C, mas nesse trecho específico sim. Enfim, uma AVLTree e uma Red-Black Tree, diferente de uma árvore de procura binária simples, são árvores balanceadas. Eu mostro no outro video a diferença de forma animada, mas em resumo, significa que o processo de ordenar e colocar na árvore exige uma operação de rotação pra fazer o balanceamento, então a inserção na árvore é um pouquinho mais lenta do que uma árvore binária simples, mas na hora de procurar vai exigir menos comparações então a pesquisa é mais rápida.&lt;/p&gt;

&lt;p&gt;Pra todas as pesquisas serem mais rápidas, o certo é ter índices pra cada expressão que queira procurar. Se quisesse fazer uma pesquisa por idade e cidade ao mesmo tempo, eu deveria ter um índice que contenha esses dois valores na mesma árvore. Se quiser procurar também separado por idade e em outras pesquisas procurar por cidade, daí preciso ter outro indice separado pra cidade e outro pra idade. Mas aí você pensa, porque diabos tenho que ficar criando índices manualmente, se tudo fica mais rápido com índices, o banco já não cria índices pra todas as colunas automaticamente como eu fiz no meu banco estúpido aqui nesse trecho?&lt;/p&gt;

&lt;p&gt;Agora é hora de falar daquela função nova de insert decorada. Eu usei um decorator só pra não ter que mexer na função de insert original e só plugar a funcionalidade de índice. Vamos ver o que faz esse &lt;code&gt;decorateInsert&lt;/code&gt;. Abrindo a biblioteca &quot;fake-sql-index&quot;, vemos que ela recebe a função &lt;code&gt;insert&lt;/code&gt; antiga e devolve outra função. Dentro ela chama o insert original, mas na sequência insere também os valores nos índices de cada coluna. Ou seja, se estivermos inserindo um novo objeto com os campos name, age, city e id, são 4 novos inserts. O que antes era só 1 operação de insert no array de users, pra atualizar todos os índices, virou um total de 5 inserts. A operação de insert ficou pelo menos 5 vezes mais lenta.&lt;/p&gt;

&lt;p&gt;Imagina uma tabela com 50 colunas. Se todas tiverem índices, cada insert vai exigir 51 operações de insert. Um pra linha na tabela principal e 50 inserts pros índices de cada coluna. Então o insert ficaria 50 vezes mais lento. E não é só isso. Concorda que toda vez que fizer update também precisa atualizar todos os índices? E se fizer delete também tem que deletar de todos os índices? Portanto insert, update e delete todos ficam lentos. E é por isso que não se faz índices pra todas as colunas.&lt;/p&gt;

&lt;p&gt;Índices é um trade-off de deixar operações de pesquisa muito mais rápidas ao custo de deixar operações de escrita muito mais lentos. E nós programadores é que temos que avaliar, testar e achar formas de criar o mínimo de índices possível pra ter a maioria das pesquisas rápidas sem prejudicar demais as operações de escrita. Da mesma forma, não ter índice nenhum significa que as pesquisas podem ser 10 vezes mais lentas, porque vai cair num table-scan. Na prática, todo select complexo vai ser uma combinação de pesquisa em índices e table-scan pras coisas que não tem índice. E é por isso que existe o comando EXPLAIN na maioria dos bancos de dados pra explicar qual estratégia que o banco vai usar pra executar sua pesquisa.&lt;/p&gt;

&lt;p&gt;Se quiser ficar bom em fazer queries que tiram o máximo dos seus índices, estudem como funciona o comando EXPLAIN. Internamente um banco de dados como Postgres pode pegar seu SELECT com condições WHERE complicadas e dividir o trabalho, por exemplo, pra rodar em paralelo em múltiplos núcleos do CPU. Ele vai tentar sempre otimizar a query pra rodar no menor tempo possível, usando a menor quantidade recursos que puder. É bem mais complicado do que eu dar um &quot;filter&quot; num array, obviamente. Mas já deve dar pra dar uma idéia.&lt;/p&gt;

&lt;p&gt;De curiosidade, além dos comandos básicos de SQL como select, insert, ainda podemos programar stored procedures, que são programinhas em SQL, com condicionais como IF, variáveis e tudo mais. No caso de Postgres, a linguagem mais comum é o PL/pgSQL, num Oracle é o PL/SQL, num SQL Server é o T-SQL. Um postgres vai passar o código por um parser mais complicado do que eu mostrei e vai usar LLVM pra fazer o just in time compile desses programinhas, incluindo a fase de transformation passes pra otimizar o código antes de tentar executar. Então existe um JIT até no seu banco de dados hoje em dia. Todo banco de dados tem um interpretador embutido.&lt;/p&gt;

&lt;p&gt;Seguindo no nosso banco estúpido ainda temos um probleminha. Notem que estamos usando o console do Repl do banco stupid pra inserir, apagar linhas, mas quando saímos e entramos de novo, os dados que mudamos não estão mais lá porque sempre carregamos do zero, do mesmo arquivo 'users-database.mjs', mas nunca salvamos em cima desse arquivo. Portanto os dados são fixos. E se a gente quisesse gravar as mudanças em disco pra persistir? Ah, isso é fácil, alguns poderiam dizer. Basta fazer o que todo tutorial de javascript já explicou: transformar a lista de objetos num arquivo JSON e gravar. Certo?&lt;/p&gt;

&lt;p&gt;Totalmente errado. E nem meu banco é stupid o suficiente pra fazer isso. Na realidade vou quebrar o problema em dois. Um banco de dados de verdade tem no mínimo dois mecanismos, pra tanto persistir dados, quanto garantir a integridade desses dados, em particular num banco SQL. Se você estudou o básico de SQL já deve ter ouvido falar que bancos relacionais tem a característica de ser ACID, certo? Atomicidade, Consistência, Isolamento e Durabilidade. Se nunca ouviram falar, é obrigatório estudar isso. Hoje vou falar só de um pequeno aspecto de consistência e durabilidade.&lt;/p&gt;

&lt;p&gt;Em particular, tudo que um banco responde &quot;gravei&quot; depois que você mandou inserir, tem que estar garantidamente gravado de verdade quando pesquisar. Todo conjunto de operações que eu mandar numa transação, ou todas executam ou nenhuma executa. Não pode ter operações feitas pela metade. Tudo isso porque uma característica importante é integridade. Tudo que o banco me confirmar eu preciso acreditar que fez realmente tudo certo, sem exceções. Essa é a característica de todo banco de dados relacional. O mesmo não se aplica a bancos NoSQL como Redis ou Dynamo, eles fazem trade-offs diferentes, mas outro dia falo disso.&lt;/p&gt;

&lt;p&gt;Um dos mecanismos pra ter algumas dessas garantias é o transaction log ou journal. Um sistema de arquivos modernos como NTFS, ext4 ou APFS também tem esse mecanismo, como já expliquei no video de sistemas de arquivos. Como o nome diz é basicamente um arquivo de log, cuja característica principal é ser um arquivo append-only, ou seja, que nunca muda o que já foi escrito e só dá pra ir apendando, só adicionando na frente. O conceito é que antes de executar uma operação você primeiro grava que operação vai fazer nesse log e depois executa. Caso dê alguma catástrofe, tipo acabar a luz e o computador desligar, da próxima vez que reiniciar ele consegue dar replay nesse log e refazer as operações pra trazer os dados de volta num estado consistente.&lt;/p&gt;

&lt;p&gt;Quero simular isso rapidinho pra vocês verem. Pra isso criei um outro módulo chamado &quot;fake-sql-transact&quot;. Abrindo o código, as duas primeiras funções são pra abrir o arquivo de log em modo append  - pra isso que serve esse “a” aqui -, e a segunda é pra apendar alguma coisa nesse arquivo. Em seguida, tenho decorators pras três operações principais que modificam os dados, um pra insert, outro pra delete e outro pra update. Repetindo, um decorator recebe como argumento o nome da função original e devolve outra função como retorno. Dentro dessa nova função vou salvar o comando no log de transações e só depois executo de fato o insert, delete ou update.&lt;/p&gt;

&lt;p&gt;Pra testar dupliquei o script de repl que estava usando até agora e fiz um &lt;code&gt;replTransact.mjs&lt;/code&gt; pra ter essa funcionalidade. Olha o que faço. Primeiro importo as funções insert, deleteFrom e updateFrom do módulo de fake-sql com outros nomes. Daí o resto importa igual. Agora chega nesse trecho onde começo abrindo o arquivo de log de transações que chamei de &lt;code&gt;transact.log&lt;/code&gt;. A primeira coisa que faço é checar se tem algum coisa nesse arquivo e, se tiver, essa função de replay vai repetir todas as operações lá dentro. Como toda vez eu carrego sempre os mesmos dados do arquivo de &lt;code&gt;users-database&lt;/code&gt;, eu sempre perdia tudo que fazia nela. Agora isso vai recuperar o que fiz baseado no log.&lt;/p&gt;

&lt;p&gt;Em seguida abro o arquivo em modo append e recrio as funções de insert, deleteFrom e updateFrom com versões decoradas que recebem o objeto do arquivo de log aberto. A partir de agora quando chamar um insert, por exemplo, ele vai salvar a operação no log e depois fazer as alterações no array de objetos. O resto do script é igualzinho, então vamos sair, ir pro terminal, e carregar esse novo repl. A primeira coisa que vou fazer é dar insert em mais dois usuários. Vamos inserir os desconhecidos de Paradis. Insert do Levi Ackerman, 37 anos, depois deixa eu inserir uma parente distante dele, Mikasa Ackerman, 20 anos.&lt;/p&gt;

&lt;p&gt;Pronto, agora posso fazer &lt;code&gt;select name from users where users.city = 'Paradis'&lt;/code&gt; e estão lá direitinho. Vamos dar Ctrl+C pra sair e abrir o Repl de novo. E olha só, ele executa a função de replay e reinsere os dois. Se eu fizer o mesmo select, eles estão lá direitinho. E com isso eu implementei um tipo estúpido de persistência desses dados. Imagina que isso fosse um servidor de banco normal, como um Posgres, e bem na hora que tá salvando essas linhas em disco, dá uma pane, acaba a energia ou o sistema crasheia. O banco ia ficar num estado inconsistente, mas como antes de tentar gravar na tabela ele salvou as operações num transaction log, no próximo restart, vai conseguir fazer um replay, como o que simulei nessa versão simples.&lt;/p&gt;

&lt;p&gt;Todo banco de dados e todo sistema de arquivos moderno tem um transaction log ou o outro nome que se usa pra isso, que é um journal, que é inglês pra diário. Óbvio que diferente da minha versão estúpida, ele não vai dar replay de todo o arquivo de transactions toda vez que restarta, isso seria estúpido. Ele só dá replay nas últimas transações que não concluíram até o fim. E isso leva à última característica de um banco de dados que quero discutir hoje, que é: como se salva esses dados numa tabela de verdade num HD?&lt;/p&gt;

&lt;p&gt;Bom, já mostrei que meu banco estúpido sempre carrega os dados de um arquivo como o &quot;users-database&quot;. E nesse caso estou persistindo os dados em formato texto, em javascript. Acho que todo mundo aqui já salvou dados assim de outra forma, como um arquivo JSON. Bastaria chamar a função &lt;code&gt;JSON.stringify&lt;/code&gt; e salvar a string num arquivo, certo? Por que a gente não faz só isso? Toda vez que insere ou modifica uma linha naquele array de usuários, basta chamar o &lt;code&gt;JSON.stringify&lt;/code&gt; e salvar tudo no arquivo. Fácil, não?&lt;/p&gt;

&lt;p&gt;E isso estaria errado. Vamos entender o raciocínio pra vocês nunca cometerem esse erro. O que faz uma função como &lt;code&gt;JSON.stringify&lt;/code&gt;? Ele executa uma operação que chamamos de serialização ou marshalling. Daí quando lemos de um JSON e recriamos o array de objetos em memória, chamamos de desserialização ou unmarshalling. Esse é o tipo de operação que tem literalmente em toda biblioteca de APIs que você já tenha usado, como APIs pra integrar com meios de pagamento, ou APIs pra fazer bots de instagram ou twitter. Toda API te devolve um XML ou um JSON e você converte pra objetos da sua linguagem pra poder manipular os dados. Daí prepara um payload qualquer e serializa de volta em XML ou JSON pra mandar pra API.&lt;/p&gt;

&lt;p&gt;Essa operação necessariamente duplica os dados em memória, pra começar. Você já tem o array de objetos em memória, e gera uma versão em JSON, em texto, que também tá em memória. Ambos vão ocupar o dobro ou mais de espaço. Os dados no array estão em binário e toda vez que transforma binário numa representação legível em texto, isso ocupa mais memória. Vamos ver quanto. Eu fiz um scriptzinho besta que usa a biblioteca &quot;object-sizeof&quot; que instalei via npm. E lá embaixo faço uma versão serializada do array de users usando o &lt;code&gt;JSON.stringify&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Olha só. A string com a conversão em JSON do array tá ocupando 4.3 kilobytes na memória. Já a versão original binária em memória do array de objetos de users ocupa uns 3 kilobytes. Portanto a versão JSON é mais de 40% maior. Imagina se fosse um array gigante, com 1 gigabyte. Significa que o JSON ocuparia algo na ordem de 1.4 gigabytes. 400 Mega a mais por causa da conversão em texto e do formato JSON. Representações em texto de estruturas de dados fazem o trade-off de ocupar mais espaço pra gerar um arquivo que é mais fácil de um ser humano como nós, programadores, conseguirmos manipular num editor de textos. Por isso é um bom formato pra arquivos curtos, que não vão crescer o tempo todo. Mas em termos puramente de performance, trafegar JSON é um desperdício de espaço, banda e processamento.&lt;/p&gt;

&lt;p&gt;Exatamente por isso que empresas como o Google preferem usar outro protocolo pra fazer comunicação entre serviços, como o ProtoBuf, que é um protocolo binário de formato de dados. Nenhum game que você joga online trafega JSON pra atualizar a posição do seu personagem no jogo, sempre é um protocolo binário. Mas pra APIs de coisas como mandar mensagens automatizadas no Whatsapp ou Twitter, eles oferecem um protocolo baseado em JSON, porque a maioria das pessoas nunca ia conseguir montar um pacote binário. E também porque costuma ser poucos dados trafegados. Num jogo precisa estar trafegando dezenas ou centenas de pacotes de dados por segundo, por isso todo byte conta. Numa API de mensagens ou pagamentos, você manda poucos pacotes de dados, por isso, mesmo que seja 40% mais pesado, não vai fazer tanta diferença assim pra você.&lt;/p&gt;

&lt;p&gt;Da mesma forma, num banco de dados, a gente primeiro não quer perder 40% de espaço gravando num formato ineficiente como JSON ou XML ou qualquer outro protocolo baseado em texto. Mesmo desconsiderando isso, ainda tem outro problema. Uma função como JSON.stringify, converte a estrutura de dados do array inteira de uma só vez pra texto. Depois não tem como inserir ou modificar só uma linha. Como que adiciona um novo elemento no JSON? Não tem como de maneira rápida. E um banco de dados vai o tempo todo aumentando de tamanho e não dá pra ficar convertendo de binário pra texto, tabelas inteiras toda vez. É inviável. Tem que ter um jeito de ir adicionando só um insert ou update de cada vez, sem mexer no que já tava gravado, sem criar outro arquivo e sobrescrever por cima.&lt;/p&gt;

&lt;p&gt;Pra gravar em disco queremos gravar em formato binário, algo próximo da tal árvore ordenada que mencionei antes e que simulei com índices usando uma árvore AVL. Você poderia pensar, ok, todo Nó de uma árvore contém a referência ou ponteiro na memória pra outros Nós. Basta gravar o nó em disco e mudar de ponteiro em memória pra endereço no disco, daí consigo navegar pelos nós no disco e consigo inserir novos nós fácil, certo?&lt;/p&gt;

&lt;p&gt;Nos videos sobre sistemas de arquivos eu explico como que funciona um sistema de partição, endereçamento em sistemas de arquivos e tudo mais. Recomendo que dêem uma olhada. Mas pros nossos propósitos basta saber que na prática o sistema operacional enxerga um disco como se fosse um array gigante de dados, começando com a posição zero e incrementando pra 1, 2, 3 etc até acabar o espaço de 64-bits, que é endereços pra caramba, terabytes e terabytes. Então beleza, é parecido como memória funciona, que pra uma linguagem é como se fosse um arrayzão gigante que começa em 0 e vai endereçando até acabar a RAM, também com endereços de 64-bits. Então tá fácil, dá pra gravar um binário em RAM pra um binário em disco só convertendo as posições de cada nó quando passa de um arrayzão em memória pro arrayzão em disco.&lt;/p&gt;

&lt;p&gt;Mas discos de HD tem uma diferença importante. Vamos pensar a árvore AVL. Deixa eu mostrar uma representação gráfica de como seria a árvore de índice de idades. Vou inserir aqui todas as idades de todo mundo que tá no array de usuários. Ela vai ficar assim. Conseguem enxergar que essa árvore é uma lista ordenada? Começa lendo da esquerda pra direita. Olha só. 22, 25, 27, 30, 30, 30, 30, 32, 33, 33, 35, 35, 35, 35 etc.&lt;/p&gt;

&lt;p&gt;E se eu quiser achar todo mundo com idade maior que 40? Começa descendo tudo pra esquerda. 22 é menor que 40, vai pra direita, 27 é menor que 40, sobe, 25 ainda é menor, 30 é menor, sobe, 30 ainda é menor, 33 é menor, sobe 30 é menor, 35 é menor, sobe, 35 é menor. sobe, 40 é menor. Desce tudo até 40, é menor, opa 41. sobe, 42. opa. Daqui pra direita é tudo maior que 40. Então eu tenho no final 10 endereços pras posições na memória onde estão esses nós.&lt;/p&gt;

&lt;p&gt;Essas posições em memória não são necessariamente consecutivas. Eu peço um espaço pro gerenciador de memória, tipo malloc, e ele me dá algum endereço que eu não me preocupo. Expliquei isso no episódio de gerenciamento de memória, dêem uma olhada depois. Mas enfim, saiba que toda vez que você inicia uma nova variável na sua linguagem, por baixo a variável é só um nome pra alguma posição na memória, um endereço, que você não sabe. Uma das utilidades de uma linguagem de programação é esconder esses endereços de você pra não ficar complicado. Em Assembly você teria que lidar direto com o endereço. Em disco é a mesma coisa, só que em disco vai ser a localização em coordenadas tridimensionais de qual dos discos, em qual trilha e em qual posição da trilha.&lt;/p&gt;

&lt;p&gt;Tão vendo esse CD aqui, um HD é como se fossem vários CDs um em cima do outro. Em cima e embaixo de cada disco desse tem um cabeçote com um leitor, que vai navegar nos diversos cilindros ou trilhas desse disco. Agora imagina que aqueles 10 endereços que retornou pras idades maior que 40 seja cada um num disco diferente, em trilhas diferentes, em posições aleatórias. Concorda que pra ler tudo, o HD vai ter um trabalhão? Especialmente porque os discos não estão parados, eles ficam girando sem parar dentro do HD e o cabeçote tem que ir lendo até bater no endereço que foi pedido, daí ele muda de cilindro, e procura outro endereço. É muito trabalhoso pegar endereços aleatórios num disco, é muito lento. HDs funcionam melhor quando queremos pegar pedaços mais longos de dados de uma só vez numa única passada do cabeçote na trilha.&lt;/p&gt;

&lt;p&gt;E essa nossa árvore AVL é super curtinha. Imagina um banco de dados de milhares ou milhões de nós. Seu HD ia fritar pra ficar procurando nós individualmente e aleatoriamente. Mas tem jeito melhor? Tem sim, na verdade tem dois jeitos, as árvores B-Tree e B+Tree que eu cheguei a mencionar no episódio de árvores e agora espero que faça mais sentido pra que eles servem. Deixa eu mostrar como ficam as mesmas idades ordenadas organizadas numa árvore B-tree.&lt;/p&gt;

&lt;p&gt;Nem vou inserir todo mundo, só as idades dos 10 primeiros usuários. Olha o formato que essa árvore tá ficando. Entenderam? Cada nó pode ter mais de um valor. Em particular se eu pesquisar só quem tem mais de 40 anos, aí vou ter só esses dois grandes nós aqui na direita. E eles vão ser lidos do disco em no máximo 2 operações, em vez de ter que pesquisar e ler de 10 endereços aleatórios como no exemplo da árvore AVL. A grande sacada é justamente isso de acumular vários valores um atrás do outro num único nó grandão, num bloco ou cluster. Isso porque um HD não lê byte a byte, ele lê em setores ou blocos de uma só vez. Todo HD vai sempre ler pelo menos um bloco de 512 bytes ou até 4 kilobytes de uma só vez e colocar num buffer.&lt;/p&gt;

&lt;p&gt;Se não ficou claro, lembra quando se formata um HD novo? Na tela de formatação ele pergunta sobre tamanho da unidade de alocação. Por padrão costuma ser 4 kilobytes que é o tamanho do setor. É a menor unidade que ele lê ou grava de uma só vez. Quanto maior for, melhor pra ler depois, mas desperdiça mais espaço, porque quando precisar gravar menos de 4 kilobytes, o que faltar, não pode reusar mais. Mas enfim, em um único setor do disco cabe nossa estrutura de usuários inteira, que a gente viu que ocupava menos de 4 kilobytes lembra? Então num HD nossa tabelinha de usuários cabe inteiro num único setor e consegue ser lido do HD de uma só vez.&lt;/p&gt;

&lt;p&gt;O trabalho pra ler um único byte não compensa pro HD, como o disco tá girando sem parar, na hora que a cabeça começa a ler, ele já lê de uma só vez vários bytes. Ler um byte ou ler 4 kilobytes, é quase o mesmo trabalho. Então se eu organizar a árvore pra já conseguir organizar em clusters, fica mais fácil de ler depois. Na hora que o HD for ler esse valor 41 do exemplo, já vai ler de uma vez o 42, 43 e 47. Como já organizei na árvore pra serem valores consecutivos, ou seja, um atrás do outro sem intervalo, ele vai ler os 4 valores corretos numa única passagem.&lt;/p&gt;

&lt;p&gt;Nesse exemplo eu fiz nós que vão ter no máximo uns 7 valores, mas numa árvore B-tree de verdade vai ter dezenas ou centenas de valores um atrás do outro em cada nó. E é assim que o HD vai gravar e vai ler de uma só vez. A diferença de uma árvore B-tree e B+tree é que no B+tree só os últimos nós da árvore que vão conter dados, os nós acima só vão conter metados, os ponteiros pros nós de baixo.&lt;/p&gt;

&lt;p&gt;Atualizar a árvore é significativamente mais complicado do que a rotação que já mostrei de uma árvore simples Red Black ou AVL, mas o layout pro HD é muito mais amigável. Recomendo depois procurar num livro de algoritmos, como o do Cormen, os detalhes sobre uma B+Tree. No Cormen inclusive ele explica exatamente sobre essa vantagem e no meu repositório no GitHub desse episódio adicionei outro repositório como sub módulo que encontrei alguém que implementou em C a B+Tree do Cormen pra gravar a árvore em disco, pra quem tiver curiosidade.&lt;/p&gt;

&lt;p&gt;E de fato, se eu procurar por &quot;btree&quot; no repositório do código fonte do banco sqlite3, vamos encontrar várias menções a árvore B-tree que ele usa pra gravar as tabelas, os índices, e tudo mais. E se sair procurando em outros bancos como MySQL, Postgres, até Redis e outros, você vai encontrar uma variação dessa árvore sendo usada.&lt;/p&gt;

&lt;p&gt;Em resumo o importante é entender que um bom banco de dados precisa levar em consideração a estratégia e custos de ler e gravar dados em disco, em blocos, vários bytes de uma só vez. Um programador iniciante que só lida com variáveis em memória, costuma pensar só um byte de cada vez, mas é bom saber que dependendo do que tá fazendo, é mais eficiente pegar mais dados do que precisa e descartar o que não precisa. Não é intuitivo mas é justamente por isso que ninguém deve sair fazendo um banco de dados do zero sem ter conhecimento pra isso. Existem vários papers e anos de pesquisa pra cada aspecto de um banco de dados modernos. Lembrem-se que a gente vem fazendo isso faz mais de 60 anos.&lt;/p&gt;

&lt;p&gt;A intenção do episódio de hoje não foi incentivar ninguém a escrever o próprio banco, mas sim entender alguns dos principais componentes que um banco de dados de verdade precisa ter. Muita gente tenta racionalizar um banco com variáveis e arrays em memória, e espero que tenha ficado mais claro como é bem mais do que isso. E também porque chamei o episódio onde explico árvores de &quot;O Começo de Tudo&quot;. Compiladores precisam de árvores. Bancos de dados precisam de árvores. Tudo precisa de árvores, e se você não consegue enxergar listas ordenadas como árvores, está perdendo o entendimento de como tudo funciona em computação.&lt;/p&gt;

&lt;p&gt;Aliás, como último exercício de tudo que fiz até agora no meu banco estúpido, faltou mostrar mais uma coisa. Como transformar esse meu console de linha de comando de SQL em um sistema cliente/servidor, como um banco de verdade. E esse nem vou explicar o código, porque é um servidor simples de TCP. Pra quem aprendeu redes na faculdade isso aqui é o primeiro dia de aula. Mas olha só. Num terminal vou executar o servidor com &lt;code&gt;./stupid-server.mjs&lt;/code&gt; e pronto, isso vai pendurar o meu servidor na porta 4000 da minha máquina.&lt;/p&gt;

&lt;p&gt;Em outro terminal, vou executar &lt;code&gt;./stupid-client.mjs&lt;/code&gt; que vai tentar se conectar na porta 4000. E no terminal do servidor, mostra que alguém se conectou. Esse cliente não tem nenhuma lógica do banco, não importa nenhum código do interpretador de SQL nem o array de users, mas eu posso mandar comandos em SQL. Esses comandos vão trafegar pela rede e chegar no servidor. O código do servidor é parecido com o REPL, e no fim devolve o resultado de volta pro cliente.&lt;/p&gt;

&lt;p&gt;E pra provar que é um servidor mesmo, posso abrir outro Terminal separado e abrir mais um &quot;stupid-client&quot;. O servidor mostra que mais alguém se conectou. Posso mandar mais SQL por aqui, e ele vai responder. E esse é o esqueleto básico de como um servidor de banco de dados funciona. Depois estudem o código no repositório do GitHub pra ver como eu fiz isso, mas se entenderam até aqui, essa foi a parte mais fácil, eu literalmente fiz esse cliente e servidor em uns 15 minutos.&lt;/p&gt;

&lt;p&gt;Fazer um banco de dados de verdade exige conhecimento bem mais avançado do que isso em ciências da computação e vai exigir centenas, milhares de horas homem de desenvolvimento, testes e ajustes. Essa minha versão estúpida eu justamente chamo de estúpido porque fiz só as coisas mais fáceis, o suficiente pra fazer esse episódio. Eu basicamente peguei o feriado e pensei &quot;hum, que tipo de banco SQL será que consigo fazer em uns 2 ou 3 dias de trabalho&quot;, e foi isso aqui que mostrei pra vocês. Em particular, o interpretador de SQL, eu nunca tinha usado Antlr4 antes, então foi a parte que gastei mais tempo pra fazer, devo ter gasto pouco mais de 1 dia pra fazer o parser e interpretador. Portanto, tendo conhecimentos básicos de algoritmos e as coisas que já vinha explicando no canal, dá pra fazer esse exercício do zero em uns 3 dias.&lt;/p&gt;

&lt;p&gt;Eu me coloquei um limite de tempo porque não dava pra ficar eternamento mexendo no código, uma hora eu precisava gravar este episódio. Mas se alguém se interessar puramente pelo exercício, o próximo passo que não fiz foi justamente mover o array de objetos do &quot;users-database&quot; pra uma árvore B+tree e gravar em disco, incluindo os índices. Só que aí precisaria também implementar um cache, pra não precisar ir toda hora no disco, e precisaria implementar um sisteminha simples de lock, caso na versão cliente e servidor, vários clientes tentem modificar os dados ao mesmo tempo.&lt;/p&gt;

&lt;p&gt;Além disso era bom fazer um limite de conexões. Javascript não tem multi-thread real, então teriamos um limite do que é possível fazer. E nesse caso sintam-se a vontade de reimplementar tudo numa linguagem diferente como Java ou C# ou qualquer outra coisa assim. Só que se fizerem numa linguagem que não é dinâmica, aí vão ter mais trabalho por não terem a trapaça que eu fiz usando &quot;eval&quot; pra executar as condições de where. Enfim, dá pra ficar brincando bastante em cima disso ainda.&lt;/p&gt;

&lt;p&gt;Pra só usar um banco de dados precisa saber em detalhes como ele foi feito? Claro que não. Justamente o que mostrei aqui já é um modelo mínimo suficiente pra ter na cabeça pra não parecer que é uma caixa preta mágica. Usem isso pra ajudar a explicar os principais conceitos técnicos de como um banco de dados funciona pra colegas e amigos iniciantes. Quero saber se isso ajudou a mudar um pouco o que vocês imaginavam que era um banco de dados. Num outro episódio vou falar mais especificamente de bancos de verdade como um Postgres, Redis ou Dynamo. Mas por hoje acho que já ficou comprido o suficiente. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e compartilhem o video pros seus amigos. A gente se vê, até mais.&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5951</id>
    <published>2022-04-15T17:34:00-03:00</published>
    <updated>2022-04-15T16:35:47-03:00</updated>
    <link href="/2022/04/15/akitando-117-linguagem-compilada-vs-interpretada-qual-e-melhor" rel="alternate" type="text/html">
    <title>[Akitando] #117 - Linguagem Compilada vs Interpretada | Qual é melhor?</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/SNyh-cubxaU&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Chegou a hora de finalmente entender qual a diferença de linguagens compiladas e interpretadas, linguagens estáticas e dinâmicas. Java é compilado? Javascript é interpretado? Qual a diferença?&lt;/p&gt;

&lt;p&gt;Hoje você vai ganhar uma fundação mais sólida pra entender linguagens da maneira correta e é o pré-requisito pros próximos videos onde finalmente vou discutir as linguagens mais famosas da atualidade.&lt;/p&gt;

&lt;p&gt;== Conteúdo&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;00:00:00 - Intro&lt;/li&gt;
&lt;li&gt;00:00:55 - Pré-Requisitos&lt;/li&gt;
&lt;li&gt;00:01:53 - Hello World em C e Java&lt;/li&gt;
&lt;li&gt;00:03:03 - ELF vs CAFE&lt;/li&gt;
&lt;li&gt;00:03:41 - 1a tentativa: compilador vs interpretador&lt;/li&gt;
&lt;li&gt;00:04:45 - Estudo de Linguagens&lt;/li&gt;
&lt;li&gt;00:07:37 - Análise Léxica&lt;/li&gt;
&lt;li&gt;00:10:59 - Análise Sintática&lt;/li&gt;
&lt;li&gt;00:14:39 - Abstract Syntax Tree (AST)&lt;/li&gt;
&lt;li&gt;00:15:48 - Notação Polonesa&lt;/li&gt;
&lt;li&gt;00:17:58 - Otimização de Bytecode&lt;/li&gt;
&lt;li&gt;00:22:55 - Pra que serve um Programador?&lt;/li&gt;
&lt;li&gt;00:27:04 - Linters&lt;/li&gt;
&lt;li&gt;00:28:00 - Backus, Naur, BNF e História&lt;/li&gt;
&lt;li&gt;00:31:57 - Parsers e &quot;DOM&quot;&lt;/li&gt;
&lt;li&gt;00:32:58 - Interpretadores e Máquinas Virtuais&lt;/li&gt;
&lt;li&gt;00:36:56 - Linguagens Dinâmicas&lt;/li&gt;
&lt;li&gt;00:38:58 - Otimização Binária&lt;/li&gt;
&lt;li&gt;00:42:52 - As Fases de um Compilador&lt;/li&gt;
&lt;li&gt;00:46:23 - Just-In-Time Compiler (JIT)&lt;/li&gt;
&lt;li&gt;00:50:33 - Linkers&lt;/li&gt;
&lt;li&gt;00:59:24 - JIT de novo&lt;/li&gt;
&lt;li&gt;01:02:30 - Google V8&lt;/li&gt;
&lt;li&gt;01:05:25 - Por que dinâmico em vez de estático?&lt;/li&gt;
&lt;li&gt;01:07:42 - 2a tentativa: compilador vs interpretador?&lt;/li&gt;
&lt;li&gt;01:10:45 - Bônus: Bloopers (novidade)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;== Links&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ANSI C Grammar (https://www.lysator.liu.se/c/ANSI-C-grammar-y.html#compound-statement)&lt;/li&gt;
&lt;li&gt;EcmaScript 2023 (https://tc39.es/ecma262/#sec-ecmascript-language-statements-and-declarations)&lt;/li&gt;
&lt;li&gt;List of Java bytecode instructions (https://en.wikipedia.org/wiki/List_of_Java_bytecode_instructions)&lt;/li&gt;
&lt;li&gt;Why the New V8 is so Damn Fast (https://nodesource.com/blog/why-the-new-v8-is-so-damn-fast/)&lt;/li&gt;
&lt;li&gt;V8 Bytecode.h (https://github.com/v8/v8/blob/master/src/interpreter/bytecodes.h)&lt;/li&gt;
&lt;li&gt;LLVM Analysis and Transform Passes (https://llvm.org/docs/Passes.html)&lt;/li&gt;
&lt;li&gt;HHVM (https://hhvm.com/)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Todo bom iniciante que se preza mais cedo ou mais tarde vai se engajar numa discussão sobre porque sua linguagem favorita é melhor que dos outros. E invariavelmente um dos argumentos que vai aparecer é que ser compilado é melhor, é mais performático, ou que ser interpretado é melhor porque é mais produtivo. E certamente todos que estão participando dessa discussão estão errados de maneiras que vocês nem imaginam.&lt;/p&gt;

&lt;p&gt;Hoje quero falar um pouco sobre o que significa uma linguagem ser compilada ou interpretada e porque a maioria dos programadores interpreta isso errado (no pun intended). O objetivo não de hoje não é dizer qual é melhor, e sim porque sua linha de raciocínio não leva em conta diversos outros fatores que são mais importantes. No final do video você vai enxergar todas as suas linguagens de uma forma diferente.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Pra todo mundo que tá estudando ou já se formou em ciências da computação: não, eu não vou falar do livro do dragão de compiladores. Também não vou falar do livro de sistemas operacionais do Tanenbaum. Porém todo mundo que estudou essas duas matérias vai ter um aproveitamento melhor desse video. Aproveitem pra discutir os detalhes técnicos com seus colegas ou fazer mais perguntas pros seus professores na faculdade e nos cursos.&lt;/p&gt;

&lt;p&gt;Compiladores e sistemas operacionais são conteúdos pra pelo menos um ano ou mais no curso de ciências da computação. Partes desses assuntos eu já expliquei em videos como o guia mais hardcore de introdução à computação e hello world como você nunca viu antes, e todos os outros na playlist de &quot;Como Seu Computador Funciona&quot;. Mas pra ter uma idéia, na faculdade, um dos trabalhos é fazer seu próprio compilador. Se nunca fez, depois do video fica de desafio tentar fazer o seu próprio. E pra isso já dou spoiler pra pesquisar tutoriais sobre a ferramenta ANTLR. Mas não vou falar especificamente de ANTLR hoje também.&lt;/p&gt;

&lt;p&gt;Vamos começar do começo. Isso aqui é um Hello World em C. Agora, isso aqui é um terminal de Linux no WSL2 do meu PC e compilamos esse arquivo texto com o código em C num binário ELF compatível com Linux usando o compilador &quot;cc&quot;, que significa &quot;Compiler Collection&quot;. E pronto, isso é tudo que a maioria tem na cabeça quando se pensa em transformar código texto em um executável e no video sobre arquivos texto e binários eu explico como realmente as coisas funcionam e como um executável funciona. Mas vamos pra outro exemplo.&lt;/p&gt;

&lt;p&gt;Agora, isso aqui é um Hello World em Java. Novamente vamos pro Terminal e é assim que se aprende na faculdade, roda &lt;code&gt;javac Hello.java&lt;/code&gt; e ele vai gerar um &lt;code&gt;Hello.class&lt;/code&gt;. E pra rodar é só fazer &lt;code&gt;java Hello&lt;/code&gt; e voilá. Muita gente costuma dizer que Java é uma linguagem &quot;compilada&quot;, entre aspas. Só que olha o que estamos fazendo: chamando o executável &quot;java&quot; antes e dando como parâmetro a classe &quot;Hello&quot; que acabamos de compilar. Se fosse realmente compilado, no sentido clássico, deveria dar pra só fazer &lt;code&gt;./Hello.class&lt;/code&gt;, mas isso não funciona, o sistema operacional não tem nenhuma idéia de como executar isso, porque um binário &lt;code&gt;.class&lt;/code&gt; não é um executável em formato ELF.&lt;/p&gt;

&lt;p&gt;Como tinha explicado no video de arquivos, pro Linux saber que um binário pode ser executado, ele precisa estar num formato chamado ELF, que significa Executable and Linkable Format. Em particular, todo binário ELF começa com a sequência que em hexadecimal seria 7f 45 4c 46, que propositalmente é ASCII pra string &quot;ELF&quot;. Isso é totalmente arbitrário e quem inventou o ELF só definiu que é assim e pronto. Mas se abrirmos os primeiros bits da classe Hello de Java, o que vemos nos primeiros bytes é a sequência hexadecimal &lt;code&gt;cafe babe&lt;/code&gt;. Obviamente uma piada de café e java que o James Gosling escolheu pra serem os primeiros bytes de todas as classes Java.&lt;/p&gt;

&lt;p&gt;Se definirmos que compilar é o processo de pegar um código fonte texto, por exemplo em C, passar por uma ferramenta como o &quot;cc&quot;, que vai cuspir um binário que o sistema operacional vai conseguir executar diretamente, então não, o Java não é compilado. Por outro lado vamos definir um interpretador. Um interpretador é um programa que vai pegar ou o arquivo texto do código ou uma representação intermediária, como esse &quot;.class&quot; e vai traduzir pro binário de máquina que, aí sim, o sistema operacional e o processador vão entender. Portanto, por essa definição, Java é uma linguagem interpretada.&lt;/p&gt;

&lt;p&gt;Isso é o que você vai encontrar na bolhadev, e na verdade o que acabei de falar não tá nem totalmente certo, nem totalmente errado. Tanto C quanto Java são linguagens compiladas. E antes de explicar qual a diferença, preciso começar explicando o que é o processo de compilação. Esse processo tem diversos passos mas minha intenção não é escovar bits hoje, mas sim fazer com que vocês tenham um modelo mental fácil na cabeça pra não cairem em pegadinhas de discussão online e conseguir entender as principais diferenças.&lt;/p&gt;

&lt;p&gt;O estudo de linguagens de programação é meio que um sub-conjunto do estudo de linguagens em geral, tipo português ou inglês. Em ciência da computação, na teoria formal de linguagens, as linguagens de programação como C ou Java ou Python são o que chamamos de linguagens regulares. Por sua vez, uma linguagem regular é uma linguagem formal que pode ser definida por expressões regulares, sabe o regex que você usa todo dia pra validar formato de emails ou cpfs? E por sua vez uma expressão regular pode ser definida como uma linguagem que pode ser reconhecida por um autômato finito. Älguns de vocês já devem ter esbarrado no termo &quot;máquinas de estado finito&quot;, que é outro nome que se dá a autômatos de estado finito.&lt;/p&gt;

&lt;p&gt;Mas relaxa, nem eu sei tudo em todos os detalhes e não vou descer na matemática por trás de autômatos não, mas é pra saber que existe esse campo de estudo. Na prática, pense assim. Temos um texto em português. O que é um texto? É uma coleção de parágrafos. E o que são parágrafos? É uma coleção de frases. E o que é uma frase? É uma coleção de palavras. Palavras são coleções de letras separadas por espaço ou outras pontuações, como vírgula ou ponto.&lt;/p&gt;

&lt;p&gt;Então, se eu juntar quaisquer letras, vou ter palavras? Se eu juntar quaisquer palavras, vou ter frases? Não. Existe uma ordem que funciona. Conjuntos de letras só formam palavras se elas existem num dicionário, que é uma coleção arbitrárias de conjuntos de letras, que a gente definiu. Se eu juntar as letras &quot;h&quot;, &quot;e&quot;, &quot;l&quot;, &quot;l&quot;, &quot;o&quot;, um atrás do outro, no dicionário em inglês, isso define uma saudação e uma forma de iniciar uma comunicação por telefone, por exemplo. Tire um &quot;l&quot; dessa palavra e já não significa a mesma coisa.&lt;/p&gt;

&lt;p&gt;Pra formar frases, não pode sair juntando quaisquer palavras. Elas precisam obedecer uma gramática. Sujeito, predicado, adjetivos, pronomes, tempos verbais e tudo mais que fazem uma frase ter sentido. Se eu disser a frase &quot;não vai ter jeito&quot;, significa que não tem mais conserto, fodeu já. Mas se na verdade faltou uma vírgula nessa frase depois do não, então seria &quot;não, vai ter jeito&quot;, que é exatamente o significado oposto. Uma única vírgula faz diferença. Quem já programou e teve bug porque esqueceu uma vírgula, em português, também tem bug se esquecer uma pontuação.&lt;/p&gt;

&lt;p&gt;Voltando pro código C. Pro computador, um texto, é só um linguição de bytes, um linguição de caracteres. Tem letras, tem espaços, tem chaves, mas não quer dizer nada. Mesmo pra você, ser humano assistindo, se nunca programou, esse texto também não quer dizer bulhufas. Eu preciso dar um jeito de fazer o computador juntar letras que formam palavras, que no caso chamamos de &quot;tokens&quot;, que fazem sentido. &quot;int&quot; é um conjunto de letras que faz sentido. &quot;printf&quot; é outro conjunto de letras que faz sentido. Então preciso conseguir quebrar esse texto todo em uma lista de tokens. Pra isso vou &quot;tokenizer&quot;, ou &quot;lexar&quot;, e isso é o trabalho de uma ferramenta que faz análise léxica.&lt;/p&gt;

&lt;p&gt;Na realidade precisamos definir os lexemes da linguagem, o equivalente de dicionário. Podemos definir que dígitos são todos os caracteres de zero até 9. E definimos como não-dígitos, todas os caracteres de &quot;a&quot; até &quot;z&quot;, em mínusculo e maiúsculo. Também definimos que pontuação são todos estes outros caracteres como parênteses, colchetes, vírgula, ponto e tudo mais. Alguns desses caracteres podem repetir em outra definição. Parênteses podem ser operadores, assim como sinal de mais, de menos, asterisco. E assim por diante. Agora um analisador léxico como um chamado &lt;code&gt;flex&lt;/code&gt; pode pegar meu código em C e entender que tem sinais, strings, digitos, operadores e traduzir isso numa lista de tokens.&lt;/p&gt;

&lt;p&gt;Uma vez tendo essa lista de tokens, precisamos saber o que significam. Eu preciso que o computador entenda que, o que vai entre parênteses depois do token &quot;printf&quot;, é um argumento. Eu preciso que ele entenda que o &quot;return 0&quot; na última linha entre chaves se refere ao tipo &quot;int&quot; declarado no começo da função &quot;main&quot;. Elas precisam estar seguindo alguma regra. Uma vez tendo uma lista de tokens, precisamos checar contra uma gramática.&lt;/p&gt;

&lt;p&gt;Antes de ficar teórico demais, deixa eu mostrar um exemplo prático. Pra isso fiz a linguagem de programação mais idiota do mundo. Ela não é turing complete, só aceita uma expressão no formato &lt;code&gt;1 add 2&lt;/code&gt; pra somar, ou &lt;code&gt;4 sub 3&lt;/code&gt; pra substrair. Só isso. Olha só, escrevi um programa nessa linguagem e salvei como &quot;hello.stupid&quot;, por que não? Pra rodar esse programa, fiz um interpretador em Javascript chamado &quot;js stupid&quot;. Vamos abrir pra ver o que faz.&lt;/p&gt;

&lt;p&gt;As primeiras linhas é o jeito mais porco de pegar um argumento da linha de comando. Vou executar fazendo &quot;./stupid.js hello.stupid&quot;, igual o Java faz com &quot;java Hello&quot; antes, lembra. Pra isso botei um shebang na primeira linha que vai executar o Node automaticamente e carregar o resto do script nele. Depois pego o primeiro argumento e uso a biblioteca &lt;code&gt;path&lt;/code&gt; pra formar o caminho absoluto até esse arquivo.&lt;/p&gt;

&lt;p&gt;Daí uso a biblioteca &quot;fs&quot;, de filesystem, pra ler o arquivo. Se eu passar um arquivo que não existe vai dar um erro e sair. Senão eu pego o conteúdo do arquivo, limpo coisas no final como o caracter de nova linha, caso tenha, sabe o &quot;\n&quot;? E aí simulo o que seria o trabalho do tal analisador léxico, que vai tokenizar o conteúdo do arquivo.&lt;/p&gt;

&lt;p&gt;Meu lexer é tão simples que é só um split de string de javascript. Ele quebra onde tem espaço e joga os tokens nesse array que chamei de &quot;tokens&quot;. Nesse ponto, converti o texto do arquivo numa lista e ainda converti o primeiro e o último token pra serem números.&lt;/p&gt;

&lt;p&gt;Essa minha linguagem é tão besta que não precisa de um analisador sintático. Eu só faço um switch case e vejo o 2o token da lista. Se for &quot;add&quot; eu faço uma soma, se for &quot;sub&quot; eu faço uma subtração. E é isso aí. Eu posso adicionar quantas operações quiser seguindo esse formato. E no final, se colocar um operador que não tá declarado, vai dar erro de sintaxe e sair.&lt;/p&gt;

&lt;p&gt;Se tiver um pouquinho de imaginação vai conseguir criar linguagens bem simplinhas usando só esse esquema. Mas dando spoilers já adianto onde que vai dar nó na cabeça. E se eu quisesse suportar uma expressão com mais de 2 números? Tipo fazer 1 + 2 + 3, como você faria? E se eu quisesse adicionar multiplicação e quiser que 1 + 2 * 3 dê a resposta correta? Note que multiplicação tem precedência, então 2 * 3 tem que vir antes de somar por 1. E dá pra ir complicando. E se eu quiser suportar variáveis? E se eu quiser suportar funções?&lt;/p&gt;

&lt;p&gt;Obviamente isso é um problema já resolvido. E o primeiro conceito é separar a análise de sintaxe da execução propriamente dita, que é o que costumamos chamar de &quot;tempo de compilação&quot; e &quot;tempo de execução&quot;. Recapitulando, a análise léxica só dividiu o texto em tokens, e agora análise sintática vai dar significado pra esses tokens. Pra isso vai precisar de uma gramática que define o que são expressões, o que são funções, o que são condicionais. Por exemplo, podemos ver a gramática da linguagem C.&lt;/p&gt;

&lt;p&gt;As ferramentas tradicionais que aprendemos na faculdade são lex ou flex, pra definir a análise léxica, e o bison ou yacc pra definir a gramática. Se procurar no GitHub é fácil de achar a gramática de todas as linguagens. A gramática do C em yacc é bem curtinha. Você imagina que definir uma linguagem deve ser milhares de linhas, mas na realidade não é mais que meia dúzia de page down, algumas centenas de linhas. Isso não é o compilador todo, só o parser, claro.&lt;/p&gt;

&lt;p&gt;Se olharmos lá no fim da gramática, eis a definição do que é uma função de C, literalmente ele nomeia como &quot;function definition&quot;. E pode ser construído de 4 formas diferentes. A 1a são especificadores de declaração, seguido de declarador, seguido de lista de declaração e terminando com composição de statements. Em português não sei como chamam &quot;statements&quot;, alguns chamam de expressão, mas a definição de expression e statement são diferentes. Eu acostumei a chamar de statement então vai assim mesmo.&lt;/p&gt;

&lt;p&gt;Mas o que tudo isso quer dizer? Esses nomes são como etiquetas, a definição de cada uma tá mais pra cima. Por exemplo, vamos ver o que &quot;compound statement&quot; quer dizer. Olha só, pode ser também 4 coisas: um bloco entre chaves vazias, ou uma lista de statements entre chaves, ou uma lista de declarações entre chaves e por último pode ser uma lista de declarações seguida de uma lista de statements, mas o principal é que estamos olhando pra declaração oficial, completa e não-ambígua do que o C chama de &quot;função&quot;. Não há espaço pra discussão, esta é gramática que define o C.&lt;/p&gt;

&lt;p&gt;Agora o que é um statement list? Aqui vemos a definição e ela pode ser um statement ou vários statements, isso é meio que uma definição recursiva, por ser ela mesma várias vezes. Mas isso só define que pode ter um ou mais statements, agora precisamos ver o que é um statement. E agora sim, um statement pode ser de diversos tipos como labeled statement, compound statement que vimos antes, mas o interessante vai ser esse iteration statement.&lt;/p&gt;

&lt;p&gt;E olha só, é isso que C chama de statements de iteração. Pode ser um while, pode ser um do while, pode ser for. E o for pode ser de dois jeitos diferentes, com ou sem uma expressão no final. E assim por diante. Se você tiver paciência pra ler a definição léxica e a gramática de yacc, tecnicamente é toda a sintaxe e semântica do C definida em 2 arquivos razoavelmente curtos, dá pra decorar e saber de cabeça. Claro, não é um iniciante que vai ler esses arquivos e entender tudo. Mas se alguma vez você já se perguntou, onde que tem a definição exata de uma linguagem? São nesses arquivos, e não em blog post.&lt;/p&gt;

&lt;p&gt;Por exemplo, o Javascript tem uma definição semelhante. No site do EcmaScript podemos ver como é definido a versão mais nova 2023. E se pularmos aqui no menu pra Notação de Gramática, olha só como é a definição de um Variable Declaration, ou declaração de variável: é um Binding Identifier e um Initializer. Um Binding Identifier são tokens como &quot;const&quot;, &quot;let&quot; ou &quot;var&quot; e um Initializer é uma expressão de assignment como &quot;x = 1&quot;. E olha aqui embaixo, ele define como é um loop com for, um For Statement, que pode ser definido de 4 maneiras diferentes, todos com um Lexical Declaration e variando a forma da Expression.&lt;/p&gt;

&lt;p&gt;Mas beleza, essa é a gramática mas e daí, pra que serve? O objetivo é quebrar seu código fonte, que é um texto, em uma listona de tokens e depois usar a gramática pra organizar esses tokens em uma estrutura de dados que podemos manipular programaticamente. O objetivo é transformar seu código em uma árvore, mais especificamente uma Parse Tree. Lembram? Eu sempre falo que uma das estruturas de dados mais importantes é uma árvore, por isso dediquei um video inteiro só sobre isso. No final seu código fonte vai ficar mais ou menos assim: (imagem)&lt;/p&gt;

&lt;p&gt;Aliás, tudo que expliquei até agora sobre analisadores léxicos e sintáticos, o processo de transformar um texto de código em tokens e reorganizar esses tokens numa árvore, é o que muita gente chama de &quot;parser&quot;, mas na realidade a primeira etapa de tokenizar é feito por um lexer, e a segunda parte de pegar os tokens e transformar em árvore é feita por um parser. Pra simplificar, vou pular a explicação do que é uma Parse Tree e ir direto pra mostrar outra representação da mesma árvore, que é a árvore de sintaxe, Syntax Tree, ou mais especificamente uma Abstract Syntax Tree ou AST.&lt;/p&gt;

&lt;p&gt;Lembra daquela minha linguagem de programação mais estúpida do mundo, que só aceita uma linha e só faz soma de dois elementos? Digamos que eu evolua ela pra fazer conta com mais de dois elementos e queira escrever aquele &lt;code&gt;1 + 2 * 3&lt;/code&gt;. Essa é a notação que chamamos de infixa onde operadores como o sinal de mais e o asterisco vão no meio dos operandos, que são os números. É a notação, ou forma de escrever, que nós humanos estamos mais acostumados a ver, mas não é a única notação e nem a melhor.&lt;/p&gt;

&lt;p&gt;Quem tá acostumado com as boas e velhas calculadoras de engenharia da HP conhece a famosa notação RPN ou notação polonesa reversa onde começamos digitando 3, depois 2, depois asterisco. Daí ele multiplica o 3 pelo 2, que dá 6. Continuamos digitando o 1 e só depois o sinal de soma, que vai somar o 1 com o resultado parcial 6, e isso vai dar 7.&lt;/p&gt;

&lt;p&gt;Notação polonesa reversa é o que chamamos de pósfixa. Portanto notação polonesa é a préfixa. E numa árvore de sintaxe escrevemos de forma préfixa. O importante é saber que o que você considera &quot;bom senso&quot;, nem sempre é, tem formas melhores. Vamos ver como fica. A árvore pra essa conta começa com um nó raiz que seria o sinal de mais. Começamos pelo operador, por isso préfixo. Daí divide em dois galhos, o nó da esquerda poderia ser o asterisco e o nó da direita o número 1. Embaixo do nó de asterisco, divide um nó a esquerda pra ser o 2 e outro a direita pra ser o 3.&lt;/p&gt;

&lt;p&gt;Essa representação seria o equivalente a escrever direto em polonês como &lt;code&gt;+(*(2, 3), 1)&lt;/code&gt;. Já viram isso antes? Vamos mudar o símbolo de &quot;+&quot; e chamar de &lt;code&gt;somar&lt;/code&gt; e trocar asterisco por &lt;code&gt;multiplicar&lt;/code&gt;, daí ficaria &lt;code&gt;somar(multiplicar(2, 3), 1)&lt;/code&gt;. Ficou mais claro agora?&lt;/p&gt;

&lt;p&gt;É assim que você programa. A maioria das linguagens de programação, no caso especial de contas aritmeticas, como soma ou multiplicação, deixa você escrever da forma infixa, mas internamente, depois do parsing, o compilador tá mudando pra forma préfixa na árvore, assim como todo o resto das suas funções. Isso tudo foi pra ilustrar rapidamente que a forma que você escreve o código não é a forma como o computador vai executar.&lt;/p&gt;

&lt;p&gt;Vamos ver como isso funciona. Vamos fazer outro programinha idiota em Java, que pegue os 3 primeiros parâmetros, converta o texto em número, que vai ser os números 1, 2 e 3 que usamos no exemplo anterior. Então precisa chamar o método estático &lt;code&gt;parseInt&lt;/code&gt; da classe &lt;code&gt;Integer&lt;/code&gt;. Daí vou imprimir no console com &lt;code&gt;System.out.print&lt;/code&gt; a soma de a com b vezes c, exatamente como no exemplo. Agora compilamos com &lt;code&gt;javac&lt;/code&gt; e podemos chamar &lt;code&gt;java Calc 1 2 3&lt;/code&gt; e deu 7, como deveria. Nenhuma surpresa aqui.&lt;/p&gt;

&lt;p&gt;Mas o que de fato executou? Pra quem não sabe existe a ferramenta &lt;code&gt;javap&lt;/code&gt; que vem no JDK de Java que serve pra desassemblar, pra mostrar o assembly de bytecodes de Java. Lembra no episódio do guia mais hardcore de introdução a computadores que eu mostrei uma parte do assembly da CPU 6502 que era usado no antigo Nintendinho? Mesma coisa, a ferramenta javac vai passar nosso código por um parser, fazer algumas mágicas que já vou explicar e cuspir instruções binárias de máquina, o que comumente se chama de bytecodes ou assembly. E o javap só mostra esses bytecodes na tela de forma que nós humanos conseguimos ler.&lt;/p&gt;

&lt;p&gt;Parece complicado, mas pra esse exemplo simples não é. Esse trecho que vai até a linha 21 são as instruções pra pegar os argumentos que passamos e converter em inteiro, aquele &lt;code&gt;Integer.parseInt&lt;/code&gt;. Percebam que a gente só escreveu uma linha de código pra cada um dos três parâmetros, mas pra cada um ele gerou pelo menos 4 instruções, isso sem contar esse &lt;code&gt;invokestatic&lt;/code&gt; que chama seja lá o que o &lt;code&gt;parseInt&lt;/code&gt; faça.&lt;/p&gt;

&lt;p&gt;Novamente, o que você escreve na sua linguagem sempre vira bem mais instruções depois de compilado, e esse é o objetivo. Antigamente, até antes dos anos 90, era super comum escrever direto em assembly, em linguagem de máquina, porque era mais eficiente. Não havia nem memória, nem processamento suficiente pra existir um compilador inteligente como os de hoje.&lt;/p&gt;

&lt;p&gt;Mas agora os compiladores evoluíram tanto que geram assembly muito melhor do que é possível fazer na mão. Além disso os programas que escrevemos hoje são muito maiores, seria impossível escrever tudo em assembly. Se alguém já se perguntou se seria mais eficiente escrever tudo em assembly, não, não é, tirando raríssimas exceções, nenhum programador hoje é melhor que um compilador.&lt;/p&gt;

&lt;p&gt;De qualquer forma, a parte que importa são as linhas 24 a 28. Aqui esse bytecode &lt;code&gt;iload&lt;/code&gt; vai empilhando na Stack os números que converteu. Daí chama o bytecode &lt;code&gt;imul&lt;/code&gt; que, como dá pra ver pelo nome, é a multiplicação. Ele desempilha os últimos 2 números, multiplica e empilha de volta o resultado. Chega na instrução &lt;code&gt;iadd&lt;/code&gt; que desempilha os últimos dois valores na Stack, soma e empilha o resultado. Lembram? Eu expliquei pilhas no episódio de Hello World como você nunca viu. Se não sabe como um sistema baseado em pilha funciona, assista esse episódio depois.&lt;/p&gt;

&lt;p&gt;Os mais espertos aí assistindo talvez tenham notado, mas porque eu fiz essa forma complicada de ter que passar os números como parâmetros na linha de comando? Por que não deixei hardcoded direto no código o &lt;code&gt;1 + 2 * 3&lt;/code&gt;? Vamos fazer isso agora. Deixa eu criar uma nova classe chamada &lt;code&gt;Calc2&lt;/code&gt; e fazer direto &lt;code&gt;System.out.print(1 + 2 * 3)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Aliás, ignorem a nomenclatura de classes que estou usando, tá bem porco mesmo só pra ir mais rápido, mas obviamente não façam nomes como &quot;Calc2&quot; em projeto de verdade, né? Enfim, agora salvamos, compilamos de novo com &lt;code&gt;javac&lt;/code&gt; e podemos executar com &lt;code&gt;java Calc2&lt;/code&gt; e devolve 7 de novo. Se for como no exemplo anterior, agora deve ter cortado aquele monte de bytecodes de parseInt mas deve ter as mesmas instruções de iload pra empilhar e imul e iadd pra multiplicar e somar, né?&lt;/p&gt;

&lt;p&gt;Vamos conferir. Fazemos &lt;code&gt;javap -c Calc2&lt;/code&gt; e olha só, ué, cadê a multiplicação e adição? Não tem, em vez disso tem aqui esse bytecode &lt;code&gt;bipush&lt;/code&gt; que quer dizer &quot;empilhe este número inteiro na Stack&quot;, no caso o número 7 que já é direto o resultado de &lt;code&gt;1 + 2 * 3&lt;/code&gt;. Sacaram? Na hora de compilar o javac já viu: cara, essa conta vai dar 7 sempre, nunca vai mudar, então pra que vou refazer o cálculo toda vez? Deixa eu já pré-calcular e dar direto o resultado. Assim ele economizou 3 chamadas de &lt;code&gt;iload&lt;/code&gt;, e as chamadas pra &lt;code&gt;imul&lt;/code&gt; e &lt;code&gt;iadd&lt;/code&gt;. Em vez disso ficou só uma chamada &lt;code&gt;bipush&lt;/code&gt; pra empilhar direto o resultado final 7 e daí já pula pra chamada que vai imprimir o texto no console. O compilador literalmente reescreveu meu código.&lt;/p&gt;

&lt;p&gt;Lembram daquele meu interpretador pra linguagem mais estúpida do mundo? Ele pega o código do meu programa, tokeniza e coloca os tokens num array, e faz um switch case, se achar o operador &quot;add&quot; executa uma soma, se achar um &quot;mul&quot; faz multiplicação. E olhando pra isso, vocês poderiam pensar, &quot;ah, isso faz sentido, pra que precisa de tanto trabalho pra converter os tokens numa árvore e depois da árvore converter em instruções de máquina em vez de direto já cuspir instruções?&quot; E esse é um dos motivos. Vamos filosofar um pouco.&lt;/p&gt;

&lt;p&gt;Não importa que sintaxe de linguagens você ache mais bonita ou mais elegante ou mais produtiva. Não importa se você gosta de usar chaves pra delimitar funções como no Java ou se prefere usar identação como em Python. Não importa se prefere dividir tudo em várias pequenas funções de poucas linhas ou prefira o jeito go-horse de entuchar o máximo de linhas dentro de uma função. Não importa se gosta ou não de colocar comentários detalhados antes de cada função. Tudo isso é totalmente irrelevante pro computador.&lt;/p&gt;

&lt;p&gt;Depois que seu código for parseado e compilado, o que vai sobrar são instruções de máquina. Muito do que você escreveu vai ser reescrito, o compilador vai pensar &quot;porque esse idiota deixou um cálculo hardcoded que sempre dá a mesma resposta? vou reescrever e deixar direto só a resposta, foda-se.&quot; Sim, o compilador vai jogar fora tudo que você escreveu e vai reescrever tudo do zero da forma que ele acha mais eficiente. É isso que o computador vai ver no final e executar.&lt;/p&gt;

&lt;p&gt;Mas então, tudo que sempre me falaram de Código Limpo, boas práticas, serve pra que então? Se no final o computador tá cagando e andando, pra que eu tô perdendo tempo organizando todo meu código, escolhendo nomes fáceis de entender e tudo mais? Porque você não está e nem deve estar programando pro computador, você programa pra outras pessoas. Entenda essa verdade: o seu trabalho não é programar pro computador e sim pra que outras pessoas entendam. Incluindo você daqui alguns dias ou meses.&lt;/p&gt;

&lt;p&gt;Se como eu você curte retrogames, já deve ter assistido videos explicando os truques que programadores faziam antigamente pra conseguir extrair o máximo de máquinas de processadores super fracos como o Z80 ou 6502, com quase nada de RAM, na faixa de 2 kilobytes. Pensa que o Super Mario World inteiro, com todos os gráficos, fases e lógica ocupa meio megabyte. Em meio megabyte hoje você não consegue fazer nada.&lt;/p&gt;

&lt;p&gt;Qualquer página web simples precisa de múltiplos megabytes. E isso porque antigamente ciclos de CPU e RAM eram hiper caras. Pelo mesmo preço que se comprava um Nintendinho nos anos 80, hoje você compra um Playstation 5. Literalmente, no começo dos anos 80 um nintendinho custava quase 180 dólares, que se ajustar pra inflação dá quase 500 dólares, que é o preço de um PS5.&lt;/p&gt;

&lt;p&gt;Por isso antigamente era hiper importante economizar ciclos e bytes ao máximo, mesmo que fosse ao custo de dificultar a vida dos programadores, porque o custo do hardware era muito maior que o custo de programadores. 10 kilobytes a mais de RAM que desperdiçasse em 1983 era 200 dólares a mais. Por isso escovar bits era a coisa mais importante. Mas o tempo passou e agora 200 dólares você compra mais de 16 giga de RAM. Por isso que desperdiçar 1 ou 2 gigas de RAM, por mais absurdo que pareça, não é mais grande coisa.&lt;/p&gt;

&lt;p&gt;Por outro lado o que subiu foi a hora de programador. Se o programador tem que gastar o dia todo pra descobrir que diabos é esse número 7 e que a conta original era 1 + 2 * 3, que se estivesse explícito no código ia custar 2 segundos pra entender, faz muita diferença. Toda boa prática de código é feito pra economizar hora de programador e não hora da máquina. E mesmo se você trabalha sozinho, as boas práticas e código limpo continuam fazendo sentido, porque o você de amanhã, às 2 horas da manhã numa emergência, vai te agradecer por ter feito tudo organizado e fácil de ler.&lt;/p&gt;

&lt;p&gt;Lógico, não estou dizendo que você não tem que saber programar direito sabendo como a máquina vai se comportar. Nem o melhor compilador do mundo vai melhorar seu código bosta que dá &quot;select *&quot; numa tabela gigante do banco só pra pegar uma linha, porque ele não tem como saber, durante a compilação, que pode ter uma tabela de um milhão de linhas sendo consumido e causando vazamento de memória. Ele vai fazer o melhor possível, mas não existe limites pra programador ruim dificultar a vida do compilador.&lt;/p&gt;

&lt;p&gt;Mas a tecnologia de compiladores não serve só pra cuspir binários otimizados. Aquela árvore de sintaxe abstrata, a AST é útil pro programador. Sabe quando no VSCode você instala uma extension pra Javascript que roda o jslinter e ele marca no seu código onde tem problemas e possíveis bugs? Essa análise é feita em cima da AST. Ele não tá olhando o texto do seu código.&lt;/p&gt;

&lt;p&gt;Por baixo dos panos ele passou pelo parser do V8, gerou a estrutura de árvore e tá processando os nós dessa árvore. Os nós podem conter metadados que apontam de volta qual linha do código que representa, e assim dá pra analisar e devolver a análise pro editor mostrar bonitinho pra você. Todo linter funciona assim. Um linter ou ferramentas de análise estática, tentam te orientar pra escrever código melhor e apontar coisas que seriam ambíguas pro compilador, daí ele pode te dizer o equivalente a &quot;tem certeza que é isso que você quer fazer?&quot;&lt;/p&gt;

&lt;p&gt;O que mostrei até aqui é a fase de parsing. Essa idéia de criar uma gramática livre de contexto veio de ninguém menos que John Backus, criador do Fortran, provavelmente a primeira linguagem de alto nível de sucesso comercial, lá nos anos 60. E não só ele fez a linguagem como se inspirou nos trabalhos de pesquisadores como o grande Turing ou Noam Chomski, cujo foco era linguagens no termo mais genérico, tipo língua portuguesa ou inglês, mas os resultados poderiam ser aplicados em linguagens de computador.&lt;/p&gt;

&lt;p&gt;O Backus inventou essa idéia de uma linguagem que descreve outra linguagem, ou seja, uma meta-linguagem, e usou pra definir uma nova linguagem chamada IAL, que viria a se tornar o ALGOL. Muita gente costuma pensar em C como o avô das linguagens, mais por causa dos blocos com chave e statements terminando com ponto e vírgula. Outros consideram o C muito novo pra ser avô e o verdadeiro avô seria o COBOL.&lt;/p&gt;

&lt;p&gt;Mas eu penso diferente. No máximo, COBOL é mais avô de coisas como linguagens de stored procedures de banco de dados. A real linguagem avô do que chamamos de &quot;linguagens de uso genérico&quot;, anterior ao C, é o ALGOL. Antes do C ser inventado, o Ritchie e Kerninghan trabalharam no BCPL que poderíamos chamar de linguagem &quot;B&quot; e pra mim a linguagem &quot;A&quot; é o ALGOL.&lt;/p&gt;

&lt;p&gt;Antes que alguém comente, sim, eu sei, não é exatamente assim. ALGOL foi um linguagem criada por comitê e, como tudo que é gerado por comitê, ficou uma linguagem bloated, complexa, gorda. Daí na universidade de Cambridge em Londres surgiu o CPL que é o Combined Programming Language, mas que obviamente muitos poderiam chamar de Cambridge Programming Language, pra ser o &quot;ALGOL com os pés no chão&quot;. Acabou não sendo muito bem aceita, a intenção foi melhor que a implementação. Povo de javascript já viu episódios desse tipo, como yarn querendo ser npm melhorado, ou Deno querendo ser Node melhorado.&lt;/p&gt;

&lt;p&gt;Ainda em Cambridge tentaram simplificar o CPL com o Basic CPL ou BCPL do Martin Richards, pra ser &quot;só as coisas boas do CPL&quot;. Poucos anos depois o Ken Thompson chefiou o projeto de UNIX pras máquinas PDP/11 e fez um compilador pra uma versão reduzida do BCPL que chamou de linguagem B. Mas tanto BCPL quanto B eram limitadas. B era alto nivel demais e como eu disse antes, isso custava recursos escassos e caros de hardware. Por esses e outros problemas, surgiu Dennis Ritchie pra reescrever a linguagem que seria a sucessora do B pra ser mais &quot;próxima da máquina&quot; e daí surgiu o C.&lt;/p&gt;

&lt;p&gt;Mas independente disso, está claro que as raízes das principais funcionalidades que reconhecemos hoje numa linguagem de uso geral surgiram com o Algol e por isso podemos considerar que ela é a verdadeira linguagem A. E a linguagem C é a versão que realmente vingou e serviu de inspiração pra linguagens mais modernas como C++, Objective-C, que inspiraram depois Java, Python, C# e todo o resto que vocês usam hoje.&lt;/p&gt;

&lt;p&gt;De qualquer forma, o cientista da computação Peter Naur reconheceu o poder da idéia de metalinguagem do Backus e como já existia o termo &quot;Chomski Normal Form&quot;, ele decidiu que a notação do Backus deveria se chamar &quot;Backus Normal Form&quot; ou BNF. Mas tecnicamente não era uma forma normal, como apontou o lendário Donald Knuth e foi ele que sugeriu que BNF deveria ser pra &quot;Backus-Naur Form&quot;, e é assim que chamamos até hoje.&lt;/p&gt;

&lt;p&gt;Mas como falei antes, poucos programadores hoje, eu incluso, estudam as definições matemáticas por trás das notações de linguagens, com o rigor que deveria, porque ferramentas como flex, bison, yacc simplificam tudo isso e a gente tem mais que se preocupar com o design da linguagem propriamente dita, que começa em escrever a tabela de lexemes e a gramática no formato que mostrei antes. Em particular, se tiver interesse em estudar mais sobre geradores de parsers, a que eu vejo que povo considera mais moderna é o ANTLR, que por acaso é feita em Java.&lt;/p&gt;

&lt;p&gt;Um parser não serve só pra compilar linguagens de programação. Pense um arquivo de configuração em YAML ou um JSON. Ambos não deixam de ser linguagens, só que declarativas e não de programação. Ambas precisam de um parser e de fato, pra transformar um YAML ou JSON num objeto de javascript, ou um Hash em Python, precisa de um lexer e de um parser.&lt;/p&gt;

&lt;p&gt;Sabe quando você carrega um arquivo texto de HTML num navegador e agora dá pra inspecionar usando as ferramentas de debug? Você que é de front-end sabe disso, ele se transforma no tal do DOM ou Document Object Model. E o que é o DOM? É uma árvore, é resultado de passar o HTML por um parser. E como o navegador desenha as coisas na tela? Navegando pode Node a Node do DOM. O DOM está pra HTML assim como AST está pra uma linguagem de programação. E da mesma forma que você pode modificar nós do DOM, o compilador manipula nós do AST pra melhorar seu código e gerar binários mais eficientes.&lt;/p&gt;

&lt;p&gt;Tanto o que você chama de compilador quanto o que chama de interpretador começam com um lexer e um parser. Mas a história não acaba aqui. Agora vem a parte interessante da história. Lembra quando fizemos o disassembly do binário compilado de Java? Ele não era uma árvore né, era uma sequência de instruções, o que em Java se chama de bytecodes. Depois que o javac gerou o AST, checou que a sintaxe tá correta e as dependências estão corretas, daí navega nó a nó da árvore e cospe os bytecodes correspondentes pra gerar o arquivo &quot;.class&quot; no final.&lt;/p&gt;

&lt;p&gt;Vamos recapitular, o que é um bytecode? É literalmente isso, um código de byte. Cada instrução como aquela aaload, iload, imul, iadd são mnemônicos pra um código binário. Por exemplo, aaload é o hexadecimal 32 ou o binário 0011 0010. Lembra, a máquina hardware, a CPU, só entende binário e internamente tem instruções que são representadas por certas sequências de bits.&lt;/p&gt;

&lt;p&gt;Assista o episódio de Hello World ou outros videos da série &quot;Como Computadores Funcionam&quot; pra relembrar disso. Mas agora vem uma pergunta, que diabos de instrução é isso de aaload, iload, imul. Isso não é assembly de x64, que é o que roda em Intel ou AMD aí no seu PC ou notebook. Também não são instruções de ARM como os Mac M1 ou Qualcomm do seu Android. Que diabo de instruções são essas? E aqui vem a parte que confunde a diferença entre um compilador puro e um interpretador. Essas instruções são pra uma máquina virtual que não existe em forma de hardware.&lt;/p&gt;

&lt;p&gt;Se existisse um chip de Java - e nos anos 90 a Sun até queria mesmo fazer um chip de verdade Java -, essas seriam as instruções. Mas esse chip não existe, o que existe é a JVM, que literalmente significa Java Virtual Machine. Todo programa Java roda numa máquina virtual. Por similaridade, todo programa Python roda numa máquina virtual Python. Todo programa Javascript roda numa máquina virtual Javascript, como o Google V8 ou Mozilla SpiderMonkey. Todo navegador web vem embutido com uma máquina virtual. E sim, máquina virtual como um Virtualbox ou VMWare, já pararam pra pensar nisso?&lt;/p&gt;

&lt;p&gt;A gente tá acostumado a pensar em máquina virtual só como um Virtualbox ou Parallels ou erroneamente um Docker - que não é máquina virtual. Pensamos como programas que podemos instalar num Mac M1 pra conseguir rodar outros programas compilados como binários de Intel e interpretar em tempo real pra rodar numa máquina ARM. Tecnicamente, toda máquina virtual é um interpretador, no sentido que ele interpreta uma instrução de uma máquina pra instrução de outra máquina.&lt;/p&gt;

&lt;p&gt;Pra quem é de Python, quando chamamos &lt;code&gt;python -c compileall .&lt;/code&gt;, ele vai pegar todos os arquivos texto com extensão &lt;code&gt;.py&lt;/code&gt; e transformar em &lt;code&gt;.pyc&lt;/code&gt; ou &lt;code&gt;.pyo&lt;/code&gt;. Pra isso o código texto vai passar por um lexer, por um parser e no final o AST resultante é transformado em instruções pra uma máquina virtual de Python. Aí ele serializa e salva esse AST num binário &quot;.pyc&quot;. O que chamamos de &quot;interpretador&quot; é um caso especial de máquina virtual. Alguns também chamam isso de &quot;runtime&quot;, mas são só nomes diferentes pra mesma coisa. Pra todos os efeitos e propósitos, seu programa Python tá rodando numa máquina virtual. O Javascript do seu navegador ou no Node.js também.&lt;/p&gt;

&lt;p&gt;Dentre várias coisas que podem ser diferentes é que normalmente quando falamos de interpretadores é quando temos algumas facilidades de desenvolvimento, como um REPL que é acrônimo pra Read-Eval-Print-Loop, que é a linha de comando que abre quando você digita &lt;code&gt;python&lt;/code&gt; ou &lt;code&gt;node&lt;/code&gt; no terminal e pode sair digitando código que ele vai executando imediatamente, ou seja, ele lê, faz evaluation, imprime o resultado e volta pro loop e fica nisso até apertar &lt;code&gt;Ctrl+D&lt;/code&gt; pra sair. Aliás, a linha de comando do seu terminal, seja Bash, ZSH ou Fish, é um interpretador também.&lt;/p&gt;

&lt;p&gt;Sendo mais específico, um interpretador costuma manter o AST num formato que podemos modificar, depois dos arquivos do programa terem todos passados pelo parser. Toda vez que dentro do console de Python a gente define um novo método, estamos adicionando nós na árvore AST. É isso que define o que muitos chamam de &quot;linguagem dinâmica&quot;. Mesmo Java, apesar de mais chatinho, tem como modificar o AST que tá em memória. Eu posso mandar o classloader da JVM carregar outros arquivos &quot;.class&quot; que ainda não tinha carregado, ou usar a API de Reflection pra modificar o código em tempo real.&lt;/p&gt;

&lt;p&gt;Claro, apesar de parecerem máquinas virtuais, interpretadores são diferentes de um VirtualBox. Um VirtualBox da vida de fato tenta ao máquina esconder tudo sobre a máquina host por baixo, pra fazer os programas rodando dentro acreditar que estão numa máquina de verdade. Quando um programa lá dentro pede pra fuçar o disco, o Virtualbox vai dar pra ele um disco virtual e não acesso ao disco de verdade.&lt;/p&gt;

&lt;p&gt;Quando o programa pede pra conectar na internet, ele não vai ter acesso ao TCP da máquina de verdade, mas sim à uma rede virtual interna, como se fosse uma rede externa separada que faz ponte com a rede da máquina de verdade. E assim por diante. Já um interpretador não tenta esconder nada. Quando o programa de python pede acesso ao disco, o interpretador dá acesso ao disco de verdade. Quando o programa pede pra abrir um socket de rede, ele passa pela rede de verdade.&lt;/p&gt;

&lt;p&gt;Então tecnicamente sim, Java é uma linguagem compilada porque no final gera um binário com instruções de máquina. Só que não são instruções de Intel ou ARM e sim pra JVM, uma máquina só virtual. Por outro lado, ela é interpretada porque a máquina pra qual ela foi compilada não existe e pra rodar depende de um interpretador que entende essas instruções e vai converter pras instruções da máquina host de verdade, como um Intel x64. O mesmo vale pra Python, Javascript, Ruby, PHP. E eu sei, já estou vendo você aí já indo ferozmente pros comentários pra discordar, porque eu ainda não mencionei JIT, segura a onda aí!&lt;/p&gt;

&lt;p&gt;Já vou voltar no assunto de interpretadores e máquinas virtuais, mas o importante é saber que depois da fase de parsing, o objetivo é transformar a árvore abstrata de sintaxe em instruções de máquina, seja lá pra qual arquitetura de CPUs: uma de verdade como x64 ou arm64 ou uma virtual como a JVM. A história não acaba aqui.&lt;/p&gt;

&lt;p&gt;Mesmo o compilador de C como GCC ou o clang de LLVM não sai da AST depois do parsing e já transforma direto em instrução de Intel. Não, acho que hoje em dia praticamente todos os compiladores trabalham com uma representação intermediária. O compilador de DotNet, por exemplo, chama isso de Intermediate Language ou IL. No mundo LLVM isso se chama Intermediate Representation ou IR. No mundo do GNU Compiler Collection ou GCC que todo mundo conhece, ele trabalha com RTL que é o Register Transfer Language. No mundo de shaders como pra rodar em Vulkan no SteamDeck, existe o padrão Standard Portable Intermediate Representation ou SPIR-V. E o que diabos é isso de linguagem intermediária?&lt;/p&gt;

&lt;p&gt;Agora é a parte da mágica que eu falei antes que vou simplificar bastante, entenda que essa parte é absurdamente extensa na realidade. Vou usar o exemplo mais besta de todos. Lembra aquele exemplo da calculadora que escrevi em Java pra dar print em &lt;code&gt;1 + 2 * 3&lt;/code&gt; e depois de compilar, fizemos o desassembly pra ver as instruções e só tinha um &lt;code&gt;bipush&lt;/code&gt; direto do resultado 7? Pois é, o compilador foi inteligente em saber que não precisa traduzir instrução a instrução, a multiplicação e soma e, em vez disso, já pré-calculou e deixou direto o resultado, economizando várias instruções. Ou seja, ele otimizou o código e deixou tanto mais rápido quanto economizou ter que guardar os números 1, 2 e 3 na memória.&lt;/p&gt;

&lt;p&gt;Essa é a otimização mais trivial e besta de todas, mas o compilador tem a capacidade de fazer centenas de outras otimizações. Por exemplo, digamos que seu código tenha trechos que nunca usa. Dependendo da linguagem e agressividade da otimização, ele pode até escolher jogar fora esse trecho e nem traduzir. Sim, o compilador pode escolher jogar seu código fora. Ele pode simplificar chamadas. Se a ordem dos fatores não influir no resultado, pode mudar a ordem do seu código pra CPU processar de forma mais eficiente.&lt;/p&gt;

&lt;p&gt;Essa fase de otimização pode passar por todo o código mais de uma vez, é o que se chama de &quot;passes&quot;. Isso pra conseguir fazer coisas como achar constantes duplicadas e fazer ambas apontarem pro mesmo lugar. Eliminação de código morto, ou seja, trechos que nunca são executados e podem ser eliminados. Tail Call elimination, pra remover chamadas recursivas que podem ser otimizadas. E muito, muito mais. Isso é o que o LLVM chama de Transformation Passes.&lt;/p&gt;

&lt;p&gt;Entre aspas é como se você tivesse um Linus Torvalds e um John Carmack juntos, reescrevendo seu código da maneira mais otimizada possível. Na prática essa etapa é o real trabalho de um compilador e onde se gasta mais tempo e mais recursos, porque tem um gerador de código varrendo tudo que você escreveu, exaustivamente, várias vezes, e reescrevendo da melhor forma que se conhece sem quebrar, nem gerar bugs.&lt;/p&gt;

&lt;p&gt;Agora, imagine o seguinte: tá vendo essa lista de transformações e otimizações? Imagine se toda vez que inventa uma nova linguagem, se precisasse reescrever todas essas estratégias tudo de novo pra linguagem nova. Seria um trabalho absurdo, que ia gerar dezenas de bugs, e as otimizações seriam ruins porque criar essas rotinas levou anos e milhares de horas homens pra aperfeiçoar até ficarem perfeitas. Por isso que o comum é que toda nova linguagem, no final do parsing, seja convertida numa linguagem intermediária, que é sempre igual. E essa linguagem costuma ser tipo um assembly pra uma máquina virtual, que não precisa se preocupar nesse ponto com as limitações de design e legado de uma CPU de verdade com um x64.&lt;/p&gt;

&lt;p&gt;Daí todas essas estratégias de otimização são executadas nessa linguagem interdiária. Um compilador como GCC ou derivados de LLVM na verdade são programas grandões que podemos dividir em 3 grandes funções diferentes. O primeiro é o front-end, que tem a responsabilidade de pegar linguagens como C, C++, Go, Rust, Swift e fazer o lexing, o parsing, gerar a árvore de sintaxe e dela cuspir as instruções de máquina virtual intermediária, sem quase nenhuma otimização. Essas instruções intermediárias são o RTL, no caso de GCC, ou IR no caso de LLVM, ou IL no caso de DotNet. O clang de LLVM seria o front-end de C pra LLVM, o conjunto das definições de lexer e gramática do parser.&lt;/p&gt;

&lt;p&gt;Agora o 2o programa pega esse IR não otimizado e gasta um tempão lendo, relendo e reescrevendo, e a saída vai ser outro IR agora totalmente otimizado. Esse programa do meio poderia ser chamado de &quot;middle-end&quot;. E relembrando, cada uma das otimizações ou transformation passes que ele faz poderia ser tema de um paper de Ph.D. Não são coisas triviais como o exemplo que eu mostrei. E finalmente temos o 3o programa, que chamamos de &quot;back-end&quot;, que vai pegar o IR otimizado e traduzir finalmente pras instruções da máquina de verdade, ou seja, converter de instruções assembly de IR pra instruções assembly de x64 ou arm64 da vida. E esse é o binário final que pode ser executado.&lt;/p&gt;

&lt;p&gt;Toda vez que que eu quiser inventar uma nova linguagem é só fazer as partes de lexer e parser, fazer só a parte que é o front-end dessa nova linguagem. É pra isso que servem ferramentas como o ANTLR que eu mencionei antes. É isso que linguagens como Rust fizeram quando construíram o compilador em cima de LLVM. Ele pode deixar um compilador que já existe otimizar o que puder. Num primeiro momento, o criador de uma nova linguagem não precisa refazer tudo do zero, pode reaproveitar o que já existe, porque uma vez tendo IR, o resto é igual. A preocupação inicial dele é só fazer um parser que converte sua linguagem em IR, como um transpiler que converte TypeScript em Javascript, por exemplo.&lt;/p&gt;

&lt;p&gt;Quando um designer de chips como uma Apple inventa um novo como os tais M1, ele só precisa fazer a parte back-end, a parte que pega o IR já otimizado e converte em instruções de máquina. Amanhã a Qualcomm lança um novo chip SnapDragon ARM, ele só precisa atualizar o back-end do compilador, todo o resto continua igual. A AMD lança a nova série 6000 do Ryzen, só precisa atualizar o back-end dos principais compiladores se teve alguma mudança nas suas instruções. Entenderam?&lt;/p&gt;

&lt;p&gt;Eu expliquei isso no video de Apple e GPL, mas além obviamente do hardware, qual é o grande segredo da Apple em conseguir migrar tudo pra M1 tão rápido? É eles terem investido quase 2 décadas melhorando suas tecnologias de compiladores em cima de LLVM. Com isso conseguiram migrar de PowerPC pra Intel nos anos 2000 e com isso migraram de Intel pra ARM muito rápido. E tudo que só o compilador não consegue resolver, eles tem como desenhar no chip pra compensar. E com isso temos as performances absurdas que vocês assistem nos diversos reviews de como programas originalmente compilados pra Intel rodam liso em ARM sem recompilar, só sendo interpretados em tempo real.&lt;/p&gt;

&lt;p&gt;Se você é um cientista da computação e tem vontade de criar uma nova linguagem, provavelmente vai começar fazendo um front-end pra LLVM. Assim saíram linguagens como Rust da Mozilla, Julia, Crystal e outros. Pouquíssimas empresas tem capacidade pra fazer um compilador do zero com todos os tipos possíveis de otimização. Java tem otimizador próprio desde a era da Sun e depois passou pra Oracle e tem dezenas de empresas que investem em pesquisa pra continuar melhorando. O DotNet tem a Microsoft, que também investe pesado em pesquisa. E no meio disso tem o Javascript e o V8 do Google.&lt;/p&gt;

&lt;p&gt;Agora, qual a diferença de C, Java e Javascript? No final do dia quer dizer que C é compilado porque gera binário nativo de máquina, mas Java e Javascript são interpretados? Não, no final do dia, todos geram binário nativo de máquina usando estratégias diferentes, pra resolver problemas diferentes. A estratégia de C se chama ahead of time compiling ou AOT. E no Java e Javascript eles usam Just in Time compiler, o famoso JIT. E mesmo JIT tem diversas estratégias diferentes.&lt;/p&gt;

&lt;p&gt;Vou fazer outro exemplinho bem besta só pra vocês terem uma imagem na cabeça. Vamos voltar pro exemplo em Java de fazer um cálculo. Quem lembra como calcula a circunferência de um círculo? Lembra? Duas vezes Pi vezes Raio? 2 pi r. Então vamos fazer uma nova classe, método estático main, e System.out.print, 2 vezes 3.141592 etc que é PI mas vou colocar poucos dígitos pro exemplo, e vezes 10 que é o raio que quero usar. Salvamos, saímos, compilamos com &lt;code&gt;javac&lt;/code&gt;, rodamos e voilá, 62.83 bla bla é a circunferência pra um raio 10.&lt;/p&gt;

&lt;p&gt;Se fizermos disassembly com &lt;code&gt;javap&lt;/code&gt; olha aqui embaixo essa instrução &lt;code&gt;ldc2_w&lt;/code&gt; que puxa uma constante do pool de constantes e manda pra stack, e no comentário gerado temos que essa constante é um double 62.83 bla bla que é o valor pré-calculado. Ele fez a mesma otimização que no caso do &lt;code&gt;1 + 2 * 3&lt;/code&gt;. Top, é o esperado. Mas como eu disse antes, olha como essa continha no código é feio, mesmo sendo simples. Quem curte código limpo tem gatilho só de ver isso. Dá pra melhorar isso.&lt;/p&gt;

&lt;p&gt;Vamos escrever uma nova classe. Eu poderia começar definindo uma constante double chamada PI e escrever ali, 3.141592. Mas isso seria idiota porque obviamente Java já tem uma constante estática double de PI, que tá na classe Math. Ah, mas como eu ia adivinhar? Sempre pense: coisas triviais assim, as chances são que já existe. Na dúvida procurar no Google, &quot;Java constant PI&quot; e olha só, nos primeiros links já vem Math.PI. Quando você não acha as coisas não é que não existem ou são difíceis de achar, você que não procurou direito.&lt;/p&gt;

&lt;p&gt;Enfim, continuamos fazendo uma função estática que retorna double chamada &lt;code&gt;circunferencia&lt;/code&gt; que recebe como argumento uma variável double chamada &lt;code&gt;raio&lt;/code&gt;. Dentro calculamos o 2 vezes &lt;code&gt;Math.PI&lt;/code&gt; vezes a variável raio. E agora sim, no método &lt;code&gt;main&lt;/code&gt; vamos dar &lt;code&gt;System.out.print&lt;/code&gt;, como antes, mas o argumento vai ser o resultado do método &lt;code&gt;circunferencia&lt;/code&gt; passando o valor 10. Pronto, salvamos, saímos, compilamos com &lt;code&gt;javac&lt;/code&gt; e executamos e tá aí, 62.83 bla bla como tinha dada na outra versão.&lt;/p&gt;

&lt;p&gt;Compara a versão antiga com a nova. Ficou maior, mas a intenção do código tá muito mais clara. Se daqui 1 mês eu for editar esse código, dá pra saber o que ele faz imediatamente. E claro, dava pra ter feito bem mais bonito que isso, fazer uma classe chamada &lt;code&gt;Circulo&lt;/code&gt; ou um módulo chamado &lt;code&gt;Geometria&lt;/code&gt; ou qualquer coisa assim, mas hoje não é design de orientação a objetos, então não impliquem muito com isso hoje.&lt;/p&gt;

&lt;p&gt;Mas vamos ver como fica fazendo disassembly com &lt;code&gt;javap&lt;/code&gt; e olha que estranho, tem a instrução &lt;code&gt;ldc2_w&lt;/code&gt; que puxa a contante 10 que é o raio que eu defini e puxa pra stack, daí invoca a função &lt;code&gt;circunferencia&lt;/code&gt; e dá jump aqui pra cima. Daí faz outro &lt;code&gt;ldc2_w&lt;/code&gt; pra puxar a constante de PI pra stack também, na verdade a constante de &lt;code&gt;2 * Pi&lt;/code&gt;, veja o comentário que ele gerou mostrando que é o dobro de PI, então ele otimizou isso. Mas aí chama a instrução &lt;code&gt;dmul&lt;/code&gt; que é multiplicação de double e desempilha os últimos dois valores que colocamos na stack pra calcular, retorna e finalmente dá &lt;code&gt;invokevirtual&lt;/code&gt; no printf pra imprimir na tela.&lt;/p&gt;

&lt;p&gt;Mas &lt;code&gt;2 * pi * 10&lt;/code&gt; é tudo constante, porque ele não otimizou o suficiente pra arrancar fora essa função &lt;code&gt;circunferencia&lt;/code&gt;, ela não seria desnecessária? Eu não disse que o compilador reescreve tudo pra ficar mais eficiente? Não dava pra ter pré-calculado como antes e só guardar a constante com o resultado final? Então, aqui que a coisa começa a complicar. Toda função que criamos no Java se torna uma interface. Se o compilador quisesse ser agressivo ele poderia arrancar tudo fora e deixar só a constante, porém a gente não sabe se ninguém mesmo vai usar essa função. Lembra? Java também é uma linguagem interpretada, dinâmica, cujo código pode ser alterado dentro da JVM.&lt;/p&gt;

&lt;p&gt;Se pensar isoladamente só nessa classe, não teria problema, mas classes Java não vivem no vácuo, elas foram pensadas pra serem reusadas. Por exemplo, no mesmo diretório posso criar um novo arquivo e escrever uma nova classe assim, fazendo de novo um &lt;code&gt;System.out.print&lt;/code&gt;, chamando &lt;code&gt;Calc3.circunferencia&lt;/code&gt; direto assim mesmo e passando 20 como raio. Se eu sair e compilar só esse arquivo calc5.java, posso executar &lt;code&gt;java Calc5&lt;/code&gt; e tudo funciona. Note que nem na hora de compilar e nem na hora de executar eu mencionei o Calc3, porque ele tá no mesmo diretório, então quando a JVM carrega a classe Calc5 automaticamente faz o Class Loader carregar o Calc3 junto. Mas ele não precisou recompilar o Calc3 de novo.&lt;/p&gt;

&lt;p&gt;E se eu apagar o Calc3.class e tentar executar o Calc5? Agora vai dar a erro de &lt;code&gt;NoClassDefFound&lt;/code&gt;. Faz sentido né? Java foi feito de tal maneira que eu não preciso recompilar tudo do zero toda vez que alguma coisa muda. Ele compila cada classe individualmente, e pra garantir que outras classes não quebrem, pelo menos mantém a interface entre eles estável, o que significa que não pode sair otimizando demais e eliminando métodos, mesmo se naquela classe ela não é usada. E mesmo se eu tivesse declarado como &lt;code&gt;private&lt;/code&gt;, ainda assim não daria pra apagar o método, porque Java tem sistema de Reflection que permite manipular métodos privados. Portanto, qualquer otimização que quebre a interface não pode ser feita.&lt;/p&gt;

&lt;p&gt;Antes de continuar, agora precisamos entender rapidamente o que é uma interface. No mundo binário, não existe o conceito de abstração, tudo são instruções e endereços. Quando precisa chamar uma função, a máquina na verdade vai guardar o endereço que estamos executando  na pilha, vai fazer o jump, vai executar o que precisa, e no final vai retornar pro endereço que tinha guardado, pra continuar de onde parou. Eu expliquei isso no video do Hello World.&lt;/p&gt;

&lt;p&gt;Agora, pra isso funcionar, quando eu compilei a classe Calc3, o compilador precisaria ter anotado que a função &lt;code&gt;circunferencia&lt;/code&gt; vai estar disponível num endereço X, faz de conta 9000 0000. Daí quando eu compilei a classe Calc5 que chama essa função, nesse ponto o compilador precisaria saber que precisa escrever um jump pro tal endereço 9000 0000. Mas lembram que eu não passei o Calc3 como parâmetro nem nada? Como que o Java sabe pra qual endereço tem que dar jump?&lt;/p&gt;

&lt;p&gt;Aqui estou especulando, mas imagino que quando a JVM carrega essas classes, nesse momento que ela vai preenchendo uma tabela interna que diz, &quot;ok, carregar classe Calc3, ela tem um método circunferencia, vou colocar ela no endereço que tem vago aqui, 9000 0000&quot;. Depois carrega a Calc5 e quando executa a função &lt;code&gt;main&lt;/code&gt; dela e de lá pede pra chamar a função &lt;code&gt;circunferencia&lt;/code&gt;, daí a JVM intercepta essa chamada e consulta sua tabelinha &quot;hmmmm, essa função é a que eu coloquei no endereço 9000 000, pula pra lá&quot;. Portanto os endereços não estão nas classes, elas são atribuídas na hora que a JVM carrega elas.&lt;/p&gt;

&lt;p&gt;Vamos abrir o disassembly de novo, começa executando lá da linha 0 e vai indo. Uma hora esbarra nessa linha 6, que tem um &lt;code&gt;invokestatic&lt;/code&gt; passando um índice 15. Ele olha lá na tabela interna dele e vê &quot;ah, número 15 é o endereço 9000 0000&quot; e aí dá o jump pro lugar certo. Eu simplifiquei mas justamente porque os endereços finais são definidos quando carrega o módulo e não quando compila, dá pra compilar as classes independentes uma da outra, como eu fiz aqui, compilando individualmente os dois arquivos, porque a JVM é quem vai gerenciar os endereços em tempo de execução.&lt;/p&gt;

&lt;p&gt;Porém, linguagens como C, C++, Swift, Go, Rust, Crystal, e outras não tem um interpretador ou máquina virtual pra controlar isso, portanto cada parte de cada módulo sendo compilado precisa já saber o endereço final das coisas, senão na hora de executar não ia achar nada. Como que faz?&lt;/p&gt;

&lt;p&gt;E aqui vem outro conceito de compilador que vou simplificar ao máximo. Pensa, se eu tivesse um projeto com mil arquivos, onde uma função num arquivo pode chamar funções em outros arquivos, seria uma zona pra compilar. Toda vez que eu mudo uma linha num arquivo, precisaria recompilar todos os mil arquivos pra garantir que os endereços de todo mundo estão corretos e eu não apaguei uma função ou deu conflito de endereços ou duplicou endereços sem querer. Então na realidade, tanto o GCC quanto um clang de LLVM quanto um rustc de Rust, vão quebrar a parte final da compilação em duas etapas.&lt;/p&gt;

&lt;p&gt;Se você é de Linux e já instalou um pacote que compila a partir do código fonte, já deve ter esbarrado num &lt;code&gt;Makefile&lt;/code&gt;, que nada mais é do que um script que automatiza a função de checar dependências, se tudo que precisa pra compilar tá instalado. Pra você que é de Javascript é o equivalente a um package.json. Pra você que é de Python é tipo um requirements.txt. Ele chama o compilador pra todos os arquivos do projeto. O que costuma aparecer na tela enquanto compila? Um monte de arquivos &lt;code&gt;.o&lt;/code&gt; sendo gerados.&lt;/p&gt;

&lt;p&gt;Esses &quot;.o&quot; são objetos, simplificando, eles são os binários compilados a partir de código fonte de C de cada arquivo. Vamos abrir o hello.c lá do começo. Tão vendo esse &lt;code&gt;include &amp;lt;stdio&amp;gt;&lt;/code&gt; na primeira linha, isso indica pro compilador procurar por um arquivo de cabeçalho chamado &lt;code&gt;stdio.h&lt;/code&gt; que tem que estar no Path do sistema. Nesse arquivo vai constar a interface da função &lt;code&gt;printf&lt;/code&gt; que usamos lá embaixo. Se olharmos nesse arquivo tá dizendo que o &lt;code&gt;printf&lt;/code&gt; é uma função que retorna inteiro e recebe como parâmetro um ponteiro constante de char, basicamente um array de chars, e tem outros argumentos opcionais.&lt;/p&gt;

&lt;p&gt;Sem entrar muito em detalhes sobre o C, ter essa interface permite o compilador saber se quando estamos usando a função &lt;code&gt;printf&lt;/code&gt;, se tá da forma correta. Porém, no momento da compilação do arquivo &lt;code&gt;hello.c&lt;/code&gt; pro objeto binário &lt;code&gt;hello&lt;/code&gt; ele ainda não sabe qual é o endereço onde encontrar a função, porque ela tá numa biblioteca externa, a libc, que é compartilhada com outros programas no mesmo sistema que dependem dela. Praticamente tudo numa distro Linux.&lt;/p&gt;

&lt;p&gt;No ponto onde chama o &lt;code&gt;printf&lt;/code&gt; o compilador vai tipo deixar um post-it dizendo &quot;confirmar depois onde fica esse endereço&quot;. Se eu dependesse de outras bibliotecas externas, ia ficar deixando vários post-its pra confirmar depois. Pra ilustrar, vamos fazer o disassembly do objeto hello usando o &lt;code&gt;objdump -d&lt;/code&gt;. Olhar assembly sempre é meio intimidante, mas não tem problema, só focar no pedaço que interessa que é a função &lt;code&gt;main&lt;/code&gt; que começa aqui a partir do endereço 1139 em hexa. O &lt;code&gt;printf&lt;/code&gt; é uma função então estamos procurando um &lt;code&gt;call&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;E no endereço 1147 temos um suspeito, um &lt;code&gt;call&lt;/code&gt;, mas pra um endereço aqui perto, em 1030. Vamos ver o que tem lá. E olha lá, tem um jump pra esse outro endereço e um comentário que é pra função &lt;code&gt;puts&lt;/code&gt; da glibc. Se não estou enganado, quando se chama o printf com o parâmetro opcional nulo, ele muda pra puts. Mas nessa linha, na primeira fase do compilador, quando ele ainda não sabe os endereços, vai ter algo como um espaço em branco pro endereço, um post-it pra segunda fase confirmar o endereço.&lt;/p&gt;

&lt;p&gt;Essa segunda fase é o trabalho de um linker, no caso do GNU LD. Até agora eu não precisei chamar o linker manualmente porque o GCC faz isso sozinho pra gente. Mas se eu quiser manipular vários objetos já compilados e transformar numa biblioteca compartilhada, como um &lt;code&gt;.so&lt;/code&gt; de Linux ou &lt;code&gt;.dll&lt;/code&gt; de Windows, posso fazer isso com o comando &lt;code&gt;ld&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Inclusive, o linker pode fazer otimizações em tempo de linking. Apesar do nome ser linker que é linkar ou grudar os pedaços, ele pode modificar mais do binário. É ele que sabe se é possível por exemplo fazer inlining. Lembra o exemplo besta de Java de circunferência? Se fosse em C, como ninguém mais chama aquela função, ele poderia simplesmente copiar o conteúdo da função e colar no lugar onde a função é chamada. Isso evita no mínimo uma instrução de jump e outra de return. Num programa maior tem muito mais ganhos.&lt;/p&gt;

&lt;p&gt;No final, o compilador de C consegue analisar seu código e realizar um monte de otimizações e reescrever de uma forma otimizada, mas isso arquivo a arquivo independentemente, daí o linker reavalia  os binários gerados, liga os endereços de cada um onde precisa e ainda otimiza mais, o que diminui mais ainda o tamanho do binário, economiza mais memória e processamento na hora de executar, e por isso programas em C e similares como Rust são tão rápidos.&lt;/p&gt;

&lt;p&gt;Linguagens como Java, Javascript, Python, e outros não tem o equivalente de um linker quando compilam do código fonte pra bytecode intermediário. Uma das razões disso é aumentar a reusabilidade desses binários, o que também torna a fase de compilação menos demorada que um C pra um projeto do mesmo tamanho. Porém o binário vai ser menos otimizado, tecnicamente mais lento do que o equivalente gerado por C. Então é isso? C sempre vai ser mais performático que qualquer coisa? Não é bem assim. E agora voltamos pra Just In Time Compiler, os JITs.&lt;/p&gt;

&lt;p&gt;Quando você carrega um projeto em Java, ele inicia a máquina virtual JVM, daí o class loader começa carregando os arquivos binários &lt;code&gt;.class&lt;/code&gt; do seu projeto e criando tipo uma tabelona interna que diz em quais endereços internos na memória estão quais classes e quais funções, de forma que quando uma classe pede pra executar uma função de outra classe, a JVM vai intermediar e arbitrar mandando executar a função correta, devolvendo o retorno e tudo mais.&lt;/p&gt;

&lt;p&gt;Um interpretador ou máquina virtual substitui a necessidade de um linker estático, porque ele cuida disso dinamicamente. Então você pode imaginar que o C, que não tem esse intermediário já precisa saber de antemão o endereço de tudo. Se de fato toda vez a JVM precisar ficar checando os endereços nessa tabela virtual pra cada chamada de função, seria tudo lento mesmo. Mas claro que no mínimo ela faz cache disso pra ir mais rápido. Mas ela pode fazer mais. Ela analisa os códigos mais executados, por exemplo baseado em quais caches são mais pedidos, os &quot;pontos mais quentes&quot; do programa, literalmente o HotSpot, que é o nome da máquina virtual de Java quando ele ganhou capacidades de just in time compiling, ou JIT.&lt;/p&gt;

&lt;p&gt;Se meu programa chamasse aquela função circunferencia diversas vezes sempre com o mesmo raio, em vez de toda vez executar o cálculo, o JIT vai ser esperto e cachear o valor final. Muitas das otimizações que eu falei que o linker faz no C em tempo de compilação, o JIT vai fazer durante a execução. Por isso que em Java você já tenha ouvido falar de &quot;tempo de aquecimento&quot;, ou warm up. Logo que inicia um servidor Java, ele vai ter uma performance X que pode até parecer baixa, mas deixe rodando um tempo, e o mesmo código começa a executar mais rápido. O JIT é um otimizador em tempo real, aplicando mais e mais otimizações nos códigos mais acessados.&lt;/p&gt;

&lt;p&gt;Esse JIT, no final, tá traduzindo o bytecode do Java em instruções nativas de máquina de verdade mais otimizadas. É literalmente um compilador e mais ou menos um linkeditor que só trabalha em cima de código que realmente é necessário. Se seu programa for grande e tem partes que raramente são acessados ou ninguém nunca acessa, eles continuam lá sem serem otimizados. E o JIT vai fazendo cache do que vai otimizado, assim da próxima vez que alguém chamar essa parte, ele só executa o binário nativo de novo. E nesse ponto, esse trecho do código pode estar quase tão rápido como se tivesse sido escrito em C. Claro, não é só isso que determina a performance final, mas o JIT diminui bastante a distância entre traduzir bytecode Java de forma interpretada e um binário nativo compilado de C.&lt;/p&gt;

&lt;p&gt;A mesma coisa acontece com Javascript no navegador. Navegadores baseados em Chromium, como o próprio Chrome, Microsoft Edge, Opera, Brave, todos acabam usando a máquina virtual Google V8 pra executar Javascript. V8 na verdade é o nome de um conjunto de tecnologias de compilador e máquina virtual, assim como Java ou Python. Internamente ele tem um lexer, um parser, que vai cuspir um AST e que por sua vez vai gerar uma representação binária intermediária em bytecodes específicos da máquina virtual do V8.&lt;/p&gt;

&lt;p&gt;Sim, o que roda no V8 não é javascript, são esses bytecodes. Repetindo, elas são como instruções de um x64 de verdade, mas pra uma máquina javascript que não existe. Nesse arquivo bytecode.h tem declarado todos os bytecodes. Vou deixar nas descrições abaixo pra quem tiver curiosidade.&lt;/p&gt;

&lt;p&gt;Uma ver convertido em bytecodes, o interpretador do V8, que se chama Igniter, começa a traduzir instrução a instrução. Não vai ser lento, mas também não vai ser a melhor performance. Assim como na JVM, ele vai ficar analisando a execução e perceber quais partes precisa otimizar. Digamos que sua aplicação web baixou trocentas bibliotecas, por exemplo o arquivo inteiro do jQuery. Mas na real você só precisa da função &lt;code&gt;$()&lt;/code&gt; pra achar um elemento da página e a função &lt;code&gt;fadeToggle&lt;/code&gt; pra criar uma transição animada de fade desse elemento. Todas as outras funções você não usa.&lt;/p&gt;

&lt;p&gt;Se o V8 fosse burro e tentasse compilar tudo que sua página baixa, ele ia gastar um tempão pra otimizar a biblioteca do jQuery inteiro, quando na verdade sua página só usa meia dúzia dessas funções. Mas à medida que o usuário navega pelo site, o V8 acha os hotspots e agora é hora do Just in Time Compiler, do JIT, entrar em ação, no caso o Google chama o JIT do V8 de TurboFan. É ele quem vai gastar memória e processamento pra otimizar as partes mais usadas e gerar binário nativo de máquina otimizado, parecido com o que o Java faz, e jogar num cache em memória ou até em disco. E por isso programas em Javascript costumam consumir muita memória.&lt;/p&gt;

&lt;p&gt;Por isso que tanto Java quanto Javascript conseguem atingir velocidades muito boas, às vezes próximas de C mesmo sem terem sido agressivamente otimizadas por um compilador antes. Mas por que se dar a tanto trabalho e não simplesmente compilar tudo antes como o C faz? E isso não tem uma resposta única, mas se eu fosse resumir é o que define a diferença entre linguagens estáticas e dinâmicas.&lt;/p&gt;

&lt;p&gt;A vantagem de C é que os compiladores amadureceram muito nos últimos 40 anos e o binário nativo final que geram tendem não só a serem os mais rápidos em geral, mas também a ocupar bem menos espaço em disco e a consumir menos memória quando carregam. Por isso que a kernel do Linux, drivers e coisas assim tendem a serem feitos com C, porque eu quero o mais otimizado possível e que escale bem, pra rodar tanto num servidor de datacenter com centenas de núcleos de processador e terabytes de RAM quanto num mero Raspberry Pi com quase nada de recursos.&lt;/p&gt;

&lt;p&gt;Uma vez que o binário tá compilado estaticamente, é tudo fixo e imutável, você não tem como mudar o comportamento dele. Sim, se tiver muita paciência pra explorar coisas como injeção de código direto na memória do processo ou fazer patches de binários, explorar exploits de segurança, aí você consegue mudar alguma coisa. Mas isso é na gambiarra. É o que a comunidade de mods de games faz também. Eles descobrem em qual endereço da memória que fica o valor de vidas e eu injeto outro valor lá. Já mostrei isso no video do Super Mario.&lt;/p&gt;

&lt;p&gt;Por outro lado pense no seu navegador web. A qualquer momento eu posso inspecionar um elemento de qualquer página e sair manipulando. Eu mesmo, quando faço capturas de tela pra mostrar a imagem de um site nos videos, se tem uma propaganda muito grande, abro o inspetor, seleciono o elemento de propaganda e apago. E essa mudança se reflete na página imediatamente. Isso só é possível porque o navegador mantém a estrutura de dados que representa essa página em memória de forma acessível via Javascript. E como o código javascript não é estático, não é fixo, porque tem um interpretador que aceita código novo sem precisar reiniciar, dá pra fazer essas coisas.&lt;/p&gt;

&lt;p&gt;A grande vantagem de linguagens dinâmicas é permitir esse tipo de flexibilidade. Se isso é bom ou ruim, depende do seu uso. Tem gente que acha que isso não deveria ser permitido, que uma vez compilado, deveria ser tudo fechado. Tem gente que acha que precisa ser tudo aberto, independente se vai ficar mais lento ou consumir mais memória. Como eu sempre digo, em programação a gente troca uma coisa por outra, sempre. Não existe resposta definitiva, tudo depende de pra que vai usar. E no caso de um navegador web acho que todo mundo vai concordar que ser aberto e dinâmico é muito mais prático e é por isso que a Web em si fez tanto sucesso.&lt;/p&gt;

&lt;p&gt;Além disso linguagens dinâmicas como Javascript, Ruby, Python, Elixir, Java, C#, possuem capacidades muito potentes de metaprogramação, onde você pode escrever código que altera que está em execução. Praticamente toda linguagem com um interpretador possibilita isso. Java tem Reflection, Javascript ES6 tem APIs como Reflect ou Proxy, Python tem a função &lt;code&gt;type&lt;/code&gt;, Ruby tem coisas como Refinements. E a vantagem de um interpretador é que podemos injetar novos bytecodes sem reiniciar o programa. Como a maioria dos interpretadores possuem algum tipo de JIT, esse novo código também pode gerar binário nativo otimizado.&lt;/p&gt;

&lt;p&gt;Pra não complicar vou pular os detalhes sobre metaprogramação e a diferença com pré-processamento estático, templates como em C++, ou macros como em Rust. Estudem se tiver interesse, mas a conclusão de tudo isso é dizer que linguagens estáticas costumam ter um compilador que gera binários estáticos, cujo bytecode não é mais modificado facilmente. Linguagens dinâmicas costumam ter um interpretados justamente pra ter essas propriedades de dinamicamente modificar o bytecode e alterar o comportamento do programa em tempo real de execução. O custo disso é serem mais lentos, porque não puderam contar com a otimização agressiva de um compilador e linkeditor.&lt;/p&gt;

&lt;p&gt;Mas hoje em dia essa diferença diminuiu drasticamente, porque quase todo interpretador vem com um compilador embutido que roda em tempo real, um Just in time compiler, ou JIT. No fim do dia, ambos acabam gerando binário nativo otimizado, um gera tudo antes, o outro gera enquanto roda. Um usa menos recursos, porque fez toda a compilação e otimização antes, e o outro usa mais recursos, porque vai continuamento compilando à medida que o programa é executado.&lt;/p&gt;

&lt;p&gt;Como o video ficou super longo já, no próximo video vou dar uma pincelada em cada linguagem individualmente, mas antes vocês precisavam entender a diferença entre compiladores e interpretadores. Ser interpretado não implica ser sempre &quot;lento&quot;, só às vezes. Um exemplo, quem já ouviu falar de Elixir e Erlang provavelmente reconhece sua reputação de ser altamente escalável, tanto que é com ele que é feito o motor de comunicação por trás do Whatsapp, que é gigantesco. Mas nem todo mundo entende que Erlang, assim como Java, é uma linguagem interpretada, que roda numa máquina virtual, que é um tipo de interpretador, e que tem um Just in Time Compiler, ou JIT, pra acelerar as coisas.&lt;/p&gt;

&lt;p&gt;Pela popularidade do Javascript, muita gente já sabe que as performances absurdas que ela consegue ter, o suficiente pra portar um game de verdade e rodar dentro do navegador é em grande parte graças ao JIT. Mas tem mais detalhes disso. E muito antes de Typescript e todas as linguagens transpiladas pra Javascript, um dos primeiros transpilers apareceu no mundo PHP com o HipHop do Facebook, que transpilava PHP pra C++. Hoje em dia eles pararam esse projeto e estão investindo mais na máquina virtual HHVM que é compatível até certo ponto com PHP 7. Python, consegue gravar os bytecodes da fase de parsing como arquivos &lt;code&gt;.pyc&lt;/code&gt;, que são instruções pra sua própria máquina virtual. Ruby também gera bytecodes internamente. Enfim, tem bem mais coisas que eu vou guardar pro próximo video.&lt;/p&gt;

&lt;p&gt;A intenção do video era só educar vocês pra não ficarem com cara de bobos argumentando coisas como &quot;ah, sua linguagem é interpretada, por isso nunca vai ser tão rápida quanto uma compilada&quot;, ou &quot;ah Java é melhor que Javascript porque ela é compilada&quot;. A grande verdade é que tecnologias de compiladores e máquinas virtuais evoluiu bastante nos últimos 20 anos e essas noções erradas que eram até corretas nos anos 90, já não é mais tão simples assim. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e não deixem de compartilhar o video com seus amigos. A gente se vê, até mais.&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5950</id>
    <published>2022-03-23T14:28:00-03:00</published>
    <updated>2022-03-23T14:50:12-03:00</updated>
    <link href="/2022/03/23/akitando-116-de-5-tera-a-25-giga-compressao-de-dados-e-multimidia" rel="alternate" type="text/html">
    <title>[Akitando] #116 - De 5 Tera a 25 Giga | Compressão de Dados e Multimídia</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/j4a1SgUWh1c&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Huffman, Lempel-Ziv, LZ77: o que esses nomes tem a ver com o video que você tá assistindo aqui no YouTube e Netflix da vida? Vamos ver de uma forma prática sobre compressão de dados e como eles são aplicados no seu dia a dia. E de quebra vai aprender uma pequena introdução a ciência de cores e processamento de imagens e videos.&lt;/p&gt;

&lt;h2&gt;Conteúdo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00:00 - O problema: Como faz streaming de 5TB em 2h??&lt;/li&gt;
&lt;li&gt;00:03:52 - Cores não é só RGB: entendendo YUV&lt;/li&gt;
&lt;li&gt;00:08:51 - Chrome Subsampling&lt;/li&gt;
&lt;li&gt;00:13:52 - Contínuo pra Discreto: Fourier&lt;/li&gt;
&lt;li&gt;00:15:22 - Resolução de Áudio&lt;/li&gt;
&lt;li&gt;00:18:50 - Transformada Discreta de Fourier&lt;/li&gt;
&lt;li&gt;00:20:11 - Transformada Discreta de Cosseno&lt;/li&gt;
&lt;li&gt;00:24:14 - Quantização&lt;/li&gt;
&lt;li&gt;00:27:37 - Run-Length Encoding&lt;/li&gt;
&lt;li&gt;00:29:51 - Huffman Encoding&lt;/li&gt;
&lt;li&gt;00:36:45 - Recapitulando até aqui&lt;/li&gt;
&lt;li&gt;00:38:06 - JPEG&lt;/li&gt;
&lt;li&gt;00:40:12 - GIF&lt;/li&gt;
&lt;li&gt;00:42:44 - Lempel-Ziv&lt;/li&gt;
&lt;li&gt;00:44:50 - Sliding Window&lt;/li&gt;
&lt;li&gt;00:46:13 - Legado do PKZIP&lt;/li&gt;
&lt;li&gt;00:49:26 - A treta da Unisys e GIF: PNG&lt;/li&gt;
&lt;li&gt;00:53:33 - Aplicando tudo isso em video&lt;/li&gt;
&lt;li&gt;00:55:00 - Codecs de video: AVC, HEVC&lt;/li&gt;
&lt;li&gt;00:57:17 - Interframes e Intraframes&lt;/li&gt;
&lt;li&gt;00:58:58 - Problema resolvido: bitrates&lt;/li&gt;
&lt;li&gt;01:01:27 - codecs profissionais vs de consumo&lt;/li&gt;
&lt;li&gt;01:04:40 - Resumo e Conclusão&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;But what is the Fourier Transform? A visual introduction. (https://www.youtube.com/watch?v=spUNpyF58BY)&lt;/li&gt;
&lt;li&gt;How are Images Compressed? [46MB ↘↘ 4.07MB] (https://www.youtube.com/watch?v=Kv1Hiv3ox8I)&lt;/li&gt;
&lt;li&gt;Original CompuServe announcement about GIF patent (Original CompuServe announcement about GIF patent)&lt;/li&gt;
&lt;li&gt;Thomas Boutell (https://twitter.com/boutell/status/1357707533612441604?s=20&amp;amp;t=A2zgWT-iY2CiZrNvMI4lBw)&lt;/li&gt;
&lt;li&gt;Huffman Visualization (https://cmps-people.ok.ubc.ca/ylucet/DS/Huffman.html)&lt;/li&gt;
&lt;li&gt;Chroma Subsampling – 4:4:4 vs 4:2:2 vs 4:2:0 (https://www.displayninja.com/chroma-subsampling/)&lt;/li&gt;
&lt;li&gt;How JPEG works (https://cgjennings.ca/articles/jpeg-compression/)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Todo mundo já zipou alguma coisa. Até quem não é programador sabe que existe isso de &quot;zipar&quot; as coisas. Um programador tem que ir um passo além de só acreditar cegamente que as coisas funcionam como mágica. Como funciona compressão de dados? Já pararam pra pensar nisso? Melhor ainda, estamos assistindo um video aqui no YouTube. Como funciona a compressão de video pra possibilitar assistir isso aqui em Full HD ou 4K? Quem estudou ciência da computação provavelmente aprendeu o básico. A idéia do episódio de hoje não é falar de tudo que existe no assunto, mas dar mais noção pra você parar de ficar no escuro e aumentar sua curiosidade pra continuar estudando.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Vamos fazer uma conta de padeiro. Digamos que você tem grana sobrando, comprou uma TV nova 4K, e quer assistir filmes em 4K, com 8-bits de cor, em pelo menos 30 quadros por segundo. Um monte de jargão, vamos traduzir. Dizer 4K significa dizer 4 vezes a densidade de pixels de Full HD, High Definition. Full HD é a mesma coisa que 1080p, ou seja 1920 pixels na vertical e 1080 pixels na horizontal. O &quot;p&quot; é de progressivo, antigamente a gente tinha “i” de interlaced ou entrelaçado que ninguém usa mais então nem vou falar disso. Então, 4K é o dobro de resolução na vertical, que dá 3840 pixels e o dobro de resolução na horizontal, que dá 2160 pixels. O agora famoso 2160p. E cuidado que 8K tá chegando.&lt;/p&gt;

&lt;p&gt;Tá claro que dobrando altura e comprimento, quadruplica a quantidade de pixels né? Geometria do primário aqui. Agora, se você é programador front-end, tá acostumado a cores de 24-bits, onde cada componente de RGB tem 8-bits cada, que são 256 tons pra cada uma das 3 cores primárias, vermelho, verde e azul. Normalmente a gente codifica em hexadecimal, então vai de 00 até FF. Preto é todas as cores zeradas então 000000. Branco é todas as cores no máximo, então FFFFFF. Esse é o básico de CSS que se aprende no dia 1, então vou assumir que todo mundo aqui tá confortável com isso.&lt;/p&gt;

&lt;p&gt;Significa que cada cor precisa de 8-bits ou 1 byte, total de 3 bytes pra guardar uma cor por pixel. Repetindo, 24-bits é a mesma coisa que 3 bytes. Uma imagem 4K então vai ter 3840 colunas vezes 2160 linhas que dá um total de 8 milhões, 294 mil e 400 pixels. Tudo isso vezes 24 bits por pixel, que vai dar 24 milhões, 883 mil e 200 bytes, ou quase 24 megabytes por imagem.&lt;/p&gt;

&lt;p&gt;Mas um video são 24 ou 30 desses quadros por segundo. A cada segundo precisaria trafegar 24 megabytes vezes 30 quadros que dá mais de 711 megabytes por segundo! De curiosidade, quanto precisaria ter de SSD aí no seu computador pra guardar um filme de 2 horas então? Só multiplicar por 7200 segundos, que é quantos segundos tem em 2 horas. Então 711 megabytes por segundo vezes 7200 segundos, vai dar a quantidade absurda de 5 milhões de megabytes, ou seja, nada menos que 5 terabytes.&lt;/p&gt;

&lt;p&gt;Pensa um segundo, se isso fosse verdade, eu tenho certeza que o seu SSD aí na sua máquina não seria capaz de armazenar nem um único filme. Mas eu também sei que você baixa um monte de filmes via Torrent e deve ter uma dúzia nesse instante aí na sua pasta de downloads. Você é um programador, sua função é escovar bits. Como que você escovaria esses 42 trilhões de bits pra caber no seu HD?&lt;/p&gt;

&lt;p&gt;O campo de compressão de video em particular é um corpo de conhecimento gigantesco, tem centenas de pesquisas e implementações e formas diferentes de resolver o problema. O que eu vou fazer a partir daqui é uma simplificação super grosseira pra vocês entenderem o básico, ter um modelo na cabeça pra ficar mais fácil quando quiserem ler sobre os detalhes avançados depois. Pra quem já é profissional da área, perdoem a simplificação mas é por uma boa causa. Vamos continuar.&lt;/p&gt;

&lt;p&gt;Eu venho falando faz vários videos que tudo no final vira um linguição de bits. Um computador só entende uma fita de bits, nada mais. E uma fita de bits é nada mais, nada menos que um numerozão gigante de milhões de bits. Veja meus videos sobre a máquina de Turing e a introdução mais hardcore de computadores que soltei alguns meses atrás. Mas entenda que um video é também é só isso, um linguição de bits. Nosso trabalho, como programadores, é representar o mundo em forma de bits e processar esses bits pra tranformar em outros bits mais úteis. Um programa é só uma função, que nem uma equação, que recebe números e cospe números, no caso números em binário, bits.&lt;/p&gt;

&lt;p&gt;E pra isso você tem que parar de assumir que as coisas são regras escritas em pedra. Por exemplo, se você é só programador de front-end e seu dia a dia é justamente escrever cores em hexadecimal pra RGB, seu mundo é estático em pensar que a única forma de representar cores é juntando 1 byte pra vermelho, 1 byte pra verde e 1 byte pra azul, e acabou. Isso não é verdade. Existem diversas formas de representar cores e RGB é a mais fácil de entender, porque todo mundo misturou tintas na escola em algum momento, mas não é a forma mais eficiente. Aqui começa aquele troço que todo iniciante não entende chamado “abstrações”. Vamos ver na prática o que isso significa.&lt;/p&gt;

&lt;p&gt;E se eu quiser gastar menos bits por cor? Podemos diminuir de 24-bits pra digamos, 12-bits. Daí em vez de gastar 3 bytes no total, vamos gastar só 1 byte e meio, metade do espaço. Só que as possibilidades de cores não cai pela metade. Lembra, são potências de 2. Ou seja, caimos de um total de mais de 16 milhões de cores pra só umas 4 mil cores. Pra economizar metade do espaço de bytes, perdemos milhões de possibilidades de cores. É parecido quando se converte uma foto pra um GIF, que vou explicar sobre mais pro fim. Mas 4 mil cores é muito pouco hoje em dia. Pra fazer jogo de Nintendinho em pixel art tá mais que bom. Mas pra editar fotos, é impossível. É muito pouca informação de cor. Até um amador consegue ver uma foto com paleta de 4 mil cores e dizer que a qualidade é inaceitável.&lt;/p&gt;

&lt;p&gt;Então não dá pra cortar bits do RGB, cada bit que eu corto, divido o total de cores por 8. Fodeu então? Não. No caso de video não se usa RGB e sim outra representação chamada YUV, alguns já devem ter visto esse nome. Mas simplificando a idéia é dividir a cor em 2 grandes componentes, a luminância que é o brilho ou luma e o chroma, que é representado por diferença de azul e diferença de vermelho. Quando se fala em color science, é aqui que isso começa. Em vez de pensar em cores como mistura de tintas, imaginamos cores como a distância pro ponto mais branco num espaço tridimensional.&lt;/p&gt;

&lt;p&gt;Se você é fotógrafo ou algo assim já deve ter ouvido falar em coisas como sRGB, Adobe RGB ou DCI-P3 que é color gamut ou gama de cores, que é um subconjunto do total teórico de cores possíveis de existir no espaço de cores. YUV representa numericamente todas as cores teóricas e color gamut como Adobe RGB é o que realmente é possível de mostrar num determinado monitor. Essa parte é bem longa, tem que passar por coisas como correção de gama, mas na prática se você não sabe a importância de DCI-P3 contra sRGB, é porque pra você isso não faz diferença. Só fotógrafos e coloristas profissionais que precisam saber os detalhes disso.&lt;/p&gt;

&lt;p&gt;YUV vem da era de TV preto e branco e como fazer broadcast de video, daí já existia o conceito de luma, já que não tinha cores, ou chroma, ainda. Em YUV o Y é o luma, U é a projeção de azul e V é a projeção de vermelho. Em resumo, converter de RGB pra YUV é multiplicar o RGB por esta matriz aqui. Eu sei, assusta pra quem nunca viu Álgebra Linear. E converter de volta pra RGB é só inverter e multiplicar YUV por esta outra matriz. É uma conversão linear, um pra um, sem perder informação nenhuma. Se nunca pensou nisso, RGB é uma coordenada em 3 eixos de cor num espaço tridimensional, um vetor.&lt;/p&gt;

&lt;p&gt;Pra visualizar, vejam esta imagem, um quadro de um video. Se separar em RGB, teríamos esse quadro representando só o componente vermelho, este só o verde e este outro só o azul. É como você pensa em canais no Photoshop, cores primárias sendo misturadas. Mas depois de converter pra YUV, agora este é o Y ou luma, que seria a imagem preto e branco e tons de cinza. Este é o quadro de projeção de azul e este outro o de projeção de vermelho, que é o componente UV ou chroma. Junte os 3 e temos a imagem original de volta.&lt;/p&gt;

&lt;p&gt;Lembram DVD player antigo ou video game antigo? Antes de existir HDMI como que vocês ligavam os aparelhos na TV? Quando era modelo barato era só um cabo com conector RCA, o cabo composite ou composto. Mas tinha modelos melhores, com 2 ou 3 cabos RCA separados. Alguns poderiam chutar que era RGB, sinal de uma das 3 cores primárias em cada cabo. Mas não, é um cabo pra luma, e os outros dois pra chroma. Por isso algumas TVs podiam até ter escrito atrás YCrCr que é a mesma coisa que YUV.&lt;/p&gt;

&lt;p&gt;Sem entrar em detalhes sobre correção de gama, color space e outros detalhes, o que ganhamos fazendo isso? Beleza, convertemos um número de RGB em outro número em YUV, mas continua usando os mesmos 24-bits. “Tomeito, tomáto”. Não mudou nada. Ah, mas agora vem o pulo do gato. Na ciência de cores, pesquisas mostraram que o olho humano é mais sensível a mudanças de luma do que de chroma. Ou seja, o componente Y é muito mais importante que U e V. Em RGB a informação de brilho vem embutido em cada cor primária, e quando convertemos em YUV o que fizemos foi retirar essa informação pra um número separado de luma.&lt;/p&gt;

&lt;p&gt;Vejam de novo a separação de canais, a imagem preto e branco é super nítida, conseguimos saber o que é. Mas agora olha os componentes de chroma, difícil de discernir o que estamos vendo né? Principalmente os detalhes. O mesmo não acontece se vermos os canais separados de RGB, porque a informação de luma está distribuido nas 3 cores primárias. Tão começando a ver a vantagem de representar a mesma informação de jeitos diferentes? RGB é mais fácil de descrever mas YUV vai ser mais fácil de manipular como vamos ver já já.&lt;/p&gt;

&lt;p&gt;Já tentou ligar seu computador no HDMI da sua TV velha, que não tem um modo game? Já notou que assistir videos é normal, fica bonito. Mas quando liga o computador pra digitar um email as letras ficam todas meio borradas? Isso acontece porque sua TV é YUV 4:2:0. E o que significa isso? Pra entender vamos voltar pra RGB. Quando falamos em RGB estamos falando de 8-bits cor primária. Quando falamos de YUV estamos falando de samples ou grupos de 4 pixels organizados em duas linhas. Nesta primeira imagem temos o formato não comprimido, logo depois da conversão de RGB, que chamamos de 4:4:4, toda a informação de luma e de chroma preservados.&lt;/p&gt;

&lt;p&gt;Agora, já disse que as pesquisas mostraram que nossos olhos são mais sensíveis a mudanças de luma do que de chroma. E se mantivermos a mesma quantidade de bits pra guardar luma então, garantindo a definição da imagem, e diminuirmos a quantidade de bits pra guardar chroma? Daí podemos pular pro esquema 4:2:2 que nem podemos ver aqui. Notem o importante, a informação de luma continua a mesma, mas cortamos chroma pela metade nesse sample, nesse grupo de pixels. Isso economiza um terço de bytes e caímos dos 48 bytes pra 36 bytes. Mas dá pra melhorar. Veja que perdemos muito menos informação do que quando tentamos cortar bits de RGB.&lt;/p&gt;

&lt;p&gt;O que as pesquisas mostraram é que podemos cortar ainda mais no chroma e com isso chegamos ao formato YUV 4:2:0 que toda TV usa, que cai dos 48 bytes originais pra 30 bytes. Eu não tô muito certo dessa conta, mas é nessa faixa. Outra coisa é que o 2 e zero no 4:2:0 não se refere ao total de pixels. É como se fosse uma proporção. O primeiro 4 se refere a todos os 4 sample de pixels originais do canal de luma. O segundo 2 é o número de samples de chroma na primeira linha. O terceiro 0 é o número de mudanças dos samples de chroma entre as duas linhas, no caso 0 mudança no formato 4:2:0.&lt;/p&gt;

&lt;p&gt;Se pareceu complicadinho é porque é mesmo. O que chamamos de YUV na realidade é o padrão YCbCr às vezes chamado de YCC e é tudo a mesma coisa. Mas não confundir com YPbPr que era o padrão mais antigo. Além disso eu falei que tem vários detalhes como correção de gamma e color space, que define padrões como o Rec. 709 que é o que todo mundo usa pra videos normalmente, como aqui no YouTube. Na prática, convertendo de RGB pra YUV 4:4:4 é um pra um, visualmente e em quantidade de bytes, não muda nada.&lt;/p&gt;

&lt;p&gt;Mas mantendo o luma em 4 e descendo chroma pra 2:0, economizamos uns 40% de dados sem mudanças perceptíveis pra um ser humano, especialmente num video em movimento. Por isso a maioria das TVs é 4:2:0, por isso que video de DVD, Blu-Ray, streaming, incluindo esse aqui do YouTube é encodado em YUV 4:2:0, segundo o padrão Rec. 709. E por isso eu falei que quando se liga um computador numa TV que não suporta modo de game, PC, ou mais corretamente, YUV 4:4:4, as letras e detalhes precisos vão ficar meio borrados. Como 4:2:0 precisa de menos dados pra representar, é ideal pra fazer broadcast ou streaming ou guardar arquivos de video, porque precisa de menos banda e menos espaço sem quase perder qualidade do video, com exceção de detalhes pequenos, como letras e fontes pequenas.&lt;/p&gt;

&lt;p&gt;Cada um dos componentes de YUV é representado por samples, e de 4:4:4 pra 4:2:0 estamos diminuindo samples do chroma então chamamos esse processo de Chroma Subsampling. Repetindo, estamos diminuindo bits de cor e mantendo bits de brilho pra ter praticamente a mesma imagem aos nossos olhos usando menos bits. Tudo que você achava que era RGB na verdade é YUV 4:2:0. Quando converte de 4:2:0 de volta pra RGB pra editar, ele completa os bits dos samples que faltam, duplicando dos que sobraram, então só aumenta o tamanho de bits mas não recupera o chroma perdido, lógico. Guarde essa informação.&lt;/p&gt;

&lt;p&gt;Agora pára e pensa, um video é basicamente um conjunto de imagens. No nosso exemplo, imagens 4K. E precisa de 30 dessas imagens todo segundo pra termos algo em movimento suave como vemos na TV. Se a gente consegue reduzir 40% de todas as imagens, o video inteiro também fica 40% menor, portanto aqueles 5 terabytes pode cair pra até 3 terabytes, olha só que sensacional. Só que 3 tera ainda é absurdo pra 2 horas de video, continua não cabendo aí no seu computador. Como que faz então?&lt;/p&gt;

&lt;p&gt;Antes de continuar deixa eu introduzir mais um jargão pra vocês. A compressão de cores ou downsampling, que é descer o números de samples de chroma, é só uma técnica pra economizar bits jogando fora informação que nossos olhos não são bons de detectar. Isso significa que uma vez feito o downsampling, não tem mais como voltar pra imagem original, porque os detalhes originais foram simplificados. Isso é uma compressão que chamamos de lossly, ou seja, que perde informação.&lt;/p&gt;

&lt;p&gt;Mas podemos ir além de chroma downsampling. Uma das principais funções de computadores é representar o mundo natural, analógico, contínuo, pro mundo digital, que é discreto e exato. Quando falamos em pixels, estamos arbitrariamente pegando a luz que chega de algum objeto e dividindo pacotes de luz em pixels, que é uma medida exata e discreta. Temos uma representação digital perto o suficiente pra ser convincente pros nossos olhos. Quanto mais pixels ou samples tivermos, quanto mais resolução, mais enxergamos a imagem como realista.&lt;/p&gt;

&lt;p&gt;O mesmo vale pra outras coisas da natureza que queremos digitalizar como música e sons em geral. Se aprendeu física acústica no colegial vai lembrar que sons ou ondas representamos com um gráfico de curvas, senos, cossenos, radianos. Se já abriu qualquer editor de áudio como um Audacity ou Adobe Audition, já viu essa representação de ondas. E qualquer engenheiro de áudio sabe o que precisa fazer quando recebe onda analógica pra processamento digital: transformada de Fourier.&lt;/p&gt;

&lt;p&gt;Eu mesmo não lembro mais quase nada de Fourier que aprendi na faculdade, mas acho que o principal é o seguinte. Da mesma forma que uma imagem não é formada de pixels discretos, uma onda de som também é contínua, mas num computador precisamos representar de forma discreta. Por isso que uma curva bonitinha como dessa imagem, uma linha contínua, pro computador fica tipo uma escadinha, que é um intervalo de valores discreto que mais ou menos representa essa curva original. Lembra quando eu falo que precisa pelo menos ter uma intuição matemática? Se gastou tempo estudando coisas como cálculo, enxergar isso deveria ser trivial.&lt;/p&gt;

&lt;p&gt;Assim como temos samples pra representar som, samples de pixels por segundo representa video. Em video também representamos um movimento contínuo em quadros por segundo discretos. Quanto mais quadros por segundo pudermos ter, mais o movimento fica suave. Mas convencionamos uns 30 quadros por segundo que já é suficiente pra gente conseguir assistir um filme sem achar estranho. Quadros por segundo é uma medida de frequência. 30 quadros por segundo seriam 30hz. Um game precisa de no mínimo uns 60 quadros por segundo pra ficar bom de jogar, por isso compramos monitores de 60hz ou mais.&lt;/p&gt;

&lt;p&gt;Quanto mais samples puder ter, maior a resolução, mais preciso vai ser o video ou áudio. A convenção da qualidade de CD de música é de 44.1khz, com samples de 16-bits, ou seja 44 mil e 100 samples por segundo, ordens de grandeza mais samples por segundo que video, mas cada sample ocupa bem menos espaço. Pelo jeito precisamos de bastante resolução pra enganar nossos ouvidos. Música HiFi que é o equivalente 4K da música, são pelo menos 192 khz com samples de 24-bits.&lt;/p&gt;

&lt;p&gt;Voltando pra CD, 16-bits vezes 44 mil e 100 dá 86 kilobytes por segundo. Um álbum de 24 músicas de 5 minutos cada, ou seja, 7200 segundos, vezes os 86 kilobytes por segundo vai dar aproximadamente 604 megabytes, que se você cresceu nos anos 90 e começo dos 2000 vai lembrar que um CD cabia 650 megabytes, por isso só isso de música já enche um CD. Era até engraçado porque antigamente tinha consoles como o Sega CD ou 3DO. Esses consoles não tinham potência pra ler arquivo de áudio comprimido tipo os mp3 de hoje e descomprimir em tempo real, por isso o áudio era sempre crú, descomprimido.&lt;/p&gt;

&lt;p&gt;Se tivesse as 24 faixas na trilha sonora, ocupando 600 mega, sobrava menos de 50 mega pro jogo em si. Por isso que cabia, porque um cartucho de megadrive não era muito mais que uns 2 megabytes, daí cabia fácil num CD de Sega CD. Mas eu tô fugindo do assunto. Mencionei áudio pra explicar que tudo analógico, imagem, video e áudio migram do mundo contínuo pro mundo discreto e no caso de áudio, todo engenheiro sabe analizar ondas usando transformada de Fourier, mais especificamente transformada discreta de Fourier pra representar um período de samples de onda como equações lineares.&lt;/p&gt;

&lt;p&gt;Eu não tenho a matemática pra conseguir explicar isso fácil mas entenda que é uma conversão de sinais analógicos contínuos num domínio de frequências discretas, ou seja, dividir uma onda contínua de som em conjuntos de bits discretos. Pra quem estudou matemática, não dá pra representar todos os infinitos pontos de uma curva em conjuntos de números discretos, mas podemos achar as equações que geram esses pontos em determinado momento do tempo.&lt;/p&gt;

&lt;p&gt;O problema de transformada discreta de Fourier é que o cálculo disso é lento. Consoles e micros antigos dos anos 80 não tinham como calcular em tempo real. Mas nos anos 60 descobriram uma aproximação que é ordens de grandeza mais fácil de calcular, pulando da complexidade quadrática da transformada discreta de Fourier pra complexidade logaritmica, N.log N que é a transformada rápida de Fourier ou Fast Fourier, que é considerado o algoritmo numérico mais importante já criado.&lt;/p&gt;

&lt;p&gt;Quando falam se é importante entender matemática e tals, esse é &quot;o&quot; caso mais importante em processamento digital e se tiver interesse em pelo menos ter uma intuição visual de como enxergar uma transformada de Fourier eu recomendo muito os videos do Grant do canal 3Blue1Brown. Ele é o melhor professor de matemática que já vi no YouTube e em particular sobre Fourier tem uma forma visual animada que vai te dar uma imagem mental muito melhor do que eu jamais conseguiria.&lt;/p&gt;

&lt;p&gt;Muito bem, vamos voltar. Porque falei todo esse mambo jambo de matemática? Porque grupos de pixels em imagens podemos tratar como séries de frequências também. Falando em resumo, bem resumido, estamos com nossa imagem gigante de 4K depois de fazer color downsampling pra YUV 4:2:0. O que podemos fazer agora?&lt;/p&gt;

&lt;p&gt;Transformada Discreta e Transformada Rápida de Fourier transformam ondas contínuas em uma representação de uma série discreta. Quando falamos em ondas, falamos de senos e cossenos. E nessas equações estamos considerando duas dimensões, por isso são equações de números complexos. Lembra número complexo, que tem um componente que é um número real e outro que é um número imaginário.&lt;/p&gt;

&lt;p&gt;Quando vamos mexer com imagens só precisamos de uma dimensão de números reais e por isso em vez de DFT ou FFT podemos usar DCT que é transformada discreta de cosseno. Em áudio usamos FFT não só pra representar o áudio analógico de forma digital mas também pra fazer DSP ou processamento digital de sinais, por exemplo, decompondo uma onda em ondas mais simples usando senos e cossenos. E de novo, eu to simplificando grosseiramente, mas é assim que fazemos coisas como denoising que é redução de ruído, equalização e tudo mais.&lt;/p&gt;

&lt;p&gt;Pois bem, com DCT que é a transformada discreta de cosseno vamos decompor o luma e chroma de cada imagem em uma soma de funções de cosseno de diferentes frequências. Fodeu né. Calma que é complicado mesmo, mas pra visualizar não é tanto assim. Como uma imagem pode ter qualquer dimensão, o algoritmo começa dividindo a imagem em blocos de 8 por 8 pixels. Não importa se é uma imagem pequena do tamanho de um ícone ou gigante do tamanho de uma TV 8K. Vamos sempre trabalhar em blocos de 64 pixels de cada vez. Em computação a gente sempre usa a estratégia de dividir e conquistar. Nenhum trabalho é tão grande que não possa ser quebrado e atacado um pedaço de cada vez.&lt;/p&gt;

&lt;p&gt;Agora o trabalho da fase de DCT é quebrar os canais de luma, chroma azul, chroma vermelho em uma soma de funções de cosseno. Simplificando, essas funções de cosseno podemos ver visualmente nesta imagem também de 8 por 8 imagens. É uma tabela pré-calculada, não se preocupem agora como ela foi criada, só aceitem que existe. Cada quadrado representa uma onda diferente, a tal função de cosseno, em duas dimensões. Lá no canto superior esquerdo é todo branco, normalmente o componente mais usado. E lá embaixo é uma pattern que parece tabuleiro de xadrez ou damas, um checkerboard.&lt;/p&gt;

&lt;p&gt;Temos que processar os 3 canais de YUV, mas vamos pegar primeiro um bloco de 8 por 8 pixels só do canal luma, que é a camada preto e branco. E vamos representar cada pixel pela intensidade de preto até branco, indo de 0 até 255. Na real acho que é de 16 até 235 ou algo assim por conta de correção de gama mas não interessa agora. Funções de cosseno, você lembra que é em radianos no eixo X, por exemplo Pi que é 180 graus e 2 Pi que é 360 graus. Mas no eixo Y vai de -1 até 1 positivo.&lt;/p&gt;

&lt;p&gt;Então a primeira transformação que precisamos fazer pra cada pixel no nosso bloco é substrair por 128 pra dar números que vão de -128 até 127. Agora usando DCT, e aqui é a parte que vai ser uma caixinha preta que vocês vão ter que acreditar que funciona, vamos criar uma nova matriz de 8 por 8 com os pesos pra cada uma das imagens de frequência de base. Cada uma das imagens dessa tabela contribui um pouco pra reconstruir a imagem original. Se não ficou claro, veja esta imagem de uma pequena letra “A”. Pegando cada uma daquelas imagens bizarras de cosseno que eu falei e aplicando os pesos ou coeficientes e somando um em cima do outro, no final eu reconstruo o “A” original, sacaram?&lt;/p&gt;

&lt;p&gt;Se você é de CSS, de novo, é como se esses coeficientes fossem o tanto de RGB, o tanto de vermelho, o tanto de verde e o tanto de azul que precisa misturar pra fazer, por exemplo, marrom escuro. É o peso de cada cor. A diferença é que em vez de misturar, ou seja, somar componentes de cor, estamos somando tipo funções de cosseno, usando esses pesos pra dizer quanto que cada função contribui pra reconstruir a imagem original.&lt;/p&gt;

&lt;p&gt;A matriz de coeficiente resultante é interessante pela forma como essa base de funções de frequência é construído, blocos mais lá pro canto superior esquerdo tentem a contribuir mais pra reconstruir nosso bloco original de 8 por 8, por isso usamos coeficientes maiores. Mas os blocos mais lá pro canto inferior direito, contribuem pouco, são funções de alta frequência. E faz sentido, essa tabela base foi construída de tal forma a separar componentes de baixa frequência e de alta frequência.&lt;/p&gt;

&lt;p&gt;Mesmo se você não for de computação já deve ter ouvido que animais, como cachorros, conseguem ouvir sons de alta frequência que nosso ouvidos humanos não conseguem ouvir. Mesma coisa pra luz, a luz visível é só uma faixa pequena de todas as frequências de cores. E sim, lembra que luz é uma onda, assim como som é uma onda. Ondas de baixa frequência começam lá atrás na faixa dos raios gama, depois raios x, depois ultravioleta, aí chegamos na minúscula faixa que é a luz que nós conseguimos enxergar, mas daí vai pro infravermelho até ondas de rádio.&lt;/p&gt;

&lt;p&gt;E mesmo na faixa visível, nas bordas com o ultravioleta e infravermelho é difícil de distinguir um limite. Essas faixas não são discretas, tipo “a partir exatamente deste ponto que a gente não consegue ver”. Vai variar. E essa tabela base é feita pra quando calcularmos a tabela de coeficientes, os números que se aproximam do canto inferior direito sejam cores de alta frequência que a gente não consegue distinguir direito, mais uma vez, depois de anos de pesquisa, sabemos o que não conseguimos enxergar. Assim como na fase de color downsampling já reduzimos o chroma, agora vamos reduzir mais ainda nos 3 canais.&lt;/p&gt;

&lt;p&gt;E isso foi pra um bloco de 8 por 8 pixels da imagem. Agora precisa repetir a mesma coisa pra todos os outros blocos que formam a imagem, em todos os 3 canais de luma e chroma. Fazendo tudo isso, até agora não comprimimos nada, só processamos os números que representam cada pixel de cada canal em outro número processando com a transformada discreta de cosseno. Mas o pulo do gato vem agora. Lembra quando você vai salvar uma imagem em JPEG e o Photoshop ou outro programa pede pra escolher se quer salvar em alta qualidade ou baixa qualidade, e tem um slider pra escolher algum valor no meio? E normalmente você escolhe lá pelos 80% porque você acha que isso vai salvar 80% dos detalhes do original?&lt;/p&gt;

&lt;p&gt;Pois é, essa é a fase chamada de quantização. Na prática é uma divisão de matriz. Um exemplo é essa tabela aqui. Notem que os números mais perto do canto superior esquerdo são menores e os próximos do canto inferior direito são maiores. Lembra as tabelas resultantes de coeficientes pra cada bloco de 8 por 8 pixels? Pois bem, vamos dividir cada número por cada número correpondente nessa tabela de quantização. Cada nível de qualidade, tipo os 80% que você escolheu pra salvar no Photoshop é uma tabela diferente. Quanto menor a qualidade, maiores os números dessas tabelas de divisão.&lt;/p&gt;

&lt;p&gt;Dividindo um número por outro grandão o resultado vai ser um número bem pequeno. Se der quebrado, arredonda pro inteiro mais próximo. Isso significa que os coeficientes mais próximos do canto interior direito vão ficar pequenos, ou melhor ainda, zerados. É aqui que começa a fase real de compressão. Olha só o resultado da divisão, uma boa parte dos números fica zerado e não tem problema porque elas contribuem pouco na reconstrução da imagem, no sentido que os detalhes que acrescentam, nossos olhos não distinguem tão bem. A tal da região da alta frequência.&lt;/p&gt;

&lt;p&gt;Vamos fazer essa divisão pra todos os blocos que compõe a imagem original. E como último truque, vamos rearranjar esses pixels de lugar. Lembra que a imagem toda é só um linguição de bits? Onde cada grupo de uns 16 bits representa um pixel em YUV? A gente só tá mostrando elas quebradas em linha, formando um retângulo, pra ficar fácil de ver mas na realidade é uma linha atrás da outra no binário. E cada bloco de 8 por 8 pixels representa um pedaço de cada linha. Em vez de escrever uma linha de cada vez, vamos reescrever esse bloco em zigue zague, começando do canto superior esquerdo e descendo até o canto inferior direito. Isso pros zeros ficarem um atrás do outro.&lt;/p&gt;

&lt;p&gt;Na transformação de cosseno a gente fez de propósito os coeficientes que virariam zero na divisão ficarem mais acumulados num canto. A ordem que se escreve os bits não é importante contanto que na hora de ler de volta seja na mesma ordem, claro. E de novo, até este ponto ainda não comprimimos nada, só fizemos preparativos. Lembra que partimos de um bloco de 8 por 8 pixels e chegamos no mesmo bloco de 8 por 8 pixels. Nada mudou ainda.&lt;/p&gt;

&lt;p&gt;Depois de fazer o zigue zague olha o que aconteceu, temos um array onde os números mais pro fim tem sequências de zeros. Agora sim, vamos pra compressão. O primeiro algoritmo é mais intuitivo. Vamos pegar um outro exemplo com uma string aleatória com sequências repetidas, tipo um &quot;AABCDDDDEFFFFFGGGH&quot; que tá aqui do lado com 18 caracteres. Concorda que poderíamos reescrever por exemplo como &quot;A2B1D4E1F5G3H1&quot;. Ou seja, em vez de repetir F, 5 vezes, escrevemos F5 pra indicar que é pra repetir 5 vezes. Descemos de 18 caracteres e comprimimos pra 14, um ganho de 22%, mas isso porque essa string é curtinha, quanto maior for e quanto mais sequência repetidas tiver, mais comprimida fica.&lt;/p&gt;

&lt;p&gt;Esse é um dos algoritmos mais simples e ingênuos de compressão lossless, ou seja, que não perde nenhuma informação, diferente de chroma subsampling que fizemos lá no começo. Da string comprimida podemos voltar pra original sem muito trabalho. Voltando pro nosso bloco de coeficientes divididos depois da quantização, aquelas sequências de zeros repetidos podemos fazer a mesma coisa e isso já vai nos dar um pequeno ganho.&lt;/p&gt;

&lt;p&gt;Esse algoritmo se chama Run Length Encoding ou codificação de corrida de comprimento. O pior caso desse algoritmo obviamente é se a string original tiver alta entropia e quase nenhuma sequência repetida. Mas o próximo algoritmo é mais interessante e se aprende na ciência da computação. E começa criando uma tabela de frequência. No mesmo exemplo da string, vamos contando letra a letra e ordenando pela frequência vamos ter B 1, C 1, E 1, H 1, A 2, G 3, D 4, F 5&lt;/p&gt;

&lt;p&gt;Agora vamos montar uma árvore binária de frequências. Começa pegando do começo dessa lista, O B e o C que só aparecem 1 vez cada e coloca embaixo de um nó que é a soma das frequências, no caso 2. Agora pegar o E e H que também só aparecem 1 vez e colocar embaixo de outro nó 2. Agora tem vários com frequência 2, meio que tanto faz, mas na ordem pegamos o nó 2 do E e H que acabamos de criar, o A que aparece 2 vezes no texto e colocamos embaixo de um nó 4 que é a soma das frequências.&lt;/p&gt;

&lt;p&gt;Continuando, pegamos o nó 2 do B e C e a próxima frequência 3 da letra G e colocamos embaixo de um nó que a soma dá 5. Temos 2 nós 4, do A, E e H e do D, colocamos embaixo de um nó com soma 8. Depois pegamos a sub-árvore do B, C e G e o F que tem frequência 5 também e colocamos embaixo de outro nó com soma 10. E finalmente pegamos os nós resultantes 8 e 10 e colocamos embaixo da raíz 18, e formamos uma árvore binária de frequências. Agora pra que tivemos esse trabalhão todo?&lt;/p&gt;

&lt;p&gt;Como que o computador enxerga aquela nossa string original &quot;AABCDDDD&quot; etc? Digamos que está representada como caracteres UTF-8. Lembra Unicode? Computadores não entendem letras, só números. Nós, programadores, que precisamos enxergar uma sequência de números e pedir pra ele desenhar ou processar alguma coisa. Sendo simplista, por convenção usamos uma tabelona gigante que é a UTF-16, que associa um número com até 2 elevado a 16 possíveis glifos, mais de 65 mil possíveis glifos.&lt;/p&gt;

&lt;p&gt;Um glifo é basicamente o desenho de um caracter. UTF é uma tabela arbitrária que usamos por convenção e todo programa e sistema operacional traduz da mesma forma. O glifo da letra A maiúsculo em inglês é o ponto de código 0041 em hexadecimal. O glifo da letra B maiúscula é 0042 e assim por diante. Então, em hexadecimal o string AABC etc fica 4141 4243 e assim por diante. Mas lá embaixo, no nível do computador mesmo, ele enxerga em binário, zeros e uns, então fica um linguição tipo  0100 0001 0100 0001 0100 0010 e assim por diante.&lt;/p&gt;

&lt;p&gt;No mínimo, o código que representa cada letra, vai ocupar 8 bits se o encoding for em UTF-8. Cada letra ocupa 1 byte portanto a string inteira ocupa 18 bytes. Mas e se eu esquecer UTF-8 e usar outra tabela com códigos que precisam de menos bits? É pra isso que criamos essa árvore binária de frequência. Ela que vai servir pra codificar o A, B, C etc com outro código diferente da UTF-8. Vamos ver como.&lt;/p&gt;

&lt;p&gt;Começamos da raíz da árvore que é o nó 18 lá em cima e vamos descendo. Cada galho vai representar zero ou um. Se eu for pra esquerda vai ser zero, pra direita vai ser um. Então vamos lá, qual vai ser o código binário pro A? Em UTF-8 é 0100 0001, 8 bits. Vamos lá começando do nó 18, um galho pra esquerda, então 0. Mais um pra esquerda, 0 de novo. Esquerda, 0. Esquerda 0. Então o código pro A vai ser 0000. Logo de cara já economizamos metade do espaço.&lt;/p&gt;

&lt;p&gt;Daí tem A de novo, então 0000. Agora o B, da raíz vai pra direita, então 1. Daí tem mais 3 galhos pra baixo pra esquerda, então 3 zeros e o código pro B vai ser 1000. O C mesma coisa, começa pra direita então 1, desce dois galhos pra esquerda, então 0 e 0, e vira pra direita no final então 1. Portanto o código do C vai ser 1001. Mas começa a ficar divertido agora, porque temos uma sequência de 4 Ds. O código do D é começando pra esquerda 0 e indo pra direita que é 1, portanto o código do D é 01. Repetido 4 vezes fica 01010101. 8 bits, pra 4 letras. Agora começaram a entender?&lt;/p&gt;

&lt;p&gt;Antes esse D seria o hexadecimal UTF-8 0044 que em binário é 0100 0100, 8 bits. Como são 4 Ds tó esse trecho ocuparia 4 bytes. Mas nesse sistema de encoding da árvore de frequência codificamos o D somente com 2 bits. Todas os 4 Ds vão ocupar um único byte, então esse trecho ficou 4 vezes menor! E pra não ficar tedioso vou parar por aqui mas é pra fazer a mesma coisa letra a letra, usando a árvore como o sistema de codificação.&lt;/p&gt;

&lt;p&gt;No final vai ficar esse binariozão. Olha como o F que repete 5 vezes também ficou curtíssimo com 2 bits cada letra. E não é por acaso que o D e o F, que são as letras que mais repetem, sejam representados só com 2 bits enquanto letras que não repetem tanto como o A ou C precisam de 4 bits. Porque a árvore foi ordenada e montada de tal forma que as letras mais comuns fiquem mais próximos da raíz da árvore, precisando de menos galhos pra chegar neles. O string original tinha 18 letras, portanto 18 bytes. Agora o binário comprimido ficou só com 60 bits que é 12 bytes, economizamos 6 bytes nessa brincadeira que é 2/3 do tamanho original.&lt;/p&gt;

&lt;p&gt;0000 0000 1000 1001 01 01 01 01 0010 11 11 11 11 11 101 0011&lt;/p&gt;

&lt;p&gt;Pra strings pequenas assim esse sistema não compensa porque pra conseguir descomprimir precisa também salvar a estrutura da árvore junto, e vai dar mais que os 18 bytes originais. Mas em textos mais longos isso pode diminuir tudo em até umas 5 vezes. Pensa um arquivo de código fonte, ou um JSON, que só de identação tem um monte de espaço em branco. Espaço em branco, tabulação, tudo é um caracter e tudo ocupa espaço, mesmo o que você não enxerga.&lt;/p&gt;

&lt;p&gt;Com uma combinação de Run Length e esse sistema de árvore de frequência que se chama codificação de Huffman, dá pra comprimir bastante. Depende do texto, claro, alguns dá pra comprimir mais, outros menos. Huffman é um bom exemplo de porque todo programador precisa saber o que é uma árvore. E por isso eu fiz um vídeo só falando de árvores algum tempo atrás. Todo mundo pensa em tabelas como uma matriz, um array de array, mas uma tabela ordenada, de chaves e valores, costuma ser implementada como uma árvore.&lt;/p&gt;

&lt;p&gt;Se estão curiosos em como descomprime, é fácil. Vamos lá, 0000 significa começar da raíz e descer pra esquerda 4 vezes, aí esbarra no A, então escreve A e volta pra raíz. Vem 0000 de novo, vai chegar no A de novo, concatena com o A anterior, volta pra raíz. Agora é 1000, direita e esquerda 3 vezes, esbarra no B, concatena. Volta pra raíz, 1001, direita, esquerda, esquerda, direita, esbarra no C, concatena. Volta pra raíz, 01, esquerda e direita e já esbarra no D, concatena. Volta pra raíz 01 01 01, esbarra no D 3 vezes. E vai fazendo assim até ter o texto original de volta. Viram como é super simples e fácil?&lt;/p&gt;

&lt;p&gt;Se lembram porque estou falando de Run Length e Huffman? Porque voltando na explicação de imagem, fizemos a compressão de cores com chroma subsampling pra converter de RGB pra YUV 4:2:0, daí dividimos o linguição de bytes de cada canal em blocos de 8 por 8 bits e aplicamos o processamento de transformada discreta de cosseno ou DCT, depois a quantização pra simplificar os coeficientes.&lt;/p&gt;

&lt;p&gt;Daí reescrevemos bloco a bloco em zigue zague pra forçar zeros repetidos mais próximos no fim de cada bloco. E agora? Uma sequência de zeros repetidos, é só aplicar Run Length pra diminuir o espaço que ocupam. Agora podemos aplicar Huffman. Criar a árvore de frequência dos números que mais se repetem perto da raíz e codificar como no exemplo da string.&lt;/p&gt;

&lt;p&gt;Recapitulando, uma imagem de video de 4K seria 3240 vezes 2160 pixels, com cada pixel ocupando 3 bytes em RGB. Isso daria uma imagem crua, descomprimida, de uns 20 megabytes. Aaplicando chroma subsampling, transformada discreta de cosseno, quantização de qualidade 8, run-length e Huffman, podemos chegar à mesma imagem ocupando nada menos que 900 kilobytes, menos de 1 mega. Uma compressão de mais de 20 pra 1, 20 vezes menor que o original e que pra 99% das pessoas vai ser difícil de distinguir com o original. Nada mau né?&lt;/p&gt;

&lt;p&gt;Esse processo todo está por trás do formato de imagem mais difundido do mundo que todo mundo, até quem não é programador usa todo dia, vocês acabaram de aprender como funciona o famoso JPEG, que é um sistema de compressão lossy, que perde informações do original, mas que em quantização lá perto de 8 pelo menos, a gente não vai perceber no dia a dia a menos que precise abrir no Photoshop pra editar. E agora você talvez comece a entender as razões das limitações do formato.&lt;/p&gt;

&lt;p&gt;Como um JPEG vai cortando informação ao longo de todo esse processo, é impossível recuperar os detalhes do original. É um processo “lossy”. E cada vez que edita um JPEG e salva num novo, vai continuar perdendo mais informação. Por isso que não se deve editar um JPEG diversas vezes. Toda vez vai perdendo mais e mais definição. Por todo esse processo que imagens que tem traços muito bem definidos, como desenhos, quadrinhos, anime e coisas assim ficam meio soft e perdem a nitidez nas bordas. Esse sistema é melhor pra fotos de natureza, pessoas, que tem coisas como céu com muitos tons de azul. Mesmo desconstruindo o tanto que fizemos, é difícil alguém não treinado notar que mudou alguma coisa.&lt;/p&gt;

&lt;p&gt;E por isso JPEGs de baixa qualidade, que o original já não era grande coisa e você ainda vai e manda salvar em qualidade baixa, tipo quantização 1, fica artefatos visíveis em formato de quadradinhos, como num grid, porque a gente divide a imagem em bloquinhos quadrados e processa um a um e depois concatena o resultado. Esse sistema dá resultados impressionantes de mais de 20 ou 30 vezes de compressão, por outro lado, quanto mais nítido, ou no outro espectro, quando mais borrado o original for, pior vai ser o resultado, especialmente em quantizações mais agressivas. Em resumo, JPEG é muito bom pra comprimir uma foto e guardar ou transmir. Mas é uma bosta pra editar. Por isso que fotógrafo profissional não tira fotos em JPEG e sim em RAW, que fica muito maior que os 20 mega que eu falei, porque guarda bem mais informação de cores e tudo mais pra possibilitar manipular e ajustar num Photoshop ou Lightroom depois.&lt;/p&gt;

&lt;p&gt;O formato BMP que o Paint ou Painbrush do Windows originalmente salvava sempre gerava arquivos grandões, porque não comprime nada. Mais parecido com o formato RAW de câmeras, era o antigo formato TIFF. Ambos são parecidos com BMP ou bitmap no sentido que não joga informação fora, mas eles aplicam um mínimo de compressão lossless, ou seja, que não perde nenhuma informação. Já vou explicar compressão lossless.&lt;/p&gt;

&lt;p&gt;Mas tem um formato que surgiu da CompuServe, um dos primeiros provedores de internet dos anos 90, mesma época da America Onine, ou AOL. Naquela época estávamos engatinhando, era final dos anos 80. E começou a ficar óbvio que queríamos trafegar imagens, seja em BBS ou nesse troço novo chamado de Web. Mas não dava pra ser em Bitmap descomprimido. Mesmo TIFF era grandão. Daí eles inventaram o famigerado GIF ou JIF, seja lá como prefere chamar. A essa altura já nem me importo mais qual é o certo. É aquele formato de imagem de memes animados que todo mundo manda no zap.&lt;/p&gt;

&lt;p&gt;O GIF é um patinho feio que continuamos usando até hoje por causa de funcionalidades que poucos tem. Em particular animações bem leves. Pra começar ele limita a quantidade de cores na imagem. Em vez de cada pixel ser representado por 24 bits de cor, limitaram a no máximo 8 bits, o que diminui o espaço de cores de mais de 16 milhões pra meras 256 cores. Sim, só 256. Cada componente do RGB tem 8 bits, mas no GIF tem que dar um jeito e enfiar tudo em só 8 bits no total. E obviamente não dá, então tem que jogar fora quase todas as cores e se virar com quase nada. Eu lembro que parte do trabalho em agência naquela época era otimizar as cores dos GIFs.&lt;/p&gt;

&lt;p&gt;O processo não é muito diferente de fazer pixel art. Dá pra automatizar o processo de jogar cores fora mas às vezes o algoritmo escolhe um tom que não fica muito bom, daí temos que ajustar manualmente. Em cima disso o GIF ainda tinha duas funcionalidades que são os diferenciais até hoje. A primeira era poder escolher uma cor dessa paleta pra ser transparente. Como não era uma canal alpha de verdade, a transparência era bem abrupta e feia, só em cima de uma única cor. Mas pra memes tá mais que bom.&lt;/p&gt;

&lt;p&gt;E a segunda funcionalidade era permitir ter mais de uma imagem dentro do mesmo arquivo, com a possibilidade de tocar animado, que é a coisa mais importante em todo meme de hoje em dia. O problema disso era a paleta de cores. Lembra que pode ter no máximo 256 cores? Pois é, e as cores valem pra todas as imagens da animação, por isso é inteligente não fazer animações que tenham quadros muito diferentes um do outro.&lt;/p&gt;

&lt;p&gt;Depois de comprimir jogando milhões de cores fora, o segundo passo era comprimir reduzindo sequências de bytes repetidos, meio parecido com a idéia do Run-Length que falei antes, mas bem mais agressivo. Pra isso GIF usa um algoritmo de compressão chamado LZW ou Lempel-Ziv-Weich. Tecnicamente o algoritmo original foi criado por Abraham Lempel e Jacob Ziv coincidentemente no ano que eu nasci, em 1977. Por isso que o algoritmo original se chama LZ77 que você vai esbarrar bastante se começar a pesquisar sobre compressão. Também vai esbarrar no LZ78 que é uma otimização da fase de descompressão. O LZW é uma variação desse algoritmo criado por Terry Weich em 1983 mas já volto a falar dele. Vamos focar só no LZ77.&lt;/p&gt;

&lt;p&gt;Vale eu explicar a idéia geral com um exemplo. Vamos pegar outro string, como esse &quot;um tigre, dois tigres, três tigres&quot;. Sem descer em detalhes temos repetido 3 vezes o trecho &quot;tigre&quot;. E se a gente substituir o 2o e 3o tigre com um ponteiro dizendo quantas letras pra trás precisa ir pra achar o 1o tigre e quantas letras queremos pegar? Então ficaria algo como &quot;um tigre, dois &amp;lt;12,5&gt;, três &amp;lt;25,5&gt;&quot;. Esse 12,5 quer dizer, ande 12 letras pra trás e leia 5 letras. E 25,5 mesma coisa, ande 25 letras pra trás e leia 5 letras, ambos vão cair na primeira palavra “tigre” e com isso dá pra remontar a frase. Se eu codificar esses ponteiros com 4 bits cada valor, em 1 byte dá pra guardar.&lt;/p&gt;

&lt;p&gt;O string original tem 33 letras, mas substituindo onde repete por esses ponteiros daria pra reduzir de cara pra 23 bytes, mais 2 bytes pra cada ponteiro, ou seja, 25 bytes. Mesmo nesse esquema simplificado já comprimimos em quase 25%, 1 quarto de economia. Imagine que num texto muito maior vai ter muito mais repetições e por isso dá pra comprimir bem. Se só o esquema de Huffman comprime umas 5 vezes, eu diria que o mesmo texto nesse esquema de ponteiros daria pra comprimir na faixa de 10 vezes. No geral acho que a eficiência é pelo menos 2 vezes maior que Huffman. Por isso vale a pena entender mais.&lt;/p&gt;

&lt;p&gt;Imagina num texto longo fazer esse esquema de ponteiros. Pensa um livro por exemplo, centenas de páginas de texto. Se estivermos lá na última página e precisar fazer um ponteiro pra uma palavra na primeira página. Pra poder apontar tão longe significa que precisaríamos manter o livro inteiro em memória, megabytes de dados ou mais. Seria um puta desperdício de memória. E quando o algoritmo foi criado, no fim dos anos 70, sabemos que memória era hiper cara. Pensa que os melhores microcomputadores do começo dos anos 80 male male tinham 32 kilobytes de RAM. Então eu só posso manter pedaços do texto em memória enquanto vou descomprimindo, e por isso esses ponteiros não podem apontar pra tão longe no começo do texto.&lt;/p&gt;

&lt;p&gt;Naquela época se reservava tipo 2 kilobytes, talvez 4. Esse bloco de buffer é o que se chama de sliding window ou janela deslizante, porque é como só desse pra ler o trecho do texto onde está essa janela. E os ponteiros só podiam referenciar dentro dessa janela, daí dá pra manter ponteiros usando poucos bits também. Quanto maior o sliding window, maiores as possibilidades de comprimir mais, mas mais RAM vai precisar. Então é um trade off. Hoje em dia RAM é barata, então dá pra ter sliding windows na faixa de megabytes. Porém quanto maior for o sliding window, maior tem que ser os ponteiros, daí ocupa mais espaço no total e comprime um pouco menos. É um balanço.&lt;/p&gt;

&lt;p&gt;O primeiro grande sucesso que me lembro desse algoritmo surgiu na era dos BBS nos anos 80 pelo Phil Katz que inventou o formato ZIP, que usa esse algoritmo LZ77. Foi o famoso PKZIP que tinha os utilitário pkzip pra comprimir e pkunzip pra descomprimir. Quem daquela época não usou? Com a adoção pela Microsoft já naquela época o formato zip se popularizou, e embora outras coisas melhores tenham surgido, meio que virou o padrão até hoje. Com o tempo ganhou funcionalidades como encriptação, os famosos zips com senha que todo mundo que manja tem medo quando baixa, porque pode ser a pornografia que queria ou um malware.&lt;/p&gt;

&lt;p&gt;Não confundir porque existem diversos formatos de compressão como zip, arj, rar, 7zip e outros que usam variações do algoritmo Lempel-Ziv. Uma coisa é o formato de arquivo, a estrutura de dados e metadados, outra coisa são os algoritmos que usam por dentro. E cada um usa estratégias diferentes pra tentar ultrapassar os concorrentes. Nos anos 80 mesmo surgiram diversas melhorias em cima do LZ77 como o LZSS ou Lempel-Ziv-Storer-Szymanski de 1982 criado por James Storer e Thomas Szymanski usado no bom e velho RAR. Lembram? WinRAR? O shareware que ninguém nunca pagou. Quem nunca baixou pirataria quebrada em vários arquivos de RAR?&lt;/p&gt;

&lt;p&gt;No mundo Windows mais recente se popularizou o formato e ferramenta 7-zip, que é versátil e consegue lidar com outros formatos como o RAR ou ZIP originais, mas tem um formato próprio que usa outras variações do LZ77 como o Lempel-Ziv-Markov chain ou LZMA e LZMA2 criado pelo russo Igor Pavlov no fim dos anos 90. O Rar foi criado por outro russo, Eugene Roschal no começo dos anos 90. Naquela época pelo menos a gente falava que não podia subestimar um russo quando a questão era compressão de dados, porque entre RAR e 7zip eles contribuíram com alguns dos melhores formatos de compressão de arquivos.&lt;/p&gt;

&lt;p&gt;Muitos desses formatos usam uma mistura de Huffman pra substituir letras ou palavras mais frequentemente usados por símbolos guardados num dicionário, aquela árvore binária de frequências e uma variação de Lempel-Ziv como LZSS ou LZMA2 pra substituir repetições por ponteiros dentro de um sliding window. E como cada um ajusta essas diversas combinações de dicionários, sliding windows, tamanho de ponteiros e tudo mais é que varia tanto quanto conseguem comprimir, quanto de CPU e RAM vão gastar no processo. Mas em geral, quanto menor consegue fazer o arquivo ficar, mais processamento e memória vai gastar.&lt;/p&gt;

&lt;p&gt;Desenvolvedores web devem estão mais acostumados com o formato gzip, que é do começo dos anos 90 e é o mais usado em servidores web. Caso não saibam, toda página web que vocês navegam provavelmente vem compactada em gzip. Se não vem, deveria. O gzip usa o algoritmo deflate, que foi criado pelo Phil Katz pro PKZIP original. A patente dele já expirou e por isso podemos usar hoje de graça. Deflate e gzip são muito comuns no mundo Linux e é uma combinação do LZ77 e Huffman como expliquei. Aliás, toda linguagem de programação ou já tem ou tem bibliotecas pra todos esses algoritmos, deflate, huffman e tudo mais. Pesquisem.&lt;/p&gt;

&lt;p&gt;Huffman, Lempel e Ziv são as fundações pra tudo que é compressão lossless, ou seja, que não jogamos fora nada dos dados originais e quando descomprime volta exatamente como era antes, diferente do esquema de JPEG em imagens. E falando em JPEG, o objetivo dessa longa tangente de história foi só pra poder voltar a falar do seu GIF de memes. Como falei antes, criado pela Compuserve em cima de outra variação do LZ77, o LZW que é Lempel-Ziv-Weich, do Terry Weich. Estamos falando de uma era antes da idéia de software livre. E naquela época patentear software era mais importante ainda, ninguém sabia como as coisas iam evoluir.&lt;/p&gt;

&lt;p&gt;O erro foi que a Compuserve criou o GIF usando o LZW do Weich e deu certo, o GIF começou a se popularizar, especialmente quando a Web comercial começou a crescer e surgiram navegadores como o antigo Netscape, que adotou o GIF com um dos formatos de imagem suportado. Antes disso, no meio dos anos 80 o Weich vendeu as patentes pra Sperry Corporation, que fez merge com a antiga Burroughs, famosa por mainframes e pelo UNIVAC e que depois se tornou a Unisys, que existe até hoje. Pois bem, a Unisys ficou sabendo desse tal de GIF usando as patentes deles sem pagar nada e em 1987 começou a negociar com a Compuserve.&lt;/p&gt;

&lt;p&gt;Ninguém tava muito ciente disso e todo mundo continuou usando. Aí em 1994 eles anunciam que esperam que todo mundo que usasse e distribuisse GIFs precisaria pagar licenças. Imagina se tivesse Twitter em 1994, todo mundo ia ficar putaço e ia cair matando cancelando a Unisys. Como assim querem taxar meus memes?? Aí em 1995 o Thomas Boutell anunciou na usenet, que era tipo nosso Reddit da época, uma proposta pra resolver isso. Que seria todo mundo largar saporra de GIF e trabalhar num formato melhor e mais moderno. E daí nasceria o PNG ou Ping.&lt;/p&gt;

&lt;p&gt;Deu tão certo que já em 1996 a W3C, que é quem define os padrões da Web, adotou e oficializou o PNG. Mas então significa que todo mundo distribuindo GIFs de memes hoje tá infringindo patente da Unisys? Felizmente não, porque a patente já expirou lá por 2004 e a gente voltou a usar GIFs porque apesar do PNG ser infinitamente superior, ele não suportava animações. A única função que GIF tem é pra memes animados. Até existe um formato de PNG animado que é o APNG mas demorou muito pros navegadores adotarem. Ele já foi criado tarde, pela Mozilla, em 2008. Mas Chromium só adotou em 2017 e por tabela navegadores baseados em Chromium como Microsoft Edge só vieram a adotar depois. Você sabia que tinha APNG? Pois é, nem eu.&lt;/p&gt;

&lt;p&gt;PNG é bem versátil, suporta imagens de poucas cores como GIF, só com 8-bits de cor, mas dá pra subir pra imagens pra 24 bits ou 32 bits. Também suporta um canal de alpha pra transparência de qualidade infinitamente superior ao GIF e por isso é muito adequado pra desenvolvimento Web. E ele usa um sistema de otimização da paleta de cores pra facilitar ter sequências de repetições, é tipo filtros que não degradam a qualidade da imagem mas facilita depois usar o algoritmo Deflate. Lembra? Baseado em Huffman e LZ77, que o gzip usa? PNG é comprimido com Deflate, portanto, diferente de JPEG, ele é lossless.&lt;/p&gt;

&lt;p&gt;Depois do PNG surgiram outros formatos como o WebP do Google. Mas nem vou falar do WebP, WebM, codecs como VP8 ou VP9 porque apesar de bons não se tornaram populares. Aliás, toda vez que esbarro numa porcaria de WebP quero xingar porque a adoção é tão baixa que pro Photoshop abrir eu preciso instalar um plugin por fora. Um JPEG faz compressão na casa de 20 pra 1. Um PNG faz compressão na casa de 4 pra um. São ordens de grandeza menos. Mas um WebP é só uns 25% melhor em compressão que um PNG. Eu sei que tem outras vantagens, mas a diferença é muito pequena pra trocar tudo de PNG pra WebP e por isso continuo usando PNG pra tudo. Se eu quero uma foto altamente comprimida, vou usar JPEG. Se quero uma foto razoavelmente comprimida mas mantendo os detalhes do original, vou usar PNG e ponto.&lt;/p&gt;

&lt;p&gt;Vamos voltar pro assunto principal. Lembram o objetivo? Eu quero assistir um filme de 2h em 4K na minha TV nova. Mas o arquivo não comprimido de video, a gente fez as contas, e ia precisar de uns 5 teras. O máximo que conseguimos fazer foi converter de RGB pra YUV 4:2:0, que joga fora informações do canal de chroma que a gente não percebe, que é o chroma subsampling. Mas mesmo assim o tamanho do arquivo cai só pra 3 teras, que continua sendo coisa pra caramba.&lt;/p&gt;

&lt;p&gt;O que é um video. É basicamente uma sequência de imagens. Por que não comprimir cada imagem em PNG então? Já cairia o tamanho de tudo 4 vezes. Não muda muita coisa porque isso só diminui o video de 5 pra 1 terabyte. 1 tera é o tamanho total dos HDs de metade de vocês aí assistindo. Eu chuto que a maioria deve estar com HDs de no máximo uns 512 giga. Ou seja, tá chegando perto, mas ainda não dá. Então a próxima escolha óbvia é comprimir cada quadro do frame em JPEG, e aí vai ficar 20 vezes menor né? Sim, isso diminui os 5 teras originais pra 256 gigabytes!&lt;/p&gt;

&lt;p&gt;Caraca olha só! Significa que se eu comprimir em JPEG já cabe 2 filmes no HD de vocês. Sucesso. Fim do episódio, deixem seu like e assinem o canal! Tô zoando. 256 gigas é um puta avanço, mas ainda não dá. Com conexão de 50 megabits que é o que a maioria de vocês aí deve ter, ia levar nada menos que 12 horas pra baixar. Não rola fazer streaming disso porque assistir um filme de 2 horas precisando de 12 horas pra baixar, só se você assistisse beeeeem em slow motion.&lt;/p&gt;

&lt;p&gt;Falando em velocidade de internet, esse é o requisito pra streaming. 50 megabits é por volta de 6 megabytes por segundo, no máximo. Pra eu conseguir assistir um filme de 2h nessa velocidade, considerando uma conexão perfeita, que não engasga nem desconecta, ou seja, sem considerar buffer, significa que o tamanho máximo do arquivo do filme teria que ser menos 6 megabytes por segundo vezes 7200 segundos, ou seja, 42 gigas. Esse é o máximo, na realidade tem que ser menos que isso. Então precisamos dar um jeito de pegar o video compactado com JPEG que deu 256 gigas e comprimir 6 vezes ainda.&lt;/p&gt;

&lt;p&gt;Aliás, realmente existe formato de video que comprime os quadros em JPEG. O Motion JPEG ou M-JPEG. É um dos componentes que depois se torna o que você conhece como MPEG1 que se usava nos antigos Video-CD, ou MPEG2 que se usava nos antigos DVDs. Ou o MPEG4 também chamado de AVC, Advanced Video Coding, que você conhece como H.264 que é o codec que a maioria dos videos aqui do YouTube, Netflix e outros usam. Na verdade a gente tá migrando pro H.265 mas já falo dele. Aliás, nomenclatura de tecnologias de imagens, áudio, video, é uma bosta. Tem três nomes diferentes pra mesma coisa, com um monte de versões diferentes, diferentes empresas usando nomes diferentes. É uma confusão mesmo. Se esses nomes te confundem, você não tá sozinho. Mas vamos nos ater ao básico.&lt;/p&gt;

&lt;p&gt;Não necessariamente a compressão é o JPEG igualzinho das suas fotos mas sim um algoritmo de compressão baseado em DCT, transformada discreta de cosseno, então seria tipo variações das técnicas usadas pelo JPEG. Mas como já vimos, precisamos de mais que isso. Como estamos chegando no final, não vou entediar demais com todos os detalhes, mas o pulo do gato é simples. Presta atenção aqui em mim. Como são meus videos? É esse meu cenário aqui atrás que fica imóvel e não muda absolutamente nada do começo ao fim do video, e sou eu bobão aqui na frente, falando e se mexendo que nem um idiota. Concorda que precisa só de um quadro com a imagem inteira e depois pode recortar só eu aqui na frente pro resto do video?&lt;/p&gt;

&lt;p&gt;O que é um video? É um conjunto de imagens. A grosso modo uma câmera de video que nem essa aqui na minha frente tá tirando fotos de mim, 60 vezes por segundo. Num video de 1 hora tem 216 mil fotos. Toda a parte da imagem que é o cenário tá repetido em todas essas fotos, 216 mil vezes. Se eu arrancar fora esse cenário de todas as fotos, só de bater o olho você pode chutar que daria pra jogar fora pelo menos metade da imagem, concorda? Lembra como é mais fácil comprimir sequências repetidas de zeros? Olha quantos zeros tem aqui ao redor de mim agora.&lt;/p&gt;

&lt;p&gt;É exatamente isso que um codec como o H.264 vai fazer. Ele vai primeiro gravar um interframe ou keyframe que é uma foto completa, como essa. Daí os próximos frames vai gravando o que se chama de intraframes, que são deltas. Sabe commit de Git, que é só o trecho que você mudou no código e não o arquivo original inteiro? Mesmo conceito. Então vai ter um interframe e vários intraframes na sequência com deltas. O correto da imagem inteira é falar interframe, mas vou falar keyframe pra ficar mais fácil de distinguir de intraframe.&lt;/p&gt;

&lt;p&gt;Eu não lembro se a quantidade de intraframes era fixo ou se o codec detecta quando teve tanta mudança que é melhor pegar um keyframe inteiro de novo. Dependendo do codec ele pode escolher ser conservador e pelo menos o keyframe comprimir usando um algoritmo lossless como equivalente de PNG pra aumentar a qualidade e os intraframes usando um derivado de DCT, como um JPEG. Indo direto aos finalmentes, quanto que aqueles 5 terabytes consegue diminuir somando um esquema de JPEG com um esquema de inter e intrarames?&lt;/p&gt;

&lt;p&gt;Normalmente um filme por volta de 2h, em H.264 que é o codec mais usado atualmente, numa qualidade boa, próxima de Blu-Ray UHD que é a versão 4K, seria por volta de 20 a 25 gigabytes. Naquela mesma conexão de 6 mega por segundo daria pra baixar esse filme em aproximadamente 1 hora e 18 minutos. Ou seja, dá pra baixar todos os bits do video em menos tempo que a duração do video, que é o pré-requisito pra ser possível fazer streaming, que é poder começar a assistir enquanto o video vai baixando no fundo. Ou seja, atingimos nosso objetivo de conseguir assistir o filme via streaming na TV nova 4K!&lt;/p&gt;

&lt;p&gt;E é exatamente isso que acontece no YouTube ou Netflix. Os videos em 4K de 30 quadros por segundo são encodados em H.264 com um bitrate de 35 a 45 megabits por segundo, dependendo do video. Lembrando que o tamanho final de um video não é linear com a duração do video. Depende do tipo de conteúdo. Desenho animado, por exemplo, vai ser mais fácil de compactar do que um filme de ação com câmera que balança o tempo todo, famoso shaky cam, como nos filmes do Jason Bourne, porque fica bem mais difícil fazer delta intraframes. A qualidade da imagem também importa porque senão fica qualidade ruim quando faz color downsampling ou DCT. Por isso que vários filmes diferentes de 2h vão dar tamanhos de arquivos bem diferentes um do outro.&lt;/p&gt;

&lt;p&gt;Tem bem mais otimizações que codecs modernos fazem. Eles avaliam movimentos rápidos entre frames pra tentar normalizar, diferente de cenas que são só pessoas paradas conversando por exemplo. Tem técnicas como compensação de movimento, que existe desde o MPEG2 dos DVDs. O H.264 ou AVC que é o codec de video mais difundido pra consumo foi lançado oficialmente por volta de 2003 e levou desde 1998 até chegar no formato final. E mesmo depois saíram revisões do formato.&lt;/p&gt;

&lt;p&gt;E já em 2004 começou os estudos pro sucessor que hoje conhecemos como H.265 ou HEVC de High Efficiency Video Coding que foi lançado por volta de 2013 e só em anos recentes a adoção ficou significativa. Na prática, se você edita videos, exporte em HEVC que os arquivos tendem a ficar quase 50% menores e com qualidade pelo menos similar ao H.264, mas pra isso ele gasta bem mais processamento do que o H.264. Lembram o que eu expliquei antes que podemos ajustar o algoritmo pra criar arquivos menores ao custo de mais processamento e mais memória?&lt;/p&gt;

&lt;p&gt;Falando em quem edita videos, H.264 e H.265 são o que se chama de codecs de consumo. Assim como eu falei que JPEG é ótimo pra guardar suas fotos mas não pra editar, esses codecs de consumo é mesma coisa, ótimo pra poder baixar rápido e assistir, mas péssimos pra editar. Primeiro porque assim como JPEG eles fazem chroma downsampling, porque pra TV YUV 4:2:0 é mais que suficiente e você não vai notar que jogamos fora metade da informação de chroma. Mas pra editar faz diferença, porque agora não temos cores suficientes pra trabalhar.&lt;/p&gt;

&lt;p&gt;Fotógrafos profissionais devem tirar fotos no formato RAW que usa compressão lossless, por causa do algoritmo deflate. Daí edita no Lightroom da vida e no final exporta em JPEG pra postar no Instagram. Mesma coisa video, o formato correto pra gravar nunca deve ser H.264 ou H.265 e sim Apple ProRes ou DNxHR ou CineForm ou outros formatos proprietários como o REDCODE, se usa câmeras da RED. Todos esses formatos são similares a zip, 7zip, rar, compressão lossless, que não joga informação fora. Eles tem diferentes taxas de compressão caso queira sacrificar qualidade de video por espaço de armazenamento.&lt;/p&gt;

&lt;p&gt;Por exemplo, eu gravo meus videos aqui em DNxHR. E gravo no perfil que comprime mais e mesmo assim dá 1 gigabyte por minuto de video. Então se eu gravar 2 horas de video vai dar pelo menos 120 gigabytes. E se gravar com menos compressão pode chegar lá no 1 terabyte fácil. Quem edita videos profissionalmente precisa disso. Num H.264 final vai estar em YUV 4:2:0, mas no meu video gravado em DNxHR vai estar em YUV 4:2:2, ou seja, mais informação de cor. E pra ser agressivo era só gravar em ProRes 4:4:4 que é zero perda de informação de chroma. Agora você sabe o que significa ProRes 4444 versus ProRes 422.&lt;/p&gt;

&lt;p&gt;Além disso H.264 usa o esquema de inter e intraframes. Acho que todo mundo já viu como se edita videos? Num programa como Adobe Premiere ou DaVinci Resolve. São programas de uma categoria que se chama NLE ou Non Linear Editing, edição não linear, ou seja, que eu posso editar qualquer ponto do video que quiser. Só que toda vez que eu peço um frame pra editar, só vai ter o intraframe, que é o delta, a imagem incompleta. Então precisa voltar alguns frames pra trás e localizar o keyframe completo, mesclar os dois e aí eu tenho a imagem inteira. Imagina navegar num clip de video pra frente e pra trás e toda horas precisar processar frames assim.&lt;/p&gt;

&lt;p&gt;Com um codec profissional como o Apple ProRes ou DNxHR ele vai ter só keyframes. Então esses codecs são mais parecidos com o M-JPEG original, o Motion JPEG onde todo frame é um JPEG completo, sem deltas. Por isso, apesar do tamanho do arquivo de video ser ordens de grandeza maior, pra editar é muito mais performático, porque não precisa reconstruir os frames o tempo todo, eles já estão inteiros, é só ler. Um H.264 atrapalha pra editar e exige mais processamento e mais memória da máquina, por isso não compensa. Espaço em disco é barato, melhor plugar um SSD externo via Thunderbolt. Lembra dos meus videos explicando NAS e armazenamento?&lt;/p&gt;

&lt;p&gt;Além de tudo isso que expliquei, nem toquei no assunto de áudio direito, mas assim como imagem crua de alta qualidade de câmera se usa o formato RAW, que comprime de forma lossless com alguma variação de LZ77, assim como video tem formato lossless como Apple ProRES, áudio também tem formato crú comprimido lossless como o FLAC ou os antigos WAV e AIFF. Mas também assim como imagem tem formato lossy de consumo como JPEG, video tem formato de consumo H.265, áudio tem alta compressão lossy com MP3 ou AAC, Advanced Audio Coding, que é o que costuma ir com video H.265 ou Blu-Ray.&lt;/p&gt;

&lt;p&gt;No final os conceitos básicos são os mesmos: não tem como guardar a informação analógica original 100%, por isso pegamos samples, como pixels no caso de imagens. Quadros por segundo no caso de video. E sample por segundo no caso de áudio. Ajustamos resolução, frequência, e fazemos transformações pra simplificar os dados quando queremos alta compressão. Ou só tentamos reduzir repetições se não queremos perder informação.&lt;/p&gt;

&lt;p&gt;Tem várias outras coisas sobre compressão que nem mencionei, por exemplo, sistemas de arquivo que suportam compressão. No Windows com NTFS é possível ligar compressão e todos os arquivos no seu sistema vão ser comprimidos. Significa que toda vez que salvar vai levar um tempo a mais pra comprimir, e toda vez que tentar ler um arquivo vai levar outro tempinho pra descomprimir, e isso vai ser o tempo todo. Não sei quanto de overhead isso adiciona, até imagino que é rápido o suficiente pra não se notar na maior parte do tempo, mas considerando que espaço em disco hoje é super barato, não acho que compensa fazer isso. É muito mais inteligente comprar um HD externo USB barato e jogar o que não precisa pra lá em vez de deixar seu sistema inteiro lento o tempo todo por conta de poucos megabytes economizados. E ele provavelmente vai usar o equivalente de um zip na menor compressão pra não demorar demais, então não comprime nem de perto tanto quanto um zip de verdade.&lt;/p&gt;

&lt;p&gt;Enfim, o objetivo do video de hoje era usar a desculpa de video pra explicar o básico de tudo que existe sobre compressão de dados, incluindo as peças básicas mais fáceis que você pode treinar em qualquer linguagem de programação como Run-Length, Huffman, Lempel-Ziv, pra comprimir textos e depois ficar curioso pra aprender mais sobre séries de Fourier e transformada discreta de cossenos que, como vimos, você usa diariamente e nem sabia disso. Agora que sabe, se é de ciências da computação, não custa nada ir perguntar pro seu professor de algoritmos ou pesquisar por conta própria. Espero que isso tenha aumentado sua percepção do que acontece por trás dos panos das atividades mais simples que fazemos no dia a dia, como assistir este video no YouTube. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e compartilhem o video pra ajudar o canal. A gente se vê, até mais.&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5949</id>
    <published>2022-03-07T12:39:00-03:00</published>
    <updated>2022-03-07T11:44:18-03:00</updated>
    <link href="/2022/03/07/akitando-115-caminho-pro-melhor-teclado-do-mundo" rel="alternate" type="text/html">
    <title>[Akitando] #115 - Caminho pro Melhor Teclado do Mundo!</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/78aw1muYWQM&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;== DESCRIÇÃO&lt;/p&gt;

&lt;p&gt;Esta é a conclusão da saga de teclados! Quero explorar quais problemas herdamos desde a era das máquinas de escrever, porque temos tantos tamanhos diferentes e tantos layouts diferentes de teclados, e se é possível chegar no teclado perfeito: o melhor teclado do mundo pra digitar rápido e evitar as dores mais comuns de digitadores como tendinite e LER. Vamos explorar!&lt;/p&gt;

&lt;p&gt;== Conteúdo&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;00:00:00 - Intro: colchão, cadeira e dores&lt;/li&gt;
&lt;li&gt;00:03:42 - IBM Model M, Model F, layout 100%&lt;/li&gt;
&lt;li&gt;00:06:44 - Problema do Mouse, layout TKL&lt;/li&gt;
&lt;li&gt;00:10:36 - Notebooks, Keychron K2, layout 75%&lt;/li&gt;
&lt;li&gt;00:13:36 - Ducky, layout 65%&lt;/li&gt;
&lt;li&gt;00:14:12 - Anne Pro 2, layout 60%&lt;/li&gt;
&lt;li&gt;00:15:27 - layout 40%&lt;/li&gt;
&lt;li&gt;00:15:58 - repensando meus teclados&lt;/li&gt;
&lt;li&gt;00:17:01 - entendendo QWERTY e máquinas de escrever&lt;/li&gt;
&lt;li&gt;00:19:31 - entendendo DVORAK&lt;/li&gt;
&lt;li&gt;00:23:11 - entendendo COLEMAK&lt;/li&gt;
&lt;li&gt;00:25:03 - entendendo WORKMAN&lt;/li&gt;
&lt;li&gt;00:27:18 - o problema de todos os layouts&lt;/li&gt;
&lt;li&gt;00:29:32 - o defeito herdado das máquinas de escrever&lt;/li&gt;
&lt;li&gt;00:32:35 - repensando alinhamento: Planck EZ&lt;/li&gt;
&lt;li&gt;00:34:41 - configurando um 40%, Oryx, layers, QMK&lt;/li&gt;
&lt;li&gt;00:41:33 - como aprender a digitar rápido? keybr, monkeytype&lt;/li&gt;
&lt;li&gt;00:48:32 - a fraude do CharaChorder e estenografia&lt;/li&gt;
&lt;li&gt;00:52:38 - o melhor teclado? Moonlander MK1&lt;/li&gt;
&lt;li&gt;00:59:55 - o melhor teclado? layout 33%!&lt;/li&gt;
&lt;li&gt;01:01:56 - conclusão&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;== Links&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The CharaChorder video is a scam - Read Description (https://www.youtube.com/watch?v=ARCPpUmKYiQ)&lt;/li&gt;
&lt;li&gt;Taking Moonlander Ergonomics To The NEXT LEVEL With The Tripod Kit (Taking Moonlander Ergonomics To The NEXT LEVEL With The Tripod Kit)&lt;/li&gt;
&lt;li&gt;A Tiny, Ultra-Affordable Keyboard You Can Build Yourself! (https://youtu.be/JqpBKuEVinw)&lt;/li&gt;
&lt;li&gt;Brad Colbow - Huion Kamvas 22 Plus Review (https://www.youtube.com/watch?v=GJxGzJgfYGA)&lt;/li&gt;
&lt;li&gt;Workman Layout (https://workmanlayout.org/)&lt;/li&gt;
&lt;li&gt;Open Steno Project (http://www.openstenoproject.org/)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;== SCRIPT&lt;/p&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Os meus videos de teclados foram alguns dos mais visualizados do canal e eu realmente achei que já tinha falado tudo que precisava sobre o assunto. Mas me enganei redondamente, e fui obrigado a rever todos os meus conceitos sobre digitação. No episódio de hoje quero falar de uma categoria de teclados especiais que acho que vai ser difícil me fazer voltar pros meus antigos. E ao mesmo tempo quero falar de alguns conceitos que muitos de vocês provavelmente nunca pensaram a respeito.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Por que muitos de nós parecemos tão obcecados com teclados? Sempre vai variar de pessoa pra pessoa, por isso nunca leve essas coisas como regras invioláveis nem nada disso. Na minha filosofia pessoal, existem no mínimo 3 coisas que eu faço absoluta questão de ter o melhor que puder conseguir. Nosso dia tem 24 horas. Um terço dessas horas, pelo menos 8 horas todo dia, a gente passa dormindo. Por isso faz sentido investir num colchão decente, com travesseiro decente, pra não acordar todo dia moído, com torcicolo e dor nas costas. Economiza em tudo, menos no seu colchão.&lt;/p&gt;

&lt;p&gt;Em segundo lugar, pra todos nós que trabalhamos com programação, passamos a maior parte do tempo sentados. E não caia na armadilha das cadeiras gamers. Eu passei um tempão testando cadeiras gamer e vou dizer que a melhor gamer ainda não é melhor que uma cadeira de escritório honesta, feita pra ser realmente ergonômica. Essa que estou sentado agora é uma Hermann Miller Aeron, mas é super cara e eu acho que tem cadeira que pela metade do preço vai ser igualmente confortável. Cadeira, assim como colchão, é um troço que você precisa ir na loja, gastar algumas horas experimentando e sentir se sua lombar e tudo mais tão sustentados de verdade, se não te força numa posição estranha. Se puder, evite comprar online sem testar antes. E isso também é tudo que vou falar sobre cadeiras.&lt;/p&gt;

&lt;p&gt;E em terceiro lugar tá o periférico que você vai usar o mesmo tanto de horas que passa sentado trabalhando, que é seu teclado. Pense que um programador gasta muito mais horas usando teclado e mouse do que usando um smartphone, mas a maioria das pessoas gasta mais tempo pesquisando sobre smartphones do que teclados, cadeiras ou colchão. E se além de programador, nas suas horas vagas você é gamer e vai virar noite no fim de semana jogando, então é bem mais que 8 horas por dia. E depois não entende porque tem 20 anos e já tem dor nas costas e tendinite.&lt;/p&gt;

&lt;p&gt;Eu não sou especialista de ergonomia então falo só por mim, mas na prática, evitar dores no corpo tem mais a ver com ficar horas travado na mesma posição do que não estar numa &quot;postura correta&quot;. O segredo é de hora em hora pelo menos, mudar de posição, pra qualquer outra posição. Dar uma levantada de 2 minutos, dar uma esticada de 1 minuto, mas com alguma frequência. Se fizer isso, 90% dos problemas você já resolveu. Por exemplo, se ficou 1 hora sentado com a postura correta, muda de posição, pode sentar torto, sentado em cima da perna como muita gente faz. Mas depois de uma hora, muda de posição de novo. O importante é menos a posição e mais mexer seu corpo pra não deixar forçando sempre na mesma posição.&lt;/p&gt;

&lt;p&gt;Vamos dar uma pausa no assunto de ergonomia e voltar a alguns conceitos básicos. Nos videos anteriores eu detalhei bastante sobre switches, keycaps, ponto de actuação e vários detalhes técnicos que na realidade tem mais a ver com a sensação da digitação e muito menos com ergonomia propriamente dita ou velocidade. Nenhum teclado mecânico vai fazer diferença nas suas dores, caso sofra de tendinite ou lesão de esforço repetitivo que é o LER. Nenhum teclado mecânico vai fazer você ser muito mais rápido. Usar um bom switch Cherry ou Gateron ou Kailh tem mais a ver, por exemplo, com a qualidade do couro do volante, mais do que se o carro vai andar mais rápido.&lt;/p&gt;

&lt;p&gt;Na realidade, o que a maioria das pessoas sofre com teclados hoje em dia tem mais a ver com legado histórico e maus hábitos, que é o que quero explorar hoje. O layout de teclados que chamamos de 100% deriva do famoso IBM Model M. É o teclado original de 101 teclas, que divide as teclas em grandes grupos. As teclas alfanuméricas, o grupo do meio que são teclas de navegação como setinhas, page up e down. Daí temos o grupo mais importante pra quem usa Excel que é o teclado numérico. Um bom contador não consegue viver sem isso. E por último, lá em cima, temos a linha de teclas de função, de F1 até F12.&lt;/p&gt;

&lt;p&gt;Quem não usou teclados antes dos anos 90 não vai lembrar disso, mas cada microcomputador e também os terminais de mainframes; cada um tinha um layout diferente. Por exemplo, olha como era o layout do teclado de um micro MSX. Eu lembro claramente como ele tinha um layout esquisito de setas. Ou veja como era o Apple II, que já era mais parecido com o que hoje chamamos de layouts 65%. E se quiser esquisito, veja como era o ZX Spectrum, com as famosas teclas chiclete, o exato oposto de ergonomia e curiosamente mais próximo do que chamamos hoje de layouts 40%. Mas isso era cruel.&lt;/p&gt;

&lt;p&gt;A IBM veio pesquisando teclados por décadas e finalmente chegaram num layout que encapsula tudo que um operador ou programador precisaria e esse foi o layout do IBM Model M, que saiu com os primeiros IBM PC; a primeira tentativa da IBM de conquistar o mercado fora das corporações. E fez tanto sucesso que literalmente todo mundo passou a adotar esse mesmo layout desde então. Se você tem um teclado 100%, com o teclado numérico na direita, a única diferença é que depois do Windows 95 passaram a colocar a tecla super de Windows entre o &quot;alt&quot; e o espaço dos dois lados e com isso temos o layout clássico de 103 teclas.&lt;/p&gt;

&lt;p&gt;Agora, esse teclado foi pensado numa época antes da adoção do mouse do lado direito. O IBM Model M descende de teclados de terminais de mainframe, como o lendário IBM Model F, que nem esse que eu comprei no eBay mas não funciona direito. Ou este outro que é um clone do Model F e vem como kit pra você se virar, mas até hoje ainda não consegui fazer funcionar. De qualquer forma, o Model F é um dos maiores tanques de guerra dos teclados já feito. O layout era diferente, e a construção é quase toda de metal. Ele pesa mais de 4 quilos e se você bater na cabeça de alguém com isso, é homicídio. É impossível agredir alguém com um teclado desses e não esperar que o outro vai morrer no ato. E por isso também esses teclados eram super caros, acho que na faixa de 2 mil dólares.&lt;/p&gt;

&lt;p&gt;Os Model M passaram a ser feitos com materiais mais baratos, menos ferro, mais plástico, e mesmo assim ainda são mini tanques. Eu tenho um antigo que também comprei no eBay e com um adaptador de PS2 pra USB até que funciona direitinho. Mas se quiser experimentar uma versão moderna, a Unicomp herdou o maquinário original da IBM e Lexmark e fazem um modelo mais moderno USB mas ainda construído do mesmo jeito que antes. Vale pesquisar eles se tiver interesse em saber como eram teclados buckling spring originais. Depois que comprei esse eles fizeram uma versão tenkeyless que eu prefiro.&lt;/p&gt;

&lt;p&gt;Os Model M são muito grandes e originalmente pensados pra serem o único periférico na frente do monitor. Mesmo quando começamos a adotar mouse, mais do meio pro fim dos anos 80, depois do lançamento do Lisa, depois do Macintosh, os Commodore Amiga, Atari ST até chegar no Windows 3.1, ainda teve uma transição até a gente passar realmente a depender de mouse pra tudo. Foi mais depois do Windows 95 que o povo não profissional se acostumou a usar mais mouse e menos teclado, depois do meio dos anos 90.&lt;/p&gt;

&lt;p&gt;Pior ainda. Foi a partir principalmente do meio pro fim dos anos 90 que passamos a usar mais mouse pra games, especialmente depois de clássicos como Doom, Duke Nukem até culminar nos Quake III, Unreal Tournment e Half-Life. Agora os teclados cheios, 100%, já passam a realmente atrapalhar. Quando a gente joga, a maioria das teclas que se usa nos jogos fica acumulado do lado esquerdo. As teclas w a s d são usados como seta, usamos espaço pra pular. E a mão direita fica no mouse. Por isso que existem até teclados gamers que são só o lado esquerdo, porque daí olha quanto de espaço que ganhamos do lado direito pra mover o mouse.&lt;/p&gt;

&lt;p&gt;Claro, não é exatamente muito prático ficar trocando de teclado toda vez que vai jogar, mesmo porque a gente fica em coisas como chat, ou dá uma pausa no jogo, dá alt-tab e vai no navegador procurar alguma coisa; agora a metade que não tem do teclado faz falta. Então eu não recomendo esses meio teclados a menos que tenha espaço pra ter os dois tipos conectados ao mesmo tempo. Pra maioria de nós que não somos gamers nem streamers profissionais, realmente mais atrapalha do que ajuda.&lt;/p&gt;

&lt;p&gt;Mesmo sem contar games, atrapalha no dia a dia deixar o mouse lá longe na direita. Mesmo que você seja do tipo que não vê nenhum problema, saiba que quanto mais tá deixando o braço esticado pra direita, mais tá forçando seu ombro e pescoço. Na realidade, quanto mais horas se passa só usando mouse, mais força o braço como um todo e o ideal é evitar. Quem trabalha com coisas como Photoshop, editor de video, 3D ou qualquer coisa assim não tem tanto como evitar, a menos que já tenha migrado pra tablet com caneta, como as famosas Wacom. Ninguém que é obrigado a usar mouse o dia todo gosta muito disso, só se for masoquista e não se importa com dor. Todo o resto, ou tenta usar o teclado o máximo possível, ou migrou pra equipamentos especializados.&lt;/p&gt;

&lt;p&gt;De novo, o foco é mais pra gente como eu que lida com programação ou similares. Galera das artes digitais vai discutir se é melhor comprar uma Wacom Cintiq ou um Huion Kamvas ou um XP-Pen da vida. Se seu foco for artes, recomendo o canal do Brad Colbow que faz reviews de equipamentos desse tipo pra desenhistas e ilustradores profissionais. Eu passei anos lidando com photoshop só no mouse, e sim, é brutal com o pulso, ombro e pescoço. Mesmo edição de video eu tenho shortcuts no teclado pra coisas como cortar e navegar pela timeline.&lt;/p&gt;

&lt;p&gt;Se nunca tinha pensado nisso, o objetivo é pelo menos não esticar tanto o braço pra direita quando é obrigado a usar. E por isso o próximo passo de um teclado 100% são o que chamamos de TKL ou Tenkeyless, literalmente &quot;sem as 10 teclas&quot; ou o grupo de teclas numéricas. Eu já consigo ver um contador chorando assistindo o video, mas pra maioria de nós, esse bloco de teclas é literalmente inútil. É muito raro precisarmos digitar tantos números assim que faz diferença; por outro lado, toda vez que precisamos do mouse, esse bloco pouco usado tá no meio do caminho, então vamos arrancar fora.&lt;/p&gt;

&lt;p&gt;Por alguma razão esse é o layout que até alguns anos atrás mais se associava a gamers. Marcas como Razer, Logitech, Asus e outros enfiaram um tanto de RGB neles e pronto: teclado gamer. Eu mostrei nos meus videos originais de teclados que eu tinha modelos como o Razer Blackwidow. E mesmo o pessoal que começou a adotar a idéia de teclados mecânicos em geral, adotou o layout tenkeyless. Ele é o meio do caminho mais fácil, porque a única modificação é tirar as teclas de números, que nenhum gamer usa, então tem zero curva de aprendizado pra mudar e automaticamente se ganha mais espaço pro mouse.&lt;/p&gt;

&lt;p&gt;Se você quer o menor esforço pra mudar de teclado, caso estivesse usando os 100%, essa é a opção com zero dor de cabeça. Outra opção que se tornou muito popular por causa dos notebooks são variantes de layout 75%. Esses layouts tentam pegar o bloco do meio de teclas de navegação como home, end, page up, page down e setas e tentam enfiar no bloco principal alfanumérico. Esse é o layout que as setas ficam grudadas em teclas como shift e control do lado direito.&lt;/p&gt;

&lt;p&gt;Se você tá acostumado com notebooks, tá acostumado com esse layout mais apertado. Nesse caso o objetivo foi menos dar espaço do lado direito e mais conseguir enfiar um teclado usável num notebook com telas menores que 15 polegadas. O problema de tentar usar um teclado cheio 100% num notebook é que você sempre vai ficar um pouco pra esquerda, em vez de estar centralizado com a tela. Alguns se acostumam, mas eu pessoalmente não gosto de não estar no centro da tela. É questão de gosto mas acho que o layout 75% é o mais correto e mais universal pra notebooks. De novo, a menos que seja um contador, aí vai querer as teclas numéricas também.&lt;/p&gt;

&lt;p&gt;E quando eu falo contador, não estou menosprezando não, mas é um caso de uso muito pequeno pra estabelecer como padrão pra todo mundo e desperdiçar tanto espaço. Não só contador, mas qualquer um que lide por exemplo com data sciences, pesquisa e coisas que exigem digitar números com velocidade e precisão. Nesse caso não tem jeito, pega um teclado 100% mesmo e seja feliz. Mas pra todo o resto de nós, vamos continuar.&lt;/p&gt;

&lt;p&gt;Um notebook tem uma vantagem em relação a desktops, a existência de trackpads embaixo do teclado. Essa é a melhor posição, porque pra maioria das atividades do dia a dia como posicionar o cursor num Word, ou só rapidamente clicar num link, dá pra fazer tudo só usando o dedão sem quase tirar a mão direita do teclado. Por muitos anos, quando usava Mac, eu preferia usar o teclado Magic Keyboard e o Magic Trackpad embaixo dele, que é parecido com o posicionamento num Macbook.&lt;/p&gt;

&lt;p&gt;Nesse segmento que cresceu marcas como a Keychron com o seu famoso K2, que até canais renomados como o MKBHD do Marques Brownlee adotaram como teclado favorito. Inclusive pra muitos, migrar pro tal mundo de teclados mecãnicos significou ir pra um Keychron K2 ou clones similares, com layout 75%. É um layout minimalista que dá uma ótima opção pros road warriors, o pessoal que viaja muito e quer carregar um bom teclado na mochila. Pelo menos antes da pandemia.&lt;/p&gt;

&lt;p&gt;E um Keychron sempre teve uma construção muito boa. Não é necessariamente o melhor, mas é o melhor conjunto. Os entusiastas poderiam ir pra opções de melhor construção como os kits Glorious GMMK. Eu pessoalmente testei vários Keychron já, do tenleyless K8 ao 75% K2 até o low profile K3, mas pra mim o melhor modelo deles é o Q1 que eu comprei ou o Q2 que saiu faz pouco tempo. Eles tem uma construção bem mais robusta, que compete com os modelos da Glorious e da Drop e lembra o peso de um IBM Model M, um teclado menos pra levar na mochila e mais pra deixar preso na mesa, porque com esse peso ele não vai se mexer.&lt;/p&gt;

&lt;p&gt;Um layout que eu pelo menos não vi tanta adoção são os 65%. É basicamente o layout 75% parecido com de notebook mas sem a linha de teclas de função só. Ainda tem as setinhas na direita mas ele é mais curto verticalmente e nesse segmento acho que a marca que sempre mais se destacou foi a lendária Ducky. Eu não tenho uma, mas vejam por esta foto. A idéia acho que é porque a linha de teclas numéricas e a linha de teclas de função são as menos usadas no teclado, então vamos juntar as duas linhas em uma. Normalmente fica ativado a função numérica, mas combinando com uma tecla modificadora, ativamos a camada de funções de F1 a F12.&lt;/p&gt;

&lt;p&gt;Mas já que tiramos teclas de função, porque não dar mais uns passos e tirar mais algumas teclas. Aí muitos vão começar a se lembrar que um nicho principalmente de programadores e gamers resolveram dar o próximo passo lógico e daí surgiram marcas reconhecidas nesse nichos a Anne Pro 2 ou esse meu Ultimate Hacking Keyboard. Até pouco tempo sempre achei esse passo desnecessário. Um 60% é igual um 65% menos as teclas de setas, page up, page down, home e end. Ou seja, é o teclado cheio sem o bloco numérico, sem o bloco de navegaçao no meio e sem a linha de teclas de função. E aqui também nasce a necessidade de começar a conseguir configurar o mapeamento das teclas pra conseguir ter o equivalente de setas e tudo mais usando combinações de teclas.&lt;/p&gt;

&lt;p&gt;Mas pensa, não parece um saco precisar apertar tipo uma tecla como &quot;function&quot;, toda vez que precisa usar setinhas ou dar um page up? Esse é o ponto que separa a população normal dos entusiastas. A população em geral consegue ir, no máximo, até 65% e mesmo isso já parece um grande sacrifício. Agora, num 60%, pra ter setas, precisa apertar uma tecla de função e o w a s d ou i j k l, o tico e o teco não consegue lembrar dessas coisas, dá curto circuito, o cérebro primitivo é incapaz de se adaptar a isso. Pelamordeus.&lt;/p&gt;

&lt;p&gt;E vocês acham que eles pararam aí? Não! precisa ter um nível que separa de verdade os primitivos homo sapiens e dá destaque ao homo ludens de verdade. E aí inventaram o layout 40%. E é o que vocês devem ter pensado mesmo, vamos arrancar mais teclas! É basicamente o 60% mas arrancando fora também a inútil linha de teclas de números, e arrancando e diminuindo as teclas da direita. Pra que diabos backspace e shift precisam ser tão largos? Minimiza, minimiza. E esse é o resultado, o teclado que faz qualquer um repensar a vida.&lt;/p&gt;

&lt;p&gt;Falando sério, eu tentei um layout desses um tempo atrás. Tentei usar o Razer Huntsman Mini. E vou dizer que foi uma experiência horrorosa. Foi tão ruim que eu doei esse teclado pra um dos desenvolvedores da minha empresa. Na verdade, eu decidi me desfazer de vários dos teclados da minha coleção e no ano passado fiz um sorteio interno entre o pessoal da minha empresa. Eles levaram esse Huntsman, meu Matias Mini Quiet Pro, o low profile tenkeyless SK630 da Coolermaster, os meus dois Razer Blackwidow, o MassDrop Ctrl, da época anterior da empresa mudar de nome só pra Drop, e meu primeiro Keychron K2. Falei que usar um 40% quebra a gente...&lt;/p&gt;

&lt;p&gt;Tô brincando, eu quis me desfazer porque realmente já tava com teclados demais, e na época chegou o Keychron Q1 que como eu falei, achei que fosse ser meu último teclado. Tinha muito pouca coisa que poderia melhorar num teclado com layout 65%, que era o máximo que tava disposto a ir. Mas no fim do ano passado eu comecei a repensar essa história de teclados por causa de um colaborador da minha empresa que curte muito teclados e me convenceu a tentar uma coisa diferente.&lt;/p&gt;

&lt;p&gt;Antes de chegar nessa história, vou precisar voltar no tempo de novo. Agora que vocês entenderam o básico sobre o tamanho dos teclados, preciso explicar sobre o layout, a disposição das teclas propriamente dito. Acho que muitos já ouviram a história de porque usamos layouts baseados em qwerty. E chamamos de qwerty porque são as primeiras 5 teclas da esquerda. O motivo de porque letras que usamos muito como &quot;a&quot; ou &quot;p&quot; ficam um lá na esquerda pro dedo mindinho e lá na direita pro outro dedo mindinho. E tem a ver com isso aqui, a máquina de escrever original.&lt;/p&gt;

&lt;p&gt;Quem nunca usou não vai saber, mas cada tecla empurra um braço embaixo dele que vai tanto levantar um tambor aqui dentro e empurrar o martelo da letra específica que apertou, pra bater na fita de tinta e carimbar no papel atrás. Quando falamos em teclados mecânicos, falamos em peso de actuação na faixa de 45 até 60 gramas. Mas nesse caso, estamos falando de no mímimo o dobro disso de força que cada dedo precisa fazer pra conseguir carimbar com força no papel. Não tem como digitar leve, senão a tinta nem mancha o papel.&lt;/p&gt;

&lt;p&gt;O problema é que, depois de treinar muito, era possível digitar tão rápido que um martelo poderia acabar batendo no anterior, que não teve tempo pra voltar, e fazendo muito isso a gente trava a máquina e precisa manualmente desenroscar os martelos. Daí o povo teve uma idéia: seria difícil fazer os martelos voltar mais rápido, então era mais fácil fazer nossos dedos digitarem mais devagar. Então vamos colocar algumas das letras mais comuns pros dedos mais fracos. Não sei dizer se foi assim que pensaram, mas consigo imaginar a linha de raciocínio.&lt;/p&gt;

&lt;p&gt;De qualquer forma, o layout que usamos hoje é derivado de uma época onde se digitava nesses troços pesados, complicados e mecânicos de verdade. Só que ninguém mais usa isso hoje em dia. Mesmo antes dos PCs se popularizarem a gente já tinha máquinas de escrever eletrônicos, que era como se fosse um teclado embutido numa impressora com um processador muito simples e poucos bytes de memória. Eu lembro que era meu sonho poder digitar numa máquina daquelas, que eu via na escola, porque era ordens de grandeza mais leves de digitar. E depois nasceu micros e PCs, e aí até essa máquina ficou pra trás.&lt;/p&gt;

&lt;p&gt;O fato é que hoje em dia as teclas são super leves. Mesmo os dedos fracos como os mindinhos, não tem que fazer quase nenhuma força. O problema hoje é que pouca gente usa os mindinhos, mas já vou falar disso. Então, não demorou muito pra alguns pesquisadores e engenheiros pararem pra pensar, &quot;cara, porque usar um layout derivado da época de máquina de escrever? tem gente que já nasceu com computadores e sequer usou uma máquina de escrever na vida. vamos criar um layout mais inteligente.&quot;&lt;/p&gt;

&lt;p&gt;E assim nasceu o DVORAK, por causa do nome do inventor August Dvorak e na verdade eu fantasiei um pouco aqui. O Dvorak foi inventado nos anos 1930 com o objetivo de criar um layout cientificamente comprovado que é superior ao qwerty em velocidade, menos erros e menos esforço do digitador. Foram milhares de testes com pessoas de verdade ao longo de muito tempo pra chegar na melhor combinação de teclas que fosse melhor pro digitador. Mas na prática, qwerty é a prova que a gente tem muito pouca paciência pra aprender coisas novas.&lt;/p&gt;

&lt;p&gt;Eu disse que o qwerty foi inventado como forma de evitar sequências de letras que pudessem emperrar os martelos, mas mesmo nos anos 30 acho que já era possível construir máquinas que emperravam menos, mas uma vez que todo mundo aprendeu a usar qwerty, era pedir demais pra mudar pra um layout novo mesmo que fosse cientificamente, comprovadamente melhor. E o mais bizarro disso, eu tenho certeza que quase 70% de vocês assistindo nunca usaram uma máquina de escrever e mesmo assim não conseguem se ver aprendendo um layout diferente de qwerty.&lt;/p&gt;

&lt;p&gt;Mesmo hoje em dia, com computadores, e sendo super fácil trocar de layouts, a grande maioria das pessoas sequer sabe digitar qwerty como ele foi originalmente pensado, com todos os dez dedos. Tenho quase certeza que a grande maioria aqui usa 3, talvez quatro dedos. Em particular na mão direita, talvez cate milho só com o indicador e dedo do meio. E até digitam razoavelmente rápido, mas eu garanto que se sua profissão exige digitar muito, como um escritor ou programador, você eventualmente vai ter problemas no seu pulso, no seu cotovelo e no seu ombro e pode ser a razão de porque tem dores musculares e de cabeça.&lt;/p&gt;

&lt;p&gt;Num mundo ideal, a gente aprenderia Dvorak, mas pra isso acontecer, essa teria que ser a primeira opção, e não a segunda. É como a dificuldade que muitos adultos tem de aprender uma segunda língua. Depois de uma certa idade, você não quer mais ter o trabalho. É pura preguiça mesmo, não tem nenhuma outra explicação. O motivo de porque eu não tento dvorak é justamente isso também, preguiça. E Dvorak ainda tem alguns problemas de ter sido criado numa era antes dos computadores. Mas além de preguiça eu ainda tenho uma outra boa justificativa que vou explicar.&lt;/p&gt;

&lt;p&gt;Mas digamos que você tente experimentar Dvorak. Nas primeiras horas vai rejeitar o layout e o motivo é simples. Quais são as teclas que você mais usa no dia a dia? Eu diria que é copy e paste, os famosos control + C e control + V. Eles ficam aqui embaixo no teclado, não muito longes da tecla control. Daí às vezes a gente quer cortar em vez de só copiar, daí fazemos control + x, que tá aqui do lado do c. E quando a gente erra, faz o que? Desfaz ou undo, com control + z, que tá aqui do lado do x. Essas 4 teclas aqui embaixo é sua vida.&lt;/p&gt;

&lt;p&gt;E onde o Dvorak coloca o C? Lá em cima, perto da tecla 9. E o V pra colar? Fica lá embaixo na direita, na direita! E cadê a porra do X pra cortar? Vou dar 1 segundo, procura, conseguiu achar? Não né. Tá aqui embaixo, no meio, em cima do espaço. E se quiser fazer control Z pra desfazer alguma coisa? Antes era lá no canto esquerto, mas no Dvorak fica lá no canto direito. Ou seja, as 4 teclas mestras do nosso dia a dia tão todas espalhadas e só isso já é suficiente pra nos deixar em pânico completo e desistir dessa brincadeira. E isso porque não sei se notou, mas lembra de onde fica o q w e no qwerty? Olha o que tem no lugar: crase, vírgula e ponto, que normalmente ficam lá embaixo na direita.&lt;/p&gt;

&lt;p&gt;O Dvorak não deu certo na época das máquinas de escrever e até hoje permanece sendo um layout de nicho, embora todo sistema operacional já venha com opção pra escolher Dvorak se quiser. Basta ir na configuração de teclado, adicionar um novo layout, e começar a treinar. Não precisa instalar nada extra. E mesmo assim, duvido que alguém aqui tente. E por isso inventaram outros layouts pra essa era de computadores.&lt;/p&gt;

&lt;p&gt;Na realidade, depois de Dvorak surgiram dezenas de outros layouts, mas nenhum ficou mais famoso em anos recentes do que o Colemak de 2006. Um layout mais moderno e mais sensível, onde o autor Shai Coleman também fez pesquisas pra chegar num layout que fosse mais eficiente pra digitar, assim como o Dvorak, mas ao mesmo tempo não tão complicado de migrar do qwerty. E de fato, só de bater o olho, ele já não ofende tanto quanto o Dvorak. Pra começar as teclas de símbolos quase não muda nada de lugar. Vírgula, ponto, tudo no mesmo lugar.&lt;/p&gt;

&lt;p&gt;Mais do que isso, copiar, cortar, colar, desfazer, as teclas Z X C V continuam também nos mesmos lugares lá embaixo. Teclas como Q W A também parecem nos mesmos lugares, mas daí o miolo muda. Qual o princípio? É a idéia de manter seus dedos o maior tempo possível no que chamamos de &quot;home row&quot; ou &quot;linha de casa&quot;, que é a linha do meio.&lt;/p&gt;

&lt;p&gt;Olha só, num mundo ideal, seus dedos deveriam no máximo mover o menos possível, uma tecla pra cima, uma tecla pra baixo, mas ficar no meio o máximo possível. Quanto mais manter os dedos na linha do meio, menos esforço precisa fazer e menos erros vamos cometer. A maior parte dos erros vem de ter que tirar a mão dessa posição, por exemplo pra apertar o backspace pra apagar, que costuma ficar lá em cima na direita, ou quando você não usa o shift da direita pra fazer maiúsculas com as teclas da esquerda.&lt;/p&gt;

&lt;p&gt;Eu mesmo ainda não tive paciência pra testar, mas Colemak pode ser uma boa alternativa pra quem quiser migrar de qwerty pra um layout que possivelmente vai te deixar mais rápido e ao mesmo tempo diminuir problemas de lesão por esforço repetitivo ou LER. Eu estou chutando, mas imagino que qualquer um que digite errado vai ter mais chances de ter lesões no futuro. Mesmo se não quiser mudar de layout, no mínimo tente treinar pra usar todos os 10 dedos no qwerty. Eu também chutaria que seja qwerty, ou dvorak ou colemak, pra maioria das pessoas, não vai fazer tanta diferença e agora vou explicar porque não é só por preguiça que não me animo de mudar pra um Colemak da vida.&lt;/p&gt;

&lt;p&gt;Alguns anos depois, um cara que experimentou dvorak e colemak, justamente por causa de lesões por esforço repetitivo, ficou curioso com uma coisa que ele notou no Colemak. De fato, ele diz que notou que os dedos fazem menos movimentos pra linha de cima ou pra linha de baixo comparado com qwerty. Mas ele cismou que alguns dedos faziam mais movimentos laterais na linha do meio mesmo. E a razão disso foi a posição da letra H no colemak, que por acaso se manteve no mesmo lugar comparado ao qwerty.&lt;/p&gt;

&lt;p&gt;Tanto dvorak quanto colemak existem na idéia de aumentar a eficiência ao tentar manter seus dedos na linha do meio o máximo possível, mas o Bucao, criador do layout Workman, acha que obcecar só nisso e ignorar o movimento natural diferente dos dedos das mãos não é eficiente. No site do workman ele tem uma enorme explicação muito bem embasada com toda a pesquisa que justifica o porque do layout dele. E aí que eu acho que pra nós, eu aqui e vocês assistindo, não faz tanta diferença a menos que seu dia a dia seja só escrever textos em inglês.&lt;/p&gt;

&lt;p&gt;Em particular a cisma que ele teve com o H foi porque o E fica duas teclas pra direita na linha do meio no Colemak. E o segundo bigrama mais usado na língua inglesa é o HE, como em &quot;The&quot;. O primeiro bigrama é &quot;TH&quot;, claro, que aparece tanto em &quot;The&quot; como &quot;there&quot; ou &quot;this&quot;. Bigrama é um termo de linguística pra estudar línguas e se refere a pares consecutivos de letras que aparecem em palavras. Todo mundo que já lidou com indexação e pesquisa de palavras já esbarrou em termos como bigrams ou n-grams, na documentação de um Elasticsearch ou full text search de bancos de dados. Mas isso é assunto pra outro dia.&lt;/p&gt;

&lt;p&gt;Otimizações desse tipo são totalmente dependentes da língua. Pensa, como que fazemos pra manter os dedos o máximo possível na linha do meio do teclado? Simples, pegamos todos os textos de livros e revistas indexados tipo no Google e fazemos uma análise estatística de bigrams, n-grams e tentamos colocar no meio as letras mais frequentes. E tentamos também colocar mais próximo as letras dos bigramas mais usados como o TH e HE que o Bucao mencionou. E vamos fazendo experimentações e medições pra ver se realmente ao digitar textos em inglês, no geral, os dedos tendem a se mexer o menos possível e fazer o menor número de repetições desconfortáveis que poderiam levar a lesões.&lt;/p&gt;

&lt;p&gt;Mas de novo, toda essa pesquisa vale pra inglês. Mesmo entre inglês britânico e americano vai ter muitas diferenças. Se colocar nosso português na mistura, pior ainda, tem que refazer a pesquisa toda. Eu não pesquisei, mas nunca esbarrei num layout que foi pensado especificamente pra nossa língua. Certamente alguém deve ter tentado. Pior ainda, e pra gente que é programador que precisamos escrever em português e em inglês ao mesmo tempo? Não tem como fazer um layout que atenda todas as possibilidades de múltiplas línguas. E por isso eu não vejo muita vantagem migrar de qwerty pra outra coisa.&lt;/p&gt;

&lt;p&gt;A grande vantagem do qwerty e o motivo de porque ninguém tem muita vontade de mudar é porque é universal. Pode não ser o melhor, mas podemos usar computador de casa, notebook do trabalho, computador na casa dos pais, amigos, em lojas, e sempre vamos achar o mesmo layout. Seja o melhor ou não, a vantagem é a universalidade. Por isso eu não vejo o qwerty morrendo tão cedo. Mesmo os layouts cientificamente comprovados como melhores como dvorak ou colemak permanecem num nicho muito pequeno de usuários.&lt;/p&gt;

&lt;p&gt;Pra quem é programador e sequer digita textos muito longos nem em português, nem inglês, e sim coisas como &quot;function&quot;, &quot;class&quot;, &quot;const&quot;, &quot;let&quot; com mais frequência, os estudos todos que foram baseados em literatura, não bate com a realidade. Se você programa em javascript eu poderia chutar que o bigrama mais comum não vai ser &quot;TH&quot; com em &quot;The&quot; e sim talvez &quot;LE&quot; de &quot;let&quot; ou &quot;CO&quot; de const, e essas seriam candidatos de letras que deveriam estar na linha do meio. Mas como vimos, se tirar o &quot;C&quot; da linha de baixo, vai dar treta, porque mais que digitar código, programador moderno vai é fazer control + C e control + V com mais frequência do que qualquer outra coisa.&lt;/p&gt;

&lt;p&gt;Eu pessoalmente não vejo vantagens em mudar de qwerty, mas se no seu dia a dia você digita mais textos em inglês o dia todo, talvez como pesquisador escrevendo papers ou gerente escrevendo relatórios, talvez um colemak ou workman sejam os mais indicados pra testar, se fizer muita questão de tentar. Mas esperem passar por pelo menos 2 semanas de muita frustração e ter que retreinar digitação do zero até conseguir chegar numa boa velocidade e com poucos erros. Na prática o problema da maioria das pessoas não é o layout e sim nunca ter treinado pra digitar nem o qwerty direito, com todos os dez dedos das mãos na posição correta. Já vou falar sobre isso.&lt;/p&gt;

&lt;p&gt;Mas falando em relíquias da época da máquina de escrever tem de fato uma coisa que é difícil de defender. E foi isso que meu amigo aqui da empresa me recomendou e me convenceu a investigar mais. Olhem pra todos esses layouts, qwerty, dvorak, colemak, workman. Conseguem ver um troço estranho em todos eles? Ou melhor ainda, olha pro seu teclado na sua frente agora assistindo aí o video. Lembra quando eu falei agora pouco sobre o layout 60% de porque diabos teclas como backspace ou shift precisam ser tão largos? Sério agora, porque você acha que muitas teclas tem tamanhos diferentes?&lt;/p&gt;

&lt;p&gt;Mas vamos um passo além, porque diabos todas as teclas precisam ser desalinhadas? Coloca sua mão na posição neutra na linha do meio. Indicador esquerdo em cima da tecla F, que costuma até ter um relevo pra você sentir que estacionou no lugar certo. E indicador direito em cima da tecla J que também tem esse relevo. Agora você vai digitar a tecla &quot;C&quot;, qual é o dedo certo? É descendo o indicador ou o dedo do meio? E a tecla &quot;Z&quot;? É o mindinho ou o anelar? Sério, consegue me dizer de cabeça sem tentar no seu teclado?&lt;/p&gt;

&lt;p&gt;E todas as teclas da linha de cima e de baixo sofrem do mesmo problema. Muitas delas são ambíguas, e se você nunca teve treinamento formal de digitação, cada um vai ter uma resposta diferente porque ao longo dos anos acostumou a usar um dedo ou outro. Mesmo quem digita com todos os dez dedos vai ter dificuldades e esse é o motivo de porque você erra tanto e porque a tecla que mais usa é o backspace pra apagar e corrigir; porque se tenta ir muito rápido eventualmente seu dedo bate na borda de uma tecla e acaba apertando a outra do lado.&lt;/p&gt;

&lt;p&gt;E por que diabos as teclas são todas desalinhadas? Já pararam pra se perguntar isso? Eu mesmo nunca tinha parado pra pensar nisso até que me fizeram essa mesma pergunta. E a resposta, pra mim que aprendeu a digitar numa máquina de escrever é super óbvia. Vamos voltar aqui pra minha máquina. Lembram que cada tecla tem um braço em L embaixo que liga com o tambor lá atrás? Concorda que seria muito complicado se as teclas fossem alinhadas verticalmente? Elas são desalinhadas pros braços das teclas da linha de baixo não cruzarem com os braços das teclas da linha de cima.&lt;/p&gt;

&lt;p&gt;Deixa eu mostrar com calma que isso é importante, olha só como cada tecla fica exatamente entre duas de cima. Pras teclas serem alinhadas verticalmente, esses bracinhos de ferro embaixo precisaria cada uma ser dobrada de jeitos diferente pra desviar das outras. Eu imagino que até seja possível, mas aí na hora de fabricar, teria que ter um desenho de braço diferente pra cada linha de teclas. E esse é o único motivo de porque as teclas são todas desalinhadas. Não é por eficiência pra digitar nem nada, mas sim pela forma que máquinas de escrever eram construídas.&lt;/p&gt;

&lt;p&gt;E de novo, ninguém mais usa máquinas de escrever. Mudar de layout eu realmente acho que o ganho não compensa a dor de cabeça pra reaprender, mas o desalinhamento realmente me atrapalha mesmo depois de digitar assim por 3 décadas. Eu sempre errei mais do que gostaria e sempre porque vira e mexe, quando começo a digitar muito rápido, acabo apertando duas teclas sem querer, e como sou obrigado a mover os dedos pra um alinhamento estranho na linha de cima e de baixo, pra voltar pra posição neutra no meio de novo, é mais devagar.&lt;/p&gt;

&lt;p&gt;E foi isso que ano passado eu fui obrigado a parar pra pensar. Não existe nenhuma razão pra teclados modernos terem teclas desalinhadas a não ser legado histórico e costume. E depois que isso me ocorreu, não consigo mais despensar e fiquei com isso na cabeça me perturbando. Eu sou do tipo que quando uma coisa óbvia aparece, não consigo mais ignorar. E por isso encomendei dois teclados novos. E agora podemos continuar a história dos tamanhos de teclado também.&lt;/p&gt;

&lt;p&gt;Lembram que brinquei que o homo sapiens vai parar no layout 65% porque a evolução só chega até esse ponto e só o homo ludens vai o caminho todo até os famigerados 40%? Pois é, conheçam o meu novo Planck EZ da ZFA, a mesma que muitos devem conhecer pelo famoso Ergodox. Se nunca tinha visto, isto é um 40%. Esse teclado é um pouco menor que um Nintendo Switch, e só um pouco mais grosso na lateral. Aliás, uma dica pra quem tem teclados desse tamanho. Ainda não existe um case oficial pra transportar o Planck na mochila mas um case de Switch serve direitinho. Eu tenho esse genérico da Insignia que comprei pra transportar meu Switch e olha só, cabe o Planck direitinho nele.&lt;/p&gt;

&lt;p&gt;Mais do que 40%, esse teclado tem as teclas todas alinhadas verticalmente. É o que se chama de teclado ortolinear, teclados tradicionais chamamos de staggered, ou escalonado. Vamos falar disso primeiro e vamos colocar os dedos na posição neutra, indicador esquerdo no F e indicador direito no J, igual antes. Agora lembram da pergunta? Qual dedo que devemos usar pra apertar a tecla C? Indicador ou dedo do meio? E aqui está absolutamente óbvio que é o dedo do meio, só deslizar ele pra baixo uma tecla. E a tecla Z, é com o mindinho ou o anelar? E de novo é óbvio, só deslizar o mindinho e ele encosta no Z. Não tem dúvidas de qual dedo vai em onde.&lt;/p&gt;

&lt;p&gt;Olha só, deslizamos verticalmente pra cima, deslizamos verticalmente pra baixo, os únicos dedos que tem mais de uma letra são os indicadores que continuam deslizando pras colunas do meio. Mas beleza, e se eu quiser digitar números? Onde eles foram parar? E aqui começa uma coisa que é específica deste teclado e pode variar em outros modelos. No caso do Planck, ele pode ser totalmente customizado via um software chamado Oryx. Vamos abrir online pra olhar, é uma aplicação web, nem precisa instalar.&lt;/p&gt;

&lt;p&gt;Teclados 40% obrigatoriamente precisam ter camadas, ou layers como eles chamam. Essa camada que vocês conseguem ver chamamos de layer base. É onde ficam todas as letras normais e teclas de modificação como shift, control, alt e outros. No meu caso em particular eu modifiquei meu layout pro enter ficar do lado direito do espaço e pro backspace ficar no dedão do lado esquerdo. O motivo é simples. Quais são os dedos mais fortes das suas mãos? O dedão, e enquanto dedos fracos como o mindinho da esquerda precisa além de apertar as telas Q A e Z ainda precisa apertar o control, o shift, o esc e tudo mais. E o dedão? A única coisa que ele faz é apertar a barra de espaço. Com certeza esse dedão consegue fazer mais que isso.&lt;/p&gt;

&lt;p&gt;Fazendo isso eu consigo colocar de volta a tecla de igual e mais lá no canto superior direito, que no layout padrão é onde fica o backspace e aí eu perderia essa tecla. Tecla de igual usamos bastante em programação. No layout padrão do Planck, temos duas teclas especiais de troca de layers. Voltando ao assunto de layers, o Planck tem 3 layers, o que ele chama de layer baixo, layer alto e layer de ajuste. No meu caso eu fiz a tecla de backspace trocar pro layer baixo quando deixa apertado.&lt;/p&gt;

&lt;p&gt;Vamos a mais detalhes. A gente costuma pensar em uma tecla como só tendo uma função. Por exemplo, a tecla de backspace, apaga uma letra. Se deixamos apertado ele sai apagando mais letras pra esquerda. Mas só com essa descrição sabemos que a tecla tem pelo menos 2 comportamentos diferentes pra 2 estados diferentes, o estado de teclar e o estado de deixar apertado, ou held. Olha só no Oryx, de fato eu posso configurar cada estado como eu quiser. E além de só apertar e deixar apertado eu ainda tenho os estados de dar duplo clique rápido e o estado de apertar rápido uma vez e deixar apertado no segundo toque que é esse &quot;tapp and then held&quot;.&lt;/p&gt;

&lt;p&gt;Ou seja, cada tecla pode ter até 4 comportamentos diferentes e foi assim que fiz meu backspace também ter o comportamento de mudar pro layer baixo, quando deixa apertado sem soltar. Justamente um dos maus hábitos que nós temos é errar e sair apagando com backspace apertado a linha toda em vez de usar combinações mais eficientes como control + backspace que apaga palavras inteiras de uma só vez. Enfim, voltando pro Oryx, veja como está meu layer baixo hoje.&lt;/p&gt;

&lt;p&gt;Quando eu deixo apertado o backspace, meu teclado muda de cor, que foi como eu configurei no Oryx. Agora as teclas da linha do meio da esquerda são os acentos mais chatos de português como til e circunflexo. E na linha do meio da direita eu deixei o mesmo esquema de navegação de setinhas como é no editor Vim, com H pra esquerda, J pra baixo, K pra cima e L pra direita. Vários outros utilitários de Linux costumam navegar no estilo Vim e esse eu acho que é o jeito mais eficiente pra navegar por um texto.&lt;/p&gt;

&lt;p&gt;Esse ponto vale explicar mais. Vamos lá, o grande motivo de porque a maioria das pessoas não tá disposta a experimentar layouts abaixo de 65% é porque perdemos as setas de navegação que ficam lá embaixo na direita; olha só neste teclado 65% por exemplo. Você tá digitando seu texto ou código aí decide que faltou digitar alguma coisa no parágrafo de cima, o que você faz? Tira a mão direita inteira do teclado e move lá pra baixo. Daí quando termina de navegar, agora precisa encontrar a posição neutra tudo de novo pra voltar a digitar.&lt;/p&gt;

&lt;p&gt;Agora vamos ver como é no modo Vim. Mesma coisa, quero navegar algumas linhas pra cima pra acrescentar alguma coisa. Agora eu aperto o backspace e deixo apertado pra mudar pro layer de baixo, sem tirar as mãos da linha do meio eu navego e imediatamente volto a digitar sem tirar as mãos do meio do teclado. Essa é a grande vantagem da navegação Vim. E some a isso esse layout de 40% e literalmente os dedos não precisam se mover mais do que uma tecla pra cima ou uma tecla pra baixo.&lt;/p&gt;

&lt;p&gt;Isso ajuda mesmo acentuação de português que é um saco. Pensa o til. Se você digitava errado como eu, a gente precisa tirar a mão esquerda toda da posição pra alcançar shift e til, ou apertar shift com mindinho da direita e til com mindinho da esquerda, puta combinação chata. No meu caso eu configurei pra ser no layer de baixo então só aperto backspace com o dedão pra ativar o layer e F que é onde configurei pra ser o til. Essa é uma combinação bem mais próxima e sem malabarismo, com dois dedos fortes, o dedão e o indicador. E no caso de acentos eu recomendo que se experimente diversas combinações pra achar o que mais se adequa pra você.&lt;/p&gt;

&lt;p&gt;No meu caso, no layer de cima é onde estão coisas como as teclas de função de F1 a F12 aqui no bloco do lado esquerdo. Eu quase nunca uso essas teclas, mas tá lá se precisar. E no lado direito tem uma curiosidade que ainda não me acostumei. Em vez de tirar a mão do teclado e pegar o mouse, pra coisas pequenas como clicar num campo num formulário de um site pra dar foco, eu posso controlar o cursor do mouse via o teclado. Olha só. Eu ativo o layer de cima e com essas teclas de baixo na direita mexo o cursor e posso até clicar com equivalente do botão direito ou esquerdo do mouse. A idéia não é substituir o mouse, mas só evitar precisar dele pra coisas pequenas e rápidas.&lt;/p&gt;

&lt;p&gt;Enfim, eu já reconfigurei esses layouts literalmente dúzias de vezes. Toda vez que vou digitar um texto novo percebo alguma coisa que quero mudar de lugar. Depende muito do tipo de coisa que mais digita. Lembra o que eu falei sobre layouts como colemak, workman e afins? Diferente de inglês, português usa muita acentuação, é um grande saco, e nenhum desses   layouts leva isso em consideração. Os acentos que a gente mais precisa ficam espalhados por todo o teclado. E com teclados como o Planck posso configurar onde Eu acho mais confortável.&lt;/p&gt;

&lt;p&gt;Aliás, o Oryx não é o único software que existe. Cada fabricante costuma ter o seu, precisa pesquisar, mas em particular dê preferência pra teclados que aceitam firmwares QMK, que é um projeto de código aberto pra ser o cérebro de teclados customizáveis. O próprio software Oryx na realidade é o QMK com uma cara mais bonita que a ZFA fez pra ela. Uma das grandes vantagens é que toda essa configuração não fica só no seu computador. Deixa eu mostrar.&lt;/p&gt;

&lt;p&gt;Toda vez que modifico alguma coisa, ele compila e me dá opção de baixar um arquivo binário. Daí eu abro esse aplicativo de Windows chamado Wally e seleciono o binário que acabei de baixar. Olhem como tem vários do planck porque eu tava brincando bastante com ele. Daí pede pra colocar o teclado num modo que aceite o upload desse binário, que é apertando um botãozinho com uma agulha. Demora alguns segundos, restarta o teclado e pronto, tá com a nova configuração. E toda vez que mudar qualquer coisa precisa fazer esse processo de novo.&lt;/p&gt;

&lt;p&gt;A grande vantagem é que agora posso plugar esse teclado em qualquer outro computador e todas as minhas customizações vão funcionar igual, sem precisar instalar nada extra. Fica tudo na memória do teclado. Marcas grandes como Razer tem software 100% proprietário como o Synapse, que costuma dar mais prioridade em mudar o RGB do que realmente customizar o teclado e foi o motivo de porque eu disse que minha experiência com o Hunstman Mini foi horrorosa. Porque não podia mudar quase nenhuma tecla. E um teclado 40% que não deixa customizar tudo, não serve pra nada. Portanto fiquem espertos com compatibilidade com QMK, isso é o mais importante.&lt;/p&gt;

&lt;p&gt;Agora, mesmo customizando tudo que quiser, mesmo assim você não vai conseguir sair digitando rápido no mesmo dia. De jeito nenhum. Mesmo se já é um bom digitador num teclado normal, se já consegue uma média de 80 palavras por minuto, tenha certeza que no dia 1 sua média vai cair pra 20 palavras por minuto ou menos. Ou seja, sua velocidade male male vai ser 25% do que era antes. E isso vai ser extremamente frustrante, porque 20 palavras por minuto é a velocidade de alguém catando milho. Velocidade de ficar olhando e procurando a tecla.&lt;/p&gt;

&lt;p&gt;E não tem jeito, seu corpo se acostuma com tudo que você faz repetidas vezes, inclusive as coisas erradas. Por isso é tão difícil quebrar um mau hábito. E mesmo sendo ruim digitar com teclas desalinhadas, sua memória muscular tá acostumado a lidar com esse problema. Ele naturalmente digita errado, naturalmente seu dedinho vai pro backspace e você naturalmente fica nessa rotina de, digita errado, apaga, conserta, digita errado, apaga, conserta. Você faz sem pensar. Mas colocando um teclado 40% e ortolinear ainda por cima, a região consciente do seu cérebro certamente não tem como ignorar. Ele vai ter que voltar pro zero.&lt;/p&gt;

&lt;p&gt;Voltar do zero significa abrir um notepad e começar a reacostumar dedo a dedo. Faz de conta que você tava acostumado a digitar a letra C com o indicador esquerdo. Então coloca as mãos na posição neutra e começa a digitar a tecla C com o dedo médio quinhentas vezes no notepad, até parecer que seu dedo médio acostumou que é Ele que vai no C. E vai fazendo isso letra a letra.&lt;/p&gt;

&lt;p&gt;Quando terminar de se familiarizar com o teclado, agora precisa de treino intensivo e pra isso servem sites como o keybr.com. Lembrando que vai ser extremamente frustrante mas você precisa ignorar a frustração e prestar atenção. Qual dedo tá tendo mais dificuldade em qual letra? Seu mau hábito tá levando o dedo pra tecla errada? Então volta pro notepad e fica treinando só esse letra, até marretar no seu cérebro que é o outro dedo que tem que ir nessa tecla. Volta pro keybr, faz algumas rodadas de palavras com as letras que tá errando. Recalibra outro dedo errado no notepad, e vai fazendo isso, dedo a dedo. Seu dedo não pode mandar em você, precisa subjugar esses dedos rebeldes.&lt;/p&gt;

&lt;p&gt;Lembra bigramas? Além de digitar as teclas com o dedo errado, tem diversas combinações que naturalmente digitamos mais rápido. Acho que isso varia de pessoa pra pessoa, mas por exemplo uma palavra como &quot;palavra&quot;, no meu caso o &quot;pala&quot; eu digito muito rápido, é o mindinho da direita, depois o mindinho da esquerda, daí o anelar da direita e de novo o mindinho da esquerda. Como alterna direita e esquerda, elas saem quase ao mesmo tempo. Se você digita com dedos errados, tá acostumado a combinações rápidas que agora vão sair devagar, e até reacostumar, vai ser a parte mais frustrante de reaprender, na minha opinião.&lt;/p&gt;

&lt;p&gt;Imagino que foi intencional, mas por isso eu acho interessante não treinar no começo com palavras que tá acostumado. Olha como os testes de prática do Keybr são palavras nada a ver. Isso força seu cérebro a desistir da memória muscular, porque nenhuma palavra tem o ritmo que você espera, e aí precisa prestar mais atenção no que tá digitando em vez de confiar na memória. Esse é um bom jeito de começar a forçar a digitar da forma correta, tirando completamente da zona de conforto.&lt;/p&gt;

&lt;p&gt;E quanto tempo precisa pra acostumar? De novo, isso não é científico porque é a experiência de uma única pessoa que sou eu mesmo. Se olhar no meu perfil no keybr, eu treinei um total de 4 horas, quase 500 testes de prática como esses que mostrei. Imagino que a maioria das pessoas não tem paciência pra ficar catando milho e se frustrando por mais que meia hora por dia, então chuto que vai levar mais de uma semana tentando, no mínimo, porque picado assim vai levar mais tempo no total. E não adianta, tem que ser um treino deliberado como falei, prestando atenção dedo a dedo e retreinando letra a letra. Não adianta querer sair digitando de qualquer jeito que aí sua memória muscular vai forçar os dedos errados nos lugares errados.&lt;/p&gt;

&lt;p&gt;Eu sou meio retardado, e já tinha ouvido falar que podia levar 2 semanas ou mais de treino e eu tava com zero paciência pra esperar tanto. Então, essas 4 horas foram ao longo de 2 dias num fim de semana. E na realidade foi mais de 4 horas. Depois das primeiras horas no keybr decidi tentar palavras de verdade em outro site, no Monkeytype. E lá gastei mais 2 horas de treino e uns 300 exercícios. No total foram pelo menos umas 3 horas num sábado e mais 3 horas no domingo, total de quase 800 exercícios. Na verdade acho que foi mais porque eu só fiz conta no keybr bem depois que comecei a treinar.&lt;/p&gt;

&lt;p&gt;Nas primeiras horas eu tava catando milho, no máximo 20 palavras por minuto. Mas é incrível o que treino deliberado prestando atenção no tempo, na métrica e disciplinadamente indo dedo a dedo fazem. No final dessas 6 horas já tinha voltado pra minha média de 80 palavras por minuto, e forçando um pouco mais já tava conseguindo ficar no range entre 90 e 100 palavras por minuto que era minha velocidade em teclados normais antes. Hoje em dia facilmente atinjo 90 palavras por minuto se estiver focado. Então a boa notícia é que se você for do tipo dedicado e focado, dá pra acostumar no layout ortolinear de boa em um único fim de semana, sem nem precisar se matar tanto assim.&lt;/p&gt;

&lt;p&gt;Considere o seguinte, eu digito no estilo qwerty com teclas desalinhadas faz mais de 30 anos, e tinha alguns poucos maus hábitos de dedos em teclas erradas. 6 horas intensivas num fim de semana foi o suficiente pra corrigir meus maus hábitos e acostumar num alinhamento novo, o suficiente pra segunda-feira estar respondendo emails, conversando com o pessoal da empresa no chat e ninguém notando diferença na minha velocidade. Lembra meu episódio de a Dor de Aprender? Foi tipo isso.&lt;/p&gt;

&lt;p&gt;Eu diria o seguinte. Os melhores digitadores conseguem fazer acima de 150 palavras por minuto se estiverem completamente focados. É muito difícil conseguir isso. O meu máximo, com 30 anos de digitação nas costas, é 100 palavras por minuto, mas isso se estou num site como monkeytype ou keybr: em condições perfeitas, foco completo, sem acentuação, sem pontuação, sem palavras em maiúsculo, ou seja, condições de laboratório.&lt;/p&gt;

&lt;p&gt;Aliás, eu facilmente chego acima de 90 palavras por minuto se forem palavras em português. Uma coisa legal do Monkeytype é que dá pra escolher dicionários em várias línguas. No caso de inglês dá pra escolher dicionário de palavras fáceis e ir escolhendo de palavras bem mais complicadas pra dificultar o treino. Em inglês minha média cai pra casa dos 70 a 80 palavras por minuto. Por alguma razão eu digito bem mais rápido em português sem acentos. E um chute que eu tenho é que em inglês eu noto que uso mais igualmente as duas mâos, mas em português parece que eu uso bem menos a mão esquerda. Minha mão direita é um pouco mais devagar, daí em inglês vou mais lento que português. A velocidade não é a mesma em qualquer língua.&lt;/p&gt;

&lt;p&gt;No dia a dia, eu devo estar muito mais próximo da média de 60 palavras por minuto. Até menos. Isso porque quando digito um texto, eu páro pra pensar, mudo de idéia, reescrevo de um jeito mais bonito ou menos agressivo. Ninguém fica digitando 100 palavras por minuto o tempo todo, só em ambiente controlado. Então eu diria que mais que digitar rápido, é mais importante digitar corretamente, cometendo menos erros. Erros é o que atrapalha seu raciocínio, porque quebra sua concentração ter que ficar dando um monte de backspace toda hora.&lt;/p&gt;

&lt;p&gt;E falando em digitar muito rápido, um tanto de gente ficou me mandando um video que eu já tinha visto também no TikTok de um teclado diferente onde o cara consegue digitar 200 ou 300 palavras por minuto. É o Charachorder. Parecem duas bolas com botões pendurados. Vou resumir pra vocês: primeiro, esse teclado não presta pra gente. Segundo, essa idéia não é nenhuma novidade. Existe faz décadas.&lt;/p&gt;

&lt;p&gt;Só existe uma profissão onde digitar extremamente rápido e sem erros é fundamental. Se você for um estenógrafo. Sabe séries de TV que tem julgamentos, tipo Suits da vida? Sempre tem aquela pessoa quieta que fica transcrevendo tudo que todo mundo fala no julgamento. Já pararam pra pensar como fazem isso? Pra conseguir transcrever uma pessoa falando em tempo real de fato você precisa ter velocidade consistente acima de 150 palavras por minuto, durante todas as horas que pode durar um julgamento. Digitar 100 já exige todo meu foco, imagina 150 ou 200 palavras por minuto durante um dia inteiro de 8 horas.&lt;/p&gt;

&lt;p&gt;Num teclado normal isso é impossível. Com todos os defeitos legados que herdamos da máquina de escrever, não tem como. Nem se eu treinar muito no meu ortolinear, também é impossível. Eu preciso juntar meu teclado com um software que pensa junto comigo. É uma combinação de macros e autocomplete, que nem o teclado do seu smartphone faz sugerindo palavras antes de terminar de digitar. Palavras super longas eu abrevio pra uma combinação de poucas letras num macro, que nem snippets de código num VSCode. Os teclados especializados de estenografia oficiais custam super caros, até 6 mil dólares. Isso porque eles são essencialmente computadores com dicionários caros.&lt;/p&gt;

&lt;p&gt;Esse cara do CharaChorder acho que vende o teclado diferentão e também tem opção só do software pra usar com um teclado normal. Ele usa características de teclados modernos como n-key rollover, que já expliquei no outro video que é digitar várias teclas de uma só vez e o sistema conseguir registrar todas elas. Ele faz isso. É semelhante à função de swipe de teclado virtual que a gente só desliza o dedo em cima das teclas e ele adivinha que palavra que a gente queria digitar.&lt;/p&gt;

&lt;p&gt;Se quiser estudar mais sobre como se faz isso, hoje em dia tem machine learning envolvido porque é um processo estatístico que aprende baseado em frequência de palavras e n-grams. Mas precisa entender o que são cadeias de Markov e processo estocástico. Lembra quando povo fala que aprender matemática é opcional? Pois é, sem entender cadeias de markov, você nunca vai conseguir fazer coisas como um teclado virtual.&lt;/p&gt;

&lt;p&gt;Mas esquece esse cara do CharaChorder. Nada do que ele tá vendendo é novidade. Você também não precisa gastar milhares de dólares e fazer curso oficial de estenografia. Teclado de estenografia tem só duas linhas principais de teclas em vez de três. Dá pra ou comprar um kit barato ou configurar seu teclado normal. Existe um projeto de código aberto chamado Open Steno que tem os softwares, documentação que ensina como usar, e uma comunidade de entusiastas que querem aprender a digitar tão rápido quanto um estenógrafo de verdade. Existe todo um campo de pesquisa e engenharia em cima disso. Isso existe faz décadas.&lt;/p&gt;

&lt;p&gt;E de novo, esses sistemas funcionam bem quando tá tudo configurado de um jeito bem diferente do que seu teclado se comporta. Um estenógrafo que transcreve casos nos Estados Unidos tem configurado um bom dicionário em inglês. Se precisasse transcrever um brasileiro falando, ele ia ter problemas. Precisa de um dicionário em português, reconfigurar todos os macros, e retreinar tudo pra uma língua nova. Agora imagina programação, que todo código tem um monte de palavras que simplesmente não existem num dicionário, com um monte de parênteses, chaves e coisas que não se usa num texto normal. Daí esse tipo de teclado vai mais atrapalhar do que ajudar. Por isso eu falei, pra gente é meio inútil. Só serve se você quiser ser um estenógrafo ou se exibir em video no TikTok.&lt;/p&gt;

&lt;p&gt;Digitar rápido não é o objetivo. Ninguém vai ganhar nada digitando 300 palavras por minuto. Você nem tem tudo isso de palavras prontas na cabeça pra digitar um texto ou código que faça sentido. Porra, a maioria das pessoas consegue digitar merda em 200 letras num tweet. Magina escrevendo 300 palavras por minuto o tanto de lixo sem sentido que vai sair. Foque em escrever correto e não em escrever rápido. O objetivo é não forçar seus nervos e evitar dores desnecessárias.&lt;/p&gt;

&lt;p&gt;O Planck EZ não é o único nesse formato. Tem outros como o OLKB Planck da Drop, que antes era Massdrop. É exatamente o mesmo layout. No momento que eu tava escrevendo o episódio eles tavam sem estoque. Mas tem outros fabricantes menores e entusiastas que fazem kits de teclados nesse e outros formatos mais exóticos, vale dar uma pesquisada. Não são baratos. Pode esperar gastar 200 dólares, sem contar frete e impostos.&lt;/p&gt;

&lt;p&gt;Mas o título do episódio é pra falar do melhor teclado do mundo, e esse ainda não é o Planck. Da mesma empresa ZFA já tinha um que muita gente elogiou que é o Ergodox e depois dele, a ZFA pegou os feedbacks e fizeram um melhor ainda, que é o Moonlander MK1. E se prestou atenção em tudo que expliquei agora, falta só voltar pra um ponto que falei lá no começo do video: melhorar a ergonomia.&lt;/p&gt;

&lt;p&gt;O Planck é muito bom pra quem precisa carregar o teclado na mochila. Literalmente cabendo num case de Switch, sabemos que é super portátil. Mas se você trabalha remoto, e só precisa de um teclado, dá pra optar por um melhor. E esse é o Moonlander. A principal diferença é ele ser dividido no meio. Ele até foi feito pra ser razoavelmente portátil, embora eu não ache que esse é o objetivo. Vamos recapitular sobre postura.&lt;/p&gt;

&lt;p&gt;Você quer sempre colocar o menor stress possível no seu ombro e nos seus braços, por isso tente sentar de tal maneira que seus ombros não fiquem assim tensionados pra cima. Eles tem que estar naturalmente pra baixo. Altura errada de mesa e cadeira que prejudicam isso. Além disso vocês já devem ter ouvido falar que quando usamos um teclado normal, rotacionamos, ou pronamos os braços de tal forma que arriscamos estar beliscando os nervos quando as mãos estão retas na horizontal. A posição natural seria o mais vertical possível.&lt;/p&gt;

&lt;p&gt;É isso que muitos teclados ergonômicos fazem, sendo mais altos no meio, a tal forma de tenda, pra aliviar essa pronação forçada. Todo mundo que já usou um teclado ergonômico da Microsoft ou Logitech já viu formatos assim. Além disso, outra coisa que pode forçar seus ombros e pescoço é trazer as duas mãos pra junto, em vez disso seria mais natural estarem separadas. E são esses problemas que um teclado split, cortado no meio, tenta ajudar a resolver. Olha como eu deixo esse Moonlander na minha mesa. Sim, agora minhas mãos ficam quase meio metro separadas uma das outras. E sim, digitar corretamente é pré-requisito, porque se um dos seus maus hábitos é digitar alguma tecla do lado esquerdo com sua mão direita, agora não tem como.&lt;/p&gt;

&lt;p&gt;Diferente do Planck, o Moonlander não é minimalista. Ele tem mais ou menos o equivalente a um teclado 60%. Não tem as teclas de função lá em cima e nem as teclas de setas. Por outro lado adiciona várias teclas auxiliares ao redor que podemos customizar como quisermos no mesmo software Oryx que mostrei antes. Em particular tem esses thumb clusters, ou grupos de teclas por dedões. Lembra quando falei que nossos dedões podiam ter mais trabalho? Pois é, agora podemos dar trabalho pra ele, se quiser cada dedão tem quatro teclas dedicadas de cada lado.&lt;/p&gt;

&lt;p&gt;Igual no caso do Planck, eu ainda fico modificando o layout pra achar alguma combinação de teclas que acho mais confortável. E ele também tem diversas camadas que podemos configurar. No caso do Moonlander, posso criar quantas camadas quiser. Isso é útil se por exemplo eu uso Photoshop, DaVinci Resolve, Blender e quiser fazer camadas especiais pra cada programa. E assim como no caso do Planck, eu deixo coisas como enter e backspace nos dedões.&lt;/p&gt;

&lt;p&gt;Diferente do Planck, que é ortolinear, ou seja, é alinhado tanto na horizontal quanto na vertical simetricamente, o Moonlander é colunar. A diferença é ser alinhado na vertical, mas não na horizontal e isso é a última forma de quebrar o legado das máquinas de escrever. Não só nunca fez sentido desalinhar as teclas na vertical, mas também não existe motivo nenhum pra serem alinhadas na horizontal. Pára e pensa, olha pras suas mãos. Seus dedos tem o mesmo comprimento? Então porque as teclas ficam alinhadas na horizontal?&lt;/p&gt;

&lt;p&gt;Alinhamento na horizontal significa que seu indicador e principalmente dedo médio ficam o dia todo muito dobrados, e também significa que seus mindinhos precisam esticar pra alcançar os cantos. Se você tiver mãos grandes, seus dedos vão dobrar mais. Se tiver mãos pequenas, seus mindinhos não alcançam os cantos. Em ambos os casos vai estar fazendo esforço extra desnecessário. Mas e se simplesmente alinharmos as colunas de teclas de acordo com o comprimento dos dedos?&lt;/p&gt;

&lt;p&gt;Eu entendi ortolinear e me convenci da teoria, mas vou dizer que estava um pouco cético do desalinhamento horizontal do colunar. Mas posso dizer que funcionou super bem. O único porém é que não é muito trivial saber como de fato usar esse alinhamento. Eu mesmo pensei que as duas metades deveriam ficar paralelas com meu braço, mas não, o certo é elas estarem levemente rotacionadas e abertas. Eu vi isso em outro canal que recomendo de teclados do Ben Vallack que vou deixar na descrição abaixo.&lt;/p&gt;

&lt;p&gt;Em resumo, se deixar da forma paralela, mesmo as colunas levando em consideração o comprimento dos dedos, ainda assim o indicador tem espaço sobrando e os mindinhos precisam esticar demais. Mas inclinando dessa outra forma, olha como fica mais fácil pros mindinhos trabalharem. E essa é a melhor forma de digitar com o Moonlander. O trabalho é mais decidir quais configurações colocar em quais camadas e como ativar cada camada do jeito mais fácil e isso vai depender só do seu treinamento. Minha configuração serve só pra mim. Tem até algumas combinações como control + shift + colchete que precisa pra editar no DaVinci Resolve, e agora fica fácil porque posso juntar tudo numa tecla só e facilita horrores pra editar.&lt;/p&gt;

&lt;p&gt;Essa idéia de considerar o comprimento dos dedos não é nova. Eu já tinha visto outras opções como o Kinesis Advantage 2. Ele também é ortolinear, mas em vez de desalinhar horizontalmente, ele foi além e colocou as teclas todas numa cuba, como dá pra ver nessa imagem, porque é até difícil de descrever em palavras. E agora que aprendi a digitar com ortolinear, consigo entender porque esse teclado funciona bem. Talvez um dia experimente, mas tem uma coisa que não gosto dele, o fato de ser gigantesco e de não poder colocar as duas metades separadas como quiser. Mas eu sei que tem projetos pra impressora 3D e kits inspirados nesse formato que tentam melhorar esses pontos, vale pesquisar.&lt;/p&gt;

&lt;p&gt;Se você leva a sério a história de ergonomia, tanto o Moonlander quanto o Advantage 2 cumprem mais ou menos as mesmas coisas que expliquei em termos de evitar pronar o braço e dar menos stress nos ombros, mas em termos de estética ainda prefiro mil vezes o Moonlander. Mais ainda porque se você realmente quiser levar pras últimas consequências, embaixo de cada metade do Moonlander tem um encaixe pra um acessório que faz ele ter o mesmo encaixe padrão do parafuso que tem em tripé de câmera fotográfica. E a ZFA tem um acessório que encaixa nele e você pode fazer um adaptador pra colocar no braço da sua cadeira.&lt;/p&gt;

&lt;p&gt;Sim, o teclado sequer precisa ficar na mesa. Dá pra encaixar cada metade em cada braço da cadeira pra posição mais bizarra de todos os tempos e vou dizer que agora que aprendi a digitar corretamente em cada metade, essa idéia até que me atrai. Sem contar que daria pra colocar num tripé de cada lado pra poder digitar de pé em mesas que levantam em altura. Tem muitas possibilidades.&lt;/p&gt;

&lt;p&gt;E falando em possibilidades e no canal do Ben Vallack, como último assunto não podia deixar de falar de uma versão de teclado que estou querendo testar. Pois bem, os homo ludens chegaram no layout 40%, mas essa ainda não é a última forma. Lembrem, o objetivo é evitar ao máximo tirar os dedos da linha do meio e no máximo mover uma tecla pra cima ou pra baixo, mas ainda tem muita tecla no Moonlander.&lt;/p&gt;

&lt;p&gt;Graças às novas funcionalidaes de firmwares de código aberto como o QMK e de aumentar a densidade de funções que uma única tecla pode ter, chegamos no ponto que podemos remover ainda mais teclas. E o Ben e diversos outros entusiastas falam agora no layout de meras 34 teclas. Seria um layout 33%, que é o menor que podemos chegar. Dá pra colocar tudo em camadas e combinações de teclas. Recomendo dar uma olhada na série de videos onde ele faz experimentações com diversas configurações até chegar nesse modelo. É fascinante ver qual é o limite dos teclados. O Ben começou tirando teclas do Moonlander e configurando no Oryx pra não usar essas teclas e foi testando pra ver até onde dava pra ir. E disso projetou um teclado próprio de só 34 teclas que ele vai mostrando no canal.&lt;/p&gt;

&lt;p&gt;E quem quiser uma versão um pouco mais extrema que o Moonlander e um pouco menos extrema que o do Ben, tem esse GergoPlex, que é até barato por 45 dólares. Bem mais acessível que os 200 do Planck ou os mais de 360 dólares do Moonlander. E além dele tem diversos outros kits. Hoje em dia povo projeta e faz impressão 3D e até os PCBs que são as placas lógicas dá pra fazer o projeto e só mandar imprimir em serviços como no JLCPCB.com. É o sonho de todo engenheiro elétrico. A comunidade de teclados abaixo de 40% tá mandando ver em opções acessíveis e com firmwares de código aberto como o QMK, só tende a aumentar as opções.&lt;/p&gt;

&lt;p&gt;E realmente é prático? Dá pra aprender a usar um Planck ou Moonlander e digitar coisas de verdade? Bom, este episódio foi digitado metade no Moonlander e metade no Planck. A parte chata é que algumas combinações de teclas são diferentes nos dois e leva alguns minutos pra eu lembrar qual era qual. Mas agora digito melhor e com menos erros e na mesma velocidade de antes. Lembra o último episódio do tutorial de WSL2 que falei que deu umas 40 páginas de Word? Também inteiro escrito com o Moonlander. E na realidade os últimos 4 episódios foram escritos totalmente com o Moonlander. Eu gostei tanto que já comprei mais um pra colocar no escritório o dia que voltarmos da pandemia.&lt;/p&gt;

&lt;p&gt;E agora sim, eu acho que finalmente falei tudo que queria falar sobre teclados. Acabou se tornando um hobby experimentar tudo que sai de novo e essa comunidade de entusiastas tá o tempo todo procurando maneiras mais eficientes de digitar. Só nesse canal, que tem mais de 100 episódios, se der uma média de 20 páginas por episódio, já soma mais de 2 mil páginas. Quase 2 livros do Cormem de algoritmos ou quase os 3 primeiros volumes de livros do Knuth. Mas lembrem-se que a escolha de qual teclado e qual layout usar depende dos seus objetivos pessoais. Se você não sente que precisa mudar nada, não tem nenhum problema. Nenhum teclado vai fazer você fica magicamente melhor. Se ficaram com dúvidas, mandem nos comentários abaixo. Se curtiram o video deixem um joinha, assinem o canal e compartilhem o video com seus amigos. A gente se vê na próxima, até mais.&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5948</id>
    <published>2022-02-15T12:52:00-03:00</published>
    <updated>2022-02-15T11:53:34-03:00</updated>
    <link href="/2022/02/15/akitando-114-o-melhor-setup-dev-com-arch-e-wsl2" rel="alternate" type="text/html">
    <title>[Akitando] #114 - O Melhor Setup Dev com Arch e WSL2</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/sjrW74Hx5Po&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;h2&gt;DESCRIPTION&lt;/h2&gt;

&lt;p&gt;Vale a pena usar WSL2? Por que Arch Linux?&lt;/p&gt;

&lt;p&gt;Se quiser conhecer a fascinante de como Linux veio parar dentro do Windows, assistam meu video original sobre WSL2 onde eu conto todos os detalhes da história do Windows que você nunca conheceu: https://www.youtube.com/watch?v=28jHuWBi72w&lt;/p&gt;

&lt;p&gt;Porém, a parte sobre a configuraçâo do Linux no WSL2 estão defasadas e pra corrigir isso fiz o video de hoje.&lt;/p&gt;

&lt;p&gt;O Guia Definitivo de Ubuntu é um dos videos que vocês mais gostaram e continua válido, assistam se ainda não viram. O de hoje vai complementar aquele video com um novo setup mais moderno pra quem usa Windows e tem máquina suficiente pra instalar Linux em cima com WSL2. E não qualquer Linux, mas o venerado Arch Linux. Vamos ver um setup para desenvolvedores web que é enxuto e poderoso!&lt;/p&gt;

&lt;p&gt;E pra quem conhece WSL2 já, vá até o fim pra uma dica de como organizar seu arquivo de projetos entre diferentes máquinas virtuais ao mesmo tempo com performance máxima!&lt;/p&gt;

&lt;p&gt;== Conteúdo&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;00:00:00 - intro&lt;/li&gt;
&lt;li&gt;00:01:03 - motivação: cuidado com Insider&lt;/li&gt;
&lt;li&gt;00:02:46 - recapitulando WSL2&lt;/li&gt;
&lt;li&gt;00:03:13 - máquina recomendada&lt;/li&gt;
&lt;li&gt;00:06:01 - instalando WSL2&lt;/li&gt;
&lt;li&gt;00:07:16 - qual distribuição Linux?&lt;/li&gt;
&lt;li&gt;00:10:26 - meu problema com APT&lt;/li&gt;
&lt;li&gt;00:12:52 - instalando ArchWSL&lt;/li&gt;
&lt;li&gt;00:17:08 - temas do Windows Terminal&lt;/li&gt;
&lt;li&gt;00:18:59 - Vim antigo e Vim moderno&lt;/li&gt;
&lt;li&gt;00:22:56 - instalando NeoVim e LunarVim&lt;/li&gt;
&lt;li&gt;00:26:38 - apresentando LunarVim&lt;/li&gt;
&lt;li&gt;00:30:48 - porque não oh-my-zsh? starship?&lt;/li&gt;
&lt;li&gt;00:32:02 - instalando YAY&lt;/li&gt;
&lt;li&gt;00:33:08 - instalando ZSH, Nerd Fonts e Powerlevel10k&lt;/li&gt;
&lt;li&gt;00:38:28 - instalando plugins (zsh-autosuggestions)&lt;/li&gt;
&lt;li&gt;00:40:05 - instalando alternativas em Rust&lt;/li&gt;
&lt;li&gt;00:42:17 - instalando e mostrando ASDF de novo&lt;/li&gt;
&lt;li&gt;00:46:47 - instalando e mostrando Docker&lt;/li&gt;
&lt;li&gt;00:49:48 - apps gráficas de Linux funcionam bem?&lt;/li&gt;
&lt;li&gt;00:53:02 - WSL, mounts P9 e HDs virtuais&lt;/li&gt;
&lt;li&gt;00:56:43 - usando HDs externos do jeito certo&lt;/li&gt;
&lt;li&gt;01:00:04 - criando HDs virtuais&lt;/li&gt;
&lt;li&gt;01:01:32 - habilitando Hyper-V (parte avançada)&lt;/li&gt;
&lt;li&gt;01:02:15 - montando e formatando HDs virtuais&lt;/li&gt;
&lt;li&gt;01:03:49 - montando HDs virtuais automaticamente&lt;/li&gt;
&lt;li&gt;01:08:37 - bônus: não esqueça das chaves ssh&lt;/li&gt;
&lt;li&gt;01:09:16 - repetindo: o que você deve fazer?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;== Links&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Windows Insider (https://insider.windows.com/en-us/about-windows-insider-program)&lt;/li&gt;
&lt;li&gt;WSL Config (https://github.com/MicrosoftDocs/WSL/blob/main/WSL/wsl-config.md)&lt;/li&gt;
&lt;li&gt;Win10 Smart Debloat (https://github.com/LeDragoX/Win10SmartDebloat)&lt;/li&gt;
&lt;li&gt;Get Windows Terminal (https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701)&lt;/li&gt;
&lt;li&gt;Windows Terminal Themes (https://windowsterminalthemes.dev/)&lt;/li&gt;
&lt;li&gt;Arch Wiki (https://wiki.archlinux.org/)&lt;/li&gt;
&lt;li&gt;ArchWSL (https://github.com/yuk7/ArchWSL)&lt;/li&gt;
&lt;li&gt;VSCodium (https://vscodium.com/)&lt;/li&gt;
&lt;li&gt;Chris@Machine (https://www.chrisatmachine.com/)&lt;/li&gt;
&lt;li&gt;LunarVim (https://www.lunarvim.org/#opinionated)&lt;/li&gt;
&lt;li&gt;Powerlevel10k (https://github.com/romkatv/powerlevel10k)&lt;/li&gt;
&lt;li&gt;zsh-autosuggestions (https://github.com/zsh-users/zsh-autosuggestions)&lt;/li&gt;
&lt;li&gt;oh-my-zsh (https://ohmyz.sh/)&lt;/li&gt;
&lt;li&gt;How to install Yay (https://www.tecmint.com/install-yay-aur-helper-in-arch-linux-and-manjaro/)&lt;/li&gt;
&lt;li&gt;ASDF (https://asdf-vm.com/guide/getting-started.html#_1-install-dependencies)&lt;/li&gt;
&lt;li&gt;Nerd Fonts (https://github.com/ryanoasis/nerd-fonts)&lt;/li&gt;
&lt;li&gt;Rewritten in Rust: Modern Alternatives of Command-Line Tools (https://zaiste.net/posts/shell-commands-rust/)&lt;/li&gt;
&lt;li&gt;Docker Desktop (https://docs.docker.com/desktop/windows/wsl/)&lt;/li&gt;
&lt;li&gt;Mount a Linux disk in WSL 2 (https://docs.microsoft.com/en-us/windows/wsl/wsl2-mount-disk)&lt;/li&gt;
&lt;li&gt;How to create advanced tasks with the Task Scheduler (https://www.digitalcitizen.life/advanced-users-task-creation-task-scheduler/)&lt;/li&gt;
&lt;li&gt;How to Shrink a WSL2 Virtual Disk (https://stephenreescarter.net/how-to-shrink-a-wsl2-virtual-disk/)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Um dos videos que vocês mais gostaram foi o meu Guia Definitivo de Ubuntu. A palavra &quot;definitivo&quot; nunca é definitivo de verdade, especialmente no mundo Linux onde as coisas mudam rápido. Faz muito tempo que fiz aquele video então hoje vou dar uma atualizada. Se você é iniciante em Linux, assista o video de Ubuntu porque tem coisas básicas que ensino lá que não vou repetir mais aqui.&lt;/p&gt;

&lt;p&gt;Quero aproveitar e falar rapidinho de WSL 2, o Windows Subsystem for Linux, que também já fiz video algum tempo atrás. Então o de hoje vai ser configurar um bom ambiente de desenvolvimento web de Linux, que independe se está no WSL ou num Linux nativo, mas vou focar um pouco mais em WSL. Se quiser pular pra configuração específica de Linux, vá direto pra este tempo aqui embaixo e se já usa WSL2 ainda assim eu ainda tenho uma dica avançada no final. Consulte os capítulos do video e os links que complementam o conteúdo na descrição abaixo.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;A motivação desse video é que recentemente eu reinstalei meu notebook do zero. O que aconteceu foi o seguinte: antes do Windows 11 ser lançado obviamente eu fiquei coçando pra experimentar. A Microsoft tem um programa chamado Insider. Se você se registra nele, vai habilitar o Windows Update pra baixar versões não-oficiais do Windows, versões beta ou mais instáveis ainda, em desenvolvimento. Eu fortemente recomendo contra fazer isso numa máquina de trabalho porque a chance de dar ruim é muito alta.&lt;/p&gt;

&lt;p&gt;E justamente deu ruim comigo. As últimas versões Insider Preview quebram alguma coisa no suporte a Docker que ele sequer consegue inicializar o serviço. Dá um erro de timeout e crasheia. E não tá errado porque o povo do Docker ainda não teve tempo de corrigir seja lá o que a Microsoft mudou no Windows na versão em desenvolvimento. Não é culpa de ninguém. Esse é o propósito do programa Insider. O desenvolvedor do Docker pode ver o que o povo dentro da Microsoft tá mexendo e ir atualizando o Docker antes da próxima atualização oficial do Windows ser lançado, que pode ser daqui algumas semanas, pode ser daqui alguns meses. Eu que tava adiantado demais. É o mesmo motivo de porque em Linux recomenda-se sempre usar uma distribuição LTS, que é Long Term Support.&lt;/p&gt;

&lt;p&gt;O problema é que a versão Insider Preview de desenvolvimento não tem como dar rollback pra versão estável anterior. Precisa fazer um clean install, instalar tudo do zero. Como o Windows 11 já foi oficialmente lançado, não tem porque eu ficar tendo dor de cabeça na versão Insider mais, então fiz o de sempre. Baixei o ISO da versão estável do Windows, queimei um pendrive, bootei e reinstalei tudo do zero. Agora estou com um Windows 11 limpo. Já que ia ter que instalar o WSL tudo de novo, aproveitei pra registrar tudo e mostrar pra vocês.&lt;/p&gt;

&lt;p&gt;Vamos recapitular esse tal de WSL2. É uma camada de suporte pra facilitar e integrar uma máquina virtual Linux rodando sobre o hypervisor da própria Microsoft chamado Hyper-V. Máquinas virtuais e containers como Docker são coisas diferentes, por isso eu já tinha feito videos antes explicando a diferença entre os dois e recomendo que assista caso não saiba a diferença. Na prática é uma máquina virtual, só mais conveniente do que rodar dentro de um Virtual Box ou Virt Manager.&lt;/p&gt;

&lt;p&gt;Por ser virtualização, significa que na mesma máquina estamos bootando um Windows e em cima dele bootando um Linux. Os recursos da sua máquina como disco, memória RAM, núcleos da CPU e GPU vão ser compartilhados entre os dois sistemas operacionais e por isso é mais pesado do que rodar só um dos dois sozinho. Não é recomendado rodar um WSL ou qualquer máquina virtual se tiver menos que 4 núcleos de CPU e menos de 8Gb de RAM.&lt;/p&gt;

&lt;p&gt;Vai rodar se tiver menos? Vai, mas vai ser mais lento porque se faltar CPU vai ficar bem lento, se faltar RAM vai fazer swap em disco e isso vai deixar bem lento. O ideal é pelo menos, pelo menos, 8 núcleos de CPU e pelo menos 16Gb de RAM. Isso dito, não fiquem mandando nos comentários &quot;ah, e minha máquina com 4Gb?&quot;, faça a conta, 4 é menos que 8 que eu acabei de falar, vai ser lento. Se você não for fizer nada pesado, não tem problema, mas pra trabalhar vai ser super sofrido.&lt;/p&gt;

&lt;p&gt;Se tiver máquina mais fraca, recomendo não usar WSL nem nenhum tipo de máquina virtual e sim instalar uma distribuição Linux mais leve como Puppy Linux ou Lubuntu. Não vai ser uma experiência bonita porque a intenção dessas distros é ser leve. Mas mesmo com uma distro que faça boot usando poucos recursos, se abrir um monte de abas no Chrome ou se começar a tentar subir muitos containers de Docker, super rápido vai faltar RAM e CPU e, de novo, vai ser uma experiência bem lenta.&lt;/p&gt;

&lt;p&gt;Então pro resto do video vou assumir que você tem uma máquina com pelo menos um Intel Core i5 de 9a geração pra cima ou um Ryzen 5 de 3a geração pra cima e pelo menos 16 Gb de RAM com pelo menos SSD como armazenamento. Aliás, se tem uma máquina fraca e não tem dinheiro pra atualizar tudo, no mínimo do mínimo tente trocar seu HD mecânico por um SSD, é o que mais vai fazer diferença na performance geral. Em segundo lugar é aumentar RAM.&lt;/p&gt;

&lt;p&gt;Meu notebook de trabalho que uso pra viajar é um Asus ROG Zephyrus G14. Ele tem um Ryzen 9 5900 HS com 8 núcleos e 16 threads, mais 40Gb de memória DDR4 e um NVME de 4 terabytes, portanto tem recursos mais que sobrando pra rodar um WSL2. Como falei antes, tá com Windows 11 recém instalado. E a primeira coisa que recomendo toda vez que se acaba de instalar Windows é rodar algum script de debloating, ou seja, um script que desabilita inutilidades como a telemetria que fica mandando dados pros servidores da Microsoft sem você saber. Também desabilita Cortana que ninguém usa e coisas assim que ficam roubando recursos da sua máquina.&lt;/p&gt;

&lt;p&gt;Um dos scripts mais conhecidos é do LeDragoX. Não vou mostrar como rodar aqui porque tá explicado na página do projeto no GitHub. Se não conseguir seguir essas instruções, também não vai conseguir entender quase nada do video então sugiro que gaste 1 minuto pra tentar abrir a interface gráfica da ferramenta dele. Precisa saber o que é PowerShell, coisa que explico no meu video original sobre WSL2. Assiste lá depois.&lt;/p&gt;

&lt;p&gt;Uma vez que desabilitamos o grosso de coisas que um Windows sobe automaticamente, mesmo assim ainda vai consumir quase 5Gb logo depois do boot. Por isso falei que com menos de 8Gb, no Windows, as coisas vão ficar lentas. Vamos começar instalando o tal WSL e se estiver nas versões mais recentes de Windows pode só abrir uma janela de PowerShell com permissões de administrador e digitar &lt;code&gt;wsl --install&lt;/code&gt;. Agora vai tomar um café até baixar tudo e instalar os pacotes que precisa.&lt;/p&gt;

&lt;p&gt;Não esqueça de instalar o Windows Terminal da loja da Microsoft ou usando algum instalador como o Chocolatey. Se você não é de Linux talvez não entenda ainda a importância de um bom emulador de terminal. No Mac temos o iTerm2, em distros Linux temos o Alacritty que é o melhor ultimamente, mas o Windows Terminal devo dizer que tá muito bem feito e na mesma categoria dos que acabei de falar. Obviamente use o WSL nesse terminal. Depois disso precisamos reiniciar o Windows, como sempre.&lt;/p&gt;

&lt;p&gt;A instalação padrão de WSL já vai instalar o Ubuntu mais novo automaticamente e quando reiniciar vai pedir pra criar um usuário Linux. Esse passo é opcional, mas como não custa nada, vamos criar. Colocamos uma senha forte e pronto, estamos dentro de um Ubuntu, dentro do Windows. Se é sua primeira vez instalando WSL, essa visão deve ser bizarra.&lt;/p&gt;

&lt;p&gt;Qualquer outro tutorial ia continuar configurando o Ubuntu, mas como sou eu, vou ignorar totalmente porque pessoalmente prefiro distros baseadas em ArchLinux. Sem me alongar demais, hoje em dia você tem 3 grandes famílias de distribuições Linux. Primeiro as baseadas em Debian, com pacotes formato DEB e o gerenciador APT como o Ubuntu, Elementary, Mint, Deepin, Pop OS.&lt;/p&gt;

&lt;p&gt;Em segundo tem as baseadas no antigo RedHat, como o Fedora, CentOS, SuSe, com pacotes formato RPM e o gerenciador DNF. O mundo mais enterprise comercial tende a suportar o ecossistema RedHat, então a gente vê marcas como Oracle rodando seus sistemas num CentOS.&lt;/p&gt;

&lt;p&gt;O Ubuntu meio que dominou os derivados de Debian. Debian era considerado uma das distribuições mais estáveis, mas pra isso ela sempre vinha com programas em versões um pouco mais defasadas. Lembra o que eu falei sobre o programa Insider do Windows? O Debian é como se fosse o Windows 10 quando já temos o Windows 11. Ele prefere estar mais atrasado pra não comprometer estabilidade.&lt;/p&gt;

&lt;p&gt;Pra gerenciar isso o mundo Ubuntu tem o conceito de LTS. Por exemplo, no momento que estou gravando este episódio a versão mais nova do Ubuntu é a 21.10, mas a versão que eles recomendam como mais estável, e que vão dar suporte por alguns anos é a 20.04 que foi lançada 1 ano atrás. A esta altura a maioria dos principais software de Linux tem suporte ao Ubuntu 20.04 mas nem todos suportam a 21.10 ainda.&lt;/p&gt;

&lt;p&gt;Ainda existem diversas outras famílias de distribuições que não são nem baseadas em Debian e nem em RedHat, como Slackware ou Gentoo. Mas essas são pra usuários mais avançados. Não recomendo pra iniciantes que ainda não tem costume de fuçar muito documentação e fóruns. Pra mim o melhor meio do caminho entre facilidade e coisas novas é a família Arch Linux que incluem distribuições como Manjaro, Endeavor, Garuda e outras.&lt;/p&gt;

&lt;p&gt;Na dúvida eu sempre falo: escolha o Manjaro e seja feliz. O padrão no Manjaro é instalar com GNOME que é meio pesado, mas tem versão com KDE ou XFCE que são gerenciadores de janela mais leves e customizáveis. E você sempre pode instalar outras interfaces gráficas como o Cinnamon ou LXQT. Enfim, tem bastante coisa pra se divertir.&lt;/p&gt;

&lt;p&gt;Uma das grandes vantagens do Arch é o gerenciador de pacotes Pacman e o AUR que é o Arch User Repository. Aqui vale uma explicação. Todo tutorial de Ubuntu por aí vai fazer você digitar um comando como &lt;code&gt;apt install&lt;/code&gt; pra instalar algum pacote. Esse comando mantém um banco de dados com nomes e versões de pacotes que é atualizado sempre que se faz &lt;code&gt;apt update&lt;/code&gt;. Ele faz isso se conectando nos diversos servidores oficiais da Canonical, que faz o Ubuntu, e atualiza os bancos de dados. Daí quando fazemos &lt;code&gt;apt install&lt;/code&gt; vai em algum servidor e baixa o pacote do programa que quer instalar. Tudo costuma ser muito simples.&lt;/p&gt;

&lt;p&gt;Mas isso implica que todo software do mundo precisaria estar registrado nesses servidores da Canonical, incluindo a versão mais nova e todas as antigas. É muita coisa pra uma empresa só conseguir catalogar e manter atualizado. Por isso é possível outras empresas terem repositórios próprios só com o arquivo de pacotes dos seus software em particular. Por exemplo, se quiser instalar o Docker, os pacotes não estão nos servidores da Canonical. Vamos abrir o tutorial de instalação oficial do Docker.&lt;/p&gt;

&lt;p&gt;Tá vendo? Não é só um simples &lt;code&gt;apt install docker&lt;/code&gt;, tem vários passos. Em resumo, primeiro precisa instalar alguns pacotes de infraestrutura do Ubuntu no passo 1. No passo 2 precisamos baixar a chave pública em formato GPG do Docker. Se não sabe o que é assinatura e chaves assimétricas, assista meus videos explicando o básico sobre criptografia. Mas em resumo, todo gerenciador de pacotes que se preza vai tentar garantir que você não instale malware de servidores falsos de hackers tentando se passar pelo Docker ou outra empresa.&lt;/p&gt;

&lt;p&gt;Pra fazer isso eles assinam os pacotes e os dados transmitidos dos seus servidores usando chaves assimétricas. Eles assinam os pacotes com uma chave privada que ninguém nunca vai ter acesso. Daí instalamos essa chave GPG que é a chave pública. Tudo que a chave privada assina, só a chave pública consegue acessar e vice-versa. Um hacker não tem acesso à chave privada, então não vai conseguir assinar malwares.&lt;/p&gt;

&lt;p&gt;No passo 3 precisamos indicar pra ferramenta APT onde está a listagem de pacotes nas mais diversas versões. Nessa lista tem coisas como versão mas também quais pacotes tem pra quais arquiteturas como x64 da Intel, ARM de Mac e assim por diante. Essa linha estranha é um comando pra adicionar um arquivo no diretório de sources do apt, que lista de onde que é pra baixar.&lt;/p&gt;

&lt;p&gt;Finalmente no passo 4 fazemos primeiro um &lt;code&gt;apt update&lt;/code&gt; pra baixar a listagem desse novo source e atualizar o banco de dados local e só agora podemos fazer o &lt;code&gt;apt install&lt;/code&gt;. Quando fizer isso várias vezes, vai ficar meio automático, mas viu quantos passos precisamos fazer pra instalar um pacote que a Canonical não controla?&lt;/p&gt;

&lt;p&gt;Como é instalar o mesmo Docker num Arch? Pra começar, a documentação existe no próprio wiki do Arch. E temos dois pacotes, ou o &lt;code&gt;docker&lt;/code&gt; ou o &lt;code&gt;docker-git&lt;/code&gt; que provavelmente é uma versão mais nova e potencialmente mais instável. Mas por padrão podemos fazer só &lt;code&gt;pacman install docker&lt;/code&gt;. Pronto, só isso.&lt;/p&gt;

&lt;p&gt;Pacman, assim como Apt ou DNF é um gerenciador e instalador de pacotes. E os repositórios do Arch costumam ter mais pacotes do que os equivalente de Ubuntu ou Fedora. E o que não tem, não precisamos fazer toda a burocracia de procurar um repositório de terceiros, instalar chaves e bla bla como fizemos no caso do Docker. A comunidade mantém um repositório de usuários que é o Arch User Repository ou AUR. A gente pode baixar um script de lá, que serve pra construir um pacote do zero e usamos pacman pra instalar. Mas tem um jeito ainda mais fácil que já vou mostrar.&lt;/p&gt;

&lt;p&gt;Isso tudo dito, a Microsoft ainda não tem suporte oficial pra distros Arch no WSL, mas existe um projeto no GitHub chamado ArchWSL. Eu uso acho que desde 2018 e nunca tive problemas. É um Arch mínimo, mais leve que um Manjaro, mais leve que um Ubuntu. E a instalação é bem trivial. Pra isso vamos na página de GitHub do projeto baixar um zip que tem tudo que precisamos. Descompacte onde quiser mas eu costumo deixar na raíz do C: mesmo.&lt;/p&gt;

&lt;p&gt;Agora abra um command prompt ou powershell no Windows Terminal, dê &lt;code&gt;cd&lt;/code&gt; pro diretório onde descompactou o Arch e execute. Em dois segundos ele vai se registrar no WSL. Como o Windows Terminal já tava aberto ele ainda não detecta o novo Arch, mas basta reiniciar e voilá, já apareceu. O que eu gosto de fazer primeiro é configurar o Terminal pra sempre abrir o Arch primeiro.&lt;/p&gt;

&lt;p&gt;Da primeira vez que entramos nele, estamos logados como usuário &lt;code&gt;root&lt;/code&gt; que, como já expliquei no guia de Ubuntu, não se deve fazer. Lembra quando o Ubuntu oficial que o WSL instalou nos forçou a criar um usuário não-root? No caso do Arch vamos fazer isso manualmente. Uma das coisas que afasta muitos iniciantes do Arch é que ele nos obriga um pouco a saber o que estamos fazendo e não faz tanta coisa automaticamente. Por isso que é bom pra aprender.&lt;/p&gt;

&lt;p&gt;No próprio GitHub do ArchWSL tem um link pra documentação. Vamos seguir e olha só. Daqui você não precisa de talento tampouco habilidade. Basta saber fazer copy e paste e saber ler. Vamos copiar esta primeira linha e colar no terminal com o Arch aberto. Isso vai configurar o arquivo sudoers pra permitir que comandos com &lt;code&gt;sudo&lt;/code&gt; possam ser executados pra elevar privilégio a partir de qualquer usuário. O ideal não é abrir tudo pra &lt;code&gt;ALL&lt;/code&gt; como fala aqui, mas como é máquina de desenvolvedor, não tem tanto problema. Depois estude sobre sudoers se não conhecia.&lt;/p&gt;

&lt;p&gt;Em seguida são comandos básicos de &lt;code&gt;useradd&lt;/code&gt; pra criar um novo usuário. Faça copy e paste mas obviamente digite um usuário com seu nome, né. Daí usamos o comando &lt;code&gt;passwd&lt;/code&gt; passando o usuário que acabamos de criar pra configurar uma senha segura. Abrimos outra aba com um command prompt, e podemos fechar a que estava com o Arch aberto, porque precisamos configurar o WSL pra quando abrir o Arch de novo use o usuário que acabamos de criar em vez de `root. Então mais um copy e paste.&lt;/p&gt;

&lt;p&gt;Pronto, se abrir o Arch no terminal de novo e dermos o comando &lt;code&gt;whoami&lt;/code&gt; diz que é o novo usuário. A partir daqui, se precisarmos fazer alguma coisa que exija permissão de root é só começar o comando com &lt;code&gt;sudo&lt;/code&gt;. E a primeira coisa que se deve fazer logo que se instala uma distro linux é atualizar todos os pacotes. No caso deste Arch, precisa fazer algumas coisas antes. De novo, a documentação já diz tudo. Só copiar e colar esses comandos de &lt;code&gt;pacman&lt;/code&gt; que vai inicializar e popular as chaves que o Arch usa pra checar os pacotes.&lt;/p&gt;

&lt;p&gt;O pacman é um pouco chatinho pra quem nunca viu porque não tem comandos fáceis como &lt;code&gt;install&lt;/code&gt;, em vez disso o equivalente é &lt;code&gt;-S&lt;/code&gt; maiúsculo que significa &quot;sync&quot;. Daí adicionamos a opção &lt;code&gt;y&lt;/code&gt; que se não me engano é pra atualizar o banco de dados com as listagens do servidor oficial do arch. E a opção &lt;code&gt;u&lt;/code&gt; é pra atualizar os pacotes. E eu faço &lt;code&gt;yyuu&lt;/code&gt; duplicado pra forçar mesmo se já estiver tudo atualizado. O normal é fazer só &lt;code&gt;-Syu&lt;/code&gt; sem duplicar, mas quando alguma coisa dá problema ou agora, que é primeira vez, acabo fazendo assim.&lt;/p&gt;

&lt;p&gt;E esqueci de fazer uma coisa, então vamos dar &lt;code&gt;control-C&lt;/code&gt; pra cancelar e abrir o arquivo &lt;code&gt;/etc/pacman.conf&lt;/code&gt; com o editor &lt;code&gt;nano&lt;/code&gt;. Procurar a palavra &lt;code&gt;Verbose&lt;/code&gt; e adicionar uma nova linha embaixo dizendo &lt;code&gt;ParallelDownloads = 5&lt;/code&gt;. Por padrão o pacman baixa um arquivo de cada vez e isso é bem demorado. Se você tem uma boa internet banda larga, pode testar, mas por volta de 5 downloads simultâneos ou até mais, aguenta. Agora sim, vamos repetir o comando do pacman pra atualizar tudo, e agora vai ser mais rápido.&lt;/p&gt;

&lt;p&gt;Ele vai checar tudo que precisa ser atualizado, mostrar a lista de pacotes que vai baixar e pedir pra ir confirmando. Olha só como mostra 5 pacotes sendo baixados simultaneamente. Bem mais rápido, mas aí temos esse problema aqui. Um erro de chaves que ele não reconheceu. E isso porque eu fui burro e a documentação do ArchWSL tá incompleta. Faltou fazer &lt;code&gt;pacman -S archlinux-keyring&lt;/code&gt; que instala as chaves mais novas que precisa pra instalar os pacotes. Então vamos instalar e repetir o comando de atualizar tudo.&lt;/p&gt;

&lt;p&gt;Agora sim, foi até o fim. E é isso. Temos um Arch Linux rodando no WSL 2. Fim do video! Brincadeira. Antes de continuar, vamos deixar nosso terminal um pouco menos feio. Tá tudo com fundo preto. Pra mudar isso podemos ir no site windows terminal themes .dev. Dêem uma fuçada. Eu pessoalmente gosto do &quot;Moonlight II&quot; ou variações de &quot;Monokai&quot;. Clique em &quot;Get Theme&quot; e ele vai copiar pro clipboard.&lt;/p&gt;

&lt;p&gt;No Windows Terminal faça &quot;ctrl-virgula&quot; ou na setinha na barra de título escolha &quot;Settings&quot;. Lá embaixo tem a opção de abrir o arquivo de configurações que é em formato JSON. Abre no Notepad mesmo por enquanto e vai lá pro fim. Logo embaixo do último tema, garante que tem uma vírgula e cole o novo tema. Cuidado pra não quebrar o JSON. Não precisa reiniciar, só salvar o arquivo. Agora seleciono o &quot;Arch&quot; no menu e vamos pra &quot;Appearance&quot;. Pronto, posso selecionar o &quot;Moonlight II&quot; e embaixo posso aumentar a fonte pra 16 só pra ficar mais fácil de ver aqui no video.&lt;/p&gt;

&lt;p&gt;Falando em editor, o próximo passo é instalar um bom editor de textos. Na prática, basta instalar o Visual Studio Code que ele se integra plug and play com o WSL. Vamos ver? Vai na loja do Windows mesmo, procura por Visual Studio, seleciona e instala. Pra ter a integração com o terminal precisa fechar e abrir de novo. Pronto, basta ir pra algum diretório e digitar &lt;code&gt;code .&lt;/code&gt; e olha só como abre o Visual Studio listando os arquivos de Linux.&lt;/p&gt;

&lt;p&gt;O VSCode funciona super bem e se você prefere uma versão mais light e com mais privacidade, procure pela alternativa chamada VSCodium, que é a mesma proposta do Chromium em relação ao Chrome. VSCode e Chrome tem a péssima mania de se comunicar com servidores da Microsoft ou do Google, respectivamente, e ficar comunicando coisas que não sabemos. O VSCodium e Chromium desligam essas comunicações desnecessárias. Se você é do tipo mais paranóico com privacidade, instale essas alternativas.&lt;/p&gt;

&lt;p&gt;Mas de vez em quando ainda prefiro ficar só no terminal mesmo. E hoje em dia ainda não tem nada melhor do que Vim na minha humilde opinião. Esse nosso ArchWSL já vem com vim pré-instalado. O básico que você tem que saber é que ele tem um modo de edição e um modo de navegação. Dá pra ficar horas falando de vim e no video de Ubuntu já mostrei o básico. Se nunca tentou, recomendo que dê uma chance de verdade. Acho importante conhecer o básico de Vim pra mudar a perspectiva do que um editor de textos realmente deveria fazer.&lt;/p&gt;

&lt;p&gt;O Vim foi criado numa época onde se usava terminais remotos em redes super lentas. Por isso não existia o conceito de IDEs como um Visual Studio. Seria pesado demais. Quando conversei com o Uncle Bob a gente falou disso, recomendo que assistam nossa live que está gravada no canal. Mas por isso o Vim padrão parece tão espartano, com comandos super curtos, pra minimizar ao máximo os comandos enviados ao servidor a partir do terminal remoto.&lt;/p&gt;

&lt;p&gt;Aliás, é por isso que este programa que estamos usando, esta janela do Windows Terminal se chama terminal ou, mais corretamente, emulador de terminal. É um terminal remoto que se conecta com o servidor de terminal rodando por baixo dos panos, localmente. O &quot;Linux&quot; em si não roda &quot;dentro&quot; desse programa, por isso podemos fechar o Terminal mas o Linux em si continua rodando por baixo. E quando abrimos um novo Windows Terminal, ele conecta de novo no servidor por baixo. Pense no Linux como se fosse um servidor web e o terminal como se fosse um navegador web, é o mesmo conceito.&lt;/p&gt;

&lt;p&gt;Enfim, o Vim na verdade é a evolução do editor original que se chamava só &quot;Vi&quot;, do Bill Joy, que era da Sun Microsystems. Já o Vim é um fork do Brian Moolenar. Depois do Vim versão 8 surgiu um fork dele chamado de NeoVim que tem a proposta de modernizar o Vim, em particular modernizar o código-fonte, que tem coisa de décadas atrás que nem precisamos mais e funcionalidades que hoje são possíveis. Uma das coisas que adicionaram faz poucos meses foi suporte à linguagem Lua.&lt;/p&gt;

&lt;p&gt;O Vim sempre teve plugins escritos numa linguagem própria chamada VimScript, que é uma linguagem bem feia e chata de trabalhar, meio como scripts em Bash. É super antiquado, super fácil de criar bugs acidentais. Por isso pouca gente tem paciência pra escrever plugins nessa linguagem. Já Lua é uma linguagem mais moderna e madura, muito mais difundida, especialmente pra quem programa games. Além de ser uma linguagem super leve e fácil de embutir em qualquer programa.&lt;/p&gt;

&lt;p&gt;Tem um canal no YouTube que recomendo que assinem, do Chris at Machine. Ele veio acompanhando essa nova integração e criou um projeto no GitHub chamado LunarVim, que é uma coleção de plugins escritos em Vim e uma configuração super completa que transforma o NeoVim praticamente num Visual Studio Code, só que mais leve e mais produtivo. E &quot;praticamente&quot; porque uma das características mais importantes do VSCode são seus servidores de linguagens.&lt;/p&gt;

&lt;p&gt;Explicando bem curto e porcamente, o VSCode foi inteligente em fazer um editor que por si só é leve e não tem suporte a nenhuma linguagem. É um editor neutro se não configurar nada. Mas quando instala um plugin pra linguagem Ruby, por baixo ele instala um SolarGraph como servidor. Daí o VSCode passa a ser um cliente desse servidor que vai analizar o código Ruby que estamos digitando e dar suporte a coisas como autocomplete, debug, e mais. E cada servidor de linguagens diferentes vai implementar o Language Server Protocol ou LSP que é a API que o VSCode entende.&lt;/p&gt;

&lt;p&gt;Agora, se eu pegar outro editor e fazer ele entender o protocolo LSP, dá pra usar os mesmos servidores. Por isso consigo trocar o cliente VSCode e substituir por NeoVim. LSP é como se fosse HTML, o VSCode é como se fosse o navegador Chrome ou Edge e o NeoVim poderia ser como o Firefox, sacaram? E o tal LunarVim que o Chris fez já pré-instala todos os plugins necessários pra integrar com LSP. Então vamos instalar pra ver.&lt;/p&gt;

&lt;p&gt;Primeiro, precisamos do NeoVim. E hoje a versão que tem no repositório oficial já serve. Meses atrás precisava compilar o NeoVim manualmente porque não estava disponível esse suporte na versão oficial, só na beta. Pra instalar no Arch é fácil, só fazer &lt;code&gt;sudo pacman -S neovim&lt;/code&gt; e veja como lista pacotes de Lua. Sabemos que é a versão que precisamos.&lt;/p&gt;

&lt;p&gt;Quando terminar de instalar, vamos pra página no GitHub do LunarVim e tem um script instalador pra facilitar nossa vida. Só copiar essa linha e colar no terminal. E de cara já começa reclamando que não temos Git instalado. Bora instalar &lt;code&gt;pacman -S git&lt;/code&gt; como ele manda.&lt;/p&gt;

&lt;p&gt;Esse script vai ajudar a instalar as dependências que precisamos. A primeira coisa são pacotes de javascript, daí reclama que não tem nodejs instalado então vamos fazer &lt;code&gt;sudo pacman -S yarn npm&lt;/code&gt; pra instalar... Agora com a seta pra cima repetimos o instalador do LunarVim e damos yes e pronto, tá usando o &lt;code&gt;yarn&lt;/code&gt; pra instalar o que precisa.&lt;/p&gt;

&lt;p&gt;No próximo passo pede pra instalar dependências de Python e como já temos &lt;code&gt;pip&lt;/code&gt; instalado, prossegue instalando o que precisa... Essa foi fácil, e no passo seguinte ele quer instalar dependências de Rust, mas não temos Rust então o instalador pára de novo. Vamos fazer &lt;code&gt;sudo pacman -S rust&lt;/code&gt; pra poder usar o &lt;code&gt;cargo&lt;/code&gt; pra instalar as ferramentas de Rust.&lt;/p&gt;

&lt;p&gt;Assim como &lt;code&gt;yarn&lt;/code&gt; é instalador de dependências de Javascript, &lt;code&gt;pip&lt;/code&gt; é instalador de dependências de Python, o &lt;code&gt;cargo&lt;/code&gt; é instalador de dependências de Rust. O Rust é uma linguagem compilada que gera binários nativos super otimizados e com segurança superior à de C puro. E no meio da instalação ele deu um erro. Parece que não conseguiu compilar o jemalloc-sys ou pam.&lt;/p&gt;

&lt;p&gt;Eu já tinha ido no Google antes e o que aconteceu é que o Rust se integra com binários compilados em C também e nesse ponto ele precisou compilar alguma coisa de C só que não instalamos o ferramental pra isso ainda. Num Ubuntu isso seria o pacote &lt;code&gt;build-essential&lt;/code&gt;, mas não existe no Arch. O equivalente é &lt;code&gt;sudo pacman -S base-devel&lt;/code&gt;. Só ir dando &lt;code&gt;yes&lt;/code&gt; no que tá perguntando, mas sempre leia antes, lógico.&lt;/p&gt;

&lt;p&gt;Pronto. Seta pra cima, vamos repetir os mesmos passos tudo de novo. Ele sempre passa pelos passos de javascript e python mas agora é mais rápido porque já instalou esses passos. O Rust volta a compilar o que falta e agora .... vai até o fim. Mas atenção que fala que precisa colocar o diretório &lt;code&gt;$HOME/.cargo/bin&lt;/code&gt; no PATH senão não vai achar as ferramentas que acabou de compilar. E no final o instalador também avisa que não vamos usar o comando &lt;code&gt;nvim&lt;/code&gt; que inicia o NeoVim normalmente mas sim o &lt;code&gt;lvim&lt;/code&gt; e pra isso precisamos adicionar o &lt;code&gt;$HOME/.local/bin&lt;/code&gt; no PATH. Eu explico o que é PATH no episódio do guia de Ubuntu.&lt;/p&gt;

&lt;p&gt;Só pra agora podermos ver o LunarVim, vamos exportar o PATH manualmente adicionando esses dois diretórios... Pronto. Agora podemos abrir &lt;code&gt;lvim&lt;/code&gt; e olha só que diferença. No LunarVim apertamos &quot;espaço&quot; pra abrir esse menu e com a opção &quot;e&quot; de &quot;explorer&quot; temos um painel com o projeto organizado como árvore, todo configurado com ícones e bem bonito. Podemos navegar pra cima e pra baixo com as teclas &quot;j&quot; e &quot;k&quot; e com a tecla &quot;o&quot; podemos abrir o arquivo embaixo do cursor.&lt;/p&gt;

&lt;p&gt;Vamos abrir o arquivo &quot;config.lua&quot; pra adicionar uma configuração pra corrigir um pequeno bugzinho que essa versão ainda tem com esse negócio de abrir menu com &quot;espaço&quot;. Ele é rápido demais e pode dar conflito com outras combinações de teclas então colocamos &lt;code&gt;vim.opt.timeoutlen = 500&lt;/code&gt; milissegundos como gambiarra pra contornar e pronto.&lt;/p&gt;

&lt;p&gt;O video de hoje não é sobre Vim então não quero me alongar demais nele, mas acho que vale a pena fazer uma pequena tangente pra mostrar um pouquinho de porque instalei o LunarVim. Então vou me adiantar um pouco e abrir um projetinho besta em Rails antigo meu só pra dar exemplo. Primeira coisa é que posso fazer ele ficar parecido com uma IDE apertando espaço, que abre esse menu principal, e com &quot;e&quot; eu abro o explorer do lado, que é o plugin NvimTree. A partir dele posso usar o mouse normalmente pra abrir arquivos.&lt;/p&gt;

&lt;p&gt;Por acaso abri um arquivo de Ruby. Lá embaixo dá pra ver a mensagem de &quot;LSP inactive&quot;, ou seja, não tem nenhum servidor de linguagem analisando este arquivo. Mas se esperar alguns segundos, olha só, assim como o VSCode, o LunarVim achou um LSP de Ruby, no caso o Solargraph, e instalou sozinho e a partir daqui todo arquivo de Ruby que eu abrir ele vai analizar.&lt;/p&gt;

&lt;p&gt;Pra abrir mais arquivos, posso voltar pro painel do explorer com o mouse, ou como eu prefiro, com o teclado fazendo ctrl+h pra ir pro painel da esquerda ou ctrl+l pro painel da direita, e os mesmos comandos de navegação como &quot;j&quot; pra ir uma linha pra baixo ou &quot;k&quot; pra uma linha pra cima funcionam igual. E com &quot;o&quot; posso abrir o arquivo embaixo do cursor.&lt;/p&gt;

&lt;p&gt;Mas esse explorer usa muito espaço, então posso fechar ele. Em vez de usar o explorer posso abrir o menu principal com &quot;espaço&quot; e apertar &quot;f&quot;, que vai abrir o plugin Telescope que usa a ferramenta &quot;fd&quot; que é uma das que ele instalou de Rust no começo, lembram? E posso fazer um fuzzy find direto do nome de algum arquivo, como &quot;archives&quot; e abro o controller.&lt;/p&gt;

&lt;p&gt;Notem que o editor parece meio poluído à primeira vista, mas isso é o solargraph que analizou o código e fica dando dicas e avisos de coisas que podem melhorar ou bugs óbvios que precisam ser corrigidos. Toda linguagem moderna hoje tem um LSP que faz essas coisas. Por exemplo, nesse trecho ele fala pra evitar usar chaves num bloco de múltiplas linhas, em Ruby é mais bonito usar &quot;do-end&quot; em vez disso, e pronto, o aviso sumiu.&lt;/p&gt;

&lt;p&gt;Outro exemplo é a sintaxe de atoms em hash que mudou desde a versão 1.9 do Ruby, antes era usando um fat arrow, hoje é usando dois pontos que nem um JSON. E mesma coisa, substituir chaves por &quot;do-end&quot;. E aqui embaixo ele fala pra não fazer comparação com zero e sim usar o método &quot;empty?&quot; pra checar se a variável está vazia ou é zero.&lt;/p&gt;

&lt;p&gt;Além disso, ele nos ajuda quando temos dúvida numa função. Aqui eu digito ponto e ele já me dá uma lista de métodos que a String aceita. E se eu sair navegando, ele até me dá a documentação da função caso eu não me lembre ou tenha dúvidas. E se for digitando, ele vai filtrando na lista até eu achar o método que eu estava precisando. Tudo coisa básica que se você já usou uma IDE moderna já tinha, mas agora disponível dentro do terminal no vim.&lt;/p&gt;

&lt;p&gt;E só como último exemplo de coisas avançadas, alguns LSPs tem recursos de refactoring. Vamos procurar o arquivo do model chamado Post. E se eu quisesse renomear essa classe de Post pra Article e não ter que ir abrindo arquivo a arquivo de todo lugar que a classe Post é usado? Posso abrir o menu principal com espaço, apertar &quot;l&quot; pra abrir o sub-menu do LSP e olha que ele tem &quot;rename&quot;. Lá embaixo eu mudo de Post pra Article e deixo o LSP trabalhar.&lt;/p&gt;

&lt;p&gt;Notem que lá em cima apareceu um monte de aba de arquivos que ele abriu. Ele ainda não salvou nada. Só abriu todo arquivo onde aparece a classe Post e substituiu por Article. Abrindo aquele mesmo arquivo do ArchivesController, olha como onde tava Post agora tem Article no lugar direitinho. Mesma coisa no PostsController e assim por diante. Eu devo ir checando e salvando um a um pra garantir que fez tudo certo e, lógico, no final eu teria testes automatizados pra garantir que nada quebrou. Mas é uma das coisas que uma IDE como o LunarVim consegue fazer. E isso foram só 2 minutos de demonstração, tem bem mais coisas por baixo pra facilitar nossa vida.&lt;/p&gt;

&lt;p&gt;Se você se animar a dar uma chance pro jeito Vim de navegar, em breve vai conseguir programar sem precisar ficar tirando a mão pro mouse toda hora. É muito mais confortável e até mais ergonômico de trabalhar. Além disso ele renderiza tudo em modo texto dentro do Terminal, o que usa muito menos recursos do seu sistema do que o VSCode que, por baixo, é tão pesado quanto um navegador web, como programas feitos em Electron. O VSCode, recém instalado, logo que abre já consome de 250 a 300 mega de RAM. O LunarVim fica na faixa de menos de 150 mega. É quase metade de recursos. Mas claro, hoje em dia depende mais dos LSP que rodam por trás analisando seu código.&lt;/p&gt;

&lt;p&gt;Voltando pra nossa instalação, precisamos configurar o PATH de forma permanente e pra isso é só adicionar o mesmo comando de export que fizemos no arquivo &lt;code&gt;.bashrc&lt;/code&gt;. Mas eu pessoalmente prefiro o shell ZSH. Eu sei, tem alguns que vão comentar sobre o Fish, mas eu ainda gosto do ZSH mesmo. Pra quem não sabe, além de Bash existem outras linhas de comando ou shells e suporte a scripts diferentes. O Bash é meio antiquado pra hoje, mas não tem nenhum problema de ficar nele.&lt;/p&gt;

&lt;p&gt;No video do guia de Ubuntu eu baixei um conjunto de dotfiles de um usuário chamado skwp pra customizar meu prompt, vim e tudo mais, mas ele é muito pouco suportado e hoje em dia existem opções muito melhores. Essa é a parte do video antigo que ficou desfasado. Mesmo na época já existia o Oh-my-zsh que eu testei por um tempo, mas nenhum dos plugins foi muito importante pra mim e acho um pouco pesado. Muita gente gosta de usar só o starship, que faz um prompt até que minimalista e bonitinho de ver.&lt;/p&gt;

&lt;p&gt;Meu preferido e o que achei mais fácil de instalar e configurar é o Powerlevel10k. Antes precisamos instalar uma outra ferramenta de Arch Linux pra facilitar nossa vida. Lembra dos pacotes que a comunidade mantém no AUR que mencionei? Pra instalar direto de lá não dá pra usar Pacman. Tem vários gerenciadores de AUR mas o que mais gosto é o Yay. E muitos tutoriais mais novos já assumem que você tem yay. Dá pra saber que um artigo aleatório é mais antigo se em vez de yay ele manda usar yaourt, que é desafado. Então vamos lá.&lt;/p&gt;

&lt;p&gt;Na página de GitHub do Yay vamos descer pras instruções. A primeira coisa é instalar os pacotes de git e base-devel, mas acabamos de instalar então pula. Próxima linha é fazer &lt;code&gt;git clone&lt;/code&gt; do projeto. Eu prefiro dar &lt;code&gt;cd&lt;/code&gt; pro diretório temporário do sistema porque não vamos usar esse clone pra mais nada depois de instalar. Feito o clone damos &lt;code&gt;cd&lt;/code&gt; pra ele e &quot;make package&quot; com opção pra instalar logo depois de construir o pacote. Lembram o que eu expliquei? O AUR não tem os binários nos servidores, só a receita de como montar o pacote de determinado programa. Uma ferramenta com o Yay vai automatizar o que acabamos de fazer manualmente, que é dar clone do projeto, montar o pacote e instalar com pacman localmente.&lt;/p&gt;

&lt;p&gt;Pronto, com o Yay instalado praticamente não precisamos usar mais o pacman porque ele serve pra instalar tanto pacotes oficiais dos repositórios do Arch quanto do AUR, usando a mesma sintaxe do pacman. De exemplo, vamos instalar o shell ZSH, que vamos usar no lugar no Bash. Basta fazer &lt;code&gt;yay -S zsh&lt;/code&gt;, nem precisa de &lt;code&gt;sudo&lt;/code&gt;, ele se vira.&lt;/p&gt;

&lt;p&gt;Agora vamos pra página de GitHub do PowerLevel10K. Mesma coisa, pulamos pra seção de Instalação e veja como tem instruções pra várias distros. Queremos do Arch. Simples, duas linhas e ele já assume que temos o &lt;code&gt;yay&lt;/code&gt; instalado, então copiamos a primeira linha e colamos no terminal... pronto. No final ele recomenda instalar algumas fontes. Acho que não faz diferença dentro do terminal, mas pode instalar como fiz aqui. Vai fazer diferença se usar aplicativos gráficos de Linux iniciados de dentro do WSL, como o VSCode de Linux. Mas falo disso depois.&lt;/p&gt;

&lt;p&gt;Seguindo as instruções é só executar essa linha de comando que vai inserir a inicialização do powerlevel10k no script &lt;code&gt;.zshrc&lt;/code&gt; que é o equivalente &lt;code&gt;.bashrc&lt;/code&gt; só que pra zsh, obviamente. Podemos fechar essa aba, abrir outra e ... nada acontece. Isso porque por padrão sempre inicia o Bash, precisamos usar o comando de Linux de &quot;change shell&quot; que é &lt;code&gt;chsh -s /usr/bin/zsh&lt;/code&gt; pra da próxima vez executar direto o zsh em vez do bash...&lt;/p&gt;

&lt;p&gt;Pronto, agora fechamos a aba, reabrimos e como é a primeira vez, o powerlevel10k abre esse wizard de configuração. E a primeira coisa que ele pede pra gente confirmar é se estamos vendo um ícone com formato de diamante. Mas em vez disso estamos vendo um quadradinho, que nem quando vemos fontes quebradas numa página.&lt;/p&gt;

&lt;p&gt;Isso porque ele espera encontrar uma família de fontes que tenham gráficos além de só letras. Chamamos essas famílias de Nerd Fonts. Se você é web designer ou desenvolvedor front-end sabe disso. Acho que foi o GitHub que inventou isso de embutir gráficos como fonte. Isso porque sites como o próprio GitHub usam dezenas de pequenos ícones em todo lugar. Em vez de fazer dúzias de PNGs que além de ser um saco de gerenciar também prejudica a performance do download de assets da página, eles tiveram a idéia de embutir todos os ícones em uma família de fonte, que a gente baixa tudo de uma vez só, como a Font Awesome e fica muito mais fácil de usar em qualquer lugar, não só em sites, porque os emuladores de terminais modernos são capazes de renderizar essas fontes também.&lt;/p&gt;

&lt;p&gt;Além disso foi a época que Emojis começaram a popularizar, que nada mais são que um padrão onde imagens são associadas com códigos Unicode dentro da família de fontes. Então o mesmo código sempre vai devolver o mesmo emoji independente da família de fontes que estamos usando. Mas a fonte que o Windows Terminal usa por padrão que é a Microsoft Consolas, não tem nenhum desses ícones, por isso devolve um quadradinho pra indicar que não encontrou. Existem várias fontes que você pode pesquisar depois que tem esses ícones com o Fira Code mas eu pessoalmente gosto do Meslo LGS.&lt;/p&gt;

&lt;p&gt;Na própria página de instruções do Powerlevel10k tem links pra baixar os arquivos de fonte, então vamos lá baixar uma a uma... Pronto, agora vamos no explorer no diretório de downloads, selecionamos todos os arquivos que acabamos de baixar, e escolhemos pra instalar as fontes no sistema... E pronto, agora podemos ir no Windows Terminal, abrir de novo a aba de configurações, escolhemos o Arch, escolhemos a aba de aparência e finalmente podemos trocar o Consolas pelo Meslo LGS.&lt;/p&gt;

&lt;p&gt;Agora sim, abrimos uma nova aba. Vai carregar o ZSH que, por sua vez, vai carregar o Powerlevel10k. Como não prosseguimos a configuração da última vez, vai pedir pra começar de novo e desta vez veja como aparece o ícone de diamante. Nos próximos passos ele pede pra confirmar que estamos vendo os outros ícones e daí podemos continuar o passo a passo. E aqui vai do gosto de cada um. Vou passar rapidinho o que eu costumo escolher, mas recomendo que vocês experimentem opções diferentes. No final vai gravar tudo que escolheu no arquivo de configuração &lt;code&gt;.p10k.zsh&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Se não gostou do que escolheu basta digitar &lt;code&gt;p10k configure&lt;/code&gt; na linha de comando pra chamar esse passo a passo tudo de novo ou editar manualmente o arquivo &lt;code&gt;.p10k.zsh&lt;/code&gt;. Mas no final, olha só como ficou o meu prompt. Com ícones aqui na esquerda, mostrando o diretório onde estou. Se eu estiver num projeto de Git vai mostrar a branch também. Lá na direita mostrando a hora, mas quando entrar num projeto de código ele mostra outros ícones indicando a linguagem do projeto, a versão dessa linguagem e coisas assim.&lt;/p&gt;

&lt;p&gt;Agora não podemos esquecer de configurar o PATH no arquivo &lt;code&gt;.zshrc&lt;/code&gt;, assim podemos ter acesso ao LunarVim e qualquer nova ferramenta instalada pelo Cargo do Rust. Pode abrir com qualquer editor como o VSCode mas neste video sempre vou usar o NeoVim. Só adicionar o export no final do arquivo e não esquecer que a variável &lt;code&gt;$HOME&lt;/code&gt; é um atalho pro diretório padrão do seu usuário e no final anexar o que já tinha no $PATH antes... Pronto, veja que neste instante se chamarmos &lt;code&gt;lvim&lt;/code&gt; não vai achar. Mas agora abrimos outro terminal e boom, abriu o LunarVim como deveria.&lt;/p&gt;

&lt;p&gt;O powerlevel10k é bem flexível pra customizar o visual como quiser, e eu gosto do passo a passo do que ter que ficar ajustando arquivo de configuração manualmente. Só tem que fazer isso uma vez e esquecer. Mas como eu disse dá pra customizar o zsh pra ficar bem mais complexo com um oh-my-zsh e colocar vários plugins. Depende de que tipo de trabalhos mais faz no terminal, então não deixe de fuçar que plugins existem. Mas tem um que eu acho útil e por isso vou instalar manualmente agora. Quem usa a shell Fish já tem isso automaticamente.&lt;/p&gt;

&lt;p&gt;Vamos instalar que eu mostro pra que serve. No diretório do seu usuário, que você sempre volta fazendo &lt;code&gt;cd&lt;/code&gt; sozinho, vamos criar um diretório chamado &lt;code&gt;.zsh&lt;/code&gt; pra instalar o plugin. Agora vamos na página de GitHub dele. Tem link pra outra página com instruções de instalação. Olha como tem instrução pro Oh-my-zsh, mas queremos a forma manual. O primeiro passo é clonar o projeto pro diretório que criamos, então é só copiar essa linha de &lt;code&gt;git clone&lt;/code&gt; e colar no terminal.&lt;/p&gt;

&lt;p&gt;Queremos que toda vez que o zsh carregue, também carregue o plugin então temos que editar o arquivo &lt;code&gt;.zshrc&lt;/code&gt; de novo. Copiamos a segunda linha das instruções e podemos colar no script. Salvamos e pronto. Mas pra carregar agora mesmo, é só executar a linha que copiamos que ele carrega o script do plugin. Pra que serve isso? Vamos fazer um comando idiota de &lt;code&gt;echo&lt;/code&gt;. E se eu quiser repetir o mesmo comando?&lt;/p&gt;

&lt;p&gt;Posso copiar o que acabei de digitar e colar ou, começo a digitar &lt;code&gt;e&lt;/code&gt; e ele consulta no histórico e vai tentando auto completar. Olha como já apareceu aqui. Agora é só dar &lt;code&gt;tab&lt;/code&gt; ou seta pra direita e vai completar tudo. Isso é super útil porque o tempo todo estamos repetindo os mesmos comandos e eles ficam no histórico da sessão. Isso é um bom exemplo de plugin útil.&lt;/p&gt;

&lt;p&gt;Como o LunarVim já pediu pra instalar algumas ferramentas feitas em Rust e já colocamos no PATH onde o Cargo instala os binários, podemos aproveitar e instalar mais algumas ferramentas modernas. Se procurar no Google vai achar vários artigos a respeito mas vou usar este entitulado Rewritten in Rust junto com os outros links na descrição do video. Depois leiam com calma, ele lista diversas novas ferramentas feitas em Rust.&lt;/p&gt;

&lt;p&gt;Por exemplo, logo de cara tem os que eu mais gosto, por exemplo o &lt;code&gt;bat&lt;/code&gt;, que é um substituto do &lt;code&gt;cat&lt;/code&gt; que usamos pra listar o conteúdo de arquivos texto. Ou o &lt;code&gt;exa&lt;/code&gt; que é substituto pro &lt;code&gt;ls&lt;/code&gt; que usamos toda hora pra listar arquivos de diretórios. Em seguida ele fala do &lt;code&gt;fd&lt;/code&gt; que é uma alternativa mais performática pro &lt;code&gt;find&lt;/code&gt; e que o LunarVim já instalou porque o plugin Telescope usa. Além do &lt;code&gt;fd&lt;/code&gt; o LunarVim também instalou o &lt;code&gt;rg&lt;/code&gt; ou RipGrep que é um grep mais performático.&lt;/p&gt;

&lt;p&gt;Podemos instalar usando o pacman ou o yay mas vamos instalar usando o Cargo mesmo. Basta fazer &lt;code&gt;cargo install&lt;/code&gt; e passar todas as ferramentas que queremos. Olha só o cargo baixando, compilando e instalando... e no final deu alguns problemas. Ele não conseguiu instalar o ytop, dust e delta. Pode ter algum bug nos repositórios mas não me incomodei pra procurar porque não são essenciais. Os principais que eu queria eram o bat e exa.&lt;/p&gt;

&lt;p&gt;Vamos testar. Primeiro, olha como é a saída do &lt;code&gt;ls&lt;/code&gt; normal de Linux. Uma listagem normal de arquivos e diretórios, nada demais. Agora vamos usar o &lt;code&gt;exa&lt;/code&gt; com as mesmas opções de &lt;code&gt;-la&lt;/code&gt;. Ele não aceita todas as opções iguais do &lt;code&gt;ls&lt;/code&gt; só as mais comuns e veja só como ficou BEM mais bonito. E pra cereja no bolo tem a opção &lt;code&gt;--icons&lt;/code&gt;, e olha só como ficou ainda MAIS bonito. Depois que a gente vê assim, não dá vontade de usar o &lt;code&gt;ls&lt;/code&gt; padrão mais. Eu não testei, mas tudo tem trade-off, não sei se um diretório com milhares de arquivos vai ficar mais lento de listar. E também se quiser capturar a listagem num arquivo texto pra trabalhar depois, daí é melhor não usar o exa.&lt;/p&gt;

&lt;p&gt;Mesma coisa com o cat. Vamos listar o conteúdo do script &lt;code&gt;.zshrc&lt;/code&gt;. E como sempre, listado, nada demais. Mas e com o &lt;code&gt;bat&lt;/code&gt;? Olha só como ficou BEM mais bonito. Mas mesma coisa: se precisar trabalhar com o conteúdo do arquivo num script é melhor usar o cat normal. O bat é só pra visualizar mais bonito.&lt;/p&gt;

&lt;p&gt;Mas como eu uso &lt;code&gt;ls&lt;/code&gt; e &lt;code&gt;cat&lt;/code&gt; só pra visualizar no terminal mesmo, prefiro até substituir eles com aliases, e pra isso vamos abrir o &lt;code&gt;.zshrc&lt;/code&gt;. Só quero adicionar &lt;code&gt;alias&lt;/code&gt; pra dizer que quando eu digitar &lt;code&gt;ls&lt;/code&gt; no terminal o zsh vai na verdade chamar o &lt;code&gt;exa&lt;/code&gt; e mesma coisa pro &lt;code&gt;cat&lt;/code&gt; sempre chamar o &lt;code&gt;bat&lt;/code&gt;. Isso é opcional, mas só pra saber que dá pra fazer isso.&lt;/p&gt;

&lt;p&gt;Agora que nosso terminal tá bonitão, hora de instalar o ASDF. Não vou me alongar nessa parte porque já expliquei em detalhes sobre isso no guia de ubuntu e vai ser exatamente a mesma coisa agora. Pra quem não lembra, no dia a dia de programador a gente nunca usa uma única versão de alguma linguagem ou framework em todos os projetos. Podemos estar programando um projeto em Node.js versão 16, mas aí alguém me reporta um bug num projeto mais antigo que tava no Node.js 14. Eu preciso ter os dois instalados na minha máquina.&lt;/p&gt;

&lt;p&gt;Pra isso existem ferramentas como o NVM pra poder ficar trocando de versão de Node. Mas e se eu precisar mexer também num projeto de Python? Daí vou precisar instalar e configurar o VirtualEnv. Mas amanhã talvez eu tenha que ajudar no front-end de um projetinho em Rails, aí preciso instalar também o RVM ou Rbenv. E assim vai. Cada linguagem tem um gerenciador de versões diferente. Em vez de ter que lidar com essa zona toda, posso instalar o asdf, que tem plugins pra dezenas de linguagens e nunca mais usar outro.&lt;/p&gt;

&lt;p&gt;No Arch é fácil, basta fazer &lt;code&gt;yay -S asdf-vm&lt;/code&gt;... Ele vai instalar no diretório &lt;code&gt;/opt/asdf-vm&lt;/code&gt; e pra ativar precisamos fazer &lt;code&gt;source /opt/asdf-vm/asdf.sh&lt;/code&gt;. Pronto, tá ativado e precisamos colocar essa linha no &lt;code&gt;.zshrc&lt;/code&gt; pra toda ver que abrirmos o terminal ter o asdf carregado. A partir daqui é como já mostrei no video de ubuntu. Com &lt;code&gt;asdf list&lt;/code&gt; podemos ver todas as linguagens e versões instaladas. Como ainda não instalamos nenhuma, tá vazio. Então vamos instalar o plugin de node com &lt;code&gt;asdf plugin add nodejs&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Pronto, agora podemos fazer &lt;code&gt;asdf list-all nodejs&lt;/code&gt; e ver que a versão mais recente hoje é a 17.4.0 então vamos fazer &lt;code&gt;asdf install nodejs 17.4.0&lt;/code&gt; e boom, instalado. Só pra fazer graça, podemos instalar uma versão mais antiga também, por exemplo com &lt;code&gt;asdf install nodejs 16.13.2&lt;/code&gt;. E depois que instalar rodamos &lt;code&gt;asdf list&lt;/code&gt; e ele mostra que temos as duas instaladas direitinho.&lt;/p&gt;

&lt;p&gt;Pra escolher uma delas configuramos uma como versão global mas o certo é configurar por projeto. Pra dar exemplo, vamos pro diretório tmp criar um diretório falso de projeto chamado de &lt;code&gt;teste&lt;/code&gt;. Dentro podemos fazer &lt;code&gt;asdf local nodejs 16.13.2&lt;/code&gt;. Vamos listar o que tem no diretório e achamos o &lt;code&gt;.tool-versions&lt;/code&gt; que acabou de ser criado. Dando um &lt;code&gt;cat&lt;/code&gt; vemos que está declarada a versão que pedimos. Toda vez que entrarmos nesse diretório o asdf vai usar essa versão específica de Node.js independente do que for a versão global. E fazendo isso em todos os projetos, sempre vai mudar pra correta que cada projeto precisa.&lt;/p&gt;

&lt;p&gt;Pra complementar, vamos adicionar Ruby agora. Só adicionar o plugin com &lt;code&gt;asdf plugin add ruby&lt;/code&gt;. Mesma coisa de antes, vamos listar todas as versões disponíveis de Ruby e é uma lista gigante porque no meio tem versões não oficiais como o truffleruby, rubinius, jruby mas lá em cima vemos que tem a versão de desenvolvimento do 3.2.0 que ainda não é estável então vamos fazer &lt;code&gt;asdf install ruby 3.1.0&lt;/code&gt;. Ruby costuma demorar um pouco mais pra instalar, acho que baixa o código fonte e compila. Boa hora pra ir pegar um café.&lt;/p&gt;

&lt;p&gt;Agora que tá instalado, se der &lt;code&gt;asdf list&lt;/code&gt; mostra o Ruby também. E assim você pode ir instalando plugins de diversas outras linguagens como Rust, Kotlin, Python, Clojure e tudo mais e instalar versões específicas de cada uma. E como disse antes posso dizer que quero que essa versão 3.1 seja o global com &lt;code&gt;asdf global ruby 3.1.0&lt;/code&gt;. Agora todas as ferramentas de ruby que usarmos, a partir de qualquer diretório, vai estar apontando pra essa versão.&lt;/p&gt;

&lt;p&gt;Por exemplo, vamos usar o comando &lt;code&gt;gem&lt;/code&gt; que vem com todo Ruby e instalar o framework Ruby on Rails fazendo &lt;code&gt;gem install rails&lt;/code&gt;. No final ele vai instalar o comando &lt;code&gt;rails&lt;/code&gt;, então vamos de novo pro diretório temporário do sistema e executar &lt;code&gt;rails new teste_rails&lt;/code&gt; pra criar um novo projeto. Vai demorar um pouquinho pra instalar todas as dependências, mas no final podemos dar &lt;code&gt;cd&lt;/code&gt; pro novo diretório. Olha só, tudo funcionando e se quisermos perguntar a versão do ruby ativo, ainda é o 3.1.0.&lt;/p&gt;

&lt;p&gt;Como eu não declarei o arquivo &lt;code&gt;.tool-versions&lt;/code&gt; se amanhã eu atualizar o ruby global pra 3.2, esse projeto vai pegar 3.2. Se eu quiser travar pra versão específica o certo é rodar o &lt;code&gt;asdf local&lt;/code&gt; pra criar o arquivo &lt;code&gt;.tool-versions&lt;/code&gt;. Estou me alongando nisso porque é comum você atualizar o sistema, vem versão nova da sua linguagem, e de repente seu projeto começa a dar problema porque você não se tocou que tá rodando uma versão diferente. Por isso é boa prática ter declarada a versão correta em todo projeto mesmo se não usar asdf.&lt;/p&gt;

&lt;p&gt;Bom, agora que temos o ASDF e acesso a instalar linguagens, o último grande passo é instalar Docker. Antigamente eu preferia instalar banco de dados como postgres ou redis nativamente no sistema operacional, mas era mais por hábito mesmo. O ideal é não instalar esse tipo de serviço localmente, mesmo porque o WSL não tem um systemd pra controlar boot de serviços. Melhor usar Docker e configurar Docker Compose em todo projeto pra subir versões específicas de tudo que precisa.&lt;/p&gt;

&lt;p&gt;Assim como várias coisas podem quebrar quando você pula de Node 16 pra Node 17, um banco como Postgres tem funcionalidades diferentes por versão. A versão mais nova do Postgres no momento que estou fazendo este video é a 14, mas ainda tem muito projeto rodando na versão 9. Você não pode desenvolver na versão mais nova porque pode acabar dependendo de funcionalidades que não existem na 9, daí quando subir em produção, vai dar pau. Repetindo, declare a versão correta de cada coisa no manifesto de um Docker Compose.&lt;/p&gt;

&lt;p&gt;Se fosse um Arch Linux nativo fora do WSL, a instalação não seria muito mais do que um &lt;code&gt;yay -S docker&lt;/code&gt;, mas no caso do WSL tem um passo extra que é baixar e instalar o Docker Desktop antes. Então vamos no site oficial, baixar o instalador e instalar... Não precisa mudar nada na instalação, vai só prosseguindo ... ele vai baixar os componentes, vai configurar o WSL pra ter os grupos e permissões direitinho e no final vai pedir pra deslogar e logar de novo. Então vamos fazer isso.&lt;/p&gt;

&lt;p&gt;Toda vez que se logar no Windows, o Docker Desktop vai ficar carregado aqui embaixo na barra da tarefas. Certifique-se que ele tá carregado antes de usar. Mas da primeira vez vai abrir essa janela aqui pra aceitar os termos de serviço e só depois inicia. Não tem nenhuma configuração muito importante que precisa mudar, do jeito que veio já tá bom, com exceção de uma coisa que vou mostrar já já.&lt;/p&gt;

&lt;p&gt;Agora podemos abrir o terminal e fazer o &lt;code&gt;yay -S docker&lt;/code&gt; pra mandar comandos pro servidor que acabamos de instalar. Mas se tentarmos rodar &lt;code&gt;docker ps&lt;/code&gt; vai dizer que não conseguiu se conectar. Vamos abrir o Docker Desktop de novo e ir nas configurações. Aqui no item de Resources e WSL Integration tá marcado pra dar suporte pra minha distro default de WSL que vai ser aquele Ubuntu que instalou automaticamente primeiro. Então precisamos habilitar aqui embaixo pro Arch. Pronto.&lt;/p&gt;

&lt;p&gt;Se tentar rodar o mesmo comando, vai dar outro erro mas agora é de permissão. Isso porque quando mudamos aquela configuração, por baixo dos panos ele criou o grupo de docker e adicionou meu usuário nele, mas pra termos a permissão precisamos sair e entrar de novo. Então fazemos isso e veja só, o &lt;code&gt;docker ps&lt;/code&gt; agora roda sem problemas e listou que não tem nenhum container rodando. Então vamos seguir o exemplo que o Docker Desktop sugere na primeira tela. Só copie essa linha e cole no terminal.&lt;/p&gt;

&lt;p&gt;Demora alguns minutos mas ele vai baixar a imagem de um site que vai carregar na porta 80. Deixa baixar e executar. Agora podemos ir no navegador e digitar &lt;code&gt;localhost&lt;/code&gt; pra conectar na porta 80 e voilá, ele baixou um site com documentação pra iniciantes. Se você ainda é novo no Docker, é um bom lugar pra começar a estudar. Leia tudo que tem aqui.&lt;/p&gt;

&lt;p&gt;Por último, o novo WSL2 tem um novo recurso que vai facilitar muito sua vida quando for rodar testes automatizados com coisas como Selenium ou Cypress, que abrem um navegador de verdade com o Chromium pra testar sua aplicação. Antes já tinha como rodar aplicações gráficas manualmente instalando um cliente de X no Windows como o X410 mas agora o WSL2 já tem suporte nativo.&lt;/p&gt;

&lt;p&gt;Só instalar a aplicação que quiser e executar que vai abrir de boa. Infelizmente ainda não tem suporte estável pra fazer pass thru pra GPU da sua máquina então tudo vai rodar 100% via CPU e isso certamente vai ser mais lento do que o navegador nativo no Windows. Mas como é pra cenários como de testes automatizados que falei, tá mais que bom.&lt;/p&gt;

&lt;p&gt;Pra instalar é normal, só ir no terminal e fazer &lt;code&gt;yay -S chromium&lt;/code&gt;, vai demorar porque o pacote é grandinho, mas é só isso. Vamo lá, vamo lá, acelerar um pouco aqui, e pronto. Agora é só digitar &lt;code&gt;chromium&lt;/code&gt; no terminal e olha só, abre bonitinho como qualquer aplicação gráfica. Podemos carregar o YouTube e carrega rápido. Se abrir um video toca sem problemas no meu notebook inclusive com som. Tá sem som aqui porque eu tirei na edição mas pode acreditar que tem som.&lt;/p&gt;

&lt;p&gt;Tem um site que costumam usar pra fazer benchmark de navegador, o Speedometer, mas do que já usei dele sei que é bem pouco confiável. Já rodei o teste múltiplas vezes no mesmo navegador e dava resultados bem diferentes, mas só por diversão vamos rodar nesse Chromium. Isso é o cenário que falei de testes automatizados. Ele vai circular por dezenas de aplicações de todo list feitos em diversos frameworks como Angular, React, Ember e outros. Acelerando um pouco o resultado final foi 172 pontos.&lt;/p&gt;

&lt;p&gt;Agora vou abrir o Microsoft Edge aqui do lado e carregar o mesmo teste. Vamos ver. Vamos acelerar um pouco aqui com um pouco de mágica de edição e boom. Olha só que bizarro, o Chromium rodando virtualizado ganhou. Mas lógico, meu Edge tá cheio de extensions, cheio de abas abertas, e o Chromium tá recém instalado, totalmente vazio. De qualquer forma, na prática, a performance dentro do WSL tá muito boa. Daria até pra usar como segundo navegador.&lt;/p&gt;

&lt;p&gt;Só pra mostrar onde o Chromium via WSL chora é se tentar rodar qualquer coisa 3D nele, por exemplo, um aplicativo em WebAssembly como o Google Earth. Olha só como fica lento, absurdamente lento, é literalmente inusável. E não tinha como ser diferente porque o WSL ainda não tem integração com aceleração 3D da máquina, é tudo renderizado pelo CPU. Mas se eu abrir o Edge que perdeu no Speedometer e carregar o mesmo Google Earth, olha a diferença. Roda liso, praticamente sem perder frames. Essa é a diferença de ter aceleração via GPU. Mas de novo, se você não estiver mexendo com games ou modelagem 3D, pra sites normais, não faz tanta diferença assim.&lt;/p&gt;

&lt;p&gt;As novas versões do WSL tem suporte experimental a drivers de paravirtualização de GPU da Nvidia e Amd pra rodar projetos de machine learning em containers de Docker, como Tensorflow ou PyTorch. Eu mesmo não brinquei com isso ainda, não é estável nem garantido que vai funcionar, mas se alguém aí tiver inclinação pra escovar bit pra ver se funciona, vou deixar o link nas descrições também. Mas pra maioria de nós significa que se tiver aplicações gráficas de Linux que você precisa rodar no Windows, agora tem mais uma opção também, só não precisar de aceleração 3D.&lt;/p&gt;

&lt;p&gt;E já que estou falando de coisas que foram adicionadas recentemente no WSL, tem uma que gostei muito. Pra mostrar, tem duas últimas coisas que eu faço sempre que reconfiguro um novo Linux. A primeira é copiar minhas chaves privadas que ficam no diretório &lt;code&gt;.ssh&lt;/code&gt;, pra ter acesso às minhas contas de Heroku, AWS, GitHub, GitLab e tudo mais. Eu expliquei sobre chaves no episódio de Ubuntu e nos de criptografia. Você precisa tomar muito cuidado pra nunca ninguém pegar essas chaves. De qualquer forma, quando configuro máquina nova, eu zipo as chaves e copio.&lt;/p&gt;

&lt;p&gt;A segunda coisa é copiar minha pasta de projetos. Normalmente você não precisa ter tudo, projetos que nem mexe mais. Com as chaves no lugar, é só dar &lt;code&gt;git clone&lt;/code&gt; dos projetos que precisa pra agora e tá ótimo. Mas eu quero dar um exemplo do que é possível fazer com o novo WSL. Vamos dar contexto. Lembram lá no começo do video que instalei o Arch no diretório &lt;code&gt;C:\arch&lt;/code&gt;? Pois é. Vamos abrir o explorer de novo e temos um arquivo chamado &lt;code&gt;ext4.vhdx&lt;/code&gt;. Esse é o &quot;HD virtual&quot; do Arch Linux que instalamos e estamos usando agora. Olhem como o Arch é levinho, com tudo que instalamos tá ocupando menos de 7 giga.&lt;/p&gt;

&lt;p&gt;Esse é o melhor jeito de lidar com HDs virtuais, eles são arquivões binários. E eis porque eu fiz uns 4 videos explicando tudo que um programador deveria saber sobre dispositivos de armazenamento, particionamento, formatação e tipos de filesystems diferentes, porque agora vou assumir você entende o que é um HD virtual. Caso não saiba definir o que é uma partição ou qual a diferença de um filesystem ntfs ou ext4, recomendo que assistam os videos. Mas vamos lá.&lt;/p&gt;

&lt;p&gt;Voltando pro terminal, o WSL automaticamente monta seus HDs de verdade, como o seu C:, dentro do diretório &lt;code&gt;/mnt&lt;/code&gt;. Então se navegarmos pra &lt;code&gt;cd /mnt/c&lt;/code&gt; e dermos um ls, vai listar exatamente o que tem no seu C:. E aí você pode ficar com a idéia errada de deixar seus diretórios de projetos no Windows e trabalhar neles de dentro do WSL. E isso seria uma péssima idéia. Isso porque esse mount que ele fez é parecido com conectar com um servidor de arquivos via rede, mais especificamente usando o protocolo P9 em vez de CIFS. Toda operação de arquivos nesse mount vai ser lenta, porque tem um overhead a mais por cima.&lt;/p&gt;

&lt;p&gt;O lado oposto também dá. Do Windows acessar os arquivos dentro do Linux. Só abrir o explorer e digitar &quot;\wsl$&quot; e pronto, podemos ficar copiando arquivos do Windows pro Linux e vice versa, tanto via terminal quanto via explorer. Mas como eu disse, é como se fosse uma pasta compartilhada na rede, mesmo sendo local, ainda vai ser uma ordem de grandeza mais lento. Especialmente projetos web que tem centenas de arquivos pequenos. O melhor sempre é transferir um zip grandão via rede do que centenas de arquivos pequenos.&lt;/p&gt;

&lt;p&gt;A melhor performance é dentro do HD virtual montado diretamente, aquele arquivão VHDX. Lá dentro a velocidade é quase a mesma que nativa, tem muito pouco overhead. Tanto que ficamos aqui instalando pacotes, editando arquivos e tudo mais e a sensação era que estava rodando tudo nativo. Então o mais prático é fazer git clone dos seus projetos lá dentro mesmo e nunca jogar pra fora a partir no &lt;code&gt;/mnt/c&lt;/code&gt;. Mesmo se conectar um HD externo que vai montar como &lt;code&gt;/mnt/d&lt;/code&gt;, é a mesma coisa, vai ter overhead.&lt;/p&gt;

&lt;p&gt;Em 90% dos casos você nunca vai ter nenhum problema se ficar dentro do hd virtual. Mas se for que nem eu que vira e mexe quer mudar de distribuição Linux, mover tudo pra outra máquina ou coisas assim, se tiver muitos arquivos pra ficar movendo, vai ser super lento e demorado, pode levar horas ou mais. O ideal seria alguma coisa tipo um HD externo montado direto. Mantém tudo fora, daí se quiser reinstalar a máquina não tem problema e não demora porque tudo que não era instalação de aplicativos já tava fora.&lt;/p&gt;

&lt;p&gt;E se quiser usar um HD externo, o WSL agora suporta montar direto. Vamos ver esse cenário primeiro. Eu tenho um SSD com um adaptador pra USB. Quando conecta no notebook, o Windows detecta e monta como drive D:. Normal, todo mundo já viu isso quinhentas vezes. O que a maioria de vocês não sabe é como o Windows controla isso por baixo. Vamos abrir um Powershell com privilégios de administrador primeiro, alguns comandos que vamos usar a partir de agora vai precisar.&lt;/p&gt;

&lt;p&gt;Desde a época do Windows 2000 existe a ferramenta &lt;code&gt;wmic&lt;/code&gt; que é o Windows Management Instrumentation Command. É uma forma de instrumentar o Windows sem precisar usar aplicativos gráficos. Quando estamos vendo o drive D: isso é um mount point, um ponto de montagem como no Linux que monta dispositivos como diretórios como no &lt;code&gt;/mnt/c&lt;/code&gt;. No Linux os dispositivos de verdade ficam declarados no diretório &lt;code&gt;/dev&lt;/code&gt; e seu HD poderia ser um &lt;code&gt;/dev/sda&lt;/code&gt; que montamos como a raíz &lt;code&gt;/&lt;/code&gt;. Ficou confuso? Eu avisei, é porque você não assistiu meus videos sobre sistemas de arquivos.&lt;/p&gt;

&lt;p&gt;Enfim, no Windows tem o mesmo conceito. Os dispositivos podem existir ligados sem ter um ponto de montagem como &quot;D:&quot; ou eu posso mudar o ponto de montagem pra outra letra se eu quiser, mas o dispositivo em si tem um identificador único que foi dado quando foi conectado. Pra ver isso, no Powershell que acabamos de abrir podemos digitar &lt;code&gt;wmic diskdrive list brief&lt;/code&gt; e olha só, nesse notebook temos dois dispositivos de armazenamento, o NVME Sabrent Rocket de 4 terabytes que é o C: e o Kingston conectado via USB de 480 gigas. E os nomes dos dispositivos são esses &lt;code&gt;\\.\PHYSICALDRIVE0&lt;/code&gt; e 1. São os equivalente no Linux a um &lt;code&gt;/dev/sda&lt;/code&gt; e &lt;code&gt;sdb&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;O WSL2 mais novo ganhou a habilidade de montar um HD externo e acessar direto sem ter aquele overhead de rede do protocolo P9 que falei antes, ele monta direto o filesystem. Pra isso, aqui no Powershell mesmo é só fazer &lt;code&gt;wsl --mount \\.\PHYSICALDRIVE1&lt;/code&gt;. Ele avisa que o disco foi anexado na máquina virtual mas falhou em criar um ponto de montagem. Notem que quando fiz isso, o Windows desmontou o drive D:, que era só um ponto de montagem. O ideal é não ter dois sistemas operacionais diferentes mexendo no mesmo HD ao mesmo tempo direto, obviamente.&lt;/p&gt;

&lt;p&gt;De dentro do WSL, por padrão ele vai montar drives externos em &lt;code&gt;/mnt/wsl&lt;/code&gt;, olha só, apareceu um novo diretório chamado &quot;PHYSICALDRIVE1&quot;, mas se entrarmos nele não tem nada. O comando de mount do WSL tinha avisado que falhou e se consultarmos o &lt;code&gt;dmesg&lt;/code&gt; que registra o log global do sistema operacional, tem de fato um erro do VFS que é o subsistema de file system virtual que falhou em tentar montar com filesystem ext4. E ia falhar mesmo porque esse SSD está formatado com NTFS, que o Linux não entende por padrão. Mesmo assim o dispositivo foi registrado dentro do Linux como &lt;code&gt;/dev/sdg&lt;/code&gt; e se eu quiser poderia formatar usando o comando &lt;code&gt;mkfs.ext4&lt;/code&gt; como qualquer outro drive.&lt;/p&gt;

&lt;p&gt;Isso é legal porque posso ter um drive externo com partição formatada em ext4 e toda vez que reiniciar meu note, é só eu dar o comando &lt;code&gt;wsl --mount&lt;/code&gt; que vai aparecer dentro do WSL e ter excelente performance, basicamente performance de um drive nativo, sem overhead nenhum. E eu posso compartilhar esse drive com meu Linux em casa e com outro Linux no escritório por exemplo. Super conveniente.&lt;/p&gt;

&lt;p&gt;Mas eu gosto de complicar um pouco mais. Eu não quero usar um drive externo, quero usar um outro HD virtual só pra projetos, pra ficar separado do HD virtual de sistema operacional. Assim, posso compartilhar esse drive virtual com mais de um WSL na mesma máquina, que nem agora que eu já tenho um Ubuntu e um Arch Linux instalados no WSL. Ambos poderiam acessar os mesmos projetos sem duplicar nada. Como fazemos isso?&lt;/p&gt;

&lt;p&gt;Pra criar um HD virtual é simples. Basta abrir o Disk Management e criar um VHD. Vou criar um arquivo chamado &lt;code&gt;test.vhdx&lt;/code&gt; de 150 giga, formato VHDX que é mais moderno e com tamanho dinâmico, ou seja, quando montarmos esse disco virtual, o sistema operacional vai achar que ocupa 150 giga mas na verdade vai ter o tamanho dos arquivos que colocarmos dentro dele, então não desperdiça espaço do seu HD de verdade. Olha no Explorer o arquivo que criamos e como não tá ocupando praticamente nada de espaço porque ainda tá vazio.&lt;/p&gt;

&lt;p&gt;De volta ao mesmo Powershell, o que precisamos agora é que esse arquivo seja reconhecido como um disco de verdade e pra isso precisa aparecer como um PHYSICALDRIVE no Windows primeiro, que nem quando espetamos o SSD via USB. E pra fazer isso usamos o comando &lt;code&gt;Mount-VHD&lt;/code&gt; passando o caminho completo pro arquivo de disco virtual e ... deu pau. Isso porque esse comando só existe se você instalou as ferramentas do Hyper-V que é a plataforma de hypervisor da Microsoft. E isso só existe se estiver usando Windows 10 ou 11 versão Pro ou versão Server.&lt;/p&gt;

&lt;p&gt;Tudo que fizemos até este ponto do video funciona na versão Home do Windows. Eu sei disso porque quando reinstalei o Windows 11 do zero ele instalou como Home sei lá porque. Mas é só ir na loja do Windows e fazer o upgrade pra versão Pro. Custa um pouco caro, mas se sua profissão é ser programador, é um custo necessário. O upgrade hoje em dia é super rápido, leva alguns minutos e um reboot e já era. Se não gosta da idéia de pagar pelo Windows, é pra isso que existe Linux.&lt;/p&gt;

&lt;p&gt;Estando no Windows Pro, agora eu procuro a opção de Turn On Features que é o instalador de funcionalidades opcionais do Windows e boom, ta aí a opção de Hyper-V que só tem na versão Pro. Leva mais alguns poucos minutos pra instalar, mais um reboot e pronto. Agora temos suporte às ferramentas do Hyper-V. Então podemos abrir o Powershell com privilégios de administrador de novo e repetir o mesmo comando de antes de &lt;code&gt;Mount-VHD&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Desta vez foi com sucesso. A partir deste ponto, pro sistema operacional, esse disco virtual se comporta igual a um SSD de verdade. Vamos rodar aquele mesmo comando &lt;code&gt;wmic diskdrive list brief&lt;/code&gt; e olha só, entrou como Microsoft Virtual Disk e se registrou como &lt;code&gt;\\.\PHYSICALDRIVE1&lt;/code&gt; igual meu SSD Kingston via USB antes. E finalmente, podemos repetir o mesmo comando &lt;code&gt;wsl --mount&lt;/code&gt;. Isso vai anexar o disco no WSL mas vai falhar a montagem de novo porque acabamos de criar esse disco virtual e ele tá vazio e nunca foi formatado.&lt;/p&gt;

&lt;p&gt;Abrindo o terminal e olhando os logs no &lt;code&gt;dmesg&lt;/code&gt;, mesmo erro de antes, não achou uma partição formatada com ext4. Mas ele tá bonitinho anexado como &lt;code&gt;/dev/sdg&lt;/code&gt;. Como é um drive zerado, podemos formatar fazendo &lt;code&gt;sudo mkfs.ext4&lt;/code&gt; que é o comando de 'make filesystem'. Pronto, agora que tá formatado, podemos dar &lt;code&gt;cd&lt;/code&gt; pro ponto de montagem que é &lt;code&gt;/mnt/wsl/PHYSICALDRIVE1&lt;/code&gt;. Mas note que ele tá com permissão pro usuário root então não vamos conseguir criar nada aqui.&lt;/p&gt;

&lt;p&gt;Só falta dar um &lt;code&gt;sudo chown&lt;/code&gt; pro meu usuário nesse Linux que é &quot;akitaonrails&quot; e pronto. Agora eu posso criar um arquivo aqui dentro com o comando &lt;code&gt;touch&lt;/code&gt; e sucesso! Arquivo criado. Pro Linux isso é um SSD normal, montado diretamente sem nenhum tipo de gambiarra de rede, com velocidade máxima. Tem a mesma performance do disco virtual principal onde o sistema operacional tá instalado. Você pode criar quantos discos virtuais quiser e montar dessa forma.&lt;/p&gt;

&lt;p&gt;Por acaso, eu tenho meu PC de casa que uso pra editar meus videos com WSL instalado e fiz um disco virtual com todos os projetos pessoais e da empresa. Virou um arquivão de mais de 20 gigas de coisas. Transferi pro notebook e agora coloquei no mesmo diretório C:\Arch. Tem que tomar cuidado se deixar em diretórios como Documents, caso Dropbox ou OneDrive estejam configurados pra fazer backup automático. Como é um arquivão gigante, toda vez pode tentar fazer upload de tudo, o que vai ser hiper lento. Faça backup mas tome cuidado de como vai fazer.&lt;/p&gt;

&lt;p&gt;E pro último truque, eu queria que esse disco virtual de projetos fosse montado e anexado automaticamente no WSL toda vez que eu bootar a máquina pra não ter que toda vez abrir o Powershell e ficar fazendo &lt;code&gt;Mount-VHD&lt;/code&gt; e &lt;code&gt;wsl --mount&lt;/code&gt; todos os dias. Já vimos que se quisermos que algo rode toda vez que inicio o Terminal, no Linux foi fácil, foi só adicionar os comandos no nosso script &lt;code&gt;.zshrc&lt;/code&gt;, aí ele inicia automaticamente o prompt powerlevel10k, o asdf. Mas no Windows não tem o equivalente fácil assim pra scripts.&lt;/p&gt;

&lt;p&gt;Vamos ter que usar o bom e velho Task Scheduler que é o agendador de tarefas do Windows, que fica no aplicativo Computer Management e só tem opção gráfica. Então, botão direito menu de Start, selecionar Computer Management e na lista tem o Task Scheduler. Botão direito nele pra Criar Nova Tarefa. Colocamos um nome descritivo como &quot;Auto Mount VHD&quot; e configuramos pra rodar independente se eu estiver logado, marcamos pra não salvar senha e pra rodar com Privilégios de Administrador.&lt;/p&gt;

&lt;p&gt;Na aba de Trigger criamos um novo que é &quot;At Startup&quot; que é pra rodar logo que o Windows carregar. Na aba seguinte de Actions é onde dizemos o que vai rodar no boot. O tipo de programa vai ser PowerShell e o argumento vai ser o comando de &lt;code&gt;Mount-VHD&lt;/code&gt; só que agora o PATH vai ser pra &quot;C:\Arch\projects-disk.vhd&quot; que é meu disco virtual de projetos. Coloque o nome do disco que você criou. Pronto, e na última aba de condições é tirar a opção dele rodar de novo se o notebook entrar em modo de economia de energia e acordar. Não tem que rodar de novo.&lt;/p&gt;

&lt;p&gt;Agora vamos fazer exatamente a mesma coisa só que pra outro comando. O nome vai ser Auto Mount VHD in WSL. Esses nomes são arbitrários, descreva como quiser. Mesma configuração. Depois o mesmo trigger at startup, mas diferente da última vez vamos colocar a opção de &quot;Delay&quot;, ou seja, eu quero um pequeno atraso. 1 minuto pra garantir. Isso porque queremos rodar este comando só depois que o anterior tiver já rodado. O Mount VHD tem que anexar o disco virtual e só depois podemos montar no WSL. Se não for nessa ordem, não vai funcionar.&lt;/p&gt;

&lt;p&gt;Agora a action vai ser de novo tipo PowerShell e o argumento vai ser o comando &lt;code&gt;wsl --mount&lt;/code&gt; e vai estar hardcoded aqui que é pra montar o PHYSICALDRIVE1. Lembrando que se eu bootar a máquina com um pendrive ou algo assim pré-conectado, quando montar o disco virtual provavelmente vai cair pra PHYSICALDRIVE2 e aí esse mount no WSL vai montar o disco errado. Mas normalmente vai ser sempre PHYSICALDRIVE1. E finalmente mesma coisa, não precisa rodar de novo se acordar de sleep.&lt;/p&gt;

&lt;p&gt;Feito isso, podemos bootar a máquina e quando voltar, esperamos 1 minuto e quando abrimos o Terminal podemos checar com o comando &lt;code&gt;lsblk&lt;/code&gt; que lista dispositivos de bloco e lá está nosso &lt;code&gt;/dev/sdg&lt;/code&gt; devidamente montado em &lt;code&gt;/mnt/wsl/PHYSICALDRIVE1&lt;/code&gt; como a gente queria. Se listarmos os arquivos lá podemos ver que tem todo meu material. Mas convenhamos que digitar esse PATH enorme toda vez é um saco, mas pra isso que todo Linux suporta links simbólicos.&lt;/p&gt;

&lt;p&gt;Vamos voltar pro meu diretório de casa com &lt;code&gt;cd&lt;/code&gt; e vamos usar o comando &lt;code&gt;ln -s&lt;/code&gt; pra mapear o link simbólico &quot;Projects&quot; apontando pro &lt;code&gt;/mnt/wsl/PHYSICALDRIVE1&lt;/code&gt;. Pronto, agora podemos fazer &lt;code&gt;cd Projects&lt;/code&gt; e ele entra no lugar certo. E agora sim, posso continuar dando &lt;code&gt;git clone&lt;/code&gt; e &lt;code&gt;git pull&lt;/code&gt; dos meus projetos pra cá e tudo vai funcionar com velocidade máxima. E tudo separado do disco virtual do sistema operacional.&lt;/p&gt;

&lt;p&gt;E mais do que isso. Lembra do Ubuntu que o WSL instalou lá no começo e eu não mexi mais? Vamos abrir uma aba pra ele e vamos pro diretório &lt;code&gt;/mnt/wsl&lt;/code&gt; e olha só, aqui também tá montado. Tanto do Arch quanto do Ubuntu, o comando &lt;code&gt;wsl --mount&lt;/code&gt; monta em todas as máquinas virtuais e eu posso acessar de qualquer uma a qualquer momento. Obviamente não recomendo abrir o mesmo arquivo dos dois lugares ao mesmo tempo, mas isso abre diversas possibilidades diferentes. Eu não preciso ficar copiando arquivos entre os dois, posso compartilhar o mesmo drive virtual. E isso só funciona porque o Arch e o Ubuntu compartilham o mesmo Kernel do Linux e esse kernel único tem acesso ao drive e por isso aparece no container dos dois sistemas operacionais diferentes. Eles não sabem que estão compartilhando a mesma kernel.&lt;/p&gt;

&lt;p&gt;E por último, eu tinha falado que faltava transferir as chaves privadas de SSH e nesse disco virtual eu tinha copiado o arquivo &lt;code&gt;ssh.tgz&lt;/code&gt;. Aqui vai depender de como você compactou mas eu preciso ir pra raíz do drive e fazer &lt;code&gt;tar xvfz /mnt/wsl/PHYSICALDRIVE1/ssh.tgz&lt;/code&gt; e vai descompactar no lugar certo e já com as permissões corretas. Se tivesse copiado de um pendrive formatado em FAT por exemplo, as permissões não teriam sido mantidas então precisaria fazer &lt;code&gt;chmod -R 600 .ssh&lt;/code&gt; que significa permissão 6 pro meu usuário que é leitura e escrita e permissão 0 pra todos os outros usuários, que é não conseguir nem ler e nem mesmo listar os arquivos desse diretório.&lt;/p&gt;

&lt;p&gt;Com isso o WSL do meu notebook está configurado e pronto pra eu poder voltar a mexer em código. Tenho o &lt;code&gt;yay&lt;/code&gt; pra poder instalar qualquer programa do repositório AUR, tenho o Cargo do Rust pra instalar programas feitos em Rust, tenho o ASDF pra conseguir instalar qualquer linguagem de programação em qualquer versão e tenho o Docker pra poder rodar qualquer projeto. Além disso tenho meu prompt bonitão com o Powerlevel10K e posso editar código tanto no VSCode quanto no meu LunarVim fodão. E isso é tudo que um programador web precisa. Se eu fosse lidar com devops precisaria instalar outras ferramentas como do google cloud, kubernetes e coisas assim, mas aí é de projeto a projeto e com o que tenho configurado aqui, é só instalar com o &lt;code&gt;yay&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Eu mencionei muito rápido no começo, mas uma das grandes vantagens de usar o Arch é a documentação da comunidade. Como no exemplo de instalar Docker, pra Ubuntu, eu precisei achar no site do próprio Docker e não no site do Ubuntu. Mas no caso do Arch tem um Wiki de Docker feito pela comunidade com um monte de informação e dicas muito úteis, seja pra quem é iniciante ou quem é avançado tentando resolver problemas mais complexos. Em vez de sair fuçando em fóruns ou lugares como o site stackexchange, normalmente você acha o que precisa no Wiki do Arch. Gaste tempo estudando tudo que não conhecia que mostrei nesse video a partir desse Wiki. Tenho certeza que vai aprender muito.&lt;/p&gt;

&lt;p&gt;Tenho certeza também que muitos vão ficar mandando nos comentários ou mensagens diretas pra mim perguntando qual distro usar. Se tem dúvida, instale Ubuntu. Por isso eu fiz o guia definitivo de Ubuntu pra iniciantes. É a resposta padrão. De novo, se você tem dúvidas e veio perguntar é porque não tá com muita disposição de pesquisar, então é Ubuntu. Quem se importa já instalou um VirtualBox ou Virt Manager e criou máquinas virtuais pra baixar os ISOs das distribuições que tem interesse pra ver com os próprios olhos.&lt;/p&gt;

&lt;p&gt;Hoje em dia é muito fácil. As ISOs você baixa de graça no site de cada distro. Daí é só bootar numa máquina virtual e usar alguns dias pra ver se gosta. Se ficou confortável, daí reserva um fim de semana, faz backup de tudo, queima a ISO num pendrive e instala como sistema operacional nativo. Não gostou? Muda pra outro. É assim que se faz. Perguntar pros outros não faz nenhuma diferença porque cada um usa pra coisas diferentes.&lt;/p&gt;

&lt;p&gt;Eu pessoalmente gosto de distros baseados em Arch como o Manjaro, mas acho o Manjaro meio feinho. Prefiro a cara de um Deepin, mas acho Deepin muito pesado e não confio muito nele. Kali Linux é seguro, mas não foi feito pra usar no dia a dia, só rodaria numa máquina virtual se precisasse fuçar coisas de segurança. E assim por diante.&lt;/p&gt;

&lt;p&gt;A parte ruim é instalar em notebooks super novos. Meu Zephyrus G14 comprei acho que lá por julho do ano passado, tinha acabado de lançar. Significa que o suporte de Linux pra ele ainda era muito insipiente. Eu achei alguns scripts, alguns hacks pra tentar fazer tudo funcionar, mas coisas como bluetooth não ia de jeito nenhum, perfil de economia de energia ainda não tava adequado. No final das contas não tive muita confiança, daí voltei pra Windows com WSL que todos os periféricos iam funcionar direitinho.&lt;/p&gt;

&lt;p&gt;Agora que já passou 1 ano, talvez já tenha mais gente que gastou tempo ajustando e talvez agora já funcione melhor, não sei. Mas no meu caso, realmente não vejo tanta vantagem. O Windows me atende bem, tenho zero problemas com coisas como Steam, e tudo que preciso pra programar roda no WSL. Como é uma máquina parruda, com CPU e RAM sobrando, mesmo numa máquina virtual como o WSL funciona tudo zero bala. Se fosse um notebook mais antigo e menos potente, daí já justifica deixar Linux nativo, porque quanto mais antigo for o hardware, maiores as chances de drivers e coisas assim já existirem e serem estáveis.&lt;/p&gt;

&lt;p&gt;Por isso que a resposta é sempre, depende. E por isso que se você é programador ou quer ser, precisa saber ir atrás da documentação e todos os componentes da sua própria máquina, pra saber quais distros são mais compatíveis, que tipos de gambiarra vai precisar fazer ou não. Este video, não tem nada que eu inventei, tudo tem na web, só saber procurar. E de novo, expliquei coisas mais básicas no meu video de Ubuntu e assistam os videos sobre máquina virtual e containers, os videos sobre sistemas de arquivos e dispositivos de armazenamento e tudo mais.&lt;/p&gt;

&lt;p&gt;Se importe um pouco mais com sua própria máquina e seu ambiente de desenvolvimento. Se ficaram com outras dúvidas, mandem nos comentários abaixo. Se curtiram o video deixem um joinha, não deixem de assinar o canal e compartilhar o video com seus amigos. A gente se vê, até mais.&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5947</id>
    <published>2022-01-29T16:59:00-03:00</published>
    <updated>2022-01-29T16:44:15-03:00</updated>
    <link href="/2022/01/29/akitando-113-a-forma-ideal-de-projetos-web-os-12-fatores" rel="alternate" type="text/html">
    <title>[Akitando] #113 - A Forma Ideal de Projetos Web | Os 12 Fatores</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/gpJgtED36U4&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;h2&gt;DESCRIPTION&lt;/h2&gt;

&lt;p&gt;Finalmente vou mostrar o que é o Heroku e como é o fluxo de trabalho mínimo de um projeto web ideal. Se você já usa Heroku, aproveite pra compartilhar o video com conhecidos que ainda não usam. Se nunca viu Heroku, prepare-se pra ficar surpreso!&lt;/p&gt;

&lt;h2&gt;Conteúdo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;01:16 - Recapitulando Heroku&lt;/li&gt;
&lt;li&gt;05:04 - Os 12 Fatores&lt;/li&gt;
&lt;li&gt;06:02 - I - Codebase&lt;/li&gt;
&lt;li&gt;07:11 - II - Dependencies&lt;/li&gt;
&lt;li&gt;08:24 - III - Config&lt;/li&gt;
&lt;li&gt;09:30 - IV - Backing Services&lt;/li&gt;
&lt;li&gt;10:47 - V - Build, release, run&lt;/li&gt;
&lt;li&gt;12:25 - VI - Processes&lt;/li&gt;
&lt;li&gt;14:50 - VII - Port Binding&lt;/li&gt;
&lt;li&gt;16:09 - VIII - Concurrency&lt;/li&gt;
&lt;li&gt;17:40 - IX - Disposability&lt;/li&gt;
&lt;li&gt;19:07 - X - Dev/Prod parity&lt;/li&gt;
&lt;li&gt;21:59 - XI - Logs&lt;/li&gt;
&lt;li&gt;22:31 - RANT: log4j&lt;/li&gt;
&lt;li&gt;23:39 - XII - Admin Processes&lt;/li&gt;
&lt;li&gt;24:33 - Heroku - alternativas e custo-benefício&lt;/li&gt;
&lt;li&gt;28:37 - Escalabilidade não é automática&lt;/li&gt;
&lt;li&gt;30:33 - 1. Farás Testes!&lt;/li&gt;
&lt;li&gt;31:05 - 2. Integrarás Continuamente!&lt;/li&gt;
&lt;li&gt;31:59 - 3. Usarás um Sistema de CI!&lt;/li&gt;
&lt;li&gt;32:44 - 4. Farás Deploy Contínuo!&lt;/li&gt;
&lt;li&gt;33:16 - Conclusão: equipes saudáveis&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The Twelve-Factor App (https://12factor.net/)&lt;/li&gt;
&lt;li&gt;Guidance for preventing, detecting, and hunting for exploitation of the Log4j 2 vulnerability  (https://www.microsoft.com/security/blog/2021/12/11/guidance-for-preventing-detecting-and-hunting-for-cve-2021-44228-log4j-2-exploitation/)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita.&lt;/p&gt;

&lt;p&gt;No episódio anterior me dei ao trabalho de fazer o passo a passo de um tutorial simples de primeiros passos usando o Heroku pra fazer deploy de um projetinho de exemplo em PHP. O objetivo foi mostrar o processo de deployment de uma aplicação moderna pra iniciantes que talvez nunca nem tenham ouvido falar de Heroku. Qualquer desenvolvedor que já trabalha com certeza já conhecia. Então, se você não sabe o que é Heroku assista o episódio anterior antes de ver este, porque precisa saber o que eu disse lá pra entender este.&lt;/p&gt;

&lt;p&gt;Hoje quero falar sobre um troço conhecido como os “12 fatores” que é meio uma metodologia que pode ser aplicada a projetos web escritos em qualquer linguagem de programação e qualquer framework web. Além disso quero complementar com alguns fatores que originalmente não se menciona mas meio que ficam implícitos. Se você quer um projeto web que tenha capacidade de escalar é meio obrigatório que no mínimo esses 12 fatores estejam satisfeitos. Satisfazer os 12 fatores não torna nenhuma aplicação automaticamente escalável, mas não satisfazer eles quase garante que não vai escalar de jeito nenhum.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Se você já é programador de sistemas legados, tenho certeza que quando viu o tutorial de Heroku pela primeira vez, brilhou seus olhos de vontade de poder trabalhar desse jeito. Se você é de agenciazinhas porcaria - e eu defino como porcaria todas que ainda atualizam sites via FTP como falei antes - mesma coisa, deve estar se coçando de vontade de poder trabalhar assim. Todo mundo que trabalha em tech startups hoje em dia ou empresas grandes que se modernizaram já trabalham desta forma. Mas cuidado, se você não trabalha assim e mesmo assistindo até aqui achou tudo desnecessário ou mesmo ruim, eu repensaria a carreira. Você vai se tornar obsoleto muito em breve.&lt;/p&gt;

&lt;p&gt;E quem acha que isso tudo é só mais uma novidade passageira, que vai acabar daqui a pouco, está bem enganado. Eu trabalho desse jeito desde aproximadamente 2010, portanto mais de uma década. Eu ajudei o quanto pude a difundir esse estilo de trabalho por anos e finalmente isso se tornou o padrão. Muitos tentaram imitar o Heroku, mas até hoje acho impressionante que poucos conseguiram chegar perto.&lt;/p&gt;

&lt;p&gt;Só resumindo se você não lembra do episódio passado, o grande lance é você não configurar um servidor manualmente, ser tudo automatizado. E principalmente não instalar todos os componentes da aplicação no mesmo lugar, deixar cada coisa num container separado, como serviços externos independentes. Além disso conseguir fazer deploys frequentes a cada nova funcionalidade ou correção de bug. Tudo tão fácil quanto fazer um &lt;code&gt;git push&lt;/code&gt;, literalmente.&lt;/p&gt;

&lt;p&gt;Na mesma época que o Heroku surgiu, o Google tinha acabado de lançar um produto que fez muito barulho, o Google App Engine pra rodar aplicações feita em Python usando serviços do Google. O problema é que você não podia usar componentes padrão de Linux como banco de dados Postgres. Precisava usar o banco do Google que era o BigTable, um NoSQL que só existe no Google e não dá pra rodar em outro lugar. Além disso precisava programar de formas específicas pro App Engine. Então, uma vez que funcionasse não dava pra rodar em nenhum outro lugar. Era super simples de subir a aplicação mas era super limitado, restritivo e você ficava preso.&lt;/p&gt;

&lt;p&gt;A grande sacada do Heroku foi abrir os padrões e ajudar a criar uma forma onde ao mesmo tempo você queria ficar no Heroku porque tudo era muito simples e seguro, mas não precisava lobotomizar seu código pra fazer rodar. Se quisesse sair e instalar sua própria infraestrutura, sua aplicação não tinha nada proprietário te prendendo. Além disso, eles criaram o conceito de marketplace de software as a service pra desenvolvedores. Você podia sair do Heroku, instalar sua aplicação manualmente num VPS do Linode e ainda usar o mesmo serviço de Papertrail que mostrei antes lá, por exemplo. Os addons também são independentes do Heroku.&lt;/p&gt;

&lt;p&gt;Mas se ainda não ficou óbvio, você não escolhe infraestrutura querendo que seja barata demais, porque você recebe exatamente aquilo que paga. Se paga barato, não espere ter garantia de nada, aliás, espere que seu sistema vai cair quando mais precisa dele e vai sair super caro colocar tudo no ar de novo pra aguentar a carga. É por isso que sempre que possível, a recomendação é começar no Heroku. No outro extremo, se tem dinheiro sobrando, não recomendo ir direto pra um Kubernetes, começa no Heroku. Só faz infra customizada quando tem dados de uso que realmente justificam isso.&lt;/p&gt;

&lt;p&gt;O Heroku mantém as coisas atualizadas. Patches de segurança e tudo mais são aplicados na sua infra, e na maioria dos casos você não precisa fazer nada. O jeito como o Heroku te obriga a organizar seu projeto, não insere nada de proprietário, mas ao mesmo tempo te força a fazer as coisas do jeito certo é a principal vantagem. Quando alguma coisa não funciona no Heroku normalmente não é um problema do Heroku, é problema da sua aplicação que foi feita de um jeito porco.&lt;/p&gt;

&lt;p&gt;E pra facilitar a educação do jeito certo de programar aplicações web, o co-fundador Adam Wiggins escreveu um site chamado The 12 Factors que lista os 12 fatores que todo projeto web deve ter pra ser minimamente possível instalar numa infraestrutura escalável como o Heroku. É como se fossem 12 patterns de gerenciamento de arquitetura de projetos. Esse site foi publicado acho que em 2011 e tudo que está nele continua válido igualzinho até hoje.&lt;/p&gt;

&lt;p&gt;Quem acompanha este canal e conseguiu acompanhar o tutorial de Heroku até aqui já entende a maioria, mas quero comentar ponto a ponto desses doze fatores. Se você é iniciante, preste muita atenção. É isso que se espera que todo programador web saiba de cor e salteado, independente da linguagem ou framework que escolher. Aliás, todos os frameworks web modernos de hoje facilitam você implementar esses 12 fatores. Um framework que não é compatível com esses fatores é um péssimo framework e você não deveria estar usando.&lt;/p&gt;

&lt;p&gt;Vamos lá, o ponto 1 declara que todo código-fonte deve estar num repositório de versionamento de código. Hoje em dia isso só tem uma resposta e se chama Git. Tudo pode estar num único repositório Git, se for um projeto gigantesco talvez esteja dividido em múltiplos repositórios ou monorepos. Não importa, contanto que esteja em Git que todo mundo da equipe tenha acesso e que se faça deploys com frequência. Esse ponto é chave.&lt;/p&gt;

&lt;p&gt;No próprio tutorial do episódio passado eu fiz pelo menos uma meia dúzia de deploys enquanto ia adicionando novas funcionalidades. Antigamente fazer deploys era um processo demorado, manual, cheio de erros humanos. Hoje em dia o certo é que se faça deploys contínuos. Múltiplos por semana, múltiplos por dia. No mínimo em ambiente que chamamos de staging, que é tipo um ambiente de testes onde tudo roda como se fosse ambiente de produção. Não pode existir uma dúzia de desenvolvedores fazendo código semanas inteiras e levar mais semanas pra subir esse código numa infraestrutura de testes. É inadmissível e é sinal claro de incompetência da gerência dessa equipe.&lt;/p&gt;

&lt;p&gt;O segundo fator se chama Dependências e foi por isso que me alonguei tanto falando de gerenciamento de dependências durante o tutorial. É fator fundamental que todas as dependências de um projeto web não dependam de pacotes específicos de sistema operacional instalado na máquina. Isso porque o sistema operacional dos containers do servidor e o sistema na máquina de cada desenvolvedor sempre vão ser diferentes. E versões diferentes de cada pacotes vão estar sendo usados em cada lugar, aí a gente nunca sabe de onde está vindo muitos bugs, se é problema no código ou se é dependência com versões erradas.&lt;/p&gt;

&lt;p&gt;Repetindo: toda dependência deve estar declarada num arquivo de manifesto como &lt;code&gt;composer.json&lt;/code&gt; no nosso projeto de exemplo, ou &lt;code&gt;package.json&lt;/code&gt; num projeto Node. E tanto na máquina do desenvolvedor quanto no script de deploy como do Heroku, vamos rodar o mesmo comando pra baixar as dependências, como o &lt;code&gt;compose update&lt;/code&gt; ou &lt;code&gt;npm install&lt;/code&gt; da vida. E em todos os lugares vamos garantidamente ter exatamente as mesmas dependências nas exatas versões necessárias. Isso é essencial e novamente, projetos que não estão assim demonstram incapacidade da gerência do projeto.&lt;/p&gt;

&lt;p&gt;O terceiro fator se chama Configuração. Em resumo, foi o que expliquei no tutorial sobre a diferença de configuração na máquina de desenvolvimento usando recursos do framework que carrega arquivos como &lt;code&gt;.env&lt;/code&gt; ou algum arquivo JSON da vida e variáveis de ambiente. Todo framework competente permite executar a aplicação em ambientes distintos pra desenvolvimento, testes e produção, no mínimo. Isso porque em desenvolvimento queremos mais detalhes de erros, mais bibliotecas de debugging e monitoramento, mas em produção não queremos divulgar nenhum detalhe de erros e nem desperdiçar memória carregando bibliotecas que só servem em desenvolvimento.&lt;/p&gt;

&lt;p&gt;Em cada um desses ambientes precisamos de variáveis de configuração diferentes, pro ambiente de testes precisamos conectar num banco de dados de teste, por exemplo. Ou não mandar SMS ou notificações de verdade e só logar que tentou mandar. Tudo isso precisa estar facilmente configurável em variáveis de ambiente. Leia os detalhes sobre configuração do seu framework, tem muitas nuances que você precisa estudar e testar com calma.&lt;/p&gt;

&lt;p&gt;O fator 4 é mais relevante se você já é um programador das antigas que instalava tudo num servidor como já expliquei inúmeras vezes que não deveria, com stacks como LAMP. Pra você havia diferença num banco de dados que roda no mesmo localhost da sua aplicação web e um serviço externo de enviar e-mails, por exemplo. Numa aplicação 12 fatores não existe essa distinção. Tudo é um serviço externo, ou um recurso externo, acessível via uma URL. Nada roda junto do container da sua aplicação web, tudo roda externamente. Esse é o quarto fator, backing services.&lt;/p&gt;

&lt;p&gt;É extritamente proibido instalar um servidor MySQL junto com um Redis ou um sistema de filas como RabbitMQ. No mínimo do mínimo, se tiver um hardware parrudo, cada serviço deve estar rodando isolado no seu próprio container ou máquina virtual. Mesmo na sua máquina de desenvolvimento, se possível não instale tudo junto. Rode cada coisa num container Docker e orquestre tudo com Docker Compose. Estude sobre Docker Compose e experimente transformar seu projeto tradicional num conjunto de containers. É até uma boa forma de garantir que você não largou conexões hardcoded pra localhost, que vai funcionar na sua máquina de desenvolvimento mas vai falhar em produção.&lt;/p&gt;

&lt;p&gt;O fator 5 é mais específico pra quem mexe com infra, mas é sobre separar a fase de build da fase de execução. Lembra como toda vez que fazemos &lt;code&gt;git push heroku&lt;/code&gt; eu falei que o Heroku criava uma nova imagem, semelhante ao que o Docker faz quando executamos &lt;code&gt;docker build&lt;/code&gt;? É isso que quero dizer. Antigamente a gente faria o equivalente build manualmente copiando arquivos via FTP e logo na sequência já executaríamos a aplicação e deixaríamos lá rodando.&lt;/p&gt;

&lt;p&gt;Pior, se tivesse algum bugzinho, o normal era abrir um SSH pro servidor ou, Deus me livre, via Telnet, editar algum arquivo manualmente direto lá e pronto. Era super fácil e também a raíz de milhares de problemas. Primeiro porque tudo que você editou na mão direto no servidor não está no repositório de código. Se der pau no servidor, você perde a modificação e tudo pára de funcionar e ninguém sabe porque. Voltamos ao fator 1 que declara que 100% de tudo deve sempre estar num repositório Git.&lt;/p&gt;

&lt;p&gt;Segundo porque digamos que queremos voltar uma release, como expliquei que o Heroku faz. Não tem como, porque todo novo deploy sobrescreve em cima da versão antiga de maneira permanente. Mas gerando builds, sempre vamos ter uma cópia da imagem antiga e podemos facilmente voltar pra ela. E como cada imagem não depende de pacotes específicos do sistema operacional, tudo que ela precisa pra rodar está na imagem. Portanto trocar imagens é uma operação trivial que pode ser feita dezenas de vezes e vai funcionar em todas as vezes, sem depender de nenhuma pessoa entrando nos servidores e subindo coisas manualmente.&lt;/p&gt;

&lt;p&gt;O fator 6 provavelmente é um dos mais difíceis de entender se você for um iniciante. Ela se chama Processos mas na verdade é o fator que define que sua aplicação deve ser share-nothing, ou mais corretamente, stateless, sem estado. O exemplo mais simples é que sua aplicação nunca deve gravar nada no sistema de arquivos. Por exemplo, digamos que você faça uma página pra fazer upload de fotos pra colocar na página de perfil de cada usuário.&lt;/p&gt;

&lt;p&gt;Você segue um tutorial qualquer que manda ir gravando tudo num diretório chamado uploads no servidor. E tudo vai funcionar perfeitamente. Até você fazer um novo deploy no Heroku ou outro sistema de containers e aí vai descobrir que todas as suas fotos sumiram. Os únicos arquivos que existem na sua aplicação são aqueles que existiam na fase de build da imagem, que é o código fonte que baixou do Git, as bibliotecas que um gerenciador de dependências como o Composer baixou, e outros arquivos gerados por scripts executados no build, como arquivos de javascript compilados por um Webpack.&lt;/p&gt;

&lt;p&gt;Uma vez que a build foi fechada, ela não pode mais ser modificada, é pra ser considerada read-only. Daí vocês vão lembrar que eu falei durante o tutorial que o Heroku vai derrubar os dynos que estavam executando a versão antiga e subir novos dynos com a imagem nova. Entenderam? Quando o container é desligado, tudo que estava nele é perdido, por exemplo as fotos no diretório uploads. Quando um novo dyno subir, os únicos arquivos que vão ser carregados são os que estão na imagem que acabamos de construir.&lt;/p&gt;

&lt;p&gt;No caso de um Docker na sua máquina local é a mesma coisa, mas você tem a opção de mapear um diretório externo fora do container e remontar esse diretório quando subir o container de novo, daí os arquivos ainda persistem, mas sempre fora do container. Tudo dentro de um container deve ser considerado efêmero.&lt;/p&gt;

&lt;p&gt;Todo dado que você quer que sobreviva a um deploy deve obrigatoriamente estar num armazenamento externo, um backing service como descrito no fator 4. Por exemplo num banco de dados como Postgres ou Redis e, no caso de arquivos, num serviço como o AWS S3 ou Azure Blob ou Google Cloud Storage da vida. Tem várias outras opções. Mesma coisa logs, por isso o certo é adicionar addons como um Papertrail. Qualquer log que for gravado como arquivo dentro do container vai ser perdido no próximo deploy, e esse é o jeito certo de se fazer aplicações web. Vamos falar mais sobre log no final.&lt;/p&gt;

&lt;p&gt;O fator 7 não é tanto uma preocupação pra nós desenvolvedores, especialmente hoje em dia. Ele só declara que todo serviço que precisamos deve estar exposto numa porta de rede. Como nossa aplicação web que costuma estar exposta em portas como 3000 ou 8080, ou MySQL que costuma estar na porta 3601 e assim por diante.&lt;/p&gt;

&lt;p&gt;Isso eu acho que é mais pra dizer que não devemos usar serviços que não estejam expostos em portas. Por exemplo, pense no serviço cron que existe em todo Linux e serve pra schedular tarefas. A gente configura num arquivo pra executar algum comando em alguma determinada hora do dia ou periodicamente, tipo todo dia às 4 da manhã se for um backup. O único ponto de contato com esse serviço é via um arquivo de configuração, por isso não podemos usar cron numa infraestrutura dessas, com vários containers que não permite modificar arquivos localmente.&lt;/p&gt;

&lt;p&gt;Em vez disso vamos usar um software as a service, um SaaS, que adiciona uma aplicação web na frente do cron, ele vai ser instalado como um backing service, um serviço externo e nossa aplicação web vai interagir com esse serviço via HTTP, conectando na porta web dele. Assim, a requisição pode sair de qualquer um dos nossos containers web e todos vão consistentemente chegar no mesmo serviço sempre, assim como já chegam em outros serviços como o banco de dados.&lt;/p&gt;

&lt;p&gt;O fator 8 se chama Concorrência e meio que só deriva dos fatores anteriores. Ele declara que a forma de escalar é horizontal. Ou seja, como nossa aplicação é share-nothing, ou seja, não depende de nenhum estado específico da máquina onde está, e como já é tudo organizado como builds que podemos subir como containers em qualquer máquina, podemos rapidamente aumentar ou diminuir a quantidade de containers horizontalmente.&lt;/p&gt;

&lt;p&gt;Esse fator é inspirado no jeito UNIX de pensar onde tudo são processos e a forma de escalar é dar fork em mais processos. Mesmo quando usamos alguma linguagem como Javascript ou Elixir, que permite fazer multiplexing via threads ou I/O assíncrono dentro de cada processo, ainda assim sempre vai estar limitada pelo tamanho da máquina onde está rodando e quando esse recurso se exaurir, vai precisa escalar horizontalmente de qualquer jeito. Escalabilidade exige expansão e contração horizontal, que é mexer na quantidade de containers e/ou na quantidade de máquinas físicas.&lt;/p&gt;

&lt;p&gt;Particularmente um Heroku facilita isso porque quando você escala horizontalmente containers web, vai precisar de um load balancer na frente pra distribuir a carga de requisições. É algo que se você só subiu aplicações pequenas nunca teve que pensar, porque só tinha um servidor web e todas as requisições iam direto pra ele. Com mais um servidor web, você precisa de um balanceador de carga como NGINX ou HAProxy na frente. Um Heroku da vida já te dá esse balanceador por padrão de forma transparente.&lt;/p&gt;

&lt;p&gt;O fator 9 é um pouco mais avançado de entender e eu recomendo estudar sobre sinais de processos como SIGTERM e o que é shutdown gracioso e conceitos como rolling restart. Mas na prática esse fator fala sobre Descartabilidade. Você já viu isso na prática no tutorial toda vez que fizemos &lt;code&gt;git push heroku&lt;/code&gt;. Ele faz a build de uma nova imagem e descarta os containers antigos pra subir novos. É isso que significa ser descartável. Uma vez que garantimos que não dependemos de nada dentro do container, todos os dados estão seguros num banco de dados ou outros serviços como o AWS S3 pra arquivos remotos, podemos facilmente descartar containers pra fazer deploy de uma nova release ou dar rollback pra releases anteriores.&lt;/p&gt;

&lt;p&gt;Antigamente, quando a gente instalava tudo num mesmo servidor, não dava pra fazer isso. E se a máquina era invadida por um hacker? É impossível recuperar uma máquina comprometida, o certo é jogar tudo fora e remontar do zero. Mas se você editava coisas via SSH direto no servidor, se parte do que rodava não estava no Git, e se sua aplicação gravava uploads e logs localmente como arquivos. E agora? Como você vai conseguir reinstalar essa máquina o mais rápido possível? E a resposta é simples: não vai. Um sistema robusto precisa ser descartável. Você precisa conseguir apagar o servidor inteiro e reinstalar tudo automaticamente via script sem perder 1 bit de dados e em poucos segundos.&lt;/p&gt;

&lt;p&gt;O ponto 10 é pra reforçar o que acabei de falar, que é manter ambientes de desenvolvimento, staging e produção o mais similares quanto possível. E isso obviamente significa tudo em repositório Git, significa builds automatizados e significa jamais editar arquivos direto no servidor. Os benefícios de fazer as coisas dessa forma é que gerenciar infraestrutura se torna muito mais fácil e quase trivial em alguns casos. É assim que nasce a área de devops de verdade.&lt;/p&gt;

&lt;p&gt;Mais do que isso, esse fator significa fazer deploys pra staging e produção o mais rápido quanto possível pra estar próximo do que está rodando na máquina dos desenvolvedores. Nada disso de demorar um mês ou mais pra fechar uma release e mandar pra produção. Estude sobre Continuous Deployment. Plataformas como GitHub e GitLab tem configurações pra criar builds e rodar os testes da sua aplicação a cada novo commit que aparece. E se tudo passar eles fazem automaticamente o &lt;code&gt;git push heroku&lt;/code&gt; pra staging ou pra produção.&lt;/p&gt;

&lt;p&gt;Parece perigoso isso de subir pros servidores automaticamente mas só é perigoso se sua gerência é uma droga. A quantidade de bugs em produção reflete inversamente a qualidade de comunicação da sua equipe. Uma equipe disfuncional vai subir porcaria pra produção não importa quantos mecanismos de gerenciamento você coloque em cima deles.&lt;/p&gt;

&lt;p&gt;Uma equipe saudável revisa o código um do outro antes de mergear um pull request na branch &lt;code&gt;main&lt;/code&gt; do Git. Uma equipe saudável sempre adiciona testes pra cada nova funcionalidade e sempre adiciona testes que simulam um bug recém reportado, dessa forma esse bug não vai aparecer de novo no futuro. Uma equipe saudável implementa novas funcionalidades atrás um &quot;feature flag&quot;. Seja via variável de ambiente ou via permissão pra determinados usuários testarem e pros demais usuários não conseguirem enxergar essa nova funcionalidade até toda equipe ter chance de testar e aprovar. Ou seja, mesmo a desculpa que uma nova funcionalidade vai levar semanas pra ficar pronto não é justificativa suficiente pra não estar integrando o código no repositório com frequência e não estar fazendo deploys com frequência. Quanto mais se demora pra integrar e deployar mais rápido aumenta a quantidade de bugs e conflitos, é inevitável.&lt;/p&gt;

&lt;p&gt;Existem diversas estratégias de comunicação e organização que uma equipe saudável pode aplicar e eu acabei de descrever algumas. Estude sobre Testes, Integração Contínua e Deploy Contínuo. Só equipes disfuncionais e/ou com péssima gerência são incapazes de trabalhar de forma integrada e contínua. Lembrem-se, não importa a linguagem ou framework, produtividade, bugs, tudo isso é reflexo da qualidade de comunicação de uma equipe. Sempre.&lt;/p&gt;

&lt;p&gt;O fator 11 de novo é meio específico pra quem desenvolve frameworks, mas fala sobre logs serem streams em vez de arquivos. Todo bom sistema manda logs pro STDOUT. Se por acaso você começou a aprender sobre logs, rotação de logs e coisas assim, esquece. Num container todo serviço deve escrever logs pro STDOUT, sempre. Daí a plataforma de infraestrutura, como um Heroku, vai capturar esses streams e mandar pra algum serviço externo de logs, como o Papertrail que mostrei antes.&lt;/p&gt;

&lt;p&gt;Fica a dica que um sistema de log que tenta fazer coisas demais uma hora vai dar catástrofe. Que foi o que aconteceu com o caso recente do Log4j. É bem coisa de javeiro enterprise fazer uma mísera biblioteca de Log que, por alguma razão do além, permite executar comandos que vem no log e ainda vem junto com bibliotecas como de lookup de JNDI que se conecta com o mundo exterior. É o cúmulo da estupidez, pra dizer o mínimo.&lt;/p&gt;

&lt;p&gt;Um log4j nem precisa muito existir, pelo menos não do jeito como é hoje. Todo log deveria ir pro STDOUT e um serviço externo, separado, como Logstash, Kibana e coisas assim que deveriam consumir esses logs e organizar. Isso é fruto daquela mentalidade de arquiteto chimfrin que pensa, &quot;ah, e se amanhã eu precisar dessa opção?&quot; E isso tá errado. Nunca faça coisas que não precisa pra hoje. O dia que a necessidade aparecer aí pensa qual a melhor forma de resolver, mas não faça nada porque tem chances de amanhã precisar. Isso é software desnecessário, e todo software desnecessário é um buraco de segurança esperando pra ser descoberto.&lt;/p&gt;

&lt;p&gt;Finalmente o fator 12, que fala de processos de administração, que eu já mostrei no tutorial pra vocês. A idéia é que você nunca abre um SSH direto pra um dos containers web. Toda tarefa administrativa fora do comum deve ser executado num container isolado e separado dos demais, sem concorrer os mesmos recursos com as requisições de usuários.&lt;/p&gt;

&lt;p&gt;Lembram quando rodei &lt;code&gt;heroku run bash&lt;/code&gt; que abriu um container direto no bash? Ou quando rodei &lt;code&gt;heroku pg:psql&lt;/code&gt; que abriu o console do Postgres pra criar a tabela que faltava? Em ambos os casos o Heroku subiu um novo container com a imagem da última release, ou seja, idêntico ao conteúdo dos containers web, mas fora do load balancer. Assim eu rodo no mesmo ambiente e quando terminar e desconectar, esse container é automaticamente destruído. E isso é importante, lembra o fator sobre descartabilidade?&lt;/p&gt;

&lt;p&gt;Os 12 fatores foram muito influentes quando foram lançados porque na época o único framework que implementava com sucesso todos os fatores era o Ruby on Rails. Levou um tempão pros outros alcançarem esse mínimo. Produtos open source que não usavam nenhum framework como Wordpress ou Magento não implementavam todos os 12 fatores por padrão e também levou um tempo pra se adequarem.&lt;/p&gt;

&lt;p&gt;O Heroku foi lançado entre 2008 e 2009, só uns 2 anos depois que a Amazon AWS apareceu. Naquela época a maioria de nós só tinha mentalidade de servidor virtual, no máximo, VPS. Ninguém pensava em particionar recursos da mesma máquina em containers e muito menos precificar por containers. Docker ainda estava a anos de ser lançado e na verdade muito do Docker foi influenciado pelo Heroku.&lt;/p&gt;

&lt;p&gt;A Amazon estava muito a frente do seu tempo, tanto que ninguém sabia exatamente qual a melhor forma de tirar proveito dessa idéia de infraestrutura elástica. O que significava ser elástico? Como lidar com máquinas voláteis e descartáveis que quando reiniciam apagam tudo que tinha dentro? O Heroku foi a primeira plataforma como serviço ou PaaS que de fato soube aproveitar a infraestrutura que a AWS estava oferecendo.&lt;/p&gt;

&lt;p&gt;O sonho de muitas empresas é ter um Heroku privado, o famoso private cloud, onde equipes internas poderiam facilmente dar &lt;code&gt;git push&lt;/code&gt; e o deploy aconteceria automaticamente. E muitas empresas já trabalham assim. É o que todo mundo que tenta instalar Kubernetes busca. O que o Heroku chama de dynos, o Kubernetes chama de pods. A linha de comando &lt;code&gt;kubectl&lt;/code&gt; da vida é inspirado na linha de comando do Heroku. Enfim, se você lida com devops de alguma forma hoje em dia, tenha certeza que muita coisa foi inspirado no Heroku.&lt;/p&gt;

&lt;p&gt;Se parece fanboy falando, pode dizer que sou mesmo, porque em mais de 10 anos entregando projetos de verdade pra clientes, ainda não vi outra solução que chega perto em termos de simplicidade, flexibilidade, organização e custo benefício. Muitos acham caro a faixa de 30 dólares por dyno e é mesmo, e fazem a conta comparando com o preço bruto da hora de uma máquina no EC2. Mas a conta tá errada. A conta certa é calcular quanto custa ter uma pessoa experiente de devops de plantão todo dia pra manter sua infra. O cálculo certo não é da máquina mas do cara de devops que você não precisa ter quando usa o Heroku.&lt;/p&gt;

&lt;p&gt;Aliás, antes que alguém vá nos comentários dizer sim, eu sei que existem soluções como o Dokku que implementa um Heroku light que você mesmo pode instalar num VPS da vida. Você também pode usar direto Docker Machine pra orquestar suas VPS. Mas de novo, você precisa adicionar o custo de alguém de infra pra dar manutenção nessa infraestrutura. Nunca vai ser plug and play que uma vez instalado ninguém mais precisa mexer. Não caia nessa armadilha.&lt;/p&gt;

&lt;p&gt;Além disso tem soluções como o Google Firebase que, como o Google App Engine antes dele, você programa especificamente pra essa plataforma, e vai depender pra sempre do que o Google te oferecer. Esquece querer mudar pro Azure ou AWS depois. E esquece querer usar qualquer ferramenta open source e bibliotecas que a plataforma não suporta. Tudo tem prós e contras e meu ponto é que o Heroku oferece o melhor custo benefício entre conveniência, segurança e independência do seu projeto.&lt;/p&gt;

&lt;p&gt;Também vale relembrar que Heroku e 12 fatores não são balas de prata, eu tenho um caso pra ilustrar. Lá por 2014 estava pessoalmente trabalhando num projeto pra cliente que envolvia uma ferramenta que seria usada por vestibulandos. Era uma plataforma de educação e os donos tinham a expectativa que ia vir uma montanha de alunos de cursinho por causa de algum evento que eu não lembro mais o que era. Mas em resumo eles pediram especificamente pro sistema ser escalável.&lt;/p&gt;

&lt;p&gt;Eu e minha equipe fizemos tudo em Ruby on Rails, que é o framework que funciona melhor no Heroku, especialmente naquela época, porque o próprio Heroku é em boa parte feito em Rails também. Fizemos deployment mas na hora do vamos ver sofremos um gargalo. Obviamente foi culpa minha que não estimou direito o tamanho do bancos de dados Postgres.&lt;/p&gt;

&lt;p&gt;O problema é o seguinte: todo servidor de banco de dados tem um máximo de conexões simultâneas que aguenta. Faz de conta que são 100. Mesmo implementando um connection pool, se eu escalar horizontalmente a aplicação, ou seja, aumentar o número de dynos web rodando, todos vão se pendurar no banco de dados e rapidamente 100 conexões vão ficar ocupados e novos dynos vão ficar travados esperando conseguir uma conexão.&lt;/p&gt;

&lt;p&gt;Pra lidar com isso a gente tenta mover o que não precisa pegar direto do banco em caches como Redis. E se precisa muito do banco, o certo é subir vários outros servidores que são réplicas só de leitura do principal. Assim cada nova réplica aguenta mais 100 conexões simultâneas, por exemplo. Mas não dá pra colocar réplicas infinitamente porque quanto mais réplicas tem, maior o trabalho de replicação dos dados, aí vai dar gargalo na sincronia de replicação.&lt;/p&gt;

&lt;p&gt;Muito iniciante que entra em plataformas como Heroku achando que vai colocar um addon de auto-scale e tudo vai escalar infinitamente sozinho, se decepciona que não funciona mágico assim. Escalabilidade é uma combinação de funcionalidades de infraestrutura e arquitetura bem implementada. Depende mais do programador do que da infraestrutura. Mas se você for um bom programador, um Heroku dá tudo que você precisa com o mínimo de esforço. É impressionante o que eu consigo fazer sozinho num ambiente desse em 5 min, coisa que no começo dos anos 2000 ia me custar dias de configuração. Já se você for um programador ruim, nenhum Heroku ou solução de cloud vai te ajudar, só vai ser um band-aid temporário.&lt;/p&gt;

&lt;p&gt;Eu já venho repetindo isso faz alguns episódios e vou repetir de novo. Eu tenho projetos pra clientes que funcionam assim, mas o certo como já disse antes é que os ambiente de staging e produção não estejam muito longe da versão em desenvolvimento na máquina dos desenvolvedores. Pra que isso seja possível, precisa obrigatoriamente existir os seguintes passos.&lt;/p&gt;

&lt;p&gt;Primeiro passo, toda nova funcionalidade e toda correção de bugs deve vir acompanhado no mínimo de testes unitários. Todo pull request sem testes deve ser automaticamente rejeitado. Sem exceções. Essa é a principal regra que vai garantir que seu projeto sobreviva por muitos anos. No começo, com pouca gente, com pouco código e com prazos apertados, parece que é uma grande perda de tempo. E é assim que amadores pensam e é assim que amadores se ferram quando o projeto rapidamente cresce fora de controle.&lt;/p&gt;

&lt;p&gt;Quando você satisfaz essa primeira condição agora precisamos do segunda passo: que é integrar o código constantemente e frequentemente. Ou seja, nenhum desenvolvedor deve ficar dias e dias com código que só existe na sua máquina, demorando pra dar push pro repositório no GitHub ou GitLab. Todo desenvolvedor deve, no mínimo uma vez por dia, dar pull, ou seja, puxar as últimas atualizações do repositório pra pegar tudo que todo mundo da equipe andou trabalhando e dar rebase com seu branch de desenvolvimento e corrigir conflitos. E com sua suite de testes unitários, rapidamente ver o que quebrou e consertar imediatamente. Se você faz isso com frequência, os conflitos são simples de resolver, se der conflitos. Quanto mais tempo demora pra integrar, mais difícil vai sendo pra resolver os conflitos do seu código com dos outros programadores da equipe, e maiores as chances de fazer bosta e subir bugs.&lt;/p&gt;

&lt;p&gt;Isso é integração contínua. Facilita muito se como regra número três você estiver usando algo como GitLab que tem suporte a rodar testes automaticamente no servidor, ou usar qualquer outra solução como CircleCI. Você deixa um Dockerfile configurado no projeto, o GitLab vai buildar toda vez que alguém der push pro repositório e colocar numa fila pra rodar todos os testes automatizados. Se falhar já notifica pra todo mundo e assim garantimos que todo código que está na branch &lt;code&gt;main&lt;/code&gt; sempre tem todos os testes rodando e passando. O objetivo é esse: que sempre que alguém der clone do branch &lt;code&gt;main&lt;/code&gt;, tudo sempre vai funcionar. O branch &lt;code&gt;main&lt;/code&gt; tem que ser sagrado. Tudo que está inacabado deve estar em branches de desenvolvimento separados.&lt;/p&gt;

&lt;p&gt;É essa condição que permite a próxima regra número quatro que é, toda vez que todos os testes automatizados passarem, deve ser feito deployment pra um ambiente de staging. O Heroku suporta criar pipelines de staging e production, assim você sempre tem uma infraestrutura idêntica de testes e de produção. Daí um GitLab da vida mesmo faz o &lt;code&gt;git push heroku&lt;/code&gt; automaticamente pra staging e sua equipe pode imeditamente testar no navegador como um usuário normal pra ver se realmente nada quebrou, se as novas funcionalidades estão mesmo funcionando.&lt;/p&gt;

&lt;p&gt;Recapitulando: uma equipe saudável e produtiva vai sempre desenvolver código com testes automatizados, vai sempre integrar continuamente, vai sempre ter um servidor de testes como GitLab que roda a cada push, de cada desenvolvedor. E finalmente, vai sempre ter deployments automáticos pra ambiente de staging toda vez que todos os testes automatizados passam. E fica como opção subir pra produção automaticamente ou deixar alguém de QA,
 por exemplo, responsável por fazer testes manuais no final e apertar o botão pra deployar pra produção e liberar pros usuários.&lt;/p&gt;

&lt;p&gt;Um modelo similar ao Heroku, com um projeto arquitetado pra atender os 12 fatores, e com minhas 4 últimas regras é o que eu chamo do framework mínimo pra toda equipe de desenvolvimento verdadeiramente Ágil. Toda outra metodologia, sistema de backlog e controles, é opcional e não relevante pra produtividade de verdade. Lembrem-se, a qualidade do software é diretamente proporcional à qualidade da comunicação da equipe. Não tem como fugir disso. Se sua equipe se comunica mau e porcamente, seu software vai ser necessariamente uma grande droga.&lt;/p&gt;

&lt;p&gt;A idéia hoje foi só apresentar pra quem é iniciante como que profissionais de verdade trabalham ou gostariam de trabalhar. E é esse nível que vocês deveriam almejar alcançar. Se você trabalhar num lugar que não funciona assim, tente evangelizar o jeito certo. Vai implementando um aspecto de cada vez, vai fazendo limpeza um pedaço de cada vez. Mesmo nos projetos mais antigos e mais porcos é possível transformar num projeto eficiente se, primeiro, a equipe entende que dá pra melhorar e se compromentem a fazer limpeza e não aumentar mais a sujeira. Por hoje é isso aí. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e compartilhem o video com seus amigos. A gente se vê, até mais.&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
  <entry>
    <id>tag:www.akitaonrails.com,2008:Post/5946</id>
    <published>2022-01-10T12:15:00-03:00</published>
    <updated>2022-01-10T11:16:59-03:00</updated>
    <link href="/2022/01/10/akitando-112-subindo-aplicacoes-web-em-producao-aprendendo-heroku" rel="alternate" type="text/html">
    <title>[Akitando] #112 - Subindo Aplicações Web em Produção | Aprendendo HEROKU</title>
    <content type="html">&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/TLRW_xTnQwY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Finalmente vou mostrar o que é o Heroku e como é o fluxo de trabalho mínimo de um projeto web ideal. Se você já usa Heroku, aproveite pra compartilhar o video com conhecidos que ainda não usam. Se nunca viu Heroku, prepare-se pra ficar surpreso!&lt;/p&gt;

&lt;h2&gt;Conteúdo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;00:00 - Intro&lt;/li&gt;
&lt;li&gt;00:41 - Mega Sena da Virada&lt;/li&gt;
&lt;li&gt;02:33 - Concorrência&lt;/li&gt;
&lt;li&gt;04:11 - Intro a Heroku&lt;/li&gt;
&lt;li&gt;04:46 - Deploy antigo&lt;/li&gt;
&lt;li&gt;06:24 - Iniciando Tutorial Heroku&lt;/li&gt;
&lt;li&gt;06:49 - Setup: Git é Básico!&lt;/li&gt;
&lt;li&gt;07:22 - Heroku Login&lt;/li&gt;
&lt;li&gt;08:18 - Primeiro Deploy&lt;/li&gt;
&lt;li&gt;10:03 - Buildpack? Dockerfile?&lt;/li&gt;
&lt;li&gt;10:35 - Entendendo Dynos e Containers&lt;/li&gt;
&lt;li&gt;11:30 - Escalando Dynos&lt;/li&gt;
&lt;li&gt;13:16 - Entendendo Load Balancer&lt;/li&gt;
&lt;li&gt;16:41 - Vendo logs dos Dynos&lt;/li&gt;
&lt;li&gt;17:22 - Entendendo Procfiles&lt;/li&gt;
&lt;li&gt;18:58 - Mais sobre escalar dynos&lt;/li&gt;
&lt;li&gt;20:21 - Entendendo Gerenciamento de Dependências&lt;/li&gt;
&lt;li&gt;24:37 - Adicionando nova dependência&lt;/li&gt;
&lt;li&gt;26:40 - Adicionando Addons&lt;/li&gt;
&lt;li&gt;28:16 - Conectando num shell remoto&lt;/li&gt;
&lt;li&gt;29:41 - Configurando com variáveis de ambiente&lt;/li&gt;
&lt;li&gt;33:39 - Adicionando Banco de Dados&lt;/li&gt;
&lt;li&gt;37:13 - Database Migrations&lt;/li&gt;
&lt;li&gt;37:47 - Conectando no Banco de Dados&lt;/li&gt;
&lt;li&gt;39:22 - Releases e Rollback&lt;/li&gt;
&lt;li&gt;40:34 - Conclusão&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Mega da Virada: Loterias Caixa colocam fila em app, e site fica fora do ar (https://tecnoblog.net/noticias/2021/12/31/mega-da-virada-loterias-caixa-colocam-fila-em-app-e-site-fica-fora-do-ar/)&lt;/li&gt;
&lt;li&gt;Getting Started on Heroku with PHP (https://devcenter.heroku.com/articles/getting-started-with-php#set-up)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SCRIPT&lt;/h2&gt;

&lt;p&gt;Olá pessoal, Fabio Akita&lt;/p&gt;

&lt;p&gt;Bem vindos ao primeiro video de 2022. O video de hoje vai ser parte 1 de 2 videos. Eu queria falar sobre os famosos 12 fatores pra criar projetos escaláveis modernos. Pra fazer isso eu preciso que todo mundo já tenha visto pelo menos uma vez como é trabalhar com uma plataforma chamada Heroku. Se você já trabalha com Heroku, o que vou falar hoje é o arroz com feijão, mas vocês podem usar pra apresentar a plataforma pra amigos seus que ainda não conhece. Pra todo o resto, esse é o modelo de trabalho que você deveria almejar se nunca usou essa plataforma.&lt;/p&gt;

&lt;p&gt;(...)&lt;/p&gt;

&lt;p&gt;Antes de começar deixa eu responder algumas coisas que vi online depois dos últimos vídeos. No caso do exemplo que dei do ingresso.com eu terminei concluindo que se eles não tem controle sobre o sistema de reserva de assentos dos cinemas - que imagino que sejam aplicativos super antigos que não foram feitos pra escala de internet - então deveriam controlar o acesso a esses sistemas usando uma fila virtual. Ninguém acessa diretamente a reserva de assentos, cai primeiro numa página mais fácil de escalar de fila e fica esperando a vez.&lt;/p&gt;

&lt;p&gt;Por acaso foi exatamente isso que a Caixa Econômica Federal fez pra Mega Sena da virada que ia sortear mais de 300 milhões de reais. E eles implementaram justamente uma fila virtual pra ver se aguentavam a multidão que ia tentar acessar tudo no último dia. Eu não perco meu tempo apostando em loteria porque é só garantia que vou estar dando dinheiro de graça pros outros, mas quem tentou parece que teve muita dor de cabeça mesmo tendo esse sistema de fila implementado.&lt;/p&gt;

&lt;p&gt;Mas é óbvio. Só porque você implementou algum pattern ou arquitetura não existe nenhuma garantia que implementou certo ou que não exista nenhum outro problema que ficou pra trás. Eu não sei como eles fizeram, mas imagino pelo menos 2 problemas. O primeiro e mais óbvio é que o tráfego gerado por uma Mega Sena dessas deve ter sido absurdamente alto. No nível que até uma Amazon da vida sofreria um pouco. Então dou o benefício da dúvida que eles tentaram o máximo mas mesmo assim o tráfego foi ainda maior do que era imaginado.&lt;/p&gt;

&lt;p&gt;O segundo problema, que acho que é o mais factível, é que o sistema foi mal feito mesmo. Não me entendam mal. Eu acho que existem sim, bons programadores, com boas capacidades técnicas e boas intenções que tentaram o máximo pra fazer isso funcionar. Porém, a grande maioria dos funcionários públicos que trabalham nos diversos departamentos e sistemas integrados provavelmente não tavam dando a mínima e foram obstáculos pros poucos que tentaram fazer as coisas direito.&lt;/p&gt;

&lt;p&gt;Entenda a seguinte verdade: nenhum sistema feito pelo estado jamais vai ser tão bom quanto os melhores sistemas feitos na iniciativa privada. É uma impossibilidade. O motivo é muito simples: não tem concorrência. A mesma coisa vale pra empresas privadas que tem conluio com governos, que passam a ser basicamente empresas públicas com fachada de empresa privada. A maioria dos grandes monopólios só existem porque foram auxiliados pelos governos.&lt;/p&gt;

&lt;p&gt;Sem concorrência, pra que eu preciso fazer um sistema melhor? As pessoas têm outra opção? Não, então foda-se, vai ser obrigado a usar o que tem. Mesmo se for uma merda. E eu, que trabalho aqui, vou ser mandado embora se não funcionar? Também não, então foda-se de novo. E vai ser sempre assim. Eu sinto pena dos poucos que trabalham lá que querem fazer as coisas funcionar direito, mas vão sempre esbarrar em um monte de departamento que tá pouco se fodendo e um monte de burocracia que impede melhorias. Mas é isso aí.&lt;/p&gt;

&lt;p&gt;Software nunca tá “pronto”. Se o lugar tem mentalidade de &quot;terminar&quot; o software e não mexer mais, sempre vai ser uma porcaria. O Software precisa estar constantemente atualizado, constantemente consertado, constantemente expandindo e remodelado. O problema em lugares como uma Caixa é que deve ter dezenas de sistemas que tá no esquema &quot;tá funcionando? então não mexe!&quot; só que eventualmente vai precisar integrar, se comunicar. Mas não tem como, daí vai nascendo dezenas de gambiarras pra fazer as coisas funcionarem ao redor dessas ilhas radioativas. Deixe passar alguns anos assim e você tem a experiência bosta que foi a Mega da virada. É simples assim.&lt;/p&gt;

&lt;p&gt;E isso me lembrou de um assunto que eu queria explicar faz algum tempo. No video sobre a história do Ruby on Rails eu mencionei rapidamente sobre Heroku e a metodologia de 12 fatores do Adam Wiggins, um dos co-fundadores do Heroku. Objetivo hoje vai ser dar uma repassada nesses conceitos pra quem nunca viu. Se você é um programador sênior certamente já sabe de tudo isso e certamente já usa no seu dia a dia. Eu não chamaria ninguém que não segue no mínimo os 12 fatores de sênior. E eu digo o mínimo porque isso é o básico do básico e na parte final do video vou até complementar mais algumas coisas que todo mundo já deveria saber.&lt;/p&gt;

&lt;p&gt;Os 12 fatores em si vou falar no próximo video. Pra entender os 12 fatores você precisa saber o que é Heroku. Se nunca usou Heroku faça um favor a você mesmo e veja qualquer tutorial básico de como subir um aplicativo nele. Pra um iniciante é uma experiência que muda sua forma de pensar em deploys. Pra quem é das antigas e nunca se atualizou também vai ser um choque. Quando precisava subir um projeto web pra produção, como a gente fazia nos anos 90 e começo dos anos 2000? A gente configurava um servidor remoto Linux, num Virtual Private Server ou VPS como um Linode da vida.&lt;/p&gt;

&lt;p&gt;Nesse servidor a gente configurava tudo manualmente via telnet ou SSH. Instalava um banco de dados como MySQL, um servidor web como Apache, alguma linguagem interpretada como Perl ou PHP, copiava os arquivos da aplicação, normalmente feita em Perl ou PHP, via FTP ou hoje em dia SFTP e pronto, tava tudo no ar. Essa stack inclusive tinha um nome, chamado LAMP, que é acrônimo pra Linux, Apache, MySQL e Perl ou PHP. Isso é tecnologia do fim dos anos 90. Entenda, se alguém fizer isso hoje em 2022, você tá parado no tempo mais de 20 anos. Ninguém em sã consciência atualiza código direto no servidor assim via FTP mais.&lt;/p&gt;

&lt;p&gt;Eu já tinha dito no episódio do ingresso.com que o jeito errado é fazer tudo rodar numa única máquina, configurar tudo manualmente. Atualizar código direto lá é a pior forma possível de colocar uma aplicação no ar. Agora vamos ver como é o estado da arte, a melhor forma de fazer deploy tanto do ponto de vista de escalabilidade quanto de segurança. E pra isso eu vou seguir um tutorial oficial do próprio Heroku pra vocês verem como é absurdamente simples.&lt;/p&gt;

&lt;p&gt;O tutorial começa com o básico. Você obviamente precisa ter uma conta criada no Heroku. Se ainda não fez isso, vai lá depois de assistir o video e cria a conta. Eles tem tutoriais pra quem usa Rails, Node, Java e muito mais, mas já que falei da stack LAMP, vamos ver como se sobe uma aplicação PHP. Vamos assumir que se você é de PHP já tem tudo instalado e obviamente usa Composer pra gerenciar suas dependências.&lt;/p&gt;

&lt;p&gt;Se você for de outras linguagens como Python ou Elixir, não importa. Se nunca viu Heroku funcionando não interessa a linguagem, se atenha ao passo a passo e o raciocínio. Qualquer bom programador tem que ser capaz de no mínimo seguir o raciocínio mesmo se for em outra linguagem. Também vamos assumir que você já sabe usar minimamente Git. Se ainda não sabe, assistam meus vídeos sobre Git depois. E vocês se lembram que no video de Conhecimentos Básicos eu falo que saber Git é básico? Pois é, repito, é básico. Não tem como trabalhar em projetos modernos sem saber Git.&lt;/p&gt;

&lt;p&gt;Agora, pra usar Heroku precisamos instalar a linha de comando do heroku. Praticamente tudo que vai precisar tem nessa linha de comando. Num archlinux da vida basta fazer um &lt;code&gt;yay -S heroku-cli&lt;/code&gt;, num Ubuntu da vida dá pra instalar com Snap. Procure como instalar pra sua distro mas no final você deve ser capaz de abrir um terminal, digitar &lt;code&gt;heroku login&lt;/code&gt; e vai abrir um navegador pra logar na conta que acabou de criar. E, claro, sempre habilite autenticação de duas etapas, é o mínimo.&lt;/p&gt;

&lt;p&gt;No próximo passo ele vai mandar você fazer o clone de um projetinho de exemplo feito em PHP Symfony, que é um dos muitos frameworks web pra PHP. Symfony tem muita inspiração tanto em Rails quanto Django. Pra projetos novos recomendo usar Laravel, que tem uma comunidade mais ativa e um ecossistema que cresceu bem em torno dele. De qualquer forma, se você sabe Git, clonar um projeto é arroz com feijão. Tudo vai acontecer dentro do diretório desse projeto, então só dar &lt;code&gt;cd&lt;/code&gt; pra ele.&lt;/p&gt;

&lt;p&gt;Agora vamos subir essa aplicação de exemplo. Pra isso precisamos cadastrar uma nova aplicação no Heroku. Você pode subir quantas aplicações quiser na sua conta e pode começar com opções gratuitas. Depois pode transferir a propriedade das aplicações que subiu pra outra conta, como do seu cliente. Pra cadastrar é simples, do diretório do projeto, num terminal, só usar a linha de comando que instalamos e fazer &lt;code&gt;heroku create&lt;/code&gt;. Se não disser que nome quer, ele vai inventar um aleatório. O nome em si não interessa tanto porque depois você vai registrar um domínio de verdade e apontar pra esse nome, o CNAME, então o usuário final mesmo nunca vai ver esse nome. Mas vai servir pra gente testar no domínio do Heroku.&lt;/p&gt;

&lt;p&gt;Uma vez a aplicação registrada na sua conta, agora é só subir o código. Quando rodou o comando anterior, ele também criou um branch remoto no Git do seu projeto. Então agora é só subir o que tem na branch principal &lt;code&gt;main&lt;/code&gt; que antigamente se chamava &lt;code&gt;master&lt;/code&gt; pra esse remote chamado &lt;code&gt;heroku&lt;/code&gt;. Relembrando, todos os remotes ficam no arquivo &lt;code&gt;.git/config&lt;/code&gt;, dá um &lt;code&gt;cat&lt;/code&gt; nele pra ver o conteúdo, olha o remote lá. Agora é só fazer &lt;code&gt;git push heroku main&lt;/code&gt;. Olha o que vai acontecer no seu terminal.&lt;/p&gt;

&lt;p&gt;Se você ainda não entendeu, o Heroku criou um repositório Git remoto associado com sua nova aplicação, esse é o remote. É como se fosse um projeto novo no GitHub que você vai dar push. Mas o Git tem uma funcionalidade que você pode configurar que ele detecte quando você faz um &lt;code&gt;push&lt;/code&gt; e rodar algum script. No caso, o Heroku detectou que tá subindo código PHP e por isso vai automaticamente instalar a buildpack pra ter as ferramentas de PHP que vai precisar como o próprio interpretador PHP, o Composer, Apache, NGINX e tudo mais. O que o Heroku chama de buildpack é mais ou menos o que você chamaria de Dockerfile.&lt;/p&gt;

&lt;p&gt;Aqui vale um adendo se você já usa Docker. Por que o Heroku reinventou a roda com esses buildpacks em vez de usar Dockerfiles? Na verdade é o contrário: o Heroku precede a invenção do Docker. Na realidade, muito do Docker foi inspirado no que o Heroku fez anos antes dele. O Heroku foi lançado por volta de 2008, o Docker foi lançado só em 2013. O objetivo da vida do Docker e tudo que saiu em torno de containers como Docker Compose, Docker Machine, Dokku, até Kubernetes, é conseguir imitar o que o Heroku fez antes de todo mundo.&lt;/p&gt;

&lt;p&gt;Vamos considerar o que tá acontecendo nesse ponto. O comando &lt;code&gt;git push&lt;/code&gt; que fizemos tá mandando o Heroku fazer o equivalente a criar uma imagem de Docker, é semelhante a um &lt;code&gt;docker build&lt;/code&gt;. Nos servidores deles rodam diversos containers, que eles chamam de &lt;code&gt;dynos&lt;/code&gt;. Tem de diversos tamanhos, a versão gratuita são dynos de 512 megabytes de RAM, se não me engano com uns 4 cores ou núcleos virtuais fraquinhos. Se precisar, dá pra subir pra versões de 1 giga até 14 giga de RAM se precisar muito.&lt;/p&gt;

&lt;p&gt;Mas cuidado, a grande maioria das aplicações deveria conseguir rodar suficientemente bem em 512 mega de RAM, se precisa de mais que isso precisa ver se não tá com vazamento de memória, ou você programou muito porcamente e tá enchendo a memória de lixo. Trabalhar em um ambiente mais restrito do que sua máquina local é uma boa prática. Qualquer notebook hoje tem 8 giga ou mais de RAM e você muitas vezes nem percebe que sua aplicação tá usando muito mais memória do que deveria.&lt;/p&gt;

&lt;p&gt;Seguindo o tutorial a próxima coisa que ele manda fazer é escalar a aplicação com o comando &lt;code&gt;heroku ps:scale web=1&lt;/code&gt; que basicamente manda o Heroku subir sua aplicação num único dyno. Se mudar pra web igual a 2, ele vai subir dois dynos. O que significa isso? Vamos fazer uma conta de padeiro. Eu disse que o dyno gratuito tem uns 4 cores virtuais. Se eu configurar o apache pra pendurar um fork de processo por núcleo significa que consigo ter até 4 requisições simultâneas. Se subir 2 dynos eu posso ter até 8 requisições simultâneas. Simultâneo significa exatamente no mesmo instante. Nessa conta de padeiro estou considerando 1 requisição por processo, sem considerar opções de threads ou I/O assíncrono.&lt;/p&gt;

&lt;p&gt;Digamos que a aplicação leve 100 milissegundos pra responder uma requisição. Então em 1 segundo daria pra responder até 10 requisições. Com 1 dyno então seria teoricamente possível responder até 40 requisições por segundo e com 2 dynos até 80 requisições por segundo, entenderam? Isso é teórico porque estou assumindo que cada core conseguiria responder 10 requisições todo segundo, mas depende se durante o processamento de cada requisição se não bloqueia algum recurso que outra requisição pode precisar, como banco de dados e coisas assim, por isso o tempo real pode variar bastante.&lt;/p&gt;

&lt;p&gt;Containers, como Docker, como Dynos de Heroku, não são máquinas virtuais. Eu explico sobre isso no meu episódio sobre Devops. Se você não sabe a diferença recomendo que assista depois. Mas na prática só entenda que é uma forma de particionar os recursos da sua máquina e cada programa rodar isoladamente achando que está sozinho nessa máquina. Assim é possível particionar uma máquina real grandona em diversos containers menores, dividir os recursos, e te cobrar de uma forma mais fácil.&lt;/p&gt;

&lt;p&gt;Mais do que isso, o Heroku já deixa muita coisa de infraestrutura preparada pra você. Por exemplo, eu expliquei que tem esse comando &lt;code&gt;ps:scale&lt;/code&gt; que permite subir a imagem da sua aplicação em múltiplos dynos. Significa que você tem vários servidores web de pé ao mesmo tempo. Agora, quando um usuário digitar a URL pra sua aplicação ele não cai direto no servidor web da sua aplicação e sim num balanceador de carga, ou load balancer, proprietário do Heroku, que vai pegar as requisições e distribuir nos dynos que você tem de pé provavelmente usando uma estratégia como round robin.&lt;/p&gt;

&lt;p&gt;Se você é iniciante isso pode parecer estranho. Quando sobe um processo de servidor web na sua máquina local, seja nginx, seja apache, seja um webpy de python, tomcat de java, puma de rails, vai ser um processo pendurado em uma porta. Toda vez que no seu navegador você carrega &lt;code&gt;localhost:3000&lt;/code&gt; ele vai direto pra quem responde nessa porta 3000 e é isso. Mas quando você sobe vários containers, cada um com seu próprio servidor na porta 3000, cada um dos containers tem um IP interno próprio. Faz de conta, 172.16.0.10 e 172.16.0.11.&lt;/p&gt;

&lt;p&gt;No seu navegador você não pode mais usar localhost porque não tem mais ninguém no ip local 127.0.0.1 na porta 3000, que seria o localhost. Quando você sobe containers de Docker localmente, ele cria uma rede virtual local. Cada container de Docker que sobe ganha um IP virtual privado e nele que o servidor web da sua aplicação vai se pendurar na porta 3000 por exemplo. Se achou confuso recomendo que estude e treine Docker na sua máquina local, em particular usando Docker Compose pra orquestar cada serviço num container separado.&lt;/p&gt;

&lt;p&gt;Você pode naturalmente digitar direto o ip privado de um dos containers como 172.16.0.10:3000 e aí vai carregar sempre só desse container, mas o segundo container vai ficar lá parado sem trabalhar. Pra conseguir acessar os dois containers, em vez de acessar direto, pode subir um terceiro container, com um load balancer como o NGINX ou HAProxy ou vários outros. Nesse load balancer você configuraria uma regra dizendo, toda vez que alguém mandar uma requisição na porta 80 eu envio pra porta 3000 de um dos dois containers que tenho cadastrado.&lt;/p&gt;

&lt;p&gt;Isso que se chama um proxy reverso. Digamos que o container do load balancer subiu com IP virtual 172.16.0.13. Agora você pode ir no navegador e digitar &lt;code&gt;172.16.0.13&lt;/code&gt; que por default vai conectar na porta 80. O load balancer vai pegar essa requisição e mandar pra um dos dois containers web respondendo nas suas portas 3000. Pro navegador é transparente. Ele não tem idéia de quantos containers web tem por baixo, nem em que portas eles respondem de verdade, nem que IPs tem. Load balancer é tanto uma forma de distribuir requisições pra mais servidores web quanto anonimizar os IPs dos servidores para que os usuários não saibam quem são.&lt;/p&gt;

&lt;p&gt;O objetivo do episódio não é ensinar redes, mas como tem muito iniciante assistindo achei melhor pelo menos dar o resumo do resumo. Pra saber mais procurem artigos sobre load balancer, em particular tentem fazer exatamente esse cenário que eu falei: subir com Docker Compose 2 ou mais containers com uma aplicação web qualquer e outro container com NGINX configurado como load balancer e veja na prática suas requisições sendo distribuídas pelos seus containers. Tem dezenas de tutoriais que você acha no Google pra fazer isso, só não ter preguiça de procurar, pra treinar qualquer um serve.&lt;/p&gt;

&lt;p&gt;Falando nisso, voltando pro tutorial do Heroku, o próximo passo é ver se sua aplicação subiu direito e está respondendo como deveria. Pra isso você precisa conseguir ver os logs em tempo real. Se fosse uma aplicação local era só fazer tipo um &lt;code&gt;tail log/application.log&lt;/code&gt; pra ficar monitorando o que entrar no log em tempo real. Tail é inglês pra rabo, e como o nome diz a gente fica seguindo o rabo, ou seja, o final do arquivo. Pro Heroku é parecido mas só usar a linha de comando deles fazendo &lt;code&gt;heroku logs --tail&lt;/code&gt;. Lógico, o &lt;code&gt;--tail&lt;/code&gt; é opcional, mas se usar vai manter o log aberto e tudo que for sendo logado vai aparecendo no seu terminal. De novo, se você tá acostumado a usar Linux isso é algo super comum.&lt;/p&gt;

&lt;p&gt;Fizemos o clone da tal aplicação de exemplo mas nem vimos o que tem nele. O principal é entender que toda aplicação que sobe no Heroku precisa ter pelo menos um arquivo na raíz do projeto chamado &lt;code&gt;Procfile&lt;/code&gt;. Nele definimos que no container de tipo &lt;code&gt;web&lt;/code&gt; vai executar o binário executável do apache apontando pra pasta &lt;code&gt;web&lt;/code&gt;. Isso é específico de cada framework. Se fosse um Rails o executável seria &lt;code&gt;bin/rails server&lt;/code&gt; e assim por diante. É assim que o Heroku sabe o que é pra executar dentro do container. Se fosse um &lt;code&gt;Dockerfile&lt;/code&gt; seria o equivalente ao parâmetro &lt;code&gt;CMD&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Não vou mostrar isso hoje, mas além de web existem outros tipos de containers que você pode declarar como &lt;code&gt;queue&lt;/code&gt; ou &lt;code&gt;job&lt;/code&gt; se quiser montar imagens que sobem workers pra uma fila assíncrona, por exemplo. Eu expliquei um pouco sobre filas assíncronas no episódio do ingresso.com e no de concorrência e paralelismo. O importante é saber que Heroku não sobe só aplicações web. Além disso, o formato de arquivo &lt;code&gt;Procfile&lt;/code&gt; meio que virou universal. No mundo Rails você pode usar uma ferramenta como o &lt;code&gt;foreman&lt;/code&gt; que vai ler esse arquivo e localmente subir sua aplicação pra simular como rodaria no Heroku.&lt;/p&gt;

&lt;p&gt;Você pode usar o foreman escrito em Ruby pra rodar localmente sua aplicação mesmo se for escrita em PHP, ou usar o node-foreman que faz a mesma coisa ou o goreman ou forego que são outros clones de foreman escritos em Go. Tanto faz no que é escrito porque só vai ler o que tem no arquivo &lt;code&gt;Procfile&lt;/code&gt; e executar o que está lá. É uma boa prática testar localmente antes de subir pro Heroku ou outra plataforma que também suporte Procfiles.&lt;/p&gt;

&lt;p&gt;O próximo passo do tutorial foi o que expliquei agora pouco. Como escalar sua aplicação subindo mais containers web. Só usar o comando &lt;code&gt;heroku ps:scale&lt;/code&gt; e fazer &lt;code&gt;web=2&lt;/code&gt; ou mais. Se quiser desligar a aplicação, tirar da web, só fazer igual a zero que vai desligar todos os containers. Você pode aumentar ou diminuir o número de containers manualmente com esse comando ou contratar um serviço de auto-escala que usa algumas estratégias pra fazer esse ajuste automaticamente dependendo da carga que sua aplicação tá recebendo.&lt;/p&gt;

&lt;p&gt;Dizendo assim pode parecer que escalar sua aplicação é algo tão simples quanto rodar esse comando e subir 100 dynos de uma só vez em horário de pico. Mas isso não é uma bala de prata. Lembre-se que se subir 100 dynos vai precisar que recursos embaixo dele, como seu banco de dados, tanto consiga aguentar esse tanto de conexões simultâneas e ter CPU e RAM suficientes pra processar tanta coisa de uma só vez.  Escalabilidade nunca é automática, você precisa estar preparado pra isso. Na prática, essa funcionalidade é mais pra você economizar custos. Digamos que no máximo, os recursos que instalou suportariam 100 dynos de pé ao mesmo tempo. Mas de madrugada seu tráfego cai um monte e só 10 dynos seriam suficientes. Então você pode fazer um script que derruba 90 dynos de madrugada pra economizar custos e de manhã cedo sobe 90 dynos novos pra aguentar o tráfego do dia.&lt;/p&gt;

&lt;p&gt;Agora vamos falar de dependências. Se você programa em Rails, obrigatoriamente usa a ferramenta Bundler pra gerenciar dependências. Toda nova biblioteca que precisa adicionar no seu projeto, primeiro você declara no arquivo &lt;code&gt;Gemfile&lt;/code&gt; e instala a tal biblioteca com o comando &lt;code&gt;bundle update&lt;/code&gt;. Você jamais vai no site da biblioteca, baixa um zip e descompacta dentro do diretório do seu projeto, isso seria uma barbárie e algo totalmente inaceitável numa sociedade civilizada moderna em 2022.&lt;/p&gt;

&lt;p&gt;Se você programa em Javascript, Node.js, obviamente adiciona todas as suas dependências com o comando &lt;code&gt;npm install&lt;/code&gt; e a opção &lt;code&gt;--save-dev&lt;/code&gt; por exemplo. Ou se usa &lt;code&gt;yarn&lt;/code&gt; usa o comando &lt;code&gt;yarn add&lt;/code&gt;. Em ambos os casos isso vai atualizar o arquivo &lt;code&gt;package.json&lt;/code&gt;, que todo projeto civilizado de Javascript tem obrigação de ter.&lt;/p&gt;

&lt;p&gt;Se você é de Java, certamente conhece Maven e usa a ferramenta Gradle, e tudo vai estar declarado no arquivo &lt;code&gt;build.gradle&lt;/code&gt; ou &lt;code&gt;pom.xml&lt;/code&gt;. Se você é de Python, eu sei que é chato e ainda não é uma solução perfeita, mas deveria estar usando &lt;code&gt;pip&lt;/code&gt; e todas as dependências deveriam estar no arquivo &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Se é de Go, só recentemente começaram a ficar mais civilizados e agora fazendo &lt;code&gt;go mod init&lt;/code&gt; você cria um arquivo &lt;code&gt;go.mod&lt;/code&gt; que é onde se declara dependências. Daí rodando o comando &lt;code&gt;go mod tidy&lt;/code&gt; vai baixar as dependências que precisa pra compilar e executar sua aplicação. O Rust sempre veio com o utilitário Cargo e você sempre tem um arquivo &lt;code&gt;Cargo.toml&lt;/code&gt; que declara os crates, que é como povo de Rust chama suas bibliotecas.&lt;/p&gt;

&lt;p&gt;Entenderam? Não importa que linguagem você usa, toda linguagem civilizada tem um gerenciador de dependências padrão, sempre tem um arquivo onde se declara essas dependências, e você nunca, jamais, deve baixar bibliotecas manualmente e jogar dentro do seu repositório. Mais do que isso, todo gerenciador competente costuma ter um arquivo de tranca, de lock. Por exemplo, projetos de Javascript tem o &lt;code&gt;package.json&lt;/code&gt; onde você declarou as bibliotecas e versões que queria, mas quando o &lt;code&gt;npm&lt;/code&gt; ou &lt;code&gt;yarn&lt;/code&gt; realmente baixam e instalam ele gera um outro arquivo chamado &lt;code&gt;package-lock.json&lt;/code&gt; que você nunca deve editar manualmente, que declara exatamente quais bibliotecas e exatamente quais versões baixou.&lt;/p&gt;

&lt;p&gt;Esse arquivo de lock é especialmente importante porque quando outro desenvolvedor dá pull e clona o repositório do projeto ou quando você dá &lt;code&gt;git push&lt;/code&gt; pro Heroku, ele roda o &lt;code&gt;npm install&lt;/code&gt; e vai baixar exatamente o que estiver nesse arquivo de lock. 1.0.0 é diferente de 1.0.2. Isso é importante pra todo mundo baixar exatamente a mesma versão de tudo. Basta uma biblioteca que era pra ser, faz de conta, versão 1.0.1 e no servidor baixar a 1.0.2 que um bug pode ser introduzido sem ninguém saber. Gerenciamento de dependências é uma ciência exata. Ela só fica caótica quando você não segue essas regras.&lt;/p&gt;

&lt;p&gt;Essa longa explicação foi só pra pular pro próximo passo do tutorial onde ele rapidamente explica que nosso projeto PHP de exemplo usa um gerenciador de dependências chamado Composer. E como esperado, existe um arquivo chamado &lt;code&gt;composer.json&lt;/code&gt; na raiz do projeto junto com um arquivo &lt;code&gt;composer.lock&lt;/code&gt;. No caso do PHP, no arquivo &lt;code&gt;web/index.php&lt;/code&gt; tem uma linha com o comando &lt;code&gt;require&lt;/code&gt; carregando um &lt;code&gt;autoload.php&lt;/code&gt; que é quem se responsabiliza por carregar as bibliotecas declaradas no &lt;code&gt;composer.json&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Na sua máquina local, se fizer &lt;code&gt;compose update&lt;/code&gt; vai baixar todas as dependências localmente e agora pode rodar o projeto na sua máquina. E quando fazemos &lt;code&gt;git push&lt;/code&gt; pro Heroku os scripts no buildpack de PHP vão procurar o arquivo &lt;code&gt;composer.json&lt;/code&gt; e se achar, vai rodar o &lt;code&gt;compose update&lt;/code&gt; pra montar a imagem. E é por isso que você precisa gerenciar as dependências dessa forma, pra que outro desenvolvedor não tenha problemas quando baixar na máquina dele, e pra quando subir num Heroku ou máquina de produção sabemos que não vai faltar nenhuma dependência. Isso é crucial pra vivermos numa sociedade civilizada. Imagina como era na época da barbárie quando não tínhamos ferramentas como essas.&lt;/p&gt;

&lt;p&gt;Vamos simular que esse projeto ainda está em desenvolvimento. Vamos adicionar uma nova funcionalidade. Pra isso começamos usando o comando &lt;code&gt;compose require&lt;/code&gt; que vai declarar e puxar a biblioteca &lt;code&gt;cowsayphp&lt;/code&gt; que é um programinha besta que só desenha uma vaca com caracteres ASCII. Depois disso só rodar &lt;code&gt;compose update&lt;/code&gt; pra garantir que foi baixada e instalada. Se abrirmos o arquivo &lt;code&gt;composer.json&lt;/code&gt; veja que ela foi declarada automaticamente pelo Composer.&lt;/p&gt;

&lt;p&gt;Agora, no &lt;code&gt;index.php&lt;/code&gt; podemos criar uma rota chamada &lt;code&gt;/cowsay&lt;/code&gt; que vai usar essa biblioteca &lt;code&gt;Cowsayphp&lt;/code&gt; e mandar ela desenhar a vaca dizendo &quot;Cool beans&quot;. Vamos só copiar e colar esse trecho do tutorial no nosso projeto, e pronto. Agora precisamos adicionar tudo isso que modificamos no repositório Git. Pra isso basta fazer &lt;code&gt;git add .&lt;/code&gt;. Lembrem-se que nos vídeos de Git eu falo pra tomar cuidado pra não adicionar coisas que não precisam. Por acaso esse projeto tem um &lt;code&gt;.gitignore&lt;/code&gt; e se dermos &lt;code&gt;git status&lt;/code&gt; podemos ver que só vamos adicionar os arquivos do Composer e o &lt;code&gt;index.php&lt;/code&gt; que modificamos. Na dúvida sempre rode &lt;code&gt;git status&lt;/code&gt; antes de dar &lt;code&gt;git commit&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finalizamos com o &lt;code&gt;git commit -m&lt;/code&gt; adicionando uma mensagem descritiva da modificação que fizemos e executamos um novo &lt;code&gt;git push heroku main&lt;/code&gt; pra subir a modificação e gerar uma nova imagem. Olhem o Heroku recebendo, rodando &lt;code&gt;compose update&lt;/code&gt; que vai baixar a biblioteca que mandamos, vai regerar a imagem, daí ele sozinho vai desligar os dynos que estavam rodando antes e subir novos dynos com a imagem nova.&lt;/p&gt;

&lt;p&gt;Se usarmos o comando &lt;code&gt;heroku open cowsay&lt;/code&gt; ele vai abrir o navegador pra gente, já apontando pra URL da aplicação /cowsay. Não precisa desse comando, você podia abrir o navegador e digitar a URL manualmente, mas assim é mais rápido. Demora um segundo pra abrir porque o Heroku está atualizando os dynos, mas voilá, tá funcionando. E é assim que subimos versões novas da nossa aplicação. Lembram deploy contínuo que eu falei? É assim que faz.&lt;/p&gt;

&lt;p&gt;Além de toda essa infraestrutura de containers, load balancers, facilidade de subir atualizações usando Git, o Heroku tem parceria com dezenas de empresas que oferecem serviços que são úteis pras nossas aplicações. No tutorial, o próximo passo é justamente instalar uma delas, o addon pra ferramenta chamada Papertrail. Addon é sinônimo de plugin e Papertrail é um serviço que recebe os logs da nossa aplicação e oferece uma interface web que podemos monitorar e, principalmente, fazer pesquisas.&lt;/p&gt;

&lt;p&gt;A maioria dos addons oferece uma versão gratuita pra gente testar e pra aplicações pequenas costuma ser suficiente também. Pra instalar basta ir no terminal e fazer &lt;code&gt;heroku addons:create papertrail&lt;/code&gt;. Esses comandos têm várias opções, por exemplo pra já instalar com um plano pago mais parrudo. Sempre leia a documentação de cada addon antes de sair instalando. De qualquer forma, com o comando &lt;code&gt;heroku addons&lt;/code&gt; podemos listar quais já temos instalado e veja como o Papertrail já aparece.&lt;/p&gt;

&lt;p&gt;Pra abrir a interface web no seu navegador, via terminal podemos rodar &lt;code&gt;heroku addons:open papertrail&lt;/code&gt; e voilá, agora temos como fazer pesquisas nos nossos logs. Isso é mais importante se considerarmos que podemos ter mais de um dyno rodando ao mesmo tempo e no Papertrail vai concentrar os logs de todos os dynos ativos. Assim podemos pesquisar os logs de todos ao mesmo tempo. Por acaso esse é um addon que eu recomendo sempre instalar em toda aplicação. Papertrail e também o Rollbar que você pode configurar pra te notificar por e-mail se alguma mensagem de erro crítico aparecer no log.&lt;/p&gt;

&lt;p&gt;O próximo passo do tutorial é um pouco mais avançado e eu não recomendo que você use isso se não for um desenvolvedor mais experiente. A linha de comando do Heroku permite abrir um container novo pra onde ele vai abrir uma conexão SSH segura. Dentro dele você pode rodar o shell interativo do seu framework como o &lt;code&gt;php -a&lt;/code&gt; no caso de PHP ou o &lt;code&gt;rails console&lt;/code&gt; no caso de Rails ou simplesmente abrir um shell bash caso queira checar alguma coisa no nível do sistema operacional. Tudo que carrega nos containers de web também carrega nesse container de console, então pode ser bom pra debugar algum bug que não acontece na máquina de desenvolvimento mas aparece quando sobe a aplicação no Heroku. Use com cuidado, mas essa opção já salvou minha vida diversas vezes.&lt;/p&gt;

&lt;p&gt;Esclarecendo, esse comando não abre SSH direto pra um dos containers web rodando. Ele abre um novo container, com a mesma imagem que sobe nos containers web, só isso. A vantagem é que de lá você tem os mesmos acessos ao banco de dados se precisar muito rodar alguma query de emergência ou algo assim. E justamente por isso eu falo pra tomar muito cuidado, porque o que você rodar no banco vai ser permanente. Quando desconecta do shell interativo, esse container é destruído, por isso não crie ou baixe arquivos pra lá porque esse container vai sumir tão logo você se desconecte dele. É especificamente pra tarefas administrativas especiais.&lt;/p&gt;

&lt;p&gt;Outro conceito que pra iniciantes pode não ser muito óbvio são variáveis de ambiente. Por exemplo, imagino que a maioria que usa Linux no mínimo já lidou com variáveis como &lt;code&gt;PATH&lt;/code&gt; num arquivo local como &lt;code&gt;.profile&lt;/code&gt; ou &lt;code&gt;.bashrc&lt;/code&gt; da vida. Você faz alguma coisa como &lt;code&gt;export ABC=blabla&lt;/code&gt;. Isso declara uma variável global na sua sessão e você pode ver o conteúdo da variável fazendo &lt;code&gt;echo $ABC&lt;/code&gt; e vai imprimir o &lt;code&gt;blabla&lt;/code&gt; no seu terminal.&lt;/p&gt;

&lt;p&gt;Pois bem, é considerado uma boa prática declarar configurações assim como variáveis de ambiente, particularmente em containers. Se você já viu um arquivo &lt;code&gt;.env&lt;/code&gt; na raíz do seu projeto, ele serve pra declarar e simular variáveis de ambiente do projeto. De novo, a gente inventou isso no Rails e todo framework web meio que adotou a mesma funcionalidade. Fazemos a configuração num arquivo pra cada desenvolvedor não precisar manualmente ficar escrevendo um monte de &lt;code&gt;exports&lt;/code&gt; no seu profile local de Bash. Além disso é boa prática ter um arquivo como &lt;code&gt;.env.example&lt;/code&gt; que quando clonamos o projeto fazemos uma cópia dele pra &lt;code&gt;.env&lt;/code&gt;, assim não precisamos adivinhar quais variáveis existem pra usar.&lt;/p&gt;

&lt;p&gt;E mais importante, é boa prática colocar &lt;code&gt;.env&lt;/code&gt; no arquivo &lt;code&gt;.gitignore&lt;/code&gt; pra nunca entrar no repositório Git. Isso porque os frameworks que suportam essa convenção, se existir o arquivo &lt;code&gt;.env&lt;/code&gt; ele tem prioridade sobre as variáveis de ambiente de verdade configuradas no seu sistema operacional. Mas quando subimos pra produção não queremos que a aplicação carregue desse arquivo e sim das variáveis de verdade no sistema operacional. Então sempre se lembre dessa regra: &lt;code&gt;.env&lt;/code&gt; sempre declarado no &lt;code&gt;.gitignore&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Entendido isso, localmente na sua máquina de desenvolvimento você edita o que precisa no arquivo &lt;code&gt;.env&lt;/code&gt; que só existe na sua máquina, mas em produção configura variáveis de verdade. No Heroku fazemos isso com o comando &lt;code&gt;heroku config:set&lt;/code&gt;. Vamos fazer um exemplo pra ilustrar isso. Naquele arquivo &lt;code&gt;index.php&lt;/code&gt; vamos adicionar mais uma rota, no caso substituir a rota raiz do site. Vamos repetir a palavra &quot;Hello&quot; X vezes, e essas X vezes vai estar declarado na variável de ambiente chamada &lt;code&gt;TIMES&lt;/code&gt;. Pra ler essa variável, em PHP, se usa a função &lt;code&gt;getenv&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Agora, não temos como editar um &lt;code&gt;export&lt;/code&gt; no arquivo de profile de um dyno, pra isso usamos a linha de comando &lt;code&gt;heroku config:set TIMES=20&lt;/code&gt;. Pra listar todas as variáveis que existem só usar o comando &lt;code&gt;heroku config&lt;/code&gt;. Olhem como tem uma variável do Papertrail que foi criado pela linha de comando que usamos pra adicionar o addon, esse é o token secreto de acesso. Não tem problema ter coisas como tokens e senhas em variáveis de ambiente, porque elas só existem dentro do container. Se alguém conseguiu ver esse token é porque ganhou acesso ao seu container de produção e seu problema é bem maior do que só ter o token exposto, você está com tudo exposto. Ninguém jamais deve ter acesso direto aos containers.&lt;/p&gt;

&lt;p&gt;E pra reforçar, é exatamente por isso que nunca se deve adicionar arquivos como &lt;code&gt;.env&lt;/code&gt; no repositório Git, porque normalmente guardamos coisas como senhas e tokens em variáveis de ambiente, e esse tipo de informação, jamais, sob nenhuma hipótese, pode aparecer dentro de um repositório Git. Nenhuma credencial ou segredo jamais deve estar no código fonte do projeto. Tem que ser um tosco de proporções bíblicas pra pensar em colocar segredos num versionador de código que todo mundo tem acesso.&lt;/p&gt;

&lt;p&gt;E pronto. Agora podemos fazer &lt;code&gt;git add .&lt;/code&gt;, &lt;code&gt;git commit -m&lt;/code&gt; e uma mensagem e finalmente &lt;code&gt;git push heroku main&lt;/code&gt; de novo. Ele vai criar uma nova imagem com as modificações que acabamos de fazer e se abrirmos o navegador com &lt;code&gt;heroku open&lt;/code&gt;, esperamos um segundo pro Heroku derrubar os dynos e subir com a imagem nova e, voilá, veja &quot;Hello&quot; repetido 20 vezes.&lt;/p&gt;

&lt;p&gt;E como mencionamos “banco de dados” algumas vezes, o último passo do tutorial é justamente adicionar Postgres na nossa aplicação. Assim como o Papertrail, Postgres é um addon que tem diversos tamanhos e preços e você deve checar as opções antes de adicionar. Isso porque se pegar um muito pequeno e precisar dar upgrade, o processo não é simples e muito menos trivial. Leia a documentação do Heroku sobre isso. Uma das melhores coisas do Heroku é justamente o serviço de banco de dados, que é um dos melhores que existem, mas você precisa ter um mínimo de noção do que tá fazendo. Eles não fazem mágica nem são à prova de idiotas.&lt;/p&gt;

&lt;p&gt;Avisos dados, pra adicionar a versão gratuita menor, que se chama &lt;code&gt;hobby-dev&lt;/code&gt;, basta digitar no terminal &lt;code&gt;heroku addons:create heroku-postgresql:hobby-dev&lt;/code&gt;. Agora vamos adicionar uma funcionalidade no nosso projeto PHP pra conectar no banco e listar o conteúdo de uma tabela em HTML, o arroz com feijão só. Pra isso começamos usando o Composer pra adicionar a biblioteca de PDO que é PHP Data Objects. Se você é de Java ou .NET da vida, é a mesma coisa que um DAO da vida.&lt;/p&gt;

&lt;p&gt;Fazemos &lt;code&gt;compose require csanquer/pdo-service-provider=~1.1dev&lt;/code&gt; que vai baixar uma versão compatível mas não necessariamente exata com 1.1dev dessa biblioteca, lembram? Por isso precisamos ter um arquivo de lock que vai registrar a versão exata que ele achou e baixou. Rodamos um &lt;code&gt;compose update&lt;/code&gt; pra garantir que baixou tudo e agora podemos alterar o arquivo &lt;code&gt;index.php&lt;/code&gt; de novo pra configurar o acesso ao banco de dados. Vamos copiar e colocar do tutorial.&lt;/p&gt;

&lt;p&gt;O importante é saber que é uma convenção do Heroku ter a URL de acesso ao banco declarado numa variável de ambiente chamada &lt;code&gt;DATABASE_URL&lt;/code&gt;. Podemos ver ela usando o comando &lt;code&gt;heroku config&lt;/code&gt; e olha só. E esse trecho de código que copiamos e colamos é pra pegar o conteúdo dessa variável com a função &lt;code&gt;getenv&lt;/code&gt; e parsear os diversos componentes dessa string e passar pra biblioteca de PDO configurar coisas como username, password, host, porta e path pra conseguir conectar no novo banco de dados Postgres. E simples assim, subimos um banco de dados seguro e funcional na nossa infraestrutura.&lt;/p&gt;

&lt;p&gt;Agora vamos criar uma nova rota lá embaixo chamada '/db'. Ele vai usar esse PDO pra mandar uma query pro banco e fazer um loop com &lt;code&gt;while&lt;/code&gt; pra ir montando um array com todos os nomes que voltaram da query. E pra montar o HTML, o framework Symfony oferece um sistema de template chamado Twig. Passamos o array de nomes pro template chamado de &lt;code&gt;database.twig&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Então precisamos criar um novo arquivo &lt;code&gt;views/database.twig&lt;/code&gt; e vamos copiar e colar o template do tutorial nele. Ele vai pegar cada nome que veio no array que passamos pra ele e montar uma lista em HTML, nada de mais, arroz com feijão. Isso tudo feito, vamos adicionar as modificações no Git com &lt;code&gt;git add .&lt;/code&gt;. Com &lt;code&gt;git status&lt;/code&gt; podemos ver que modificamos os arquivos do Composer, o &lt;code&gt;index.php&lt;/code&gt; e criamos um arquivo novo &lt;code&gt;database.twig&lt;/code&gt;. Está correto então podemos fazer &lt;code&gt;git commit -m&lt;/code&gt; com uma mensagem descritiva e finalmente &lt;code&gt;git push heroku main&lt;/code&gt; pra mandar as modificações pro Heroku.&lt;/p&gt;

&lt;p&gt;Espero que a essa altura você já esteja acostumado com o processo de criar uma nova funcionalidade, adicionar no Git, mandar pro Heroku e conseguir testar de verdade. O Heroku recebe a modificação, monta uma nova imagem, derruba os dynos que estavam rodando e sobe de novo com a nova imagem. Só tem um probleminha.&lt;/p&gt;

&lt;p&gt;Note que criamos um novo banco de dados, mas ele está vazio. Em qualquer framework web moderno existe um recurso chamado Migrations, mais uma coisa que nasceu no Ruby on Rails e todos os outros frameworks imitaram, onde criamos scripts que vão criar as tabelas e índices que precisamos, caso já não existam, e também já pré-cadastram coisas como conta de administrador, se precisar. Isso tudo fica em scripts que declaramos pro Heroku executar na próxima vez que subir dynos novos. Procure a documentação do seu framework e aprenda sobre Migrations porque é super importante.&lt;/p&gt;

&lt;p&gt;Como isso é só um tutorial simples, tentando ser didático pra ensinar sobre o Heroku e não sobre seu framework, ele pula qualquer coisa sobre Migrations e manda você abrir um novo container de shell, lembra? Que nem abrimos o bash remoto? Só que no caso agora é abrindo o console do Postgresql que é o &lt;code&gt;psql&lt;/code&gt;, então vamos fazer isso usando o comando &lt;code&gt;heroku pg:psql&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Agora abriu o console remoto lá no servidor do Heroku já conectado no nosso novo banco de dados. Só precisamos fazer isso uma vez, mas vamos copiar do tutorial o comando pra criar a tabela chamada test_table e em seguida vamos dar insert em alguns valores aleatórios como esse 'hello database' e também um 'hello world', por que não? Pronto, pra sair do console do Postgres é só digitar &lt;code&gt;\q&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Podemos abrir o navegador de novo usando o comando &lt;code&gt;heroku open db&lt;/code&gt; que é a nova rota que criamos e voilá, veja que conseguiu executar a query no banco, trazer as duas linhas que acabamos de inserir, e montar o HTML com elas. E pronto, nesse estágio fomos capazes de criar uma nova aplicação no Heroku, ir subindo novas funcionalidades à medida que fomos codificando, configurando o ambiente e cadastrando novos addons como o Papertrail e Postgres. Esse é o básico do básico sobre Heroku que todo mundo deveria saber. Mas o Heroku faz bem mais que isso. No fim do tutorial, no último passo, ele dá links pra outros artigos como o &quot;How Heroku Works&quot; que vai começar a explicar muito mais detalhes sobre o ciclo de vida de uma aplicação em dynos.&lt;/p&gt;

&lt;p&gt;Por exemplo, lembram que durante o tutorial repetimos &lt;code&gt;git push heroku&lt;/code&gt; várias vezes? Todas as vezes ele cria uma nova imagem com as últimas modificações, derruba os dynos rodando a versão antiga e sobe tudo de novo com a imagem nova. O que eu não expliquei é que as imagens antigas continuam disponíveis se precisar. O Heroku chama isso de releases ou lançamentos. Se rodar o comando &lt;code&gt;heroku releases&lt;/code&gt; podemos ver tudo que subimos desde a primeira vez. Ele vai numerando cada release e acho que você pode cadastrar tags nelas também pra facilitar achar depois.&lt;/p&gt;

&lt;p&gt;A vantagem mais imediata é: digamos que você subiu uma nova versão que não testou muito bem e, surpresa, começou a dar pau em produção. Em vez de ficar tentando consertar na tentativa e erro durante o sufoco, podemos rapidamente dar rollback pra última release que sabemos que tava funcionando, por exemplo &lt;code&gt;heroku releases:rollback v10&lt;/code&gt;. A versão 11 que acabamos de subir dá pau, mas a versão 10 funcionava, então voltamos pra ela. É o tipo de coisa que você nunca ia conseguir fazer rápido e organizado assim se estivesse atualizando código fonte manualmente usando FTP. E é por isso que você paga mais por isso também.&lt;/p&gt;

&lt;p&gt;Eu normalmente não gosto de fazer videos de tutoriais de ferramentas específicas porque elas envelhecem muito rápido. Mas assim como no caso de Git, o Heroku existe faz mais de dez anos e continua funcionando basicamente do mesmo jeito. Esse mesmo tutorial que funcionaria em 2010 continua funcionando em 2022 e provavelmente vai continuar funcionando por muitos mais anos do mesmo jeito e toda nova solução de devops que aparece é pra tentar se aproximar desse ideal de fluxo de trabalho. Por isso, mesmo que você não use no seu trabalho atual, acho muito importante que se familiarize com esse jeito de trabalhar.&lt;/p&gt;

&lt;p&gt;No próximo episódio vamos pegar o que aprendemos hoje pra discutir um pouco mais sobre arquitetura e gerenciamento de projetos ágeis de verdade. Se ficaram com dúvidas mandem nos comentários abaixo, se curtiram o video deixem um joinha, assinem o canal e compartilhem o video com seus amigos. A gente se vê, até mais!&lt;/p&gt;
</content>
    <author>
      <name>Fabio Akita</name>
    </author>
  </entry>
</feed>