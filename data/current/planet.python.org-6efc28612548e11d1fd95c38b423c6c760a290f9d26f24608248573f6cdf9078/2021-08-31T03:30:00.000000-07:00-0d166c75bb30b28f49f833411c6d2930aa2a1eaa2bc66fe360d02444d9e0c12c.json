{
  "title":"Stack Abuse: Random Projection: Theory and Implementation in Python with Scikit-Learn",
  "date":"2021-08-31T03:30:00.000000-07:00",
  "author":null,
  "id":"https://stackabuse.com/random-projection-theory-and-implementation-in-python-with-scikit-learn/",
  "link":"https://stackabuse.com/random-projection-theory-and-implementation-in-python-with-scikit-learn/",
  "content":"<h3 id=\"introduction\">Introduction</h3>\n<p>This guide is an in-depth introduction to an unsupervised dimensionality reduction technique called <strong><em>Random Projections</em></strong>. A Random Projection can be used to reduce the complexity and size of data, making the data easier to process and visualize. It is also a preprocessing technique for input preparation to a classifier or a regressor.</p>\n<blockquote>\n<p><strong>Random Projection</strong> is typically applied to highly-dimensional data, where other techniques such as <strong><a href=\"https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/\">Principal Component Analysis (PCA)</a></strong> can't do the data justice.</p>\n</blockquote>\n<p>In this guide, we'll delve into the details of <strong><em>Johnson-Lindenstrauss lemma</em></strong>, which lays the mathematical foundation of Random Projections. We'll also show how to perform Random Projection using Python's Scikit-Learn library, and use it to transform input data to a lower-dimensional space.</p>\n<blockquote>\n<p><strong>Theory is theory, and practice is practice</strong>. As a practical illustration, we'll load the <a rel=\"nofollow noopener\" href=\"https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf\">Reuters Corpus Volume I Dataset</a>, and apply Gaussian Random Projection and Sparse Random Projection to it.</p>\n</blockquote>\n<h3 id=\"whatisarandomprojectionofadataset\">What is a Random Projection of a Dataset?</h3>\n<p>Put simply:</p>\n<blockquote>\n<p>Random Projection is a method of <strong>dimensionality reduction</strong> and <strong>data visualization</strong> that simplifies the complexity of high-dimensional datasets.</p>\n</blockquote>\n<p>The method generates a new dataset by taking the projection of each data point along a randomly chosen set of directions. The projection of a single data point onto a vector is mathematically equivalent to taking the <strong>dot product of the point with the vector</strong>.</p>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-1.png\" alt=\"random projections illustration\"></p>\n<p>Given a data matrix \\(X\\) of dimensions \\(mxn\\) and a \\(dxn\\) matrix \\(R\\) whose columns are the vectors representing random directions, the Random Projection of \\(X\\) is given by \\(X_p\\).</p>\n\n  \n    X\n    p\n  \n  =\n  X\n  R\n\n<p>Each vector representing a random direction, has dimensionality \\(n\\), which is the same as all data points of \\(X\\). If we take \\(d\\) random directions, then we end up with a \\(d\\) dimensional transformed dataset. For the purpose of this tutorial, we'll fix a few notations:</p>\n<ul>\n<li><code>m</code>: Total example points/samples of input data.</li>\n<li><code>n</code>: Total features/attributes of the input data. It is also the dimensionality of the original data.</li>\n<li><code>d</code>: Dimensionality of the transformed data.</li>\n</ul>\n<p>The idea of Random Projections is very similar to <strong>Principal Component Analysis (PCA)</strong>, fundementally. However, in PCA, the projection matrix is computed via <strong>eigenvectors</strong>, which can be computationally expensive for large matrices.</p>\n<blockquote>\n<p>When performing Random Projection, the vectors are chosen randomly making it very efficient. The name <em>&quot;projection&quot;</em> may be a little misleading as the vectors are chosen <strong>randomly</strong>, the transformed points are mathematically not true projections but close to being true projections.</p>\n</blockquote>\n<p>The data with reduced dimensions is easier to work with. Not only can it be visualized but it can also be used in the pre-processing stage to reduce the size of the original data.</p>\n<h3 id=\"asimpleexample\">A Simple Example</h3>\n<p>Just to understand how the transformation works, let's take the following simple example.</p>\n<p>Suppose our input matrix \\(X\\) is given by:</p>\n\n  X\n  =\n  \n    [\n    \n      \n        \n          1\n        \n        \n          3\n        \n        \n          2\n        \n        \n          0\n        \n      \n      \n        \n          0\n        \n        \n          1\n        \n        \n          2\n        \n        \n          1\n        \n      \n      \n        \n          1\n        \n        \n          3\n        \n        \n          0\n        \n        \n          0\n        \n      \n    \n    ]\n  \n\n<p>And the projection matrix is given by:</p>\n\n  R\n  =\n  \n    1\n    2\n  \n  \n    [\n    \n      \n        \n          1\n        \n        \n          −\n          1\n        \n      \n      \n        \n          1\n        \n        \n          1\n        \n      \n      \n        \n          1\n        \n        \n          −\n          1\n        \n      \n      \n        \n          1\n        \n        \n          1\n        \n      \n    \n    ]\n  \n\n<p>The projection of X onto R is:</p>\n\n  \n    X\n    p\n  \n  =\n  X\n  R\n  =\n  \n    1\n    2\n  \n  \n    [\n    \n      \n        \n          6\n        \n        \n          0\n        \n      \n      \n        \n          4\n        \n        \n          0\n        \n      \n      \n        \n          4\n        \n        \n          2\n        \n      \n    \n    ]\n  \n\n<blockquote>\n<p>We started with three points in a <strong>four-dimensional space</strong>, and with clever matrix operations ended up with three transformed points in a <strong>two-dimensional space</strong>.</p>\n</blockquote>\n<p>Note, some important attributes of the projection matrix \\(R\\). Each column is a unit matrix, i.e., the norm of each column is one. Also, the dot product of all columns taken pairwise (in this case only column 1 and column 2) is zero, indicating that both column vectors are orthogonal to each other.</p>\n<p>This makes the matrix, an <strong><em>Orthonormal Matrix</em></strong>. However, in case of the Random Projection technique, the projection matrix does not have to be a true orthonormal matrix when very high-dimensional data is involved.</p>\n<p>The success of Random Projection is based on an <em>awesome</em> mathematical finding known as <strong><em>Johnson-Lindenstrauss lemma</em></strong>, which is explained in detail in the following section!</p>\n<h4 id=\"thejohnsonlindenstrausslemma\">The Johnson-Lindenstrauss lemma</h4>\n<p>The Johnson-Lindenstrauss lemma is the mathematical basis for Random Projection:</p>\n<blockquote>\n<p>The Johnson-Lindenstrauss lemma states that if the data points lie in a <strong>very high-dimensional space</strong>, then projecting such points on simple <strong>random directions preserves their pairwise distances</strong>.</p>\n</blockquote>\n<p><em>Preserving pairwise distances</em> implies that the pairwise distances between points in the original space are the same or almost the same as the pairwise distance in the projected lower-dimensional space.</p>\n<blockquote>\n<p>Thus, the structure of data and clusters within data are maintained in a lower-dimensional space, while the complexity and size of data are reduced substantially.</p>\n</blockquote>\n<p>In this guide, we refer to the difference in the actual and projected pairwise distances as the <strong><em>&quot;distortion&quot;</em></strong> in data, which is introduced due to its projection in a new space.</p>\n<p>Johnson-Lindenstrauss lemma also provides a <strong><em>&quot;safe&quot;</em></strong> measure of the number of dimensions to project the data points onto so that the error/distortion lies within a certain range, so finding the target number of dimensions is made easy.</p>\n<p>Mathematically, given a pair of points \\((x_1,x_2)\\) and their corresponding projections \\((x_1',x_2')\\) defines an <strong><em>eps-embedding</em></strong>:</p>\n<p>$$<br>\n(1 - \\epsilon) |x_1 - x_2|^2 &lt; |x_1' - x_2'|^2 &lt; (1 + \\epsilon) |x_1 - x_2|^2<br>\n$$</p>\n<p>The Johnson-Lindenstrauss lemma specifies the minimum dimensions of the lower-dimensional space so that the above <em>eps-embedding</em> is maintained.</p>\n<h4 id=\"determiningtherandomdirectionsoftheprojectionmatrix\">Determining the Random Directions of the Projection Matrix</h4>\n<p>Two well-known methods for determining the projection matrix are:</p>\n<ul>\n<li>\n<p><strong>Gaussian Random Projection</strong>: The projection matrix is constructed by choosing elements randomly from a Gaussian distribution with mean zero.</p>\n</li>\n<li>\n<p><strong>Sparse Random Projection</strong>: This is a comparatively simpler method, where each vector component is a value from the set {-k,0,+k}, where k is a constant. One simple scheme for generating the elements of this matrix, also called the <code>Achlioptas</code> method is to set \\(k=\\sqrt 3\\):</p>\n</li>\n</ul>\n\n  \n    R\n    \n      i\n      j\n    \n  \n  =\n  \n    3\n  \n  \n    {\n    \n      \n        \n          +\n          1\n        \n        \n           with probability \n        \n        \n          \n            1\n            6\n          \n        \n      \n      \n        \n          0\n        \n        \n           with probability \n        \n        \n          \n            2\n            3\n          \n        \n      \n      \n        \n          −\n          1\n        \n        \n           with probability \n        \n        \n          \n            1\n            6\n          \n        \n      \n    \n    \n  \n\n<p>The method above is equivalent to choosing the numbers from <code>{+k,0,-k}</code> based on the outcome of the roll of a dice. If the dice score is <em>1</em>, then choose <em>+k</em>. If the dice score is in the range <code>[2,5]</code>, choose <em>0</em>, and choose <em>-k</em> for a dice score of <em>6</em>.</p>\n<p>A more general method uses a <code>density</code> parameter to choose the Random Projection matrix. Setting \\(s=\\frac{1}{\\text{density}}\\), the elements of the Random Projection matrix are chosen as:</p>\n\n  \n    R\n    \n      i\n      j\n    \n  \n  =\n  \n    {\n    \n      \n        \n          +\n          \n            \n              s\n              d\n            \n          \n        \n        \n           with probability \n        \n        \n          \n            1\n            \n              2\n              s\n            \n          \n        \n      \n      \n        \n          0\n        \n        \n           with probability \n        \n        \n          1\n          −\n          \n            1\n            s\n          \n        \n      \n      \n        \n          −\n          \n            \n              s\n              d\n            \n          \n        \n        \n           with probability \n        \n        \n          \n            1\n            \n              2\n              s\n            \n          \n        \n      \n    \n    \n  \n\n<p>The general recommendation is to set the <code>density</code> parameter to \\(\\frac{1}{\\sqrt n}\\).</p>\n<p>As mentioned earlier, for both the Gaussian and sparse methods, the projection matrix is not a true orthonormal matrix. However, it has been shown that in high dimensional spaces, the randomly chosen matrix using either of the above two methods is <strong><em>close to</em></strong> an orthonormal matrix.</p>\n<h3 id=\"randomprojectionusingscikitlearn\">Random Projection Using Scikit-Learn</h3>\n<p>The Scikit-Learn library provides us with the <code>random_projection</code> module, that has three important classes/modules:</p>\n<ul>\n<li><code>johnson_lindenstrauss_min_dim()</code>: For determining the minimum number of dimensions of transformed data when given a sample size <code>m</code>.</li>\n<li><code>GaussianRandomProjection</code>: Performs Gaussian Random Projections.</li>\n<li><code>SparseRandomProjection</code>: Performs Sparse Random Projections.</li>\n</ul>\n<p>We'll demonstrate all the above three in the sections below, but first let's import the classes and functions we'll be using:</p>\n<pre><code class=\"hljs\"><span class=\"hljs-keyword\">from</span> sklearn.random_projection <span class=\"hljs-keyword\">import</span> SparseRandomProjection, johnson_lindenstrauss_min_dim\n<span class=\"hljs-keyword\">from</span> sklearn.random_projection <span class=\"hljs-keyword\">import</span> GaussianRandomProjection\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\">from</span> matplotlib <span class=\"hljs-keyword\">import</span> pyplot <span class=\"hljs-keyword\">as</span> plt\n<span class=\"hljs-keyword\">import</span> sklearn.datasets <span class=\"hljs-keyword\">as</span> dt\n<span class=\"hljs-keyword\">from</span> sklearn.metrics.pairwise <span class=\"hljs-keyword\">import</span> euclidean_distances\n</code></pre>\n<h4 id=\"determiningtheminimumnumberofdimensionsviajohnsonlindenstrausslemma\">Determining the Minimum Number of Dimensions Via Johnson Lindenstrauss lemma</h4>\n<p>The <code>johnson_lindenstrauss_min_dim()</code> function determines the minimum number of dimensions <code>d</code>, which the input data can be mapped to when given the number of examples <code>m</code>, and the <code>eps</code> or \\(\\epsilon\\) parameter.</p>\n<p>The code below experiments with a different number of samples to determine the minimum size of the lower-dimensional space, which maintains a certain <strong><em>&quot;safe&quot;</em></strong> distortion of data.</p>\n<p>Additionally, it plots <code>log(d)</code> against different values of <code>eps</code> for different sample sizes <code>m</code>.</p>\n<p>An important thing to note is that the Johnson Lindenstrauss lemma determines the size of the lower-dimensional space \\(d\\) only based on the number of example points \\(m\\) in the input data. The number of attributes or features \\(n\\) of the original data is irrelevant:</p>\n<pre><code class=\"hljs\">eps = np.arange(<span class=\"hljs-number\">0.001</span>, <span class=\"hljs-number\">0.999</span>, <span class=\"hljs-number\">0.01</span>)\ncolors = [<span class=\"hljs-string\">'b'</span>, <span class=\"hljs-string\">'g'</span>, <span class=\"hljs-string\">'m'</span>, <span class=\"hljs-string\">'c'</span>]\nm = [<span class=\"hljs-number\">1e1</span>, <span class=\"hljs-number\">1e3</span>, <span class=\"hljs-number\">1e7</span>, <span class=\"hljs-number\">1e10</span>]\n<span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">4</span>):\n    min_dim = johnson_lindenstrauss_min_dim(n_samples=m[i], eps=eps)\n    label = <span class=\"hljs-string\">'Total samples = '</span> + <span class=\"hljs-built_in\">str</span>(m[i])\n    plt.plot(eps, np.log10(min_dim), c=colors[i], label=label)\n    \nplt.xlabel(<span class=\"hljs-string\">'eps'</span>)\nplt.ylabel(<span class=\"hljs-string\">'log$_{10}$(d)'</span>)\nplt.axhline(y=<span class=\"hljs-number\">3.5</span>, color=<span class=\"hljs-string\">'k'</span>, linestyle=<span class=\"hljs-string\">':'</span>)\nplt.legend()\nplt.show()\n</code></pre>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-2.png\" alt=\"how to determine size of lower dimensional space for random projections\"></p>\n<p>From the plot above, we can see that for small values of <code>eps</code>, <code>d</code> is quite large but decreases as <code>eps</code> approaches one. The dimensionality is below 3500 (the dotted black line) for mid to large values of <code>eps</code>.</p>\n<blockquote>\n<p>This shows that applying <strong>Random Projections</strong> only makes sense to <strong>high-dimensional data</strong>, of the order of thousands of features. In such cases, a high reduction in dimensionality can be achieved.</p>\n</blockquote>\n<p>Random Projections are, therefore, very successful for text or image data, which involve a large number of input features, where Principal Component Analysis would</p>\n<h4 id=\"datatransformation\">Data Transformation</h4>\n<p>Python includes the implementation of both Gaussian Random Projections and Sparse Random Projections in its <code>sklearn</code> library via the two classes <code>GaussianRandomProjection</code> and <code>SparseRandomProjection</code> respectively. Some important attributes for these classes are (the list is not exhaustive):</p>\n<ul>\n<li><code>n_components</code>: Number of dimensions of the transformed data. If it is set to <code>auto</code>, then the optimal dimensions are determined before projection</li>\n<li><code>eps</code>: The parameter of Johnson-Lindenstrauss lemma, which controls the number of dimensions so that the distortion in projected data is kept within a certain bound.</li>\n<li><code>density</code>: Only applicable for <code>SparseRandomProjection</code>. The default value is <code>auto</code>, which sets \\(s=\\frac{1}{\\sqrt n}\\) for the selection of the projection matrix.</li>\n</ul>\n<p>Like other dimensionality reduction classes of <code>sklearn</code>, both these classes include the standard <code>fit()</code> and <code>fit_transform()</code> methods. A notable set of attributes, which come in handy are:</p>\n<ul>\n<li><code>n_components</code>: The number of dimensions of the new space on which the data is projected.</li>\n<li><code>components_</code>: The transformation or projection matrix.</li>\n<li><code>density_</code>: Only applicable to <code>SparseRandomProjection</code>. It is the value of <code>density</code> based on which the elements of the projection matrix are computed.</li>\n</ul>\n<h5 id=\"randomprojectionwithgaussianrandomprojection\">Random Projection with <em>GaussianRandomProjection</em></h5>\n<p>Let's start off with the <code>GaussianRandomProjection</code> class. The values of the projection matrix are plotted as a histogram and we can see that they follow a Gaussian distribution with mean zero. The size of the data matrix is reduced from 5000 to 3947:</p>\n<pre><code class=\"hljs\">X_rand = np.random.RandomState(<span class=\"hljs-number\">0</span>).rand(<span class=\"hljs-number\">100</span>, <span class=\"hljs-number\">5000</span>)\nproj_gauss = GaussianRandomProjection(random_state=<span class=\"hljs-number\">0</span>)\nX_transformed = proj_gauss.fit_transform(X_rand)\n\n<span class=\"hljs-comment\"># Print the size of the transformed data</span>\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">'Shape of transformed data: '</span> + <span class=\"hljs-built_in\">str</span>(X_transformed.shape))\n\n<span class=\"hljs-comment\"># Generate a histogram of the elements of the transformation matrix</span>\nplt.hist(proj_gauss.components_.flatten())\nplt.title(<span class=\"hljs-string\">'Histogram of the flattened transformation matrix'</span>)\nplt.show()\n</code></pre>\n<p>This code results in:</p>\n<pre><code class=\"hljs\">Shape of transformed data: (100, 3947)\n</code></pre>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-3.png\" alt=\"gaussian random projection scikit learn\"></p>\n<h5 id=\"randomprojectionwithsparserandomprojection\">Random Projection with <em>SparseRandomProjection</em></h5>\n<p>The code below demonstrates how data transformation can be made using a Sparse Random Projection. The entire transformation matrix is composed of three distinct values, whose frequency plot is also shown below.</p>\n<p>Note that the transformation matrix is a <code>SciPy</code> sparse <code>csr_matrix</code>. The following code accesses the non-zero values of the <code>csr_matrix</code> and stores them in <code>p</code>. Next, it uses <code>p</code> to get the counts of the elements of the sparse projection matrix:</p>\n<pre><code class=\"hljs\">proj_sparse = SparseRandomProjection(random_state=<span class=\"hljs-number\">0</span>)\nX_transformed = proj_sparse.fit_transform(X_rand)\n\n<span class=\"hljs-comment\"># Print the size of the transformed data</span>\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">'Shape of transformed data: '</span> + <span class=\"hljs-built_in\">str</span>(X_transformed.shape))\n\n<span class=\"hljs-comment\"># Get data of the transformation matrix and store in p. </span>\n<span class=\"hljs-comment\"># p consists of only 2 non-zero distinct values, i.e., pos and neg</span>\n<span class=\"hljs-comment\"># pos and neg are determined below</span>\np = proj_sparse.components_.data\ntotal_elements = proj_sparse.components_.shape[<span class=\"hljs-number\">0</span>] *\\\n                  proj_sparse.components_.shape[<span class=\"hljs-number\">1</span>]\npos = p[p&gt;<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">0</span>]\nneg = p[p&lt;<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">0</span>]\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">'Shape of transformation matrix: '</span>+ <span class=\"hljs-built_in\">str</span>(proj_sparse.components_.shape))\ncounts = (<span class=\"hljs-built_in\">sum</span>(p==neg), total_elements - <span class=\"hljs-built_in\">len</span>(p), <span class=\"hljs-built_in\">sum</span>(p==pos))\n<span class=\"hljs-comment\"># Histogram of the elements of the transformation matrix</span>\nplt.bar([neg, <span class=\"hljs-number\">0</span>, pos], counts, width=<span class=\"hljs-number\">0.1</span>)\nplt.xticks([neg, <span class=\"hljs-number\">0</span>, pos])\nplt.suptitle(<span class=\"hljs-string\">'Histogram of flattened transformation matrix, '</span> + \n             <span class=\"hljs-string\">'density = '</span> +\n             <span class=\"hljs-string\">'{:.2f}'</span>.<span class=\"hljs-built_in\">format</span>(proj_sparse.density_))\nplt.show()\n</code></pre>\n<p>This results in:</p>\n<pre><code class=\"hljs\">Shape of transformed data: (100, 3947)\nShape of transformation matrix: (3947, 5000)\n</code></pre>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-4.png\" alt=\"sparse random projections scikit learn\"></p>\n<p>The histogram is in agreement with the method of generating a sparse Random Projection matrix as discussed in the previous section. The zero is selected with probability (1-1/100 = 0.99), hence around 99% of values of this matrix are zero. Utilizing the data structures and routines for sparse matrices makes this transformation method very fast and efficient on large datasets.</p>\n<h3 id=\"practicalrandomprojectionswiththereuterscorpusvolume1dataset\">Practical Random Projections With the <em>Reuters Corpus Volume 1 Dataset</em></h3>\n<p>This section illustrates Random Projections on the <a rel=\"nofollow noopener\" href=\"https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf\">Reuters Corpus Volume I Dataset</a>. The dataset is freely accessible online, though for our purposes, it's easiest to looad via Scikit-Learn.</p>\n<p>The <a rel=\"nofollow noopener\" href=\"https://scikit-learn.org/stable/datasets/real_world.html#rcv1-dataset\"><code>sklearn.datasets</code></a> module contains a <code>fetch_rcv1()</code> function that downloads and imports the dataset.</p>\n<blockquote>\n<p><strong>Note:</strong> The dataset may take a few minutes to download, if you've never imported it beforehand through this method. Since there's no progress bar, it may appear as if the script is hanging without progressing further. Give it a bit of time, when you run it initially.</p>\n</blockquote>\n<p>The RCV1 dataset is a multilabel dataset, i.e., each data point can belong to multiple classes at the same time, and consists of 103 classes. Each data point has a dimensionality of a <strong>whopping 47,236</strong>, making it an ideal case for applying fast and cheap Random Projections.</p>\n<p>To demonstrate the effectiveness of Random Projections, and to keep things simple, we'll select 500 data points that belong to at least one of the first three classes. The <code>fetch_rcv1()</code> function retrieves the dataset and returns an object with data and targets, both of which are sparse <code>CSR</code> matrices from <code>SciPy</code>.</p>\n<p>Let's fetch the Reuters Corpus and prepare it for data transformation:</p>\n<pre><code class=\"hljs\">total_points = <span class=\"hljs-number\">500</span>\n<span class=\"hljs-comment\"># Fetch the dataset</span>\ndat = dt.fetch_rcv1()\n<span class=\"hljs-comment\"># Select the sparse matrix's non-zero targets</span>\ntarget_nz = dat.target.nonzero()\n<span class=\"hljs-comment\"># Select only indices of target_nz for data points that belong to </span>\n<span class=\"hljs-comment\"># either of class 1,2,3</span>\nind_class_123 = np.asarray(np.where((target_nz[<span class=\"hljs-number\">1</span>]==<span class=\"hljs-number\">0</span>) |\\\n                                    (target_nz[<span class=\"hljs-number\">1</span>]==<span class=\"hljs-number\">1</span>) |\\\n                                    (target_nz[<span class=\"hljs-number\">1</span>] == <span class=\"hljs-number\">2</span>))).flatten()\n<span class=\"hljs-comment\"># Choose only 500 indices randomly</span>\nnp.random.seed(<span class=\"hljs-number\">0</span>)\nind_class_123 = np.random.choice(ind_class_123, total_points, \n                                 replace=<span class=\"hljs-literal\">False</span>)\n\n<span class=\"hljs-comment\"># Retreive the row indices of data matrix and target matrix</span>\nrow_ind = target_nz[<span class=\"hljs-number\">0</span>][ind_class_123]\nX = dat.data[row_ind,:]\ny = np.array(dat.target[row_ind,<span class=\"hljs-number\">0</span>:<span class=\"hljs-number\">3</span>].todense())\n</code></pre>\n<p>After data preparation, we need a function that creates a visualization of the projected data. To have an idea of the quality of transformation, we can compute the following three matrices:</p>\n<ul>\n<li><code>dist_raw</code>: Matrix of the pairwise Euclidean distances of the actual data points.</li>\n<li><code>dist_transform</code>: Matrix of the pairwise Euclidean distances of the transformed data points.</li>\n<li><code>abs_diff</code>: Matrix of the absolute difference of <code>dist_raw</code> and <code>dist_actual</code></li>\n</ul>\n<p>The <code>abs_diff_dist</code> matrix is a good indicator of the quality of the data transformation. Close to zero or small values in this matrix indicate low distortion and a good transformation. We can directly display an image of this matrix or generate a histogram of its values to visually assess the transformation. We can also compute the average of all the values of this matrix to get a single quantitative measure for comparison.</p>\n<p>The function <code>create_visualization()</code> creates three plots. The first graph is a scatter plot of projected points along the first two random directions. The second plot is an image of the absolute difference matrix and the third is the histogram of the values of the absolute difference matrix:</p>\n<pre><code class=\"hljs\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">create_visualization</span>(<span class=\"hljs-params\">X_transform, y, abs_diff</span>):</span>\n    fig,ax = plt.subplots(nrows=<span class=\"hljs-number\">1</span>, ncols=<span class=\"hljs-number\">3</span>, figsize=(<span class=\"hljs-number\">20</span>,<span class=\"hljs-number\">7</span>))\n\n    plt.subplot(<span class=\"hljs-number\">131</span>)\n    plt.scatter(X_transform[y[:,<span class=\"hljs-number\">0</span>]==<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">0</span>], X_transform[y[:,<span class=\"hljs-number\">0</span>]==<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>], c=<span class=\"hljs-string\">'r'</span>, alpha=<span class=\"hljs-number\">0.4</span>)\n    plt.scatter(X_transform[y[:,<span class=\"hljs-number\">1</span>]==<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">0</span>], X_transform[y[:,<span class=\"hljs-number\">1</span>]==<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>], c=<span class=\"hljs-string\">'b'</span>, alpha=<span class=\"hljs-number\">0.4</span>)\n    plt.scatter(X_transform[y[:,<span class=\"hljs-number\">2</span>]==<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">0</span>], X_transform[y[:,<span class=\"hljs-number\">2</span>]==<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>], c=<span class=\"hljs-string\">'g'</span>, alpha=<span class=\"hljs-number\">0.4</span>)\n    plt.legend([<span class=\"hljs-string\">'Class 1'</span>, <span class=\"hljs-string\">'Class 2'</span>, <span class=\"hljs-string\">'Class 3'</span>])\n    plt.title(<span class=\"hljs-string\">'Projected data along first two dimensions'</span>)\n\n    plt.subplot(<span class=\"hljs-number\">132</span>)\n    plt.imshow(abs_diff)\n    plt.colorbar()\n    plt.title(<span class=\"hljs-string\">'Visualization of absolute differences'</span>)\n\n    plt.subplot(<span class=\"hljs-number\">133</span>)\n    ax = plt.hist(abs_diff.flatten())\n    plt.title(<span class=\"hljs-string\">'Histogram of absolute differences'</span>)\n\n    fig.subplots_adjust(wspace=<span class=\"hljs-number\">.3</span>) \n</code></pre>\n<h4 id=\"reutersdatasetgaussianrandomprojection\">Reuters Dataset: Gaussian Random Projection</h4>\n<p>Let's apply Gaussian Random Projection to the Reuters dataset. The code below runs a <code>for</code> loop for different <code>eps</code> values. If the minimum safe dimensions returned by <code>johnson_lindenstrauss_min_dim</code> is less than the actual data dimensions, then it calls the <code>fit_transform()</code> method of <code>GaussianRandomProjection</code>. The <code>create_visualization()</code> function is then called to create a visualization for that value of <code>eps</code>.</p>\n<p>At every iteration, the code also stores the mean absolute difference and the percentage reduction in dimensionality achieved by Gaussian Random Projection:</p>\n<pre><code class=\"hljs\">reduction_dim_gauss = []\neps_arr_gauss = []\nmean_abs_diff_gauss = []\n<span class=\"hljs-keyword\">for</span> eps <span class=\"hljs-keyword\">in</span> np.arange(<span class=\"hljs-number\">0.1</span>, <span class=\"hljs-number\">0.999</span>, <span class=\"hljs-number\">0.2</span>):\n\n    min_dim = johnson_lindenstrauss_min_dim(n_samples=total_points, eps=eps)\n    <span class=\"hljs-keyword\">if</span> min_dim &gt; X.shape[<span class=\"hljs-number\">1</span>]:\n        <span class=\"hljs-keyword\">continue</span>\n    gauss_proj = GaussianRandomProjection(random_state=<span class=\"hljs-number\">0</span>, eps=eps)\n    X_transform = gauss_proj.fit_transform(X)\n    dist_raw = euclidean_distances(X)\n    dist_transform = euclidean_distances(X_transform)\n    abs_diff_gauss = <span class=\"hljs-built_in\">abs</span>(dist_raw - dist_transform) \n\n    create_visualization(X_transform, y, abs_diff_gauss)\n    plt.suptitle(<span class=\"hljs-string\">'eps = '</span> + <span class=\"hljs-string\">'{:.2f}'</span>.<span class=\"hljs-built_in\">format</span>(eps) + <span class=\"hljs-string\">', n_components = '</span> + <span class=\"hljs-built_in\">str</span>(X_transform.shape[<span class=\"hljs-number\">1</span>]))\n    \n    reduction_dim_gauss.append(<span class=\"hljs-number\">100</span>-X_transform.shape[<span class=\"hljs-number\">1</span>]/X.shape[<span class=\"hljs-number\">1</span>]*<span class=\"hljs-number\">100</span>)\n    eps_arr_gauss.append(eps)\n    mean_abs_diff_gauss.append(np.mean(abs_diff_gauss.flatten()))\n</code></pre>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-5.png\" alt=\"RCV1 dataset gaussian random projections\"></p>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-6.png\" alt=\"RCV1 dataset gaussian random projections\"></p>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-7.png\" alt=\"RCV1 dataset gaussian random projections\"></p>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-8.png\" alt=\"RCV1 dataset gaussian random projections\"></p>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-9.png\" alt=\"RCV1 dataset gaussian random projections\"></p>\n<p>The images of the absolute difference matrix and its corresponding histogram indicate that most of the values are close to zero. Hence, a large majority of the pair of points maintain their actual distance in the low dimensional space, retaining the original structure of data.</p>\n<p>To assess the quality of transformation, let's plot the mean absolute difference against <code>eps</code>. Also, the higher the value of <code>eps</code>, the greater the dimensionality reduction. Let's also plot the percentage reduction vs. <code>eps</code> in a second sub-plot:</p>\n<pre><code class=\"hljs\">fig,ax = plt.subplots(nrows=<span class=\"hljs-number\">1</span>, ncols=<span class=\"hljs-number\">2</span>, figsize=(<span class=\"hljs-number\">10</span>,<span class=\"hljs-number\">5</span>))\nplt.subplot(<span class=\"hljs-number\">121</span>)\nplt.plot(eps_arr_gauss, mean_abs_diff_gauss, marker=<span class=\"hljs-string\">'o'</span>, c=<span class=\"hljs-string\">'g'</span>)\nplt.xlabel(<span class=\"hljs-string\">'eps'</span>)\nplt.ylabel(<span class=\"hljs-string\">'Mean absolute difference'</span>)\n\nplt.subplot(<span class=\"hljs-number\">122</span>)\nplt.plot(eps_arr_gauss, reduction_dim_gauss, marker = <span class=\"hljs-string\">'o'</span>, c=<span class=\"hljs-string\">'m'</span>)\nplt.xlabel(<span class=\"hljs-string\">'eps'</span>)\nplt.ylabel(<span class=\"hljs-string\">'Percentage reduction in dimensionality'</span>)\n\nfig.subplots_adjust(wspace=<span class=\"hljs-number\">.4</span>) \nplt.suptitle(<span class=\"hljs-string\">'Assessing the Quality of Gaussian Random Projections'</span>)\nplt.show()\n</code></pre>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-10.png\" alt=\"RCV1 random projections reduction quality\"></p>\n<p>We can see that using Gaussian Random Projection we can reduce the dimensionality of data to <strong>more than 99%</strong>! Though, this <em>does</em> come at the cost of a higher distortion of data.</p>\n<h4 id=\"reutersdatasetsparserandomprojection\">Reuters Dataset: Sparse Random Projection</h4>\n<p>We can do a similar comparison with sparse Random Projection:</p>\n<pre><code class=\"hljs\">reduction_dim_sparse = []\neps_arr_sparse = []\nmean_abs_diff_sparse = []\n<span class=\"hljs-keyword\">for</span> eps <span class=\"hljs-keyword\">in</span> np.arange(<span class=\"hljs-number\">0.1</span>, <span class=\"hljs-number\">0.999</span>, <span class=\"hljs-number\">0.2</span>):\n\n    min_dim = johnson_lindenstrauss_min_dim(n_samples=total_points, eps=eps)\n    <span class=\"hljs-keyword\">if</span> min_dim &gt; X.shape[<span class=\"hljs-number\">1</span>]:\n        <span class=\"hljs-keyword\">continue</span>\n    sparse_proj = SparseRandomProjection(random_state=<span class=\"hljs-number\">0</span>, eps=eps, dense_output=<span class=\"hljs-number\">1</span>)\n    X_transform = sparse_proj.fit_transform(X)\n    dist_raw = euclidean_distances(X)\n    dist_transform = euclidean_distances(X_transform)\n    abs_diff_sparse = <span class=\"hljs-built_in\">abs</span>(dist_raw - dist_transform) \n\n    create_visualization(X_transform, y, abs_diff_sparse)\n    plt.suptitle(<span class=\"hljs-string\">'eps = '</span> + <span class=\"hljs-string\">'{:.2f}'</span>.<span class=\"hljs-built_in\">format</span>(eps) + <span class=\"hljs-string\">', n_components = '</span> + <span class=\"hljs-built_in\">str</span>(X_transform.shape[<span class=\"hljs-number\">1</span>]))\n    \n    reduction_dim_sparse.append(<span class=\"hljs-number\">100</span>-X_transform.shape[<span class=\"hljs-number\">1</span>]/X.shape[<span class=\"hljs-number\">1</span>]*<span class=\"hljs-number\">100</span>)\n    eps_arr_sparse.append(eps)\n    mean_abs_diff_sparse.append(np.mean(abs_diff_sparse.flatten()))\n</code></pre>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-11.png\" alt=\"RCV1 dataset sparse random projections\"></p>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-12.png\" alt=\"RCV1 dataset sparse random projections\"></p>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-13.png\" alt=\"RCV1 dataset sparse random projections\"></p>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-14.png\" alt=\"RCV1 dataset sparse random projections\"></p>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-15.png\" alt=\"RCV1 dataset sparse random projections\"></p>\n<p>In the case of Random Projection, the absolute difference matrix appears similar to the one of Gaussian projection. The projected data on the first two dimensions, however, has a more interesting pattern, with many points mapped on the coordinate axis.</p>\n<p>Let's also plot the mean absolute difference and percentage reduction in dimensionality for various values of the <code>eps</code> parameter:</p>\n<pre><code class=\"hljs\">fig,ax = plt.subplots(nrows=<span class=\"hljs-number\">1</span>, ncols=<span class=\"hljs-number\">2</span>, figsize=(<span class=\"hljs-number\">10</span>,<span class=\"hljs-number\">5</span>))\nplt.subplot(<span class=\"hljs-number\">121</span>)\nplt.plot(eps_arr_sparse, mean_abs_diff_sparse, marker=<span class=\"hljs-string\">'o'</span>, c=<span class=\"hljs-string\">'g'</span>)\nplt.xlabel(<span class=\"hljs-string\">'eps'</span>)\nplt.ylabel(<span class=\"hljs-string\">'Mean absolute difference'</span>)\n\nplt.subplot(<span class=\"hljs-number\">122</span>)\nplt.plot(eps_arr_sparse, reduction_dim_sparse, marker = <span class=\"hljs-string\">'o'</span>, c=<span class=\"hljs-string\">'m'</span>)\nplt.xlabel(<span class=\"hljs-string\">'eps'</span>)\nplt.ylabel(<span class=\"hljs-string\">'Percentage reduction in dimensionality'</span>)\n\nfig.subplots_adjust(wspace=<span class=\"hljs-number\">.4</span>) \nplt.suptitle(<span class=\"hljs-string\">'Assessing the Quality of Sparse Random Projections'</span>)\nplt.show()\n</code></pre>\n<p><img src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-17.png\" alt=\"sparse random projections reduction quality\"></p>\n<p>The trend of the two graphs is similar to that of a Gaussian Projection. However, the mean absolute difference for Gaussian Projection is lower than that of Random Projection.</p>\n<h3 id=\"conclusions\">Conclusions</h3>\n<p>In this guide, we discussed the details of two main types of Random Projections, i.e., Gaussian and sparse Random Projection.</p>\n<p>We presented the details of the <strong><em>Johnson-Lindenstrauss lemma</em></strong>, the mathematical basis for these methods. We then showed how this method can be used to transform data using Python's <code>sklearn</code> library.</p>\n<p>We also illustrated the two methods on a real-life <a rel=\"nofollow noopener\" href=\"https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf\">Reuters Corpus Volume I Dataset</a>.</p>\n<p>We encourage the reader to try out this method in supervised classification or regression tasks at the pre-processing stage when dealing with very high-dimensional datasets.</p>"
}