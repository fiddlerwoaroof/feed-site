{
  "title":"Reinforcement Learning for Open Games",
  "date":"2022-11-10T00:00:00.000000Z",
  "author":null,
  "id":"https://tweag.io/blog/2022-11-10-learning-games-with-rllib/",
  "link":"https://tweag.io/blog/2022-11-10-learning-games-with-rllib/",
  "content":"In this post we illustrate how we built “Learning Games”,\nan integration between the open-games-hs framework and rllib,\nin order to gain access to the entire Python ecosystem and train agents for games written in Haskell.\nopen-games-hs is a Haskell library and DSL for defining,\noperating, and analyzing certain kinds of game-theoretic games. It comes\nwith a rich type theory and implementation that allows for the\nspecification of agent strategies as Haskell functions.\nFor some games, encoding strategies in Haskell is fairly simple. But one could\nimagine another way: what if we could learn those strategies? Utilizing the\ntheories of reinforcement learning? Well, one idea might be to try and build\na series of algorithms in Haskell implementing these strategies; or, we\ncould try and connect the Learning Games ecosystem to the Python AI ecosystem,\nand see if we can leverage both, at the same time!\nIn this post, we illustrate how we combined Learning Games and rllib to easily model games using Haskell while training agents with Python.\nRunning example: The Prisoner’s Dilemma\nTo keep things simple, we focus on a single game throughout the post, but, as we discuss at the end, the same approach we used for this game can be applied to a whole family of games.\nBroadly speaking, the concept of a Prisoner’s Dilemma (PD) refers to a well-known paradoxical situation in which “rational” agents are incentivized to not cooperate with each other, even if it might be in their best interest to do so.\nGame description\nTwo people (the potential “prisoners”, or “agents”), Agatha and Bibi, are thought to have committed a crime.\nThey are picked up and each is held in a holding cell.\nThey have no way to communicate with each other, and each has the option to either betray the other, or stay silent.\nThe possible outcomes, known by all parties, are:\n\nIf both Agatha and Bibi betray one another, each of them serves two years.\nIf Agatha betrays Bibi but Bibi stays silent, Agatha serves nothing and Bibi serves three years.\nIf Agatha stays silent but Bibi betrays Agatha, Agatha serves three years and Bibi serves nothing.\nIf Agatha and Bibi both stay silent, each of them serves one year (on a lesser charge).\n\nNash equilibrium vs. cooperative solution\nThis game has what is known as a Nash equilibrium: assuming that each agent has chosen their strategy, no agent can increase their expected payoff (i.e. reduce time served) assuming that the other agent keeps their strategy unchanged.\nThe equilibrium for this game is mutual betrayal, which is the best option from a “rational”, self-interested perspective: defecting always results in a better payoff than cooperating, regardless of the other player’s choice.\nHowever, despite the unfavorable individual incentive to betray each other, the collectively better result would be for both agents to stay silent, so that they both serve a shorter sentence.\nIterated variant\nA more interesting version of the game — known as the iterated prisoner’s dilemma — is that in which the game is played more than once in succession, and each agent remembers the previous moves of the other agent.\nThis is the variant of the game we experimented with, since it is amenable to reinforcement learning: agents can be trained so that they can play optimally against their opponent’s strategy and even learn to trust and cooperate with each other.\nThe latter is possible because the neural network can be trained and learn from historical data.\nPrisoner’s dilemma as an open game\nFirst, we encode the players’ actions using a Haskell datatype.\nUsing standard nomenclature, each agent can either Cooperate (i.e. stay silent), or Defect (i.e. betray):\ndata Action = Cooperate | Defect\nNext, we implement the payoff calculation.\nAs is common when modelling this game, we use non-negative payoffs (i.e. rewards), as opposed to costs used in the original description of the game.\nThe possible values of (PayoffA, PayoffB) are summarized as follows:\n\n\n\n\nB stays silent\nB betrays A\n\n\n\n\nA stays silent\n(3, 3)\n(0, 5)\n\n\nA betrays B\n(5, 0)\n(1, 1)\n\n\n\nThe function below implements this table, computing the payoff for agent i, given i’s and j’s actions.\nprisonersDilemmaMatrix :: Action -&gt; Action -&gt; Double\nprisonersDilemmaMatrix Cooperate Cooperate = 3\nprisonersDilemmaMatrix Cooperate Defect    = 0\nprisonersDilemmaMatrix Defect    Cooperate = 5\nprisonersDilemmaMatrix Defect    Defect    = 1\nGame specification\nLearning Games comes with a tutorial illustrating the semantics of the Template Haskell DSL in terms of string diagrams and shows how to specify games using it.\nOur intention in this post is not to explain this library in detail; for details please refer to the Learning Games modelling tutorial.\nIn this DSL, the prisoner’s dilemma looks like this:\nprisonersDilemma = [opengame|\n   inputs    :                                                        ;\n   feedback  : (payoff0,payoff1)                                      ;\n   :----------------------------:\n   inputs    :                                                        ;\n   feedback  : payoff0                                                ;\n   operation : interactWithEnv                                        ;\n   outputs   : decisionPlayer0                                        ;\n   returns   : prisonersDilemmaMatrix decisionPlayer0 decisionPlayer1 ;\n\n   inputs    :                                                        ;\n   feedback  : payoff1                                                ;\n   operation : interactWithEnv                                        ;\n   outputs   : decisionPlayer1                                        ;\n   returns   : prisonersDilemmaMatrix decisionPlayer1 decisionPlayer0 ;\n   :----------------------------:\n   outputs   : (decisionPlayer0, decisionPlayer1)                     ;\n   returns   :                                                        ;\n  |]\nIt describes the prisoner’s dilemma as the composition of two standalone sub-games.\nInterestingly, each subgame needs to know the decision of the opponent to calculate its payoff (using the prisonersDilemmaMatrix function).\nWe can represent this information flow graphically as follows:\n\n  \n\nGiven a strategy of type List '[Action, Action] (i.e. a heterogeneous list containing the decisions of the first and the second agent), we can use the framework to run the game once and observe the resulting payoffs.\nextractPayoffAndNextState (play prisonersDilemma strategy) () ()\n  :: IO ((Double, Double), (Action, Action)) -- (feedback, output)\nNote that, given we want to play the games step-by-step, we need to break games up so that, if they consist of several rounds, they output their internal state, and that state is then passed into the next game.\nThis is a technical limitation that we aim to clean up in subsequent work.\nThe game server\nA general challenge in utilizing rllib for playing open games is figuring out\nhow to make the two frameworks speak to each other, given that they are\nwritten in different languages. Our strategy for addressing this issue is the\nfollowing:\n\nTurn the open games side into a server and the rllib side\ninto a client.\nHave the server and the client exchange information (agent strategies,\npayoffs, etc.) encoded as JSON objects.\n\nTo account for the possibility of rllib utilizing parallelism during\ntraining, as well as getting a performance edge, we chose to use\nwebsockets instead of plain http. This way we avoid\nmanaging game sessions and reduce communication overhead between the server\nand the client.\nMost of the server infrastructure is standard for servant-based applications,\nand at its core sits the following function:\nwsPlay :: PendingConnection -&gt; Handler ()\nwsPlay pending = do\n  liftIO $ do\n    connection "
}