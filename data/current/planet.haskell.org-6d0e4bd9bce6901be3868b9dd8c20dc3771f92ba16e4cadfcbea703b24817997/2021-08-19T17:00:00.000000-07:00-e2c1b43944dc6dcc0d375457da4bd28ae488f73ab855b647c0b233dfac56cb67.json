{
  "title":"Avoiding quadratic core code size with large records",
  "date":"2021-08-19T17:00:00.000000-07:00",
  "author":"edsko",
  "id":"http://www.well-typed.com/blog/2021/08/large-records",
  "link":"https://well-typed.com/blog/2021/08/large-records",
  "content":"\nEdsko will be talking about the problems discussed in this blog post in his Haskell Implementors’ Workshop talk this Sunday, Aug 22. The talk will be broadcast live on YouTube.\n\nConsider a module that contains nothing but the definition of a single large record and some type class instances:\n{-# OPTIONS_GHC -fplugin=RecordDotPreprocessor #-}\n\nmodule Example where\n\nimport Data.Aeson\nimport Generics.SOP.JSON\nimport Generics.SOP.TH\nimport GHC.TypeLits\n\nnewtype T (i :: Nat) = MkT Word\n  deriving (Show, Eq, ToJSON)\n\ndata R = MkR {\n      f00 :: T 00\n    , f01 :: T 01\n      -- .. lots more ..\n    , f98 :: T 98\n    , f99 :: T 99\n    }\n  deriving (Eq, Show)\n\nderiveGeneric ''R\n\ninstance ToJSON R where\n  toJSON = gtoJSON defaultJsonOptions\nAs it stands—using ghc’s standard representation for records, along with the code generated by the RecordDotPreprocessor plugin and the Generic instance generated by generics-sop—this results in a core representation of a whopping 450,000 terms/types/coercions, takes 3 seconds to compile, and requires 500M of RAM to compile.1\nIf we change this module to\nmodule Example where\n\nimport Data.Aeson (ToJSON(..))\nimport Data.Record.Generic.JSON\nimport Data.Record.TH\n\nimport Test.Record.Size.Infra\n\nlargeRecord defaultLazyOptions [d|\n    data R = MkR {\n          f00 :: T 00\n        , f01 :: T 01\n          -- .. lots more ..\n        , f98 :: T 98\n        , f99 :: T 99\n        }\n      deriving (Eq, Show)\n  |]\n\ninstance ToJSON R where\n  toJSON = gtoJSON\nwe get a module with essentially the same functionality, but with a core size of a mere 14,000 terms/types/coercions, which compiles within 1 second and requires roughly 100M of RAM.\nIn this blog post we describe why this simple module generates so much code, and how the large-records library manages to reduce this by more than an order of magnitude.\n\nWe wrote this library because Juspay recently engaged Well-Typed’s services, and one of their requests to us was to try and improve compilation time and compilation memory requirements for their code base. Juspay very generously allowed us to make large-records open source, and it is now available on Hackage.\nQuadratic code size at every level\nThe reason the core representation of our example module is so large is that unfortunately there are many examples of ghc and other libraries being accidentally quadratic. Before we look at some concrete examples, let’s first investigate where this quadratic code size is coming from. As we will see, it arises at every level: terms, types, type classes, and type level programming.\nWarmup: terms\nFor our running example, we will want to have a record with lots of fields. To avoid some “accidental optimizations”, we’ll give each of those fields a different type. To make that a little bit easier, we’ll just introduce a single type that is indexed by a natural number, so that this one type definition gives us as many different types as we need:\ndata T (n :: Nat) = MkT Int\nThat out of the way, consider a record with lots of fields, such as\ndata R = MkR {\n    f00 :: T 00\n  , f01 :: T 01\n  , f02 :: T 02\n  -- .. lots more ..\n  , f98 :: T 98\n  , f99 :: T 99\n  }\nWhen we define a record, ghc will generate field accessors for all fields in the record. In other words, it will derive functions such as\nf00 :: R -&gt; T 0\nThese functions are not difficult to generate, of course. Each function is just a simple case statement:\nf00 = \\(r :: R) -&gt;\n    case r of\n      MkR x00 x01 x02 x03 x04 x05 x06 x07 x08 x09\n          x10 x11 x12 x13 x14 x15 x16 x17 x18 x19\n          -- .. lots more ..\n          x90 x91 x92 x93 x94 x95 x96 x97 x98 x99 -&gt;\n        x00\nAlthough simple, this case statement mentions a lot of unused variables (99 of them, in fact). Moreover, each of those variables is annotated with their type. This means that this one function is actually rather large; ghc reports that it contains 5 terms and 202 types. The size of this function is clearly linear in the number n of fields we have; moreover, ghc will generate one function for each of those n fields; that means that simply declaring the record will already generate code that is O(n²) in size.\nMore subtle: types\nSuppose we define an applicative “zip” function for R, something like\nzipMyRecordWith ::\n     Applicative f\n  =&gt; (forall n. T n -&gt; T n -&gt; f (T n))\n  -&gt; R -&gt; R -&gt; f R\nzipMyRecordWith f r r' =\n        pure MkR\n    &lt;*&gt; f (f00 r) (f00 r')\n    &lt;*&gt; f (f01 r) (f01 r')\n    &lt;*&gt; f (f02 r) (f02 r')\n    -- .. lots more ..\n    &lt;*&gt; f (f98 r) (f98 r')\n    &lt;*&gt; f (f99 r) (f99 r')\nClearly the size of this function is at least linear in the number of record fields, that much is expected. However, -ddump-simpl tells us that this function contains 50,818 types! Where are all of those coming from?\nRecall the type of (&lt;*&gt;):\n(&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b\nThose type variables need to be instantiated; f is always instantiated to the f type parameter passed to zipMyRecordWith, a is the type of the next field we’re applying, but what about b? Let’s annotate zipMyRecordWith with the types of a and b in pseudo-Haskell:\nzipMyRecordWith ::\n     Applicative f\n  =&gt; (forall n. T n -&gt; T n -&gt; f (T n))\n  -&gt; R -&gt; R -&gt; f R\nzipMyRecordWith f r r' =\n        pure MkR\n    &lt;*&gt; @(T 00) @(T 01 -&gt; T 02 -&gt; T 03 -&gt; .. -&gt; T 99 -&gt; R) f (f00 r) (f00 r')\n    &lt;*&gt; @(T 01) @(        T 02 -&gt; T 03 -&gt; .. -&gt; T 99 -&gt; R) f (f01 r) (f01 r')\n    &lt;*&gt; @(T 02) @(                T 03 -&gt; .. -&gt; T 99 -&gt; R) f (f02 r) (f02 r')\n    -- .. lots more ..\n    &lt;*&gt; @(T 98) @(                              T 99 -&gt; R) f (f98 r) (f98 r')\n    &lt;*&gt; @(T 99) @(                                      R) f (f99 r) (f99 r')\nThe first instantiation of (&lt;*&gt;) mentions the type of every single field; the second mentions the types of all-but-one field, the next of all-but-two, etc. This means that the size of this single function is once again O(n²) in the number of record fields.\nType class dictionaries\nSuppose we wanted to capture the concept “some constraints c applied to the types of all fields in our record”:\nclass (\n    c (T 00)\n  , c (T 01)\n  , c (T 02)\n  -- .. lots more ..\n  , c (T 98)\n  , c (T 99)\n  ) =&gt; Constraints_R c\nThis should be fine right? Right? Wrong.\nWhen we declare a type class, we’re effectively constructing a record type with fields for each of the methods of the class; this record is known as the “dictionary” for the class. Superclass constraints translate to “subdictionaries”: when a class such as Ord has a superclass constraint on Eq, then the dictionary for Ord will have a field for an Eq dictionary.\nThis means that this definition of Constraints_R is actually of a very similar nature to the definition of R itself: it defines a record with 100 fields. And just like for the record, ghc will generate “field accessors” to extract the fields of this dictionary; put another way, those field accessors “prove” that if we know Constraints_R c, we also know c (T 00), c (T 01), etc. What do those field accessors look like? You guessed it, a big pattern match; in pseudo-Haskell:\n$p1Constraints_R :: Constraints_R c =&gt; c (T 0)\n$p1Constraints_R = \\dict -&gt;\n    case dict of\n      Constraints_R d00 d01 d02 d03 d04 d05 d06 d07 d08 d09\n                    d10 d11 d12 d13 d14 d15 d16 d17 d18 d19\n                    -- .. lots more ..\n                    d90 d91 d92 d93 d94 d95 d96 d97 d98 d99 -&gt;\n        d00\nSince ghc generates a projection like this for each superclass constraint, this once again results in code of size that is O(n²) in the number of record fields.\nType level induction\nSo far all our examples have been simple Haskell; for our next example, we’ll get a bit more advanced. Just like we can have list values, we can also have list types; for example, here is a type-level list of the indices of the T types used inside our running example record R:\ntype IndicesR = '[\n    00, 01, 02, 03, 04, 05, 06, 07, 08, 09\n  , 10, 11, 12, 13, 14, 15, 16, 17, 18, 19\n  -- .. lots more ..\n  , 90, 91, 92, 93, 94, 95, 96, 97, 98, 99\n  ]\nThere are many use cases for type level lists. For example, we can define a type NP such that NP f [x, y, .., z] is basically the same as (f x, f y, .., f z):\ndata NP (f :: k -&gt; Type) (xs :: [k]) where\n  Nil  :: NP f '[]\n  (:*) :: f x -&gt; NP f xs -&gt; NP f (x ': xs)\nIf we have a T for every index in IndicesR, we can construct a value of our record:\nnpToR :: NP T IndicesR -&gt; R\nnpToR (  f00 :* f01 :* f02 :* f03 :* f04 :* f05 :* f06 :* f07 :* f08 :* f09\n      :* f10 :* f11 :* f12 :* f13 :* f14 :* f15 :* f16 :* f17 :* f18 :* f19\n       -- .. lots more ..\n      :* f90 :* f91 :* f92 :* f93 :* f94 :* f95 :* f96 :* f97 :* f98 :* f99\n      :* Nil ) = MkR {..}\nThe compiled size of npToR is large, but it is linear in size (total size of 4441 terms, types and coercions for 100 fields, and a total size of 2241 for 50 fields). So far so good.\nIn order to get to the problem I’d like to illustrate in this section, we need one more concept. Suppose wanted to write a function that can construct a value of NP T xs for any xs:\nmkNP :: NP T xs\nThis should be possible, since T is just a wrapper around an Int, and so all we need to do is generate as many Ts as there are elements in the type level list xs. However, xs is a type level list, and we cannot pattern match on types in Haskell; indeed, they do not even exist at all at run-time.\nTherefore we somehow need to reflect the type level list at the term level: we need a value that corresponds exactly to the type. We do this by introducing a new type, indexed by a type level list, so that given a type level list of a particular length, our new type has exactly one value. Such a type—a type with exactly one value—is known as a singleton type:\ndata SList :: [k] -&gt; Type where\n  SNil  :: SList '[]\n  SCons :: SList xs -&gt; SList (x ': xs)\nSList gives us a value that we can pattern match on, and when we do we discover something about the shape of the type level list xs:\nmkNP' :: SList xs -&gt; NP T xs\nmkNP' SNil      = Nil\nmkNP' (SCons s) = MkT 0 :* mkNP' s\nThe closest we can come to mkNP is to make this singleton value implicit:\nclass SListI (xs :: [k]) where\n  sList :: SList xs\n\nmkNP :: SListI xs =&gt; NP T xs\nmkNP = mkNP' sList\nIf we now try to use mkNP to construct a value of our record\nr0 :: R\nr0 = npToR mkNP\nwe will of course find that we need an instance of SListI for IndicesR. Our first instinct might be to write something like\ninstance SListI IndicesR where\n  sList =\n      SCons\n    $ SCons\n    $ SCons\n    -- .. lots more ..\n    $ SCons\n    $ SCons\n    $ SNil\nbut if we do that, we will soon discover that the compiled code is quadratic in size. We could have predicted that: it’s the same problem as in the “Types” section above, with ($) playing the role of (&lt;*&gt;). But even if we write it as\ninstance SListI IndicesR where\n  sList =\n    SCons (\n    SCons (\n    SCons (\n    -- .. lots more ..\n    SCons (\n    SCons (\n    SNil\n    ))) {- .. lots more brackets .. -} ))\nwe’re still in trouble: each of those SCons applications has two type argument x and xs (the type level list of the tail). So with some type annotations, this code is\ninstance SListI IndicesR where\n  sList =\n    SCons @00 @'[01, 02, 03, .., 99] (\n    SCons @01 @'[    02, 03, .., 99] (\n    SCons @02 @'[        03, .., 99] (\n    -- .. lots more ..\n    SCons @98 @'[                99] (\n    SCons @99 @'[                  ] (\n    SNil\n    ))) {- .. lots more brackets .. -} ))\nSo this code is again O(n²) in size (actually, the real code generated by ghc is much worse than this, due to the fact that SList is a GADT; after desugaring, the function has a total size of 15,352, and after the simplifier runs (in -O0) that expands to a whopping 46,151).\nExperienced type-level Haskellers might be surprised that we’d try to write this SListI instance by hand. After all, the definition of a singleton type is that it is a type with only a single value, and so we should be able to just derive it automatically. Indeed we can:\ninstance SListI '[] where\n  sList = SNil\n\ninstance SListI xs =&gt; SListI (x ': xs) where\n  sList = SCons sList\nSurely we should be good now, right? These definitions are small, and don’t deal with concrete large lists, and so we avoid quadratic code size. Right? Wrong.\nAlthough it is true that the two instances for SListI are unproblematic, the moment that we use npToR mkNP, ghc needs to prove SListI '[00, 01, .. 99]. In other words, it must generate code that produces a dictionary for SListI [00, 01, .. 99]. Since SListI for (x ': xs) has a superclass constraint SListI xs, the dictionary for SListI [00, 01, .., 99] will have a field for the dictionary for SListI [01, .., 99], all the way down to the empty type level list. This means that ghc will generate 100 dictionaries; each of those dictionaries contains an SCons application with the same type annotation as in hand-written code above. This means that we still have code that is O(n²) in size.\nConcrete examples\nIn the previous section we discussed the ways in which we might end up with accidentally quadratic code size. In this section we will consider some examples of code generated by specific libraries. We will start with GHC generics, which is actually a good example: it generates code of size O(n log n) rather than O(n²). After that we will discuss record-dot-preprocessor and generics-sop, both of which do generate code of O(n²) size.\nGHC Generics\nThe goal of generic programming is to be able to write a single function that can be applied to values of lots of different types. Generics libraries such as GHC.Generics and generics-sop (discussed below) do this by translating the value to a representation type; since every type can be translated to only a handful of different representation types, it suffices to write a function over all of those representation types.\nHere we will discuss a simplified form of GHC.Generics that still illustrates the same point. The generic representation of a record such as the one above is essentially just a large nested tuple. For the GHC library itself it does not actually matter terribly how this tuple is created; for example, this would work:\ntype family GHC_Rep (a :: Type) :: Type\n\ntype instance GHC_Rep R = (T 00, (T 01, (T02, ... (T98, T99))))\nAlthough it would work, it would not be great from a code size perspective. Consider the function that would translate R to GHC_Rep R:\nghcTo :: R -&gt; GHC_Rep R\nghcTo MkR{..} =\n    (,) @(T 00) @(T 01, (T02, ... (T98, T99))) f00 (\n    (,) @(T 01)        @(T02, ... (T98, T99))  f01 (\n    (,) @(T 02)             @(... (T98, T99))  f02 (\n    -- .. lots more ..\n    (,) @(T 99)                         @(())  f99 (\n    () ))))\nThis pattern is starting to get familiar at this point; with this representation, ghcTo would be O(n²) in size. Fortunately, GHC generics avoids this problem by instead generating a balanced representation, something like\ntype instance GHC_Rep R =\n ( ( ( ( ( ( T 00, ( T 01, T 02 ) )\n         , ( T 03, ( T 04, T 05 ) )\n         )\n -- .. lots more ..\n         , ( ( T 46, T 47 )\n           , ( T 48, T 49 )\n   ) ) ) ) )\n , ( ( ( ( ( T 50, ( T 51, T 52 ) )\n         , ( T 53, ( T 54, T 55 ) )\n         )\n -- .. lots more ..\n         , ( ( T 96, T 97 )\n           , ( T 98, T 99 )\n ) ) ) ) ) )\nWith this representation, the number of branches is still the same, but in the translation function the type annotations are now halved in size at each branch, rather than reduced by 1:\nghcTo :: R -&gt; GHC_Rep R\nghcTo MkR{..} =\n  ( ( ( ( ( ( f00, ( f01, f02 ) )\n          , ( f03 , ( f04, f05 ) )\n          )\n  -- .. lots more ..\n          , ( ( f96, f97 )\n            , ( f98, f99 )\n  ) ) ) ) ) )\nAs a consequence, the size of this version, and the cost of GHC generics in general, is actually O(n log n) in the number of record fields, although the constant factor is reasonably high.\nIt is worth emphasizing how much better O(n log n) is than O(n²). Here is a plot of the cost of GHC generics (in terms of AST size: terms, types and coercions) as we vary the number of record fields from 100 fields to 1000 fields:\n\nThis almost looks linear. It isn’t; the cost per field is roughly 460 terms/types/coercions when we have 100 fields, and that increases to roughly 625 when we get to 1000 fields, but the cost only goes up very slowly.\nThe RecordDotSyntax preprocessor\nThe record-dot-preprocessor is a preprocessor and GHC plugin for the RecordDotSyntax GHC proposal. The preprocessor interprets specialized “record syntax”; for example, it translates\nexpr{lbl1.lbl2 = val}\nto\nsetField @&quot;lbl1&quot; expr $ setField @&quot;lbl2&quot; (getField @&quot;lbl1&quot; expr) val\nThese two functions getField and hasField come from a HasField class currently provided by record-hasfield (although now that the Add setField to HasField proposal is accepted, this should eventually move to base):\nclass HasField x r a | x r -&gt; a where\n  hasField :: r -&gt; (a -&gt; r, a)\nWhen you include\n{-# OPTIONS_GHC -fplugin=RecordDotPreprocessor #-}\nat the top of your Haskell file, the RecordDotPreprocessor plugin will generate HasField instances for you. They look innocuous enough:\ninstance HasField &quot;f00&quot; R (T 00) where\n  hasField r = (\\x -&gt; r { f00 = x }, f00 r)\nUnfortunately, once we get to ghc’s internal representation, this is much less innocent:\nhasField_f00 :: R -&gt; (T 0 -&gt; R, T 0)\nhasField_f00 r = (\n      \\new -&gt; case r of\n        MkR x00 x01 x02 x03 x04 x05 x06 x07 x08 x09\n            x10 x11 x12 x13 x14 x15 x16 x17 x18 x19\n            -- .. lots more ..\n            x90 x91 x92 x93 x94 x95 x96 x97 x98 x99 -&gt;\n          MkR new x01 x02 x03 x04 x05 x06 x07 x08 x09\n              x10 x11 x12 x13 x14 x15 x16 x17 x18 x19\n              -- .. lots more ..\n              x90 x91 x92 x93 x94 x95 x96 x97 x98 x99\n    , case r of\n        MkR x00 x01 x02 x03 x04 x05 x06 x07 x08 x09\n            x10 x11 x12 x13 x14 x15 x16 x17 x18 x19\n            -- .. lots more ..\n            x90 x91 x92 x93 x94 x95 x96 x97 x98 x99 -&gt;\n          x00\n    )\nWe saw this before when we discussed the record field accessors; the same linear cost that a field accessor induces is induced here as well, with a larger constant factor. As for field accessors, we need to generate a HasField instance for every field, and hence altogether we once again have code that is O(n²) in size.\nThis really matters: a module containing just the declaration of our record size has a total size of 22,065 terms/types/coercions after desugaring, and 22,277 after the simplifier (with -O0). This is already much bigger than it should be, due to the quadratic nature fo the field accessors. Generating HasField instances for all fields results in a total code size of 58,665 after desugaring and 78,977 after the simplifier. And all we’ve done is define the record! (For comparison, with large-records, a module containing a single record with 100 fields has a mere total size of 8,305 after desugaring, expanding to 13,958 after the simplifier, and that is including support for generics).\nSOP generics\nThe generics-sop library is similar in nature to GHC generics, but it uses a different generic representation. It is described in detail in the paper True Sums of Products; here we give a simplified presentation.\nIn fact, we have already seen most ingredients. The generics-sop representation for a record is essentially the NP type that we discussed in “Type level induction”, above. In that section we saw that the function npToR that translates from the generic representation to R is linear in size. Unfortunately, the same is not true for the inverse function:\nnpFromR :: R -&gt; NP T IndicesR\nnpFromR MkR{..} = (\n       f00 :* f01 :* f02 :* f03 :* f04 :* f05 :* f06 :* f07 :* f08 :* f09\n    :* f10 :* f11 :* f12 :* f13 :* f14 :* f15 :* f16 :* f17 :* f18 :* f19\n    -- .. lots more ..\n    :* f90 :* f91 :* f92 :* f93 :* f94 :* f95 :* f96 :* f97 :* f98 :* f99\n#endif\n    :* Nil\n    )\nThat wildcard pattern match MkR{..} will expand to a pattern match for a variable for every field, but that doesn’t matter here: we only generate one translation function, and it’s fine if that is linear in size. The problem however is in the body of this function. After the previous examples, perhaps you can spot the problem already: (:*) has a bunch of type arguments, and one of those is the list of indices at the tail; so this code looks something like\nnpFromR MkR{..} =\n    (:*) @00 @'[1, 2, .., 98, 99] f00 (\n    (:*) @01 @'[   2, .., 98, 99] f01 (\n    (:*) @02 @'[      .., 98, 99] f02 (\n    -- .. lots more ..\n    (:*) @98 @'[              99] f98 (\n    (:*) @99 @'[                ] f99 (\n    Nil )))))\na depressingly familiar sight at this point (and again, the real code is worse, due to the fact that NP is a GADT). This matters: the size of npToR is 4,441 terms/types/coercions, the size of npFromR is 46,459.\nThe generics-sop library suffers from quadratic code size in other places as well. It makes heavy use of type-level lists, in a similar style to SListI above, and with the same kinds of problems. It also represents metadata at the type-level, rather than just at the term level, which are more type-level lists. These are not fundamental problems; generics-sop simply wasn’t designed with the goal to optimize for code size reduction in mind. As we saw in the section on GHC generics, these costs can probably be brought down to O(n log n), though this will require careful thought.\nThe large-records library\nWhen you use the large-records library and define\nlargeRecord defaultLazyOptions [d|\n  data R = MkR {\n      f00 :: T 00\n    , f01 :: T 01\n    , f02 :: T 02\n    -- .. lots more ..\n    , f98 :: T 98\n    , f99 :: T 99\n    }\n  |]\nYou get the definition of a type R with field accessors, HasField instances for every field, and a Generic instance (albeit for a custom generics library), but the code will be entirely linear—O(n)—in the size of the record. In this section we will see how large-records achieves this for the basic definitions; we will discuss generics separately in the next section.\nRepresentation\nAs we saw, the moment that we declare a record, ghc will generate field accessors for each field of the record, resulting in code that is O(n²) in size. It follows that we cannot use the normal representation of the record. Instead, large-records generates the following:\nnewtype R = LR__MkR { vectorFromR :: Vector Any }\nThat is, we are representing the record basically by an untyped vector which will have an entry for every field in the record.2 Of course, typically users will never deal with this untyped representation directly, but use the field accessors or HasField instances, which we will discuss next.\nField accessors\nAlong with the definition of R, large-records generates an unsafe function that can return any element of the vector at any type:\nunsafeGetIndexR :: Int -&gt; R -&gt; a\nunsafeGetIndexR n t = noInlineUnsafeCo $ vectorFromR t n\nwhere noInlineUnsafeCo is a non-inlinable form of unsafeCoerce3. Just like the internal representation of R, this function is not intended for normal use. Instead, it is used to define field accessors for each field. For example, here is the definition of the accessor f00:\nf00 :: R -&gt; T 0\nf00 = unsafeGetIndexR 0\nOne of these accessors is generated for every field, but the size of each accessor is constant (and tiny), so the generation of all accessors results in code that is O(n) in size.\nHasField instance\nThe HasField instance is very similar. Along with the unsafe accessor, we also define an unsafe update function:\nunsafeSetIndexR :: Int -&gt; R -&gt; a -&gt; R\nunsafeSetIndexR n r x = LR__MkR $\n    unsafeUpd (vectorFromR r) [(n, noInlineUnsafeCo x)]\nThe HasField instance is now easy, and once again constant in size and tiny:\ninstance HasField &quot;f00&quot; R (T 0) where\n  hasField r = (unsafeSetIndexR 0 r, unsafeGetIndexR 0 r)\nPattern synonym\nBy default large-records does not generate a pattern synonym for R. It can do, if requested:\nlargeRecord (defaultLazyOptions {generatePatternSynonym = True}) [d|\n  data R = MkR {\n      f00 :: T 00\n    , f01 :: T 01\n    , f02 :: T 02\n    -- .. lots more ..\n    , f98 :: T 98\n    , f99 :: T 99\n    }\n  |]\nWith the generatePatternSynonym option, large-records generates two new definitions. First, a function that constructs a tuple containing all fields of the record:\ntupleFromR :: R\n           -&gt; ( (T 00, T 01, T 02, {- .. lots more .. -}, T 60, T 61)\n              , (T 62, T 63, T 64, {- .. lots more .. -}, T 98, T 99)\n              )\ntupleFromR r = (\n      unsafeGetIndexR 00 r\n    , unsafeGetIndexR 01 r\n    , unsafeGetIndexR 02 r\n    -- .. lots more ..\n    , unsafeGetIndexR 60 r\n    , unsafeGetIndexR 61 r\n    )\n  , ( unsafeGetIndexR 62 r\n    , unsafeGetIndexR 63 r\n    , unsafeGetIndexR 64 r\n     -- .. lots more ..\n    , unsafeGetIndexR 98 r\n    , unsafeGetIndexR 99 r\n  ) )\n(It was careful to use a nested tuple, because ghc does not support tuples with more than 62 fields.) It then uses this function as a view pattern in an explicitly bidirectional pattern synonym:\npattern MkR :: T 0 -&gt; T 1 -&gt; T 2 -&gt; {- .. lots more .. -} -&gt; T 98 -&gt; T99 -&gt; R\npattern MkR{f00, f01, f02, {- .. lots more .. -}, f98, f99} "
}