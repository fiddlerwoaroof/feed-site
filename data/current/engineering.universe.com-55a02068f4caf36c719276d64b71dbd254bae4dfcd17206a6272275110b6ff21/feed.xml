<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Universe Engineering - Medium]]></title>
        <description><![CDATA[Helping you build memories and experiences that last a lifetime. Check out our career opportunities at https://careers.universe.com - Medium]]></description>
        <link>https://engineering.universe.com?source=rss----a1cd8e0d60b2---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Universe Engineering - Medium</title>
            <link>https://engineering.universe.com?source=rss----a1cd8e0d60b2---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 17 Nov 2022 12:57:38 GMT</lastBuildDate>
        <atom:link href="https://engineering.universe.com/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Improving Browser Performance 10x]]></title>
            <link>https://engineering.universe.com/improving-browser-performance-10x-f9551927dcff?source=rss----a1cd8e0d60b2---4</link>
            <guid isPermaLink="false">https://medium.com/p/f9551927dcff</guid>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[react]]></category>
            <category><![CDATA[graphql]]></category>
            <category><![CDATA[puppeteer]]></category>
            <category><![CDATA[elixir]]></category>
            <dc:creator><![CDATA[exAspArk]]></dc:creator>
            <pubDate>Thu, 06 Jun 2019 15:08:10 GMT</pubDate>
            <atom:updated>2019-07-12T13:57:03.417Z</atom:updated>
            <content:encoded><![CDATA[<p>We recently improved the performance of the <a href="https://www.universe.com">Universe.com</a> homepage by more than ten times. Let’s explore the techniques we used to achieve this result.</p><p><em>Here is a translated </em><a href="https://www.infoq.cn/article/XSKxPByXUVu1-O3OQsxD"><em>Chinese version</em></a><em> of the blog post on InfoQ.</em></p><p>But first, let’s find out why website performance is important (there are links to the case studies at the end of the blog post):</p><ul><li><strong>User experience</strong>: poor performance leads to unresponsiveness, which may be frustrating for users from a UI and UX perspective.</li><li><strong>Conversion and revenue</strong>: very often slow websites can lead to lost customers and have a negative impact on conversion rates and revenue.</li><li><strong>SEO</strong>: Starting <a href="https://developers.google.com/search/mobile-sites/mobile-first-indexing">July 1, 2019</a>, Google will enable mobile-first indexing by default for all new websites. Websites will be ranked lower if they are slow on mobile devices, and don’t have mobile-friendly content.</li></ul><p>In this blog post, we will briefly cover these main areas, which helped us to improve the performance on our pages:</p><ul><li><strong>Performance measurement</strong>:<strong> </strong>lab and field instruments.</li><li><strong>Rendering</strong>: client-side and server-side rendering, pre-rendering, and hybrid rendering approaches.</li><li><strong>Network</strong>: CDN, caching, GraphQL caching, encoding, HTTP/2 and Server Push.</li><li><strong>JavaScript in the browser</strong>: bundle size budget, code-splitting, async and defer scripts, image optimizations (WebP, lazy loading, progressive), and resource hints (preload, prefetch, preconnect).</li></ul><p>For some context, our homepage is built with React (TypeScript), Phoenix (Elixir), Puppeteer (headless Chrome), and <a href="https://engineering.universe.com/why-were-betting-on-graphql-233ddf1a0779">GraphQL API</a> (Ruby on Rails). This is how it looks like on mobile:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tVIdR8S8QOox_oMjFzNFdQ.png" /><figcaption>Universe <a href="https://www.universe.com">homepage</a> and <a href="https://www.universe.com/explore">explore</a></figcaption></figure><h3>Performance measurement</h3><blockquote>Without data, you’re just another person with an opinion. ― W. Edwards Deming</blockquote><h4>Lab instruments</h4><p>Lab instruments allow collecting data within a controlled environment with the predefined device and network settings. With these instruments, it is much simpler to debug any performance issues and have well-reproducible tests.</p><p><a href="https://developers.google.com/web/tools/lighthouse/">Lighthouse</a> is an excellent tool for auditing webpages in Chrome on a local computer. It also provides some useful tips on how to improve performance, accessibility, SEO, etc. Here are some Lighthouse performance audit reports with a Simulated Fast 3G and 4x CPU Slowdown:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2k4GOCN8mGhXGD8amp3Tvw.png" /><figcaption>Before and after: 10x improvement for the <a href="https://developers.google.com/web/tools/lighthouse/audits/first-contentful-paint">First Contentful Paint</a> (FCP)</figcaption></figure><p>There is, however, a disadvantage of using just lab instruments: they don’t necessarily capture real-world bottlenecks which may depend on the end-users’ devices, network, location, and many other factors. That is why it is also important to use field instruments.</p><h4>Field instruments</h4><p>Field instruments allow to simulate and measure real user page loads. There are multiple services which can help to get real performance data from the actual devices:</p><ul><li><a href="https://www.webpagetest.org/">WebPageTest</a> — allows performing tests from different browsers on real devices from various locations.</li><li><a href="https://www.thinkwithgoogle.com/feature/testmysite">Test My Site</a> — uses Chrome User Experience Report (<a href="https://developers.google.com/web/tools/chrome-user-experience-report/">CrUX</a>) which is based on Chrome usage statistics; it is publicly available and updated monthly.</li><li><a href="https://developers.google.com/speed/pagespeed/insights/">PageSpeed Insights</a> — combines both lab (Lighthouse) and field (CrUX) data.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4I--Fz3Rd9qy8_3QJv4cNg.png" /><figcaption>WebPageTest report</figcaption></figure><h3>Rendering</h3><p>There are multiple approaches for rendering content, and each has its pros and cons:</p><ul><li><strong>Server-side rendering</strong> (SSR) is a process of getting the final HTML documents for browsers on the server-side. <em>Pros</em>: search engines can crawl the website without executing JavaScript (SEO), fast initial page load, code lives only on the server-side. <em>Cons</em>: non-rich website interactions, the full page reloads, limited access to the browser features.</li><li><strong>Client-side rendering</strong> is a process of rendering content in the browser by using JavaScript. <em>Pros</em>: rich website interactions, fast rendering on route changes after the initial load, access to modern browser features (e.g. offline support with <a href="https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API/Using_Service_Workers">Service Workers</a>). <em>Cons</em>: not SEO-friendly, slow initial page load, usually requires implementing a Single Page Application (SPA) and an API on the server-side.</li><li><strong>Pre-rendering</strong> is similar to server-side rendering but happens during buildtime in advance instead of runtime. <em>Pros</em>: serving built static files is usually simpler than running a server, SEO-friendly, fast initial page load. <em>Cons</em>: requires pre-rendering all possible pages in advance on any code changes, the full page reloads, non-rich website interactions, limited access to the browser features.</li></ul><h4>Client-side rendering</h4><p>Previously, we had our homepage implemented with Ember.js framework as a SPA with client-side rendering. One issue we had was a big bundle size of our Ember.js application. It means that users see just a blank screen while the browser downloads JavaScript files, parses, compiles, and executes them:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EuN5a83NwTLEJZEek3ccjg.png" /><figcaption>White screen of death</figcaption></figure><p>We decided to rebuild some parts of the app by using <a href="https://reactjs.org/">React</a>.</p><ul><li>Our developers are already familiar with building React applications (e.g. <a href="https://www.universe.com/sell-on-your-site">embedded widgets</a>).</li><li>We already have a few React component libraries which can be shared across multiple projects.</li><li>The <a href="https://www.universe.com/explore">new pages</a> have some interactive UI elements.</li><li>There is a huge React ecosystem with lots of tools.</li><li>With JavaScript in the browser, it is possible to build a <a href="https://developers.google.com/web/progressive-web-apps/">Progressive Web App</a> with lots of nice features.</li></ul><h4>Pre-rendering and server-side rendering</h4><p>The issue with client-side rendered applications built, for example, with <a href="https://reacttraining.com/react-router/web/guides/quick-start">React Router DOM</a> is still the same as with Ember.js. JavaScript is expensive, and it takes a while to see the First Contentful Paint in the browser.</p><p>Once we decided on using React, we started experimenting with other potential rendering options to allow browsers to render the content faster.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7dboVK-29_p73R9rp88CaQ.png" /><figcaption>Conventional rendering options with React</figcaption></figure><ul><li><a href="https://www.gatsbyjs.org">Gatsby.js</a> allows pre-rendering pages with React and GraphQL. Gatsby.js is a great tool which supports many performance optimizations out of the box. However, using pre-rendering doesn’t work for us since we have a potentially unlimited number of pages with user-generated content.</li><li><a href="https://nextjs.org">Next.js</a> is a popular Node.js framework which allows server-side rendering with React. However, Next.js is very opinionated, requires to use its router, CSS solution, and so on. And our existing component libraries were built for browsers and are not compatible with Node.js.</li></ul><p>That is why we decided to experiment with some <a href="https://www.youtube.com/watch?v=k-A2VfuUROg">hybrid approaches</a>, which try taking the best from each rendering option.</p><h4>Runtime pre-rendering</h4><p><a href="https://github.com/GoogleChrome/puppeteer">Puppeteer</a> is a Node.js library allows working with a headless Chrome. We wanted to give Puppeteer a try for pre-rendering in runtime. That enables using an interesting hybrid approach: server-side rendering with Puppeteer and client-side rendering with the <a href="https://reactjs.org/docs/react-dom.html#hydrate">hydration</a>. Here are some useful tips by Google on <a href="https://developers.google.com/web/tools/puppeteer/articles/ssr">how to use a headless browser</a> for server-side rendering.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZxsknHfPnPWPBF5s2Q0QwQ.png" /><figcaption>Puppeteer for runtime pre-rendering a React application</figcaption></figure><p>Using this approach has some pros:</p><ul><li>Allows SSR, which is good for SEO. Crawlers don’t need to execute JavaScript to be able to see the content.</li><li>Allows building a simple browser React application once, and using it both on the server-side and in browsers. Making the browser app faster automatically makes SSR faster, win-win.</li><li>Rendering pages with Puppeteer on a server is usually faster than on end-users’ mobile devices (better connection, better hardware).</li><li>Hydration allows building rich SPAs with access to the JavaScript browser features.</li><li>We don’t need to know about all possible pages in advance in order to pre-render them.</li></ul><p>However, we faced a few challenges with this approach:</p><ul><li><strong>Throughput </strong>is the main issue. Having each request executed in a separate headless browser process uses up a lot of resources. It is possible to use a single headless browser process and run multiple requests in separate tabs. However, using multiple tabs decreases the performance of the whole process.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jhXxF-C5U3bYARpv2ylgqg.png" /><figcaption>The architecture of server-side rendering with Puppeteer</figcaption></figure><ul><li><strong>Stability</strong>. It is challenging to scale up or scale down many headless browsers, keep the processes “warm” and balance the workload. We tried different hosting approaches: from being self-hosted in a Kubernetes cluster to serverless with AWS Lambda and Google Cloud Functions. We noticed that the latter had some performance <a href="https://github.com/GoogleChrome/puppeteer/issues/3120">issues with Puppeteer</a>:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6QKWg6cnoMIyxwyVj7vrQA.png" /><figcaption>Puppeteer response time on AWS Lambdas and GCP Functions</figcaption></figure><p>As we’ve become more familiar with Puppeteer, we’ve iterated our initial approach (read below). We also have some interesting ongoing experiments with rendering PDFs through a headless browser. It is also possible to use Puppeteer for automated end-to-end testing, even <a href="https://github.com/checkly/puppeteer-recorder">without writing any code</a>. It now supports <a href="https://github.com/GoogleChrome/puppeteer/tree/master/experimental/puppeteer-firefox">Firefox</a> in addition to Chrome.</p><h4>Hybrid rendering approach</h4><p>Using Puppeteer in runtime is quite challenging. That’s why we decided to use it in buildtime with the help of a tool which could return an actual user-generated content in runtime from the server-side. Something which is more stable and has a better throughput than Puppeteer.</p><p>We decided to try the Elixir programming language. Elixir looks like Ruby but runs on top of BEAM (Erlang VM), which was created to allow building fault-tolerant and stable systems.</p><p>Elixir uses the <a href="https://engineering.universe.com/introduction-to-concurrency-models-with-ruby-part-ii-c39c7e612bed">Actor concurrency model</a>. Each “Actor” (Elixir process) has a tiny memory footprint of about around 1–2KB. That allows for running many thousands of isolated processes concurrently. <a href="https://phoenixframework.org">Phoenix</a> is an Elixir web framework which enables high throughput and allows handling each HTTP request in a separate Elixir process.</p><p>We combined these approaches by taking the best from each world, which satisfies our needs:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1xL_bMidK0I-_llAkqDzow.png" /><figcaption>Puppeteer for pre-rendering and Phoenix for server-side rendering</figcaption></figure><ul><li><strong>Puppeteer</strong> pre-renders React pages the way we want during buildtime and saves them in HTML files (app shell from the <a href="https://developers.google.com/web/fundamentals/performance/prpl-pattern/">PRPL pattern</a>).</li></ul><p>We can keep building a simple browser React application and have a fast initial page load without waiting for JavaScript on end-users’ devices.</p><ul><li>Our <strong>Phoenix</strong> application serves these pre-rendered pages and dynamically injects the actual content to the HTML.</li></ul><p>That makes the content SEO friendly, allows processing a huge number of various pages on demand and scaling more easily.</p><ul><li>Clients receive and start showing the HTML immediately, then hydrate the <strong>React DOM</strong> state to continue as a regular SPA.</li></ul><p>That way, we can build highly interactive applications and have access to the JavaScript browser features.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DUo0OJU_R1MoisQWdx5DIg.png" /><figcaption>The architecture of pre-rendering with Puppeteer, server-side rendering with Phoenix, and hydration on the client-side with React</figcaption></figure><h3>Network</h3><h4>Content delivery network (CDN)</h4><p>Using a CDN enables content caching and allows to speed up its delivery across the world. We use <a href="https://www.fastly.com/">Fastly.com</a>, which serves over 10% of all internet requests and is used by companies such as GitHub, Stripe, Airbnb, Twitter, and many others.</p><p>Fastly allows us to write custom caching and routing logic by using the configuration language called <a href="https://varnish-cache.org/intro/index.html#intro">VCL</a>. Here is how a basic request flow works where each step can be customized depending on the route, request headers, and so on:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*n9ikPrah4AVBcFSkfHK9Ag.png" /><figcaption><a href="https://book.varnish-software.com/3.0/VCL_Basics.html">VCL request flow</a></figcaption></figure><p>Another option to improve performance is to use WebAssembly (WASM) at the edge with <a href="https://wasm.fastlylabs.com/">Fastly</a>. Think of it like using serverless but at the edge with such programming languages as C, Rust, Go, TypeScript, etc. Cloudflare has a similar project to support WASM on <a href="https://blog.cloudflare.com/webassembly-on-cloudflare-workers/">Workers</a>.</p><h4>Caching</h4><p>It is important to cache as many requests as possible to improve performance. Caching on a CDN level allows delivering responses faster for new users. Caching by sending a <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control">Cache-Control</a> header allows speeding up response time for the repeated requests in the browser.</p><p>Most of the build tools such as <a href="https://webpack.js.org/guides/caching/">Webpack</a> allow adding a hash to the filename. These files can be safely cached since changing the files will create a new output filename.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZHCCu0P_YJk4cjb9oIo6rg.png" /><figcaption>Cached and encoded files through HTTP/2</figcaption></figure><h4>GraphQL caching</h4><p>One of the most common ways of sending GraphQL requests is to use the POST HTTP method. One approach we use is to cache some GraphQL requests on Fastly level:</p><ul><li>Our React app annotates the GraphQL queries which can be cached.</li><li>Before sending an HTTP request, we append a URL argument by building a hash from a request body, which includes the GraphQL query and variables (we use custom fetch with <a href="https://www.apollographql.com/docs/link/links/http/">Apollo Client</a>).</li><li>Varnish (and Fastly) by default uses the full URL as part of the <a href="https://varnish-cache.org/docs/trunk/users-guide/vcl-hashing.html">cache key</a>.</li><li>That allows us to keep sending POST requests with GraphQL query in the request body and cache at the edge without hitting our servers.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HZUWoW17DrxddIfgPffbBA.png" /><figcaption>Sending POST GraphQL requests with a SHA256 URL argument</figcaption></figure><p>Here are some other potential GraphQL cache strategies to consider:</p><ul><li>Cache on the server-side: the whole GraphQL requests, on the resolver level or declaratively by annotating the schema.</li><li>Using persisted GraphQL queries and sending GET /graphql/:queryId to be able to rely on HTTP caching.</li><li>Integrate with CDNs by using automated tools (e.g. <a href="https://blog.apollographql.com/automatic-persisted-queries-and-cdn-caching-with-apollo-server-2-0-bf42b3a313de">Apollo Server 2.0</a>) or use GraphQL-specific CDNs (e.g. <a href="https://fastql.io">FastQL</a>).</li></ul><h4>Encoding</h4><p>All major browsers support gzip with the <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Encoding#Compressing_with_gzip">Content-Encoding</a> header to compress data. That allows sending fewer bytes to browsers, which usually means faster content delivery. It is also possible to use a more effective <a href="https://caniuse.com/#search=brotli">brotli</a> compression algorithm in supported browsers.</p><h4>HTTP/2 protocol</h4><p><a href="https://http2.github.io/faq/">HTTP/2</a> is a new version of the HTTP network protocol (h2 in DevConsole). Switching to HTTP/2 may improve performance, thanks to these differences compared to HTTP/1.x:</p><ul><li>HTTP/2 is binary, not textual. It is more efficient to parse, more compact.</li><li>HTTP/2 is multiplexed, which means that that HTTP/2 can send multiple requests in parallel over a single TCP connection. It allows us not to worry about browser <a href="https://www.browserscope.org/?category=network&amp;v=top">connections per host limits</a> and domain sharding.</li><li>It uses header compression to reduce request / response size overhead.</li><li>Allows servers to push responses proactively. This feature is particularly interesting.</li></ul><h4>HTTP/2 Server Push</h4><p>There are a lot of programming languages and libraries which don’t fully support all HTTP/2 features because they introduce breaking changes for existing tools and the ecosystem (e.g. <a href="https://github.com/tenderlove/the_metal/issues/5">rack</a>). But even in this case, it is still possible to use HTTP/2, at least partially. For example:</p><ul><li>Set up a proxy server such as <a href="https://h2o.examp1e.net">h2o</a> or <a href="https://www.nginx.com/">nginx</a> with HTTP/2 in front of a regular HTTP/1.x server. E.g. Puma and Ruby on Rails can send <a href="https://eileencodes.com/posts/http2-early-hints/">Early Hints</a>, which can enable HTTP/2 Server Push with some <a href="https://tools.ietf.org/html/draft-ietf-httpbis-early-hints-05">limitations</a>.</li><li>Use a CDN which support HTTP/2 to serve static assets. For instance, we use this approach to push fonts and some JavaScript files to the clients.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*CWybQ36g0z0jGr2eBKaIWg.png" /><figcaption>HTTP/2 Push fonts</figcaption></figure><p>Pushing critical JavaScript and CSS can also be very useful. Just don’t over-push and be aware of some <a href="https://jakearchibald.com/2017/h2-push-tougher-than-i-thought/">gotchas</a>.</p><h3>JavaScript in the browser</h3><h4>Bundle size budget</h4><blockquote>The #1 JavaScript performance rule is not to use JavaScript. ― me</blockquote><p>If you already have an existing JavaScript application, setting a budget can improve visibility of the bundle size and keep everybody on the same page. Exceeding the budget forces developers to think twice about the changes and to minimize the size increase. These are some examples of how to set a budget:</p><ul><li>Use numbers based on your needs or some recommended values. For instance, <a href="https://infrequently.org/2017/10/can-you-afford-it-real-world-web-performance-budgets/">&lt; 170KB</a> minified and compressed JavaScript.</li><li>Use the current bundle size as a baseline or try to reduce it by, for example, 10%.</li><li>Try to have the fastest website among your competitors and set the budget accordingly.</li></ul><p>You could use the <a href="https://github.com/siddharthkp/bundlesize">bundlesize</a> package or Webpack performance <a href="https://webpack.js.org/configuration/performance/">hints and limits</a> to keep track of the budget:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0ZB70_gEdwiRejXOY1GerA.png" /><figcaption>Webpack performance hints and limits</figcaption></figure><h4>Kill your dependencies</h4><p>That’s the title of the popular <a href="https://www.mikeperham.com/2016/02/09/kill-your-dependencies/">blog post </a>written by the author of Sidekiq.</p><blockquote>No code runs faster than no code. No code has fewer bugs than no code. No code uses less memory than no code. No code is easier to understand than no code.</blockquote><p>Unfortunately, the reality with JavaScript dependencies is that your project most probably uses many hundreds of dependencies. Just try ls node_modules | wc -l.</p><p>In some cases adding a dependency is necessary. In this case, the dependency bundle size should be one of the criteria when <a href="https://engineering.universe.com/building-a-google-map-in-react-b103b4ee97f1">choosing between multiple packages</a>. I highly recommend using <a href="https://bundlephobia.com">BundlePhobia</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Zih5TOhOO3cmvif0-cqTOQ.png" /><figcaption>BundlePhobia finds the cost of adding an npm package to your bundle</figcaption></figure><h4>Code-splitting</h4><p>Using code-splitting is perhaps the best way to significantly improve JavaScript performance. It allows splitting the code and shipping only the part which a user needs at the moment. Here are some examples of code-splitting:</p><ul><li>Routes are loaded separately in separate JavaScript chunks.</li><li>Components on a page which are not visible immediately. E.g. modals, footer which is below the fold.</li><li><a href="https://en.wikipedia.org/wiki/Polyfill_(programming)">Polyfills</a> and <a href="https://github.com/sindresorhus/ponyfill">ponyfills</a> to support the latest browser features in all major browsers.</li><li>Prevent code duplication by using Webpack’s <a href="https://webpack.js.org/guides/code-splitting/#prevent-duplication">SplitChunksPlugin</a>.</li><li>Locales files on demand to avoid shipping all our supported languages at once.</li></ul><p>You can use code-splitting with Webpack <a href="https://webpack.js.org/guides/code-splitting/#dynamic-imports">dynamic imports</a> and <a href="https://reactjs.org/docs/code-splitting.html">React.lazy with </a><a href="https://reactjs.org/docs/code-splitting.html">Suspense</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Zhc6IuCP83J0b_VsU2r2GQ.png" /><figcaption>Code-splitting with dynamic import and React.lazy with Suspense</figcaption></figure><p>We built a function instead of React.lazy to support named exports rather than <a href="https://humanwhocodes.com/blog/2019/01/stop-using-default-exports-javascript-module/">default exports</a>.</p><h4>Async and defer scripts</h4><p>All major browsers support async and defer attributes on script tags:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DWQMEjjJOtUckpzxfPnU5Q.png" /><figcaption>Different ways of loading JavaScript</figcaption></figure><ul><li>Inline scripts are useful for loading small critical JavaScript code.</li><li>Using a script with async is useful for fetching JavaScript without blocking HTML parsing when the script is not required for your users or any other scripts (e.g. analytics scripts).</li><li>Using scripts with defer is probably the best way from a performance point of view for fetching and executing non-critical JavaScript without blocking HTML parsing. Additionally, it guarantees the execution order as the scripts are called, which is useful if one script depends on another.</li></ul><p>Here is a visualized difference between the scripts in a head tag:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Cv943YbZsYv3ca1j9mugeg.png" /><figcaption>Different ways of script fetching and execution</figcaption></figure><h4>Image optimizations</h4><p>Although 100 KB of JavaScript has a very different performance cost compared to 100 KB of images, it is in general important to keep the images as light as possible.</p><p>One way of reducing the image size is to use a more lightweight <a href="https://caniuse.com/#search=webp">WebP</a> image format in supported browsers. For browsers which don’t support WebP, it is possible to use one of the following strategies:</p><ul><li>Fallback to regular JPEG or PNG formats (some CDNs do it automatically based on a browser’s Accept request header).</li><li>Loading and using <a href="https://webpjs.appspot.com/">WebP polyfill</a> after <a href="https://github.com/Modernizr/Modernizr/blob/7231dd32a3e30b4c2ec337bebcd822e3397ae9b6/feature-detects/img/webp.js">detecting</a> browser support.</li><li>Using Service Workers to listen to fetch requests and the changing actual URLs to use WebP if it is supported.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2-ctSFlE7tn3E3r2gkWD0w.png" /><figcaption>WebP images</figcaption></figure><p>Loading the images lazily only when they are in or near the viewport is one of the most significant performance improvements for initial page loads with lots of images. You can either use the <a href="https://developers.google.com/web/fundamentals/performance/lazy-loading-guidance/images-and-video/">IntersectionObserver</a> feature in supported browsers or use some alternative tools to achieve the same result, for example, <a href="https://github.com/twobin/react-lazyload">react-lazyload</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HnO4pIDXGh2Y_z4Ylw88TQ.gif" /><figcaption>Lazy loading images during the scroll</figcaption></figure><p>Some other image optimizations may include:</p><ul><li>Reducing the quality of images to reduce the size.</li><li>Resizing and loading the smallest possible images.</li><li>Using the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/img#attr-srcset">srcset</a> image attribute for automatically loading high-quality images for high-resolution retina displays.</li><li>Using progressive images to show a blurry image immediately.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*R2BsJEZDYxcyZ4lxff8S5g.png" /><figcaption>Loading regular vs progressive images</figcaption></figure><p>You can consider using some generic CDNs or specialized image CDNs which usually implement most of these image optimizations.</p><h4>Resource hints</h4><p><a href="https://www.w3.org/TR/resource-hints/">Resource hints</a> allow us to optimize the delivery of resources, reduce round trips, and fetch resources to deliver content faster while a user is browsing a page.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gBecKBYO3iWwZkoxZN1eaQ.png" /><figcaption>Resource hints with link tags</figcaption></figure><ul><li><a href="https://caniuse.com/#search=preload">Preload</a> downloads resources in the background for the current page load before they are actually used on the current page (high priority).</li><li><a href="https://caniuse.com/#search=prefetch">Prefetch</a> works similarly to preload to fetch the resources and cache them but for future user’s navigations (low priority).</li><li><a href="https://caniuse.com/#search=preconnect">Preconnect</a> allows to set up early connections before an HTTP request is actually sent to the server.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vWY-CYuzPei-KsAD3ajAzQ.png" /><figcaption>Preconnect in advance to avoid DNS, TCP, TLS roundtrip latencies</figcaption></figure><p>There are also some other resource hints such as <a href="https://caniuse.com/#search=prerender">prerender</a> or <a href="https://caniuse.com/#search=dns-prefetch">dns-prefetch</a>. Some of these resource hints can be specified in response headers. Just be careful when using resource hints. It is quite simple to start making too many unnecessary requests and downloading too much data, especially if users use a <a href="https://developer.mozilla.org/en-US/docs/Web/API/Network_Information_API">cellular connection</a>.</p><h3>Conclusion</h3><p>Performance in a growing application is a neverending process which usually requires constant changes across the whole stack.</p><blockquote>This video reminds me of you wanting to decrease the app bundle size. — My colleague</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TKFLqi1OlFIm32wdZAhGXg.gif" /><figcaption>Strip everything out of this plane you don’t need now! – <a href="https://en.wikipedia.org/wiki/Pearl_Harbor_(film)">Pearl Harbor</a> movie</figcaption></figure><p>Here is a list of other potential performance improvements we use or are planning to try which were not mentioned previously:</p><ul><li>Using Service Workers for caching, offline support, and offloading the main thread.</li><li>Inlining critical CSS or using functional CSS to decrease the size over the long-term.</li><li>Using font formats such as WOFF2 instead of WOFF (up to 50%+ compression).</li><li>Keeping the <a href="https://github.com/browserslist/browserslist">browserslist</a> up to date.</li><li>Using the <a href="https://github.com/webpack-contrib/webpack-bundle-analyzer">webpack-bundle-analyzer</a> to analyze build chunks visually.</li><li>Preferring smaller packages (e.g. <a href="https://github.com/date-fns/date-fns">date-fns</a>) and plugins which allow reducing the size (e.g. <a href="https://github.com/lodash/lodash-webpack-plugin">lodash-webpack-plugin</a>).</li><li>Trying <a href="https://github.com/preactjs/preact">preact</a>, <a href="https://github.com/Polymer/lit-html">lit-html</a> or <a href="https://github.com/sveltejs/svelte">svelte</a>.</li><li>Running <a href="https://github.com/GoogleChromeLabs/lighthousebot">Lighthouse in CI</a>.</li><li><a href="https://github.com/GoogleChromeLabs/progressive-rendering-frameworks-samples">Progressive hydration</a> and <a href="https://twitter.com/dan_abramov/status/1079352276433715200?lang=en">streaming with React</a>.</li></ul><p>There is an endless number of exciting ideas to try. I hope this information and some of these case studies will inspire you to think about performance in your application:</p><blockquote><a href="https://www.fastcompany.com/1825005/how-one-second-could-cost-amazon-16-billion-sales">Amazon has calculated</a> that a page load slowdown of just 1 second could cost it $1.6 billion in sales each year.</blockquote><blockquote><a href="https://wpostats.com/2015/11/04/walmart-revenue.html">Walmart saw</a> up to a 2% increase in conversions for every 1 second of improvement in load time. Every 100ms improvement also resulted in up to a 1% increase in revenue.</blockquote><blockquote><a href="https://www.fastcompany.com/1825005/how-one-second-could-cost-amazon-16-billion-sales">Google has calculated</a> that by slowing its search results by just 0.4 of a second, they could lose 8 million searches per day.</blockquote><blockquote><a href="https://medium.com/@Pinterest_Engineering/driving-user-growth-with-performance-improvements-cfc50dafadd7">Rebuilding Pinterest pages</a> for performance resulted in a 40% decrease in wait time, a 15% increase in SEO traffic, and a 15% increase in conversion rate to signup.</blockquote><blockquote><a href="https://www.creativebloq.com/features/how-the-bbc-builds-websites-that-scale">BBC has seen</a> that they lose an additional 10% of users for every additional second it takes for their site to load.</blockquote><blockquote>Tests of the new faster <a href="https://www.wsj.com/articles/financial-times-hopes-speedy-new-website-will-boost-subscribers-1475553602">FT.com showed</a> users were up to 30% more engaged — meaning more visits and more content being consumed.</blockquote><blockquote><a href="https://instagram-engineering.com/performance-usage-at-instagram-d2ba0347e442">Instagram increased</a> impressions and user profile scroll interactions by 33% for the median by decreasing the response size of the JSON needed for displaying comments.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/478/1*2hpMzRgc8T4BoUcevMdC1g.png" /></figure><blockquote>Universe.com increased the number of crawled pages 10 times by improving browser performance 10 times.</blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f9551927dcff" width="1" height="1" alt=""><hr><p><a href="https://engineering.universe.com/improving-browser-performance-10x-f9551927dcff">Improving Browser Performance 10x</a> was originally published in <a href="https://engineering.universe.com">Universe Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building a Google Map in React]]></title>
            <link>https://engineering.universe.com/building-a-google-map-in-react-b103b4ee97f1?source=rss----a1cd8e0d60b2---4</link>
            <guid isPermaLink="false">https://medium.com/p/b103b4ee97f1</guid>
            <category><![CDATA[google-map-api]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[react]]></category>
            <dc:creator><![CDATA[Kimberly Oleiro]]></dc:creator>
            <pubDate>Wed, 01 May 2019 15:30:24 GMT</pubDate>
            <atom:updated>2019-05-01T19:49:18.545Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*464eKImbR4lVQv991VHJ1g.jpeg" /><figcaption>Somewhat relevant stock photo</figcaption></figure><p>In this article I’m going to walk you through building a Google Map Component in React <em>without</em> a 3rd party library like <a href="https://www.npmjs.com/package/google-map-react">google-map-react</a> and <a href="https://www.npmjs.com/package/google-maps-react">google-maps-react</a>. Instead we’re going to use Google’s <a href="https://developers.google.com/maps/documentation/javascript/tutorial">Maps JavaScript API</a> directly in our component with some good old fashioned vanilla JavaScript. When evaluating whether to use a dependency or to write your own, it’s a good idea to ask yourself the following:</p><p>1. How many dependencies does this library use?<br> 2. What is the bundle size of this library (<a href="https://bundlephobia.com">Bundle Phobia</a>)?<br> 3. Can I implement the required minimal functionality myself?<br> 4. Do I need this library to be battle tested?</p><p>In our case, both libraries used minimal dependencies (0–3) and had a minified bundle size of 6–12KB. These numbers are by no means outrageous but given the low-risk nature of the task and the minimal functionality required, we decided it would be a good (and fun) choice to write our own.</p><p><strong><em>** Disclaimer</em></strong><em>: This tutorial assumes a working knowledge of JavaScript and React. We’ll follow the steps laid out in Google’s </em><a href="https://developers.google.com/maps/documentation/javascript/tutorial"><em>Maps JavaScript API Tutorial</em></a><em> and add our own React-y spin on things as we go</em>.</p><p><strong>Step One: Create a div element named “map” to hold the map.</strong></p><p>For the map to display on a web page, we must reserve a spot for it. Commonly, we do this by creating a named div element and obtaining a reference to this element in the browser’s document object model (DOM). We could obtain this reference using the <strong>React Ref API</strong> or the DOM method <strong>getElementById. </strong>The snippet below demonstrates both:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*596KUW9EtfGJic4nzsO5hA.png" /></figure><h4><strong>Step Two: Define a JavaScript function that creates a map in the div.</strong></h4><p>Since we’re doing this the React-y way, we’ll create an instance method on our <strong>GoogleMap</strong> class. To instantiate Google’s Map class we’ll pass in a reference to our map element (from step 1) alongside two mandatory options: <strong><em>centre</em></strong> and <strong><em>zoom</em></strong>. This particular map will be zoomed into the streets of Toronto and centred on the CN Tower landmark.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0v0bdLR2c2_nPvZPoYWIbg.png" /></figure><p>We’ve chosen to disable the <strong><em>defaultUI </em></strong>to keep the map as simple as possible. You can customize your own map by adding or removing controls, which is described in more detail in the<strong><em> </em></strong><a href="https://developers.google.com/maps/documentation/javascript/controls#DisablingDefaults">documentation</a>.</p><h4><strong>Step Three: Load the Maps JavaScript API using a script tag.</strong></h4><p>This can be done in one of two ways. The first is to add your script tag directly into your<strong> <em>index.html</em></strong> file and the second is to create a script tag on the fly inside our Google Map Component. For the purposes of encapsulating all responsibility within our React component, we’ll opt for the latter.</p><p>Using some vanilla JavaScript we’ll create a script element, set it’s src property to location of the JavaScript file that loads all the resources needed to use the Maps API and then append it to the document body.</p><h4><strong>Step Four: Create our Map</strong></h4><p>We’re almost there! The second last step in creating our map is to call our <strong>createGoogleMap</strong> function (from step 2). One caveat to mention when calling this function is that it is very possible to encounter a race condition where we try to instantiate a new Map class before our script has finished loading and receive an error where <strong><em>window.google </em></strong>is undefined. To prevent this from happening we will add an event listener to our script tag to ensure that all the required files have loaded before executing <strong>createGoogleMap.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5iAfV83Ckl6KueE9P1YCBQ.png" /></figure><h4>Step Five: <strong>Adding a Location Marker</strong></h4><p>You may have noticed in the snippet above that we stored the instance of our map object. The reason behind this is that we’ll need this same instance to instantiate Google’s Marker class. Similar to how we wrote a function to create a map (in Step 2) we will write a function to create a marker, this time passing in the mandatory options: <strong><em>position</em></strong> and <strong><em>map</em></strong>. This particular marker will be dropped on our map in the exact location of the CN Tower landmark.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WNCJopEvNRCk9uljlthBtw.png" /></figure><p>The final step here is to execute this function inside our event listener from step 4 and voilà ✨, you have a Google Map completely encapsulated in one React component without any 3rd party libraries 😎.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jBXXqmqItEV5jkoHmruRAA.png" /><figcaption>Your Google Map 👏🏻</figcaption></figure><p>Thanks for reading. Please reach out for any questions or feedback — We’d love to hear it.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/000a3c21260a4993a2641967342fe4c0/href">https://medium.com/media/000a3c21260a4993a2641967342fe4c0/href</a></iframe><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b103b4ee97f1" width="1" height="1" alt=""><hr><p><a href="https://engineering.universe.com/building-a-google-map-in-react-b103b4ee97f1">Building a Google Map in React</a> was originally published in <a href="https://engineering.universe.com">Universe Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Common patterns with Redux-Saga]]></title>
            <link>https://engineering.universe.com/common-patterns-with-redux-saga-ed68f89dfecf?source=rss----a1cd8e0d60b2---4</link>
            <guid isPermaLink="false">https://medium.com/p/ed68f89dfecf</guid>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[redux]]></category>
            <category><![CDATA[react]]></category>
            <category><![CDATA[redux-saga]]></category>
            <category><![CDATA[es6]]></category>
            <dc:creator><![CDATA[Alex Richardson]]></dc:creator>
            <pubDate>Thu, 26 Oct 2017 14:51:05 GMT</pubDate>
            <atom:updated>2017-10-26T14:49:50.308Z</atom:updated>
            <content:encoded><![CDATA[<p>In my first blog post on this topic “<a href="https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1"><em>What is Redux-Saga?</em></a><em>”, </em>I compared the basic differences between <a href="https://github.com/redux-saga/redux-saga">redux-saga</a> and <a href="https://github.com/gaearon/redux-thunk">redux-thunk</a>, two middleware libraries to help manage asynchronous code in our react-redux apps at Universe. If the saga pattern is unfamiliar to you, I recommend checking out the <a href="https://redux-saga.js.org/">redux-saga docs</a> along with <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function%2A">ES6 Generators</a>.</p><p>In this post I’m going to outline a few common patterns with redux-saga we use here at Universe on a day-to-day basis. These include:</p><ol><li>Running multiple requests in parallel.</li><li>Using the race keyword to timeout multiple requests gracefully.</li><li>Using sagas to manage a queue of events.</li></ol><h4>Running multiple requests in parallel</h4><p>This is a common pattern we rely on, since most of the data we need from our backend is across multiple endpoints (<a href="https://developers.universe.com/page/graphql-explorer">although we’ve launched a GraphQL beta</a> 😃) and we don’t care which request finishes first.</p><p>Note, this is <strong>different</strong> from the <a href="https://redux-saga.js.org/docs/advanced/RunningTasksInParallel.html">recipe in the docs</a>, which describes waiting for each request to complete before being able to access any of the responses (like Promise.all). The following pattern allows you to start many requests at once, but they are all individual tasks that can finish at any time.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/191e0f6c8abc893a9c1ddcb71a5d4c2f/href">https://medium.com/media/191e0f6c8abc893a9c1ddcb71a5d4c2f/href</a></iframe><p>This pattern can be a little bit hard to digest at first glance; lets break it down a bit:</p><ol><li>First we have a watcher (onPageInit) that will fire our saga (fetchDataForPage ) any time the ON_PAGE_MOUNT action type is seen.</li><li>In the fetchDataForPage saga, we first gather any dependencies we need to make our requests (userIds, etc)</li><li>Using redux-saga’s all and fork APIs, we start as many requests as we need in parallel. We pass our reusable requestAndPut side effect function, along with the actual request function we want to call eg: firstRequest, any parameters we need, and finally the action creator we want to call if the request succeeds.</li><li>If any of the requests fail, we can use the catch block for our error handling.</li><li>Else, the action creators will dispatch our success actions, leaving it up to our reducers to handle these and update our store accordingly.</li></ol><p>The magic here is in the requestAndPut helper function. The fork API expects a function to call, along with any parameters you wish to pass to the function. In this case, we always pass in requestAndPut as our function to call, along with a list of parameters, and an action creator. Because these are coupled together, the success actions fire right after our requests finish. They are all independent of one another.</p><h4>Using the race keyword to timeout multiple requests gracefully</h4><p>Continuing in the spirit of our first pattern, sometimes all of those nice parallel requests you just started, might have to get cancelled 😃. Luckily this is quite easy when using redux-saga!</p><p>The docs have a nice <a href="https://redux-saga.js.org/docs/advanced/RacingEffects.html">recipe for cancelling a single request</a> which I highly recommend checking out first. Here is that same recipe augmented slightly to handle multiple requests (using the pattern from above).</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8c0f2f1d6b52effdc9b1ccb5e877eaa4/href">https://medium.com/media/8c0f2f1d6b52effdc9b1ccb5e877eaa4/href</a></iframe><p>In this example, we are cancelling all of our requests if we receive an action type ( ON_PAGE_CHANGED in this case ) <em>before</em> all of the requests complete. This makes it simple to re-use this pattern elsewhere, as all that really changes is the action type we are listening for when we want to cancel our tasks. This could be as simple as a cancel button being clicked, a tab layout change, or even a set time limit (5 seconds for example).</p><h4>Using sagas to manage a queue (array) of events</h4><p>Here at Universe we show a wide range of notifications to our users depending on the state of the action they are trying to complete. Put simply, we show different success, warning, or error messages in the form of a flash message, like so:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/458/1*4D5-H7P-QN52m2Sp--i6Kw.gif" /><figcaption>Sample error &amp; success flash messages</figcaption></figure><p>At first glance this may seem simple (it is!), but we can leverage a lot of utility functions from redux-saga to help make this flow even simpler. This way we stay out of setInterval callbacks which can be tricky to debug at times.</p><p>This is the exact code that runs the flash messages in the GIF above.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bea6ebfc66d2f61a46a0ddcfb3bd2cd4/href">https://medium.com/media/bea6ebfc66d2f61a46a0ddcfb3bd2cd4/href</a></iframe><p>This is a saga (onAddFlashMessage) that will be constantly running while our app is in use. It first checks if we have any events in the queue we want to process. If we do, we loop through each event and process it, then we wait until we get a new event in the queue (ADD_FLASH_MESSAGE). Rinse and repeat.</p><p>It’s important to note that this pattern could be used for any queue of events, like network requests, disk operations, etc. All that really has to change is how we want to handle each event (showFlashMessage), and our selector functions we use to get data from our store ( getFlashQueue ).</p><p>Hopefully these three patterns can be of use to you or your team to help manage side effects in redux apps!</p><p>Also, I hope this unlocks different ways of augmenting the recipes in the docs to help solve those harder problems.</p><p><em>You can check out part 1 of the redux-saga series here: </em><a href="https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1">https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ed68f89dfecf" width="1" height="1" alt=""><hr><p><a href="https://engineering.universe.com/common-patterns-with-redux-saga-ed68f89dfecf">Common patterns with Redux-Saga</a> was originally published in <a href="https://engineering.universe.com">Universe Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introduction to Concurrency Models with Ruby. Part II]]></title>
            <link>https://engineering.universe.com/introduction-to-concurrency-models-with-ruby-part-ii-c39c7e612bed?source=rss----a1cd8e0d60b2---4</link>
            <guid isPermaLink="false">https://medium.com/p/c39c7e612bed</guid>
            <category><![CDATA[stm]]></category>
            <category><![CDATA[concurrency]]></category>
            <category><![CDATA[csp]]></category>
            <category><![CDATA[guild]]></category>
            <category><![CDATA[actors]]></category>
            <dc:creator><![CDATA[exAspArk]]></dc:creator>
            <pubDate>Wed, 06 Sep 2017 14:45:05 GMT</pubDate>
            <atom:updated>2017-09-11T12:43:18.331Z</atom:updated>
            <content:encoded><![CDATA[<p>In the second part of our series we will take a look at more advanced concurrency models such as Actors, Communicating Sequential Processes, Software Transactional Memory and of course Guilds — a new concurrency model which may be implemented in Ruby 3.</p><p>If you haven’t read our <a href="https://engineering.universe.com/introduction-to-concurrency-models-with-ruby-part-i-550d0dbb970">first post</a> in the series, I’d definitely recommend reading it first. There I described Processes, Threads, GIL, EventMachine and Fibers which I’ll be referring to in this post.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wTtV8K5ZTTA8B0jv8aZqnQ.jpeg" /></figure><h3>Actors</h3><p>Actors are concurrency primitives which can send messages to each other, create new Actors and determine how to respond to the next received message. They keep their own private state without sharing it, so they can affect each other only through messages. Since there is no shared state, there is no need for locks.</p><blockquote>Do not communicate by sharing memory; instead, share memory by communicating.</blockquote><p>Erlang and Scala both implement the Actor model in the language itself. In Ruby, <a href="https://github.com/celluloid/celluloid">Celluloid</a> is one of the most popular implementations. Under the hood, it runs each Actor in a separate thread and uses fibers for every method invocation to avoid blocking methods while waiting for responses from other Actors.</p><p>Here is a basic example of Actors with Celluloid:</p><pre># actors.rb<br>require &#39;celluloid&#39;<br>class <strong>Universe</strong><br>  include Celluloid<br>  def <strong>say</strong>(msg)<br>    puts msg<br>    Celluloid::Actor<strong>[:world].say</strong>(&quot;#{msg} World!&quot;)<br>  end<br>end<br>class <strong>World</strong><br>  include Celluloid<br>  def <strong>say</strong>(msg)<br>    puts msg<br>  end<br>end<br>Celluloid::Actor[:world] = World.new<br>Universe.new.say(&quot;Hello&quot;)</pre><pre>$ ruby actors.rb<br>Hello<br>Hello World!</pre><h4>Pros:</h4><ul><li>No manual multithreaded programming and no shared memory mean almost deadlock-free synchronization without explicit locks.</li><li>Similarly to Erlang, Celluloid makes Actors fault-tolerant, meaning that it’ll try to reboot crashed Actors with <a href="https://github.com/celluloid/celluloid/wiki/Supervisors">Supervisors</a>.</li><li>The Actor model is designed to address the problems of distributed programs, so it is great for scaling across multiple machines.</li></ul><h4>Cons:</h4><ul><li>Actors may not work if a system needs to use shared state or you need to guarantee a behavior that needs to occur in a specific order.</li><li>Debugging can be tricky — imagine following system flow through multiple Actors, or what if some of the Actors mutate the message? Remember that Ruby is not an immutable language, right?</li><li>Celluloid allows to build complex concurrent systems much quicker compared to dealing with threads manually. But it does it with a <a href="http://www.mikeperham.com/2015/10/14/optimizing-sidekiq/">runtime cost</a> (e.g. 5x slower and 8x more memory).</li><li>Unfortunately, Ruby implementations are not that great at using distributed Actors across multiple servers. For example, <a href="https://github.com/celluloid/dcell">DCell</a>, which uses 0MQ, is still not production ready.</li></ul><h4>Examples:</h4><ul><li><a href="https://github.com/celluloid/reel/">Reel</a> — event-based web server, which works with Celluloid-based apps. Uses one Actor per connection. Can be used for streaming or WebSockets.</li><li><a href="https://github.com/celluloid/celluloid-io/">Celluloid::IO</a> — brings Actors and evented I/O loops together. Unlike EventMachine, it allows to use as many event loops per process as you want by creating multiple Actors.</li></ul><h3>Communicating Sequential Processes</h3><p>Communicating Sequential Processes (CSP) is a paradigm which is very similar to the Actor model. It’s also based on message-passing without sharing memory. However, CSP and Actors have these 2 key differences:</p><ul><li>Processes in CSP are anonymous, while actors have identities. So, CSP uses explicit channels for message passing, whereas with Actors you send messages directly.</li><li>With CSP the sender cannot transmit a message until the receiver is ready to accept it. Actors can send messages asynchronously (e.g. with <a href="https://github.com/celluloid/celluloid/wiki/Basic-usage">async calls</a> in Celluloid).</li></ul><p>CSP is implemented in such programming languages as Go with <a href="https://blog.golang.org/share-memory-by-communicating">goroutines and channels</a>, Clojure with the <a href="http://clojure.com/blog/2013/06/28/clojure-core-async-channels.html">core.async</a> library and Crystal with <a href="https://crystal-lang.org/docs/guides/concurrency.html">fibers and channels</a>. For Ruby, there are a few gems which implement CSP. One of them is the Channel class implemented in <a href="https://github.com/ruby-concurrency/concurrent-ruby/blob/df482db36caf1b0c1d69a8ff97a2407469e1e315/doc/channel.md">concurrent-ruby</a> library:</p><pre># csp.rb<br>require &#39;concurrent-edge&#39;<br>array = [1, 2, 3, 4, 5]<br>channel = Concurrent::Channel.new<br>Concurrent::Channel.go do<br>  puts &quot;Go 1 thread: #{Thread.current.object_id}&quot;<br>  <strong>channel.put(array[0..2].sum)</strong> # Enumerable#sum from Ruby 2.4<br>end<br>Concurrent::Channel.go do<br>  puts &quot;Go 2 thread: #{Thread.current.object_id}&quot;<br>  <strong>channel.put(array[2..4].sum)</strong><br>end<br>puts &quot;Main thread: #{Thread.current.object_id}&quot;<br>puts <strong>channel.take + channel.take</strong></pre><pre>$ ruby csp.rb<br>Main thread: 70168382536020<br>Go 2 thread: 70168386894280<br>Go 1 thread: 70168386894880<br>18</pre><p>So, we basically ran 2 operations (sum) in 2 different threads, synchronized the results and calculated the total value in the main thread. Everything is done through the Channel without any explicit locks.</p><p>Under the hood, each Channel.go runs in a separate thread from the thread pool, which increases its size automatically if there is no free thread left. In this case, using this model is useful during blocking I/O operations, which release GIL (see <a href="https://engineering.universe.com/introduction-to-concurrency-models-with-ruby-part-i-550d0dbb970">the previous post</a> to find more information). On the other hand, core.async in Clojure, for example, uses a limited number of threads and tries to “park” them, but this approach may be a problem during I/O operations which may <a href="https://martintrojer.github.io/clojure/2013/07/07/coreasync-and-blocking-io">block out any other work</a>.</p><h4>Pros:</h4><ul><li>CSP Channels can only hold maximum one message, which makes it much easier to reason about. While with the Actor model it’s more like having a potentially infinite mailbox with messages.</li><li>Communicating Sequential Processes allow you to avoid coupling between the producer and the consumer by using channels; they don’t have to know about each other.</li><li>In CSP messages are delivered in the order they were sent.</li></ul><blockquote>Clojure may eventually support the actor model for distributed programming, paying the price only when distribution is required, but I think it is quite cumbersome for same-process programming. <a href="https://clojure.org/about/state#actors">Rich Hickey</a></blockquote><h4>Cons:</h4><ul><li>CSP is generally used on a single machine, it’s not that great as the Actor model for distributed programming.</li><li>In Ruby, most of the implementations don’t use M:N threading model, so each “goroutine” actually uses a Ruby thread, which is equal to an OS thread. This means that Ruby “goroutines” are not so lightweight.</li><li>Using CSP with Ruby in not so popular. So, there are no actively developed, stable and battle-tested tools.</li></ul><h4>Examples:</h4><ul><li><a href="https://github.com/igrigorik/agent">Agent</a> — another Ruby implementation of CSP. This gem runs each go-block in a separate Ruby thread as well.</li></ul><h3>Software Transactional Memory</h3><p>While Actors and CSP are concurrency models which are based on message passing, Software Transactional Memory (STM) is a model which uses shared memory. It’s an alternative to lock-based synchronization. Similarly to DB transactions, these are the main concepts:</p><ol><li>Values within a transaction can be changed, but these changes are not visible to others until the transaction is <strong>committed</strong>.</li><li>Errors that happened in transactions <strong>abort</strong> them and rollback all the changes.</li><li>If a transaction can’t be committed due to conflicting changes, it <strong>retries</strong> until it succeeds.</li></ol><p>The <a href="https://github.com/ruby-concurrency/concurrent-ruby">concurrent-ruby</a> gem implements <a href="https://ruby-concurrency.github.io/concurrent-ruby/Concurrent/TVar.html">TVar</a>, which is based on Clojure’s <a href="https://clojure.org/reference/refs">Refs</a>. Here is an example, which implements money transferring from one bank account to another:</p><pre># stm.rb<br>require &#39;concurrent&#39;<br>account1 = <strong>Concurrent::TVar.new(100)</strong><br>account2 = <strong>Concurrent::TVar.new(100)</strong><br><strong>Concurrent::atomically</strong> do<br>  account1.value -= 10<br>  account2.value += 10<br>end<br>puts &quot;Account1: #{account1.value}, Account2: #{account2.value}&quot;</pre><pre>$ ruby stm.rb<br>Account1: <strong>90</strong>, Account2: <strong>110</strong></pre><p>TVar is an object which contains a single value. Together with atomically they implement data mutation in transactions.</p><h4>Pros:</h4><ul><li>Using STM is much simpler compared to lock-based programming. It allows to avoid deadlocks, simplifies reasoning about concurrent systems since you don’t have to think about race conditions.</li><li>Much easier to adapt as you don’t need to restructure your code as it’s required with Actors (use models) or CSP (use channels).</li></ul><h4>Cons:</h4><ul><li>Since STM relies on transaction rollbacks, you should be able to undo the operation in the transaction at any point in time. In practice, it’s difficult to guarantee if you make I/O operations (e.g. POST HTTP requests).</li><li>STM doesn’t scale well with Ruby MRI. Since there is GIL, you can’t utilize more than a single CPU. At the same time, you also can’t use the advantage of running concurrent I/O operations in threads because it’s hard to undo such operations.</li></ul><h4>Examples:</h4><ul><li>TVar from <a href="https://github.com/ruby-concurrency/concurrent-ruby">concurrent-ruby</a> — implements STM and also contains some <a href="https://ruby-concurrency.github.io/concurrent-ruby/Concurrent/TVar.html">benchmarks</a> which compare lock-based implementations and STM in MRI, JRuby and Rubinius.</li></ul><h3>Guilds</h3><p>Guild is a new concurrency model proposed for Ruby 3 by Koichi Sasada — a Ruby core developer who designed the current Ruby VM (virtual machine), fibers and GC (garbage collector). These are the main points which lead to creating Guilds:</p><ul><li>The new model should be compatible with Ruby 2 and allow better concurrency.</li><li>Forcing immutable data structures similar to Elixir may be unacceptably slow since Ruby utilizes many “write” operations. So, it’s better to copy shared mutable objects similarly to Racket (<a href="https://docs.racket-lang.org/reference/places.html">Place</a>) but the copying must be fast for this to be successful.</li><li>If it’s necessary to share mutable objects, there should be special data structures similar to Clojure (e.g. STM).</li></ul><p>These ideas resulted in the following main concepts of Guilds:</p><ul><li>A Guild is a concurrency primitive which may contain multiple threads, which may contain multiple fibers.</li><li>Only a Guild-owner can access its mutable objects, so there is no need to use locks.</li><li>Guilds can share data by copying objects or by transferring membership (“moving” objects) from one Guild to another.</li><li>Immutable objects can be accessed from any Guild by using a reference without copying. E.g. numbers, symbols, true, false, deeply frozen objects.</li></ul><p>So, our example of money transferring from one bank account to another with Guilds may look like:</p><pre>bank = Guild.new do<br>  accounts = ...<br>  while acc1, acc2, amount, channel = Guild.default_channel.<strong>receive</strong><br>    <strong>accounts[acc1].balance += amount</strong><br>    <strong>accounts[acc2].balance -= amount</strong><br>    channel.transfer(:finished)<br>  end<br>end</pre><pre>channel = Guild::Channel.new<br>bank.<strong>transfer</strong>([acc1, acc2, 10, channel])<br>puts channel.receive<br># =&gt; :finished</pre><p>All data about account balances are stored in a single Guild (bank), so, only this Guild is responsible for data mutations which can be requested through channels.</p><h4>Pros:</h4><ul><li>No mutable shared data between Guilds means there is no need for locking mechanisms, so there are no deadlocks. Communication between Guilds is designed to be safe.</li><li>Guilds encourage using immutable data structures since they are the fastest and the easiest way to share data across multiple Guilds. Start freezing as much data as possible now, for example, by adding # frozen_string_literal: true at the beginning of your files.</li><li>Guilds are fully compatible with Ruby 2, meaning that your current code will simply run within a single Guild. You are not required to use immutable data structures or make any changes in your code.</li><li>At the same time, Guilds enable better concurrency with MRI. It’ll finally allow us to use multiple CPUs within a single Ruby process.</li></ul><h4>Cons:</h4><ul><li>It’s too early to make predictions about performance, but communicating and sharing mutable objects between Guilds will probably have a bigger overhead compared to threads.</li><li>Guilds are more complex concurrency primitives because they allow using multiple concurrency models at once. For example: CSP for inter-Guild communication through channels, STM with special data structures for sharing mutable data for better performance, multi-threaded programming within a single Guild, etc.</li><li>Even though running multiple Guilds within a single process will be cheaper compared to running multiple processes from a resource usage point of view, Guilds are not so lightweight. They will be heavier than Ruby threads, meaning that you won’t be able to handle, let’s say, tens of thousands of WebSocket connections with just Guilds.</li></ul><h4>Examples:</h4><p>There are no examples since Ruby 3 wasn’t released yet. But I see a bright future where developers will start building Guild-friendly tools like web servers, background job processings, etc. Most probably all these tools will allow using hybrid approaches: running multiple processes with multiple Guilds with multiple threads in each. But for now, you can read the original <a href="http://www.atdot.net/~ko1/activities/2016_rubykaigi.pdf">PDF presentation</a> by Koichi Sasada.</p><h3>Conclusion</h3><p>There is no silver bullet. Each concurrency model described in the post has its own pros and cons. The CSP model works best on a single machine without deadlocks. The Actor model can easily scale across several machines. STM allows to write concurrent code much simpler. But all these models are not first class citizens in Ruby and can’t be fully adapted from other programming languages; mostly because in Ruby they are all implemented with standard concurrency primitives like threads and fibers. However, there is a chance that Guilds will be released with Ruby 3, which is a big step towards a much better concurrency model!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/350/1*hqm1ijYWLsw8fCV2KmGvAg.png" /><figcaption><a href="https://careers.universe.com/">We’re hiring!</a></figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c39c7e612bed" width="1" height="1" alt=""><hr><p><a href="https://engineering.universe.com/introduction-to-concurrency-models-with-ruby-part-ii-c39c7e612bed">Introduction to Concurrency Models with Ruby. Part II</a> was originally published in <a href="https://engineering.universe.com">Universe Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introduction to Concurrency Models with Ruby. Part I]]></title>
            <link>https://engineering.universe.com/introduction-to-concurrency-models-with-ruby-part-i-550d0dbb970?source=rss----a1cd8e0d60b2---4</link>
            <guid isPermaLink="false">https://medium.com/p/550d0dbb970</guid>
            <category><![CDATA[concurrency]]></category>
            <category><![CDATA[gil]]></category>
            <category><![CDATA[threads]]></category>
            <category><![CDATA[eventmachine]]></category>
            <category><![CDATA[fiber]]></category>
            <dc:creator><![CDATA[exAspArk]]></dc:creator>
            <pubDate>Wed, 23 Aug 2017 15:19:00 GMT</pubDate>
            <atom:updated>2017-09-06T18:38:31.265Z</atom:updated>
            <content:encoded><![CDATA[<p>In this first post, I would like to describe the differences between Processes, Threads, what the GIL is, EventMachine and Fibers in Ruby. When to use which of the models, which open-source projects use them, what the pros and cons are.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cKZ7MlP2o3M-IeB1SVcFoA.jpeg" /><figcaption>What is concurrency?</figcaption></figure><h3>Processes</h3><p>Running multiple processes is not actually about concurrency, it’s about parallelism. Although parallelism and concurrency are often confused, they are different things. I like this simple analogy:</p><ul><li><strong>Concurrency</strong>: having a person juggle many balls with only 1 hand. Regardless of how it seems, the person is only catching / throwing one ball at a time.</li><li><strong>Parallelism</strong>: is having multiple people juggle their own set of balls simultaneously.</li></ul><h4>Sequential execution</h4><p>Imagine, we have a range of numbers, which we need to convert to an array and find an index for the specific element:</p><pre># sequential.rb<br>range = 0...10_000_000<br>number = 8_888_888<br>puts range.to_a.index(number)</pre><pre>$ time ruby sequential.rb                                                   <br>8888888<br>ruby test.rb  0.41s user 0.06s system <strong>95% cpu</strong> <strong>0.502 total</strong></pre><p>Executing this code takes approximately 500ms and utilizes 1 CPU.</p><h4>Parallel execution</h4><p>We can rewrite the code above by using multiple parallel processes and splitting the range. With the fork method from the standard Ruby library we can create a child process and execute the code in the block. In the parent process we can wait until all child processes are finished with Process.wait:</p><pre># parallel.rb<br>range1 = 0...5_000_000<br>range2 = 5_000_000...10_000_000<br>number = 8_888_888<br>puts &quot;Parent #{Process.pid}&quot;<br><strong>fork</strong> { puts &quot;Child1 #{Process.pid}: #{range1.to_a.index(number)}&quot; }<br><strong>fork</strong> { puts &quot;Child2 #{Process.pid}: #{range2.to_a.index(number)}&quot; }<br><strong>Process.wait</strong></pre><pre>$ time ruby parallel.rb<br>Parent 32771<br>Child2 32867: 3888888<br>Child1 32865:<br>ruby parallel.rb  0.40s user 0.07s system <strong>153% cpu 0.309 total</strong></pre><p>Because each process works in parallel with just a half of the range, the code above works a bit faster and consumes more than 1 CPU. The process tree during the execution may look like:</p><pre># \ - 32771 ruby parallel.rb (parent process)<br>#  | - 32865 ruby parallel.rb (child process)<br>#  | - 32867 ruby parallel.rb (child process)</pre><h4>Pros:</h4><ul><li>Processes don’t share memory, so you can’t mutate data from one process in another. It makes it much easier to code and debug.</li><li>Processes in the <a href="https://en.wikipedia.org/wiki/Ruby_MRI">Ruby MRI</a> are the only way to utilize more than a single-core since there is a GIL (global interpreter lock, find more information below in the post). It may be useful if you’re doing, let’s say, some math calculation.</li><li>Forking child processes may help avoid unwanted memory leaks. Once the process finishes, it releases all the resources.</li></ul><h4>Cons:</h4><ul><li>Since processes don’t share memory, they use a lot of memory—meaning that running hundreds of processes may be a problem. Note that since Ruby 2.0 fork uses OS <a href="https://en.wikipedia.org/wiki/Copy-on-write">Copy-On-Write</a>, which allows processes to share memory as long as it doesn’t have different values.</li><li>Processes are slow to create and destroy.</li><li>Processes may require inter-process communication. For example, <a href="https://ruby-doc.org/stdlib-2.4.1/libdoc/drb/rdoc/DRb.html">DRb</a>.</li><li>Beware of <a href="https://en.wikipedia.org/wiki/Orphan_process">orphan</a> processes (a child process whose parent has finished or terminated) or <a href="https://en.wikipedia.org/wiki/Zombie_process">zombie</a> processes (a child process which completed execution but still occupies space in the process table).</li></ul><h4>Examples:</h4><ul><li><a href="https://bogomips.org/unicorn/">Unicorn</a> server — it loads the application, forks the master process to spawn multiple workers which accept HTTP requests.</li><li><a href="https://github.com/resque/resque">Resque</a> for background processing — it runs a worker, which executes each job sequentially in a forked child process.</li></ul><h3>Threads</h3><p>Even though Ruby uses native OS threads since version 1.9, only one thread can be executing at any given time within a single process, even if you have multiple CPUs. This is due to the fact that MRI has GIL, which also exists in other programming languages such as Python.</p><h4>Why does the GIL exist?</h4><p>There are a few reasons, for example:</p><ul><li>Avoids race conditions within C extensions, no need to worry about thread-safety.</li><li>Easier to implement, no need to make Ruby data structures thread-safe.</li></ul><p>Back in 2014, Matz started thinking about <a href="https://twitter.com/yukihiro_matz/status/495219763883163648">gradually removing GIL</a>. Because GIL doesn’t actually guarantee that our Ruby code is thread-safe and doesn’t allow us to use better concurrency.</p><h4>Race-conditions</h4><p>Here is a basic example with a race-condition:</p><pre># threads.rb<br>@executed = false<br>def ensure_executed<br>  <strong>unless @executed</strong><br>    puts &quot;executing!&quot;<br>    <strong>@executed = true</strong><br>  end<br>end<br>threads = 10.times.map { Thread.new { ensure_executed } }<br><strong>threads.each(&amp;:join)</strong></pre><pre>$ ruby threads.rb<br>executing!<br>executing!</pre><p>We create 10 threads which execute our method and call join for each of them, so the main thread will be waiting until all other threads are finished. The code printed executing! twice because our threads share the same @executed variable. Our read (unless @executed) and set (@executed = true) operations are not atomic, meaning that once we read the value it could be changed in other threads before we set a new value.</p><h4>GIL and Blocking I/O</h4><p>But having GIL, which doesn’t allow to execute multiple threads at once, doesn’t mean that threads can’t be useful. Thread releases GIL when it hits blocking I/O operations such as HTTP requests, DB queries, writing / reading from disk and even sleep:</p><pre># sleep.rb<br>threads = 10.times.map do |i|<br>  Thread.new { <strong>sleep 1</strong> }<br>end<br>threads.each(&amp;:join)</pre><pre>$ time ruby sleep.rb                                                    <br>ruby sleep.rb  0.08s user 0.03s system 9% cpu <strong>1.130 total</strong></pre><p>As you can see, all 10 threads slept for 1 second and finished almost at the same time. When one thread hit sleep, it passed the execution to another thread without blocking GIL.</p><h4>Pros:</h4><ul><li>Uses less memory than processes; it’s possible to run thousands of threads. They are also fast to create and destroy.</li><li>Threads are useful when there are slow blocking I/O operations.</li><li>Can access the memory area from other threads if necessary.</li></ul><h4>Cons:</h4><ul><li>Requires very careful synchronization to avoid race-conditions, usually by using locking primitives, which sometimes may lead to deadlocks. All this makes it quite difficult to write, test and debug thread-safe code.</li><li>With threads you have to make sure that not only your code is thread-safe, but that any dependencies you’re using are also thread-safe.</li><li>The more threads you spawn, the more time and resources they’ll be spending by switching the context and spending less time doing the actual job.</li></ul><h4>Examples:</h4><ul><li><a href="https://github.com/puma/puma">Puma</a> server — allows to use multiple threads in each process (clustered mode). Similarly to Unicorn it preloads the app and forks the master process, where each child process has its own thread pool. Threads work fine in most cases because each HTTP request can be handled in a separate thread and we don’t share a lot of resources between the requests.</li><li><a href="https://github.com/mperham/sidekiq">Sidekiq</a> for background processing — runs a single process with 25 threads by default. Each thread processes one job at a time.</li></ul><h3>EventMachine</h3><p><a href="https://github.com/eventmachine/eventmachine">EventMachine</a> (aka EM) is a gem which is written in C++ and Ruby. It provides event-driven I/O using the <a href="https://en.wikipedia.org/wiki/Reactor_pattern">Reactor pattern</a> and can basically make your Ruby code looks like Node.js :) Under the hood EM uses Linux <a href="http://man7.org/linux/man-pages/man2/select.2.html">select()</a> during its run through the event loop to check for new inputs on file descriptors.</p><p>One common reason to use EventMachine is the case when you have a lot of I/O operations and you don’t want to deal with threads manually. Manually handling threads can be difficult or often too expensive from a resource usage point of view. With EM you can handle multiple HTTP requests with a single thread by default.</p><pre># em.rb<br>EM.run do<br>  EM.add_timer(1) do<br>    puts &#39;sleeping...&#39;<br>    EM.system(&#39;<strong>sleep 1</strong>&#39;) { puts <strong>&quot;woke up!&quot;</strong> }<br>    puts &#39;continuing...&#39;<br>  end<br>  EM.add_timer(3) { EM.stop }<br>end</pre><pre>$ ruby em.rb<br>sleeping...<br>continuing...<br>woke up!</pre><p>The example above shows how to run asynchronous code by executing EM.system (I/O operation) and passing a block as a callback, which will be executed once the system command has finished.</p><h4>Pros:</h4><ul><li>It’s possible to achieve great performance for slow networked apps such as web servers and proxies with a single thread.</li><li>It allows you to avoid complex multithreaded programming, the disadvantages of which were described above.</li></ul><h4>Cons:</h4><ul><li>Every I/O operation should support EM asynchrony. This means that you should use specific versions of system, DB adapter, HTTP client, etc. which can result in monkey-patched versions, lack of support and limited options.</li><li>Work done within the main thread per event-loop tick should be small. Also, it’s possible to use <a href="http://www.rubydoc.info/github/eventmachine/eventmachine/EventMachine.defer">Defer</a>, which executes the code in separate threads from the thread pool, however, it may lead to the multithreaded problems discussed earlier.</li><li>Hard to program complex systems because of the error handling and callbacks. Callback Hell is also possible in Ruby, but it can be prevented with Fibers, see below.</li><li>EventMachine itself is a huge dependency: 17K LOC (lines of code) in Ruby and 10K LOC in C++.</li></ul><h4>Examples:</h4><ul><li><a href="https://github.com/postrank-labs/goliath/">Goliath</a> — single threaded asynchronous server.</li><li><a href="https://github.com/ruby-amqp/amqp">AMQP</a> — RabbitMQ client. However, creators of the gem suggest using the non-EM-based version <a href="http://rubybunny.info/">Bunny</a>. Note that migrating tools to EM-less implementations is a general trend. For example, creators of <a href="https://github.com/rails/rails/tree/master/actioncable">ActionCable</a> decided to use low-level <a href="https://github.com/socketry/nio4r">nio4r</a>, creator of <a href="https://github.com/kyledrake/sinatra-synchrony">sinatra-synchrony</a> rewrote it with <a href="https://github.com/celluloid/celluloid">Celluloid</a>, etc.</li></ul><h3>Fibers</h3><p><a href="https://ruby-doc.org/core-2.4.1/Fiber.html">Fibers</a> are light weight primitives in the Ruby standard library which can be paused, resumed and scheduled manually. They are pretty much the same as ES6 Generators if you’re familiar with JavaScript (we also wrote a post about <a href="https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1">Generators and Redux-Saga</a>). It’s possible to run tens of thousands of Fibers within a single thread.</p><p>Often, Fibers are used with EventMachine to avoid callbacks and make code look synchronous. So, the following code:</p><pre>EventMachine.run do<br>  page = EM::HttpRequest.new(&#39;https://google.ca/&#39;).get       </pre><pre>  page.<strong>errback</strong> { puts &quot;Google is down&quot; }<br>  page.<strong>callback</strong> {<br>    url = &#39;https://google.ca/search?q=universe.com&#39;<br>    about = EM::HttpRequest.new(url).get</pre><pre>    about.<strong>errback</strong>  { ... }<br>    about.<strong>callback</strong> { ... }     <br>  }<br>end</pre><p>Can be rewritten like:</p><pre>EventMachine.run do<br>  <strong>Fiber.new</strong> {<br>    page = http_get(&#39;http://www.google.com/&#39;)     <br>    if page.response_header.status == 200<br>      about = http_get(&#39;https://google.ca/search?q=universe.com&#39;) <br>      # ... <br>    else <br>      puts &quot;Google is down&quot;<br>    end  <br>  }.<strong>resume</strong> <br>end</pre><pre>def http_get(url)<br>  current_fiber = <strong>Fiber.current</strong><br>  http = EM::HttpRequest.new(url).get    <br>  http.callback { current_fiber.<strong>resume</strong>(http) }   <br>  http.errback  { current_fiber.<strong>resume</strong>(http) }    <br><strong>  Fiber.yield</strong><br>end</pre><p>So, basically, Fiber#yield returns control back to the context that resumed the Fiber and returns the value which was passed to Fiber#resume.</p><h4>Pros:</h4><ul><li>Fibers allow you to simplify asynchronous code by replacing nested callbacks.</li></ul><h4>Cons:</h4><ul><li>Don’t really solve concurrency problems.</li><li>They are rarely used directly in application-level code.</li></ul><h4>Examples:</h4><ul><li><a href="https://github.com/igrigorik/em-synchrony">em-synchrony</a> — a library, written by Ilya Grigorik, a performance engineer at Google, which integrates EventMachine with Fibers for different clients such as MySQL2, Mongo, Memcached, etc.</li></ul><h3>Conclusion</h3><p>There is no silver bullet, so choose a concurrency model depending on your needs. For example, need to run CPU and memory intensive code and have enough resources — use processes. Have to execute multiple I/O operations such as HTTP requests — use threads. Need to scale up to the maximum throughput — use EventMachine.</p><p>In the <a href="https://engineering.universe.com/introduction-to-concurrency-models-with-ruby-part-ii-c39c7e612bed">second part</a> of this series we will take a look at such concurrency models as Actors (Erlang, Scala), Communicating Sequential Processes (Go, Crystal), Software Transactional Memory (Clojure) and of course Guilds — a new concurrency model which may be implemented in Ruby 3. Stay tuned!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/350/1*hqm1ijYWLsw8fCV2KmGvAg.png" /><figcaption><a href="https://careers.universe.com/">We’re hiring!</a></figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=550d0dbb970" width="1" height="1" alt=""><hr><p><a href="https://engineering.universe.com/introduction-to-concurrency-models-with-ruby-part-i-550d0dbb970">Introduction to Concurrency Models with Ruby. Part I</a> was originally published in <a href="https://engineering.universe.com">Universe Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Batching  – A powerful way to solve N+1 queries every Rubyist should know]]></title>
            <link>https://engineering.universe.com/batching-a-powerful-way-to-solve-n-1-queries-every-rubyist-should-know-24e20c6e7b94?source=rss----a1cd8e0d60b2---4</link>
            <guid isPermaLink="false">https://medium.com/p/24e20c6e7b94</guid>
            <category><![CDATA[ruby]]></category>
            <category><![CDATA[restful-api]]></category>
            <category><![CDATA[n-plus-1-queries]]></category>
            <category><![CDATA[batching]]></category>
            <category><![CDATA[graphql]]></category>
            <dc:creator><![CDATA[exAspArk]]></dc:creator>
            <pubDate>Wed, 16 Aug 2017 15:00:15 GMT</pubDate>
            <atom:updated>2019-07-12T13:57:37.792Z</atom:updated>
            <content:encoded><![CDATA[<h3>Batching – A powerful way to solve N+1 queries every Rubyist should know</h3><p>In this post, I’m going to tell you about batching as a technique to help avoid N+1 queries, existing battle-tested tools like Haskell Haxl and JavaScript DataLoader, and how similar approaches can be used in any Ruby program.</p><p><em>Here is a translated </em><a href="https://techracho.bpsinc.jp/hachi8833/2017_09_07/45077"><em>Japanese version</em></a><em> of the blog post.</em></p><h3>What are N+1 queries?</h3><p>First, let’s find out what N+1 queries are and why they are called that. Imagine, we have 2 SQL tables: users and posts. If we run the following code by using ActiveRecord models:</p><pre>posts = Post.where(id: [1, 2, 3])<br># SELECT * FROM posts WHERE id IN (1, 2, 3)</pre><pre>users = posts.map { |post| post.user }<br># SELECT * FROM users WHERE id = 1<br># SELECT * FROM users WHERE id = 2<br># SELECT * FROM users WHERE id = 3</pre><p>This makes 1 query to select all posts, then N queries to select users for each post. So, the code produces 1+N queries. Because changing the order of the addends does not change the sum, it’s common to call it N+1 queries.</p><h3>How do people usually solve N+1 queries?</h3><p>Generally, there are 2 popular ways to solve the problem of N+1 queries in the Ruby world:</p><ul><li><strong>Eager loading data in models</strong></li></ul><pre>posts = Post.where(id: [1, 2, 3]).<strong>includes(:user)</strong><br># SELECT * FROM posts WHERE id IN (1, 2, 3)<br># SELECT * FROM users WHERE id IN (1, 2, 3)</pre><pre>users = posts.map { |post| post.user }</pre><p>It preloads the specified associations under the hood, which makes it easy to use. But ORM can’t always help. For example, in a case when we need to load data from different sources such as another database.</p><ul><li><strong>Preloading data and passing it through arguments</strong></li></ul><pre>class Post &lt; ApplicationRecord<br>  def rating(like_count, angry_count)<br>    like_count * 2 - angry_count<br>  end<br>end</pre><pre>posts = Post.where(id: [1, 2, 3])<br># SELECT * FROM posts WHERE id IN (1, 2, 3)</pre><pre>post_emoticons = Emoticon.where(post_id: posts.map(&amp;:id))</pre><pre><strong>like_count_by_post_id</strong> = post_emoticons.like.group(:post_id).count<br><strong>angry_count_by_post_id</strong> = post_emoticons.angry.group(:post_id).count<br># SELECT COUNT(*) FROM emoticons WHERE name = &#39;like&#39; AND<br># post_id IN (1, 2, 3) GROUP BY post_id<br># SELECT COUNT(*) FROM emoticons WHERE name = &#39;angry&#39; AND<br># post_id IN (1, 2, 3) GROUP BY post_id</pre><pre>posts.map do |post|<br>  post.rating(<br>    <strong>like_count_by_post_id</strong>[post.id],<br>    <strong>angry_count_by_post_id</strong>[post.id]<br>  )<br>end</pre><p>This method is very flexible and can work when the simpler method of using includes can’t. And it may also be more memory efficient — in our example above we don’t load all emoticons to calculate a rating for each post. Instead, our database can do all the hard work and return just a count for each post, which is passed as an argument. However, passing the data through arguments can be complicated, especially when there are several layers below (e.g. load Emoticons, pass through Users to Posts).</p><p>Both of these approaches can help us to avoid N+1 queries. But the problem is that we should know in advance on the top level which data we need to preload. And we will preload it every time, even if it’s not necessary. For example, with GraphQL these approaches don’t work since with GraphQL we can’t predict which fields users are going to ask in a query.<strong> Is there any other way to avoid N+1 queries? Yes, it’s called batching.</strong></p><h3>What is batching?</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*g3WjikcHIkon-5lAjCKQRw.jpeg" /></figure><p>Batching isn’t a new way to solve N+1 queries. Facebook <a href="https://code.facebook.com/posts/302060973291128/open-sourcing-haxl-a-library-for-haskell/">released the Haskel Haxl</a> library in 2014, but the technique has been used long before. It uses such concepts as Monad, Applicative and Functor. I won’t get into explaining these ideas as it deserves a separate post (would you be interested in learning more about functional programming in Ruby?).</p><p>The idea of batching was also implemented in other programming languages. One of the most well-known libraries is <a href="https://github.com/facebook/dataloader">JavaScript DataLoader</a>, which became very popular with the rise of GraphQL. Here is a <a href="https://www.youtube.com/watch?v=OQTnXNCDywA">great video</a> about its source code by Lee Byron, an engineer at Facebook. The code is pretty straight forward and contains just 300 lines.</p><p><strong>These are the general steps for batching</strong>:</p><ol><li>Passing an item to load in any part of the app.</li><li>Loading and caching values for the passed items in batch.</li><li>Getting the loaded value where the item was passed.</li></ol><p>The main advantage of using this technique is that batching is isolated. It allows to load data where and when it’s needed.</p><p>Here is a basic example of using JavaScript DataLoader:</p><pre>var batch = (userIds) =&gt; ...;<br>var loader = new DataLoader(userIds =&gt; <strong>batch(userIds)</strong>);</pre><pre>// “load” schedules a job to dispatch a queue with<br>// Node.js “process.nextTick” and returns a promise</pre><pre>loader.<strong>load(userId1).</strong>then(<strong>user1</strong> =&gt; console.log(user1));<br>loader.<strong>load(userId2).</strong>then(<strong>user2</strong> =&gt; console.log(user2));<br>loader.<strong>load(userId3).</strong>then(<strong>user3</strong> =&gt; console.log(user3));</pre><p>First, we are creating a loader with a function which accepts all the collected items to load (userIds). These items are passed to our batch function which loads all users at once. Then we can call the loader.load function which returns a promise with a loaded value (user). OK, but what about Ruby?</p><h3>Batching in Ruby</h3><p>At Universe, we have monthly hackathons when everyone is free to experiment with any ideas and technologies such as Ethereum, Elixir, Progressive Web App, etc. During my last hackathon, which I won in 1 of the nominations, I was learning about existing techniques to avoid N+1 queries in GraphQL and building a tool to transform GraphQL query to MongoDB Aggregation Pipeline for batching. Check out our <a href="https://engineering.universe.com/mongo-aggregations-in-5-minutes-b8e1d9c274bb">previous post</a> which describes how to use Aggregation Pipeline to “join” different collections, filter them, serialize and so on.</p><p>At the same time, while <a href="https://engineering.universe.com/why-were-betting-on-graphql-233ddf1a0779">we’re migrating to GraphQL</a>, we’re still supporting our RESTful APIs. Usually, N+1 DB queries and HTTP requests are the main problems which cause bottlenecks in our applications as we continue to scale our platform.</p><p>That is why we decided to create a new tool which will allow us to solve N+1 queries in our existing RESTful APIs as well as in GraphQL. A simple tool which every Ruby developer will be able to understand and use. It’s called <a href="https://github.com/exAspArk/batch-loader">BatchLoader</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bb8bLX7LscBiAyfzQrKRMQ.png" /><figcaption><a href="https://github.com/exaspark/batch-loader">BatchLoader repository</a></figcaption></figure><h4><strong>Laziness</strong></h4><pre>class Post &lt; ApplicationRecord<br>  belongs_to :user<br>  <br>  def user_lazy<br>    # something cool with BatchLoader<br>  end<br>end</pre><pre>posts = Post.where(id: [1, 2, 3])<br># SELECT * FROM posts WHERE id IN (1, 2, 3)</pre><pre><strong>users_lazy</strong> = posts.map { |post| post.user_lazy }</pre><pre><strong>BatchLoader.sync!</strong>(users_lazy)<br># SELECT * FROM users WHERE id IN (1, 2, 3)</pre><p>BatchLoader doesn’t try to mimic implementations in other programming languages which have an asynchronous nature. So, it doesn’t use any extra primitives such as Promises. There is no reason to use them in Ruby unless you’re using something like EventMachine.</p><p>Instead, it uses the idea of “lazy objects”, which are used in Ruby standard library. For example, “lazy arrays” — they allow to manipulate with elements and resolve them at the end when it’s necessary:</p><pre>range = 1..Float::INFINITY</pre><pre><strong>values_lazy</strong> = range.lazy.map { |i| i * i }.take(10)</pre><pre>values_lazy.<strong>force<br></strong># =&gt; [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]</pre><p>As you can see, the 2 code blocks above have the same pattern:</p><ol><li>Collect lazy objects.</li><li>Resolve them at the end.</li></ol><h4><strong>Batching</strong></h4><p>Now let’s have a closer look at Post#user_lazy, which returns a lazy BatchLoader instance:</p><pre># app/models/post.rb<br>def user_lazy<br>  BatchLoader.for(<strong>user_id</strong>).batch do |<strong>user_ids</strong>|<br>    User.where(id: user_ids)<br>  end<br>end</pre><p>BatchLoader.for accepts an item (user_id) which should be collected and used for batching later. Then we call the batch method where we pass a block which will use all the collected items (user_ids). Inside the block, we execute a batch query for our items (User.where).</p><p>JavaScript DataLoader maps the passed item and loaded value implicitly. But it relies on these 2 constraints:</p><ul><li>The array of passed items (user_ids) must be the same length as the array of loaded values (users). This usually means that we have to add nils in the array for absent values.</li><li>Each index in the array of passed items must correspond to the same index in the array of loaded values. This usually means that we should sort the loaded values.</li></ul><p>BatchLoader, in this case, provides a load method which can be simply called to map a passed item (user_id) to loaded value (user):</p><pre># app/models/post.rb<br>def user_lazy<br>  BatchLoader.for(user_id).batch do |user_ids, <strong>batch_loader</strong>|<br>    User.where(id: user_ids).each { |u| batch_loader.<strong>load</strong>(<strong>u.id</strong>, <strong>u</strong>) }<br>  end<br>end</pre><h4><strong>RESTful API example</strong></h4><p>Now imagine we have a regular Rails application with N+1 HTTP requests:</p><pre># app/models/post.rb<br>class Post &lt; ApplicationRecord<br>  def <strong>rating</strong><br>    HttpClient.request(:get, &quot;https://example.com/ratings/#{id}&quot;)<br>  end<br>end<br><br># app/controllers/posts_controller.rb<br>class PostsController &lt; ApplicationController<br>  def index<br>    posts = Post.limit(10)<br>    serialized_posts = posts.map do |post| <br>      {id: post.id, rating: post.<strong>rating</strong>} # &lt;== N+1 HTTP requests<br>     end</pre><pre>    render json: serialized_posts<br>  end<br>end</pre><p>We can batch the requests with a gem called <a href="https://github.com/grosser/parallel">parallel</a> by executing all HTTP requests concurrently in threads. Thankfully, MRI releases GIL (global interpreter lock) when a thread hits blocking I/O – HTTP request in our case.</p><pre># app/models/post.rb<br>def <strong>rating_lazy</strong><br>  BatchLoader.for(post).batch do |posts, batch_loader|<br>    <strong>Parallel.each</strong>(posts, <strong>in_threads</strong>: 10) do |post|<br>      batch_loader.load(post, post.<strong>rating</strong>)<br>    end<br>  end<br>end</pre><pre># app/controllers/posts_controller.rb<br>class PostsController &lt; ApplicationController<br>  def index<br>    posts = Post.limit(10)<br>    serialized_posts = posts.map do |post| <br>      {id: post.id, rating: post.<strong>lazy_rating</strong>}<br>    end</pre><pre>    render json: <strong>BatchLoader.sync!</strong>(serialized_posts)<br>  end<br>end</pre><h4><strong>Thread-safety</strong></h4><p>The example with concurrent HTTP requests in threads will work only if HttpClient is thread-safe. BatchLoader#load is thread-safe out of the box, so it doesn’t need any extra dependencies.</p><h4><strong>GraphQL example</strong></h4><p>Batching is particularly useful with GraphQL. Using such techniques as preloading data in advance to avoid N+1 queries can be very complicated since a user can ask for any available fields in a query. Let’s take a look at the simple <a href="https://github.com/rmosolgo/graphql-ruby">graphql-ruby</a> schema example:</p><pre>Schema = GraphQL::Schema.define do<br>  query QueryType<br>end<br><br>QueryType = GraphQL::ObjectType.define do<br>  name &quot;Query&quot;<br>  field <strong>:posts</strong>, !types[PostType], resolve: -&gt;(obj, args, ctx) do<br>    <strong>Post.all</strong><br>  end<br>end<br><br>PostType = GraphQL::ObjectType.define do<br>  name <strong>&quot;Post&quot;</strong><br>  field <strong>:user</strong>, !UserType, resolve: -&gt;(post, args, ctx) do<br>    <strong>post.user</strong> # &lt;== N+1 queries<br>  end<br>end<br><br>UserType = GraphQL::ObjectType.define do<br>  name &quot;User&quot;<br>  field :name, !types.String<br>end</pre><p>If we want to execute a simple query like the following, we will get N+1 queries for each post.user:</p><pre>query = &quot;<br>{<br>  posts {<br>    user {<br>      name<br>    }<br>  }<br>}<br>&quot;<br>Schema.execute(query)<br># SELECT * FROM posts WHERE id IN (1, 2, 3)<br># SELECT * FROM users WHERE id = 1<br># SELECT * FROM users WHERE id = 2<br># SELECT * FROM users WHERE id = 3</pre><p>To avoid this problem, all we have to do is to change the resolver to use BatchLoader:</p><pre>PostType = GraphQL::ObjectType.define do<br>  name &quot;Post&quot;<br>  field :user, !UserType, resolve: -&gt;(post, args, ctx) do<br>    <strong>BatchLoader.for(post.user_id).batch </strong>do |ids, batch_loader|<br>      User.where(id: ids).each { |u| batch_loader.load(u.id, u) }<br>    end<br>  end<br>end</pre><p>And setup GraphQL with the built-in lazy_resolve method:</p><pre>Schema = GraphQL::Schema.define do<br>  query QueryType<br>  <strong>lazy_resolve BatchLoader, :sync</strong><br>end</pre><p>That’s it. GraphQL lazy_resolve will basically call a resolve lambda on the field. If it returns an instance of a lazy BatchLoader, it’ll call BatchLoader#sync later to get the actual loaded value automatically.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6PVehyXwF0__uUHqTE-q8w.png" /><figcaption>Approved by the creator of graphql-ruby, an engineer at GitHub</figcaption></figure><h4><strong>Caching</strong></h4><p>BatchLoader also provides a caching mechanism out of the box. So, it won’t make queries for already loaded values. For example:</p><pre>def user_lazy(id)<br>  BatchLoader.for(id).batch do |ids, batch_loader|<br>    User.where(id: ids).each { |u| batch_loader.load(u.id, u) }<br>  end<br>end<br><br>user_lazy(1)      # no request<br># =&gt; &lt;#BatchLoader&gt;<br><br>user_lazy(1).<strong>sync</strong> # SELECT * FROM users WHERE id IN (1)<br># =&gt; &lt;#User&gt;<br><br>user_lazy(1).<strong>sync</strong> # no request<br># =&gt; &lt;#User&gt;</pre><h3>Conclusion</h3><p>Overall, batching is a powerful technique to avoid N+1 queries. I believe that every Ruby developer should know about it, and not just people who use GraphQL or other programming languages. It will allow you to decouple unrelated parts of your application and load your data where and when it’s needed in batch without sacrificing the performance.</p><p>At Universe, we’re using BatchLoader on production for both RESTful API and GraphQL by sharing the same code. For more information, check out the <a href="https://github.com/exAspArk/batch-loader/blob/master/README.md">README</a> and the source code, which has just 150 lines.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/350/1*hqm1ijYWLsw8fCV2KmGvAg.png" /><figcaption><a href="https://careers.universe.com/">We’re hiring!</a></figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=24e20c6e7b94" width="1" height="1" alt=""><hr><p><a href="https://engineering.universe.com/batching-a-powerful-way-to-solve-n-1-queries-every-rubyist-should-know-24e20c6e7b94">Batching  – A powerful way to solve N+1 queries every Rubyist should know</a> was originally published in <a href="https://engineering.universe.com">Universe Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why We’re Betting on GraphQL]]></title>
            <link>https://engineering.universe.com/why-were-betting-on-graphql-233ddf1a0779?source=rss----a1cd8e0d60b2---4</link>
            <guid isPermaLink="false">https://medium.com/p/233ddf1a0779</guid>
            <category><![CDATA[api]]></category>
            <category><![CDATA[graphql]]></category>
            <category><![CDATA[flow]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[apollo]]></category>
            <dc:creator><![CDATA[Joshua Kelly]]></dc:creator>
            <pubDate>Tue, 15 Aug 2017 16:31:37 GMT</pubDate>
            <atom:updated>2017-08-15T16:50:24.784Z</atom:updated>
            <content:encoded><![CDATA[<p>Universe is betting big on GraphQL. We’ve been hard at work this quarter aiming to deliver a richly typed, documented, publicly available GraphQL interface into our existing event ticketing platform.</p><p>Today, we’re excited to announce general availability of our beta GraphQL API. <a href="https://developers.universe.com/page/graphql-explorer">You can explore this API with our handy GraphiQL Explorer and documentation</a>.</p><p>In making this beta, and through the course of our early development, we’ve identified five compelling use cases for GraphQL that might be less obvious, but compelling to those new to this paradigm:</p><ol><li>Strong client contracts</li><li>Domain layer abstraction</li><li>Typed request/response interfaces</li><li>Expensive fields</li><li>Documentation</li></ol><h3>Strong client contracts</h3><p><strong>Problem</strong>: <em>Eventually, you will get tired of designing systems that introduce type impedence bugs between servers and clients. Universe has been very API-contract oriented since its inception. We’ve built several large-scale single page JavaScript apps including </em><a href="https://www.universe.com"><em>universe.com</em></a><em>, and four mobile apps across two operating systems. Every single project encounters </em>null is undefined<em> type errors, when server APIs fail to produce responses that a client expected.</em></p><p><strong>Solution</strong>: With the schema introspection provided by GraphQL, we can provide strong guarantees to clients about the format of our server responses. With tools like apollo-codegen, we can generate platform-specific type declarations for our client apps (<a href="http://dev.apollodata.com/react/using-with-types.html">Flow types for JavaScript</a>, <a href="http://dev.apollodata.com/ios/index.html">native types for Swift</a>, etc). With type declarations for our API, we can eliminate entire classes of errors, where apps accidentally look for missing fields in JSON responses, producing runtime crashes.</p><p>Clients can also lint the queries they make by transforming stringified queries into the GraphQL AST (<a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">abstract syntax tree</a>) and comparing it against the introspected schema. An NPM package, <a href="https://github.com/apollographql/graphql-tag">graphql-tag</a>, lets us do this as a build step during Webpack preprocessing in our frontend JavaScript projects.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f0f16a559a70c0885101eec8874e6b50/href">https://medium.com/media/f0f16a559a70c0885101eec8874e6b50/href</a></iframe><p>Here, for example, we see a query named DemoQuery, which accepts an argument of id, expecting that value to be typed as an ID scalar. The response to this query will be an object mapping the same structure of the query itself. With <a href="https://github.com/apollographql/graphql-tag">graphql-tag</a>, we pre-process all of our queries by saving them in separate files with the *.graphql extension.</p><p>A second step with <a href="https://github.com/apollographql/apollo-codegen">apollo-codegen</a> reads our *.graphql files and produces matching Flow types for each query and its variables, like so:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5fd6d6329ba46c6dd41537f214def930/href">https://medium.com/media/5fd6d6329ba46c6dd41537f214def930/href</a></iframe><p>With this type definition, we can make type assertions for props in React Components, or even just plain functions that transform this map into something else. Giving those functions a means of proving the type of their input or output makes our code resilient to the most common class of data error.</p><h3>Domain layer abstraction</h3><p><strong>Problem:</strong> <em>A fast-moving startup transitions between markets and its domain language gets taken along for the ride. Abstractions that made sense and sounded descriptive now leave developers scratching their heads. A mountain of legacy external-party commitments make boundary-area refactoring difficult.</em></p><p><strong>Solution:</strong> Pave it over! GraphQL provides a way for us to abstractly layer new interfaces over our existing data structures. We don’t have to assume the same old business models are related in the same ways — we don’t even have to name them the same things.</p><p>For us, this create an opportunity for a fresh <em>conceptual</em> start while being able to maintain existing interface boundaries in legacy APIs indefinitely. This is a big deal because the Universe of 2011 has very different customer data needs from the Universe of 2017.</p><h3>Typed Request/Response Interfaces</h3><p><strong>Problem</strong>: <em>After building several versions of an API, we’ve created many different patterns, and followed different JSON API design trends. The structure of our responses varies: sometimes the payload is wrapped in a key like </em>data<em> or </em>payload<em> or </em>message<em>, often it has a complicated relationship sideloading strategy where each root key is an enumerable set of a type (this is internally known as Old School Active Model Serializers format, and mimics an early version of the JSON API spec). Our request interface is similarly fragmented: pagination strategies, search, and ordering all differ depending on API version, and our implementation strategy at the time of original development.</em></p><p><strong>Solution</strong>: GraphQL gives us standardized, typed interfaces. Our response interface is designed <a href="https://facebook.github.io/graphql/#sec-Response-Format">according to a specification</a>. Query responses are formatted as maps matching the structure of the query request, and returned in a key called data. Instead of needing to entity-map the response, we can predictably traverse the response given knowledge of only the query from the request. Similarly, <a href="https://facebook.github.io/graphql/#sec-Errors">errors are returned in a standardized format</a> — locations, path, and message values for each error in the operation are returned are added to a root key called errors.</p><p>While GraphQL doesn’t, on its own, define a pagination interface, we’ve discovered compelling patterns defined in the <a href="https://facebook.github.io/relay/graphql/connections.htm">Relay spec</a>, and supported by popular GraphQL client SDKs in the wild.</p><h3>Expensive Fields</h3><p><strong>Problem</strong>: <em>With previous JSON API spec-inspired API designs, we found static model-based serializers to be a significant performance limitation. We have to be extra careful about excluding “expensive” fields from our responses given that a model can be serialized in the context of both a single resource (ex. </em>/posts/1<em>) and in the context of a set or list (ex. </em>/posts<em>).</em></p><p><strong>Solution</strong>: In GraphQL, each field on a type must be able to resolve to a value. The cost of each resolution is only incurred if a query actually requests the field. Previously, as is common in MVC-style web servers, our models would be marshalled into JSON by a view layer (objects commonly referred to as serializers). Fields that were expensive to render would have to be monitored diligently. With GraphQL, we can be much more permissive about per-field resolution costs.</p><p>Additionally, because each field is individually resolved we can instrument precise performance metrics. <a href="https://optics.apollodata.com">Apollo Optics</a> collects this data for us out-of-box with effectively no configuration necessary:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9lA0cMb_OerjNhN-o1zMxA.png" /><figcaption>Apollo Optics in action</figcaption></figure><h3>Documentation</h3><p><strong>Problem:</strong> <em>Developer friendliness is really important if you care about public adoption of your API. But unless you’re writing a public API deliberately, it’s easy for internal product teams to rely on folklore, Google Docs, and private Slack messages as functioning documentation, instead of high quality technical writing.</em></p><p><strong>Solution:</strong> All GraphQL types support a description field according to the spec. Description is a field designated for use by implementers to describe the functionality of the field:</p><blockquote>All types in the introspection system provide a description field of type String to allow type designers to publish documentation in addition to capabilities.</blockquote><p>In our server implementation, we’ve taken advantage of this functionality to provide rich, English-language descriptions of all of the type objects in our system — right alongside our code. With this form of inline documentation in our schema, we can automatically generate developer-friendly docs. A number of GraphQL community projects exist which transform the description fields into navigable documentation. GraphiQL Explorer demonstrates this functionality nicely:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wIdhsO5406nAZUCBDain7Q.png" /><figcaption>GraphiQL Explorer with documentation generated from our schema</figcaption></figure><p>If you’re looking to test out a production GraphQL API, try out our public GraphiQL Explorer: <a href="https://developers.universe.com/page/graphql-explorer">developers.universe.com/page/graphql-explorer</a> — and follow us here on Medium!</p><p>Excited about GraphQL? <a href="https://careers.universe.com/">We’re hiring!</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=233ddf1a0779" width="1" height="1" alt=""><hr><p><a href="https://engineering.universe.com/why-were-betting-on-graphql-233ddf1a0779">Why We’re Betting on GraphQL</a> was originally published in <a href="https://engineering.universe.com">Universe Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is Redux-Saga?]]></title>
            <link>https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1?source=rss----a1cd8e0d60b2---4</link>
            <guid isPermaLink="false">https://medium.com/p/c1252fc2f4d1</guid>
            <category><![CDATA[redux]]></category>
            <category><![CDATA[redux-thunk]]></category>
            <category><![CDATA[redux-saga]]></category>
            <category><![CDATA[react]]></category>
            <category><![CDATA[react-redux]]></category>
            <dc:creator><![CDATA[Alex Richardson]]></dc:creator>
            <pubDate>Tue, 13 Jun 2017 15:43:49 GMT</pubDate>
            <atom:updated>2017-06-13T15:43:48.912Z</atom:updated>
            <content:encoded><![CDATA[<p><a href="https://redux-saga.js.org/">Redux-saga</a> is a redux middleware library, that is designed to make handling side effects in your redux app nice and simple. It achieves this by leveraging an ES6 feature called <a href="https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Statements/function*">Generators</a>, allowing us to write asynchronous code that looks synchronous, and is very easy to test.</p><p>Before we get any deeper, it’s important to note, that the actual term “saga” has a history in the world of computer science, and isn’t strictly limited to the world of JavaScript. If you’re interested in learning more about Sagas, I recommend checking out this amazing talk by Caitie McCaffrey, titled ‘Applying the Saga Pattern’.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FxDuwrtwYHu8%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DxDuwrtwYHu8&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FxDuwrtwYHu8%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/8cb47e6b85d3a5963b1b834fe5de32ad/href">https://medium.com/media/8cb47e6b85d3a5963b1b834fe5de32ad/href</a></iframe><p>To sum up rather quickly, the Saga pattern is way a to handle long running transactions with side effects or potential failures. For each transaction we wish to complete, we will also need a counter-transaction to revert the original if anything went wrong. Another great resource on this is a blog post by <a href="https://medium.com/u/31daca79e798">Roman Liutikov</a> called <a href="https://medium.com/@roman01la/confusion-about-saga-pattern-bbaac56e622">‘Confusion about Saga pattern’</a>.</p><h3><strong>Why should we use Sagas?</strong></h3><p>Now, when creating a new react-redux app, you’ll generally be guided to use <a href="https://github.com/gaearon/redux-thunk">redux-thunk</a> or <a href="https://github.com/redux-saga/redux-saga">redux-saga</a> to handle asynchronous actions. So why should we use redux-saga?</p><p>Well, straight from the docs:</p><blockquote>“Contrary to redux thunk, you don’t end up in callback hell, you can test your asynchronous flows easily and your actions stay pure.”</blockquote><p>Let’s unpack each of these by comparing a simple example using sagas and thunks. Let’s make an API request to fetch some data on a button click.</p><p>Redux Thunk:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a4df0d1fe4e13088feaa8fd6a5e44e5b/href">https://medium.com/media/a4df0d1fe4e13088feaa8fd6a5e44e5b/href</a></iframe><p>Here we have an action creator getDataFromAPI() that would be called from some Button component’s onClick event, which starts our async process:</p><ul><li>First we fire an action letting our store know that we are about to make an API request ( dispatch(getDataStarted() )</li><li>Next we actually make the API request, which returns a promise.</li><li>Then we fire a success action along with our API response or an error action with an error response if anything went wrong.</li></ul><p>Redux Saga:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0433e0b128bd875b0185df1afb80a6f0/href">https://medium.com/media/0433e0b128bd875b0185df1afb80a6f0/href</a></iframe><p>The exact same process, but the code looks a little bit different.</p><ul><li>Instead of calling dispatch in our thunk example, we now call put (we can think of these as equivalent).</li><li>We have a ‘watcher’ function that listens to our ‘start’ action. This assumes a Button’s onClick handler would fire a simple redux action with a type API_BUTTON_CLICK</li><li>We use redux-saga’s call effect, which is a way to get data from any async function (promise, different saga, etc)</li></ul><p>Fairly simple, right? On some button click, we’re going to make an API request to some endpoint. If it succeeds we dispatch a new action with our payload. Otherwise we dispatch an error action with an error message.</p><p>Right away, it should be noted how much easier it is to read and to understand the saga example. That’s not to say the thunk example is hard to understand, but we can get rid of a few intricacies like returning a function and dealing with a promise chain. Instead we can simply write a try/catch block to catch any errors involved with our network request, and to put (or dispatch) an action to notify our reducers.</p><p>Secondly, and arguably most importantly, our Saga side effect function is now <em>pure</em>. This is because the call(getDataFromAPI) doesn’t actually execute the API request, it returns a pure object that looks like this:{ type: &#39;CALL&#39;, func, args } The actual fetch execution is taken care of by the redux-saga middleware, and will return the value back into our generator (hence the yield keyword) or throw an error if there was one.</p><p>This is a pretty powerful concept, as we can now test our saga functions with ease. A test for the above snippet would look something like this</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5376081f9d9da949036e13a98d3a9520/href">https://medium.com/media/5376081f9d9da949036e13a98d3a9520/href</a></iframe><p>Whereas testing the thunk snippet above gets a little trickier. We would have to mock the actual getDataFromAPI call, and any other functions we call inside the thunk. This may not seem like big task, but as our actions grow in complexity, our tests surely will too, which is best to be avoided.</p><p>Hopefully this illustrated the three main points of redux-saga to be a little bit clearer. We don’t end up in callback-hell, we keep our actions pure, and our asynchronous flows are easy to test. To learn more, I recommend to check out these resources:</p><ul><li><a href="https://medium.freecodecamp.com/async-operations-using-redux-saga-2ba02ae077b3">Async operations using redux-saga</a></li><li><a href="https://riad.blog/2015/12/28/redux-nowadays-from-actions-creators-to-sagas/">Redux-Nowadays: From actions creators to sagas</a></li><li><a href="http://konkle.us/master-complex-redux-workflows-with-sagas/">Master Complex Redux Workflows with Sagas</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c1252fc2f4d1" width="1" height="1" alt=""><hr><p><a href="https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1">What is Redux-Saga?</a> was originally published in <a href="https://engineering.universe.com">Universe Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Mongo Aggregations in 5 Minutes]]></title>
            <link>https://engineering.universe.com/mongo-aggregations-in-5-minutes-b8e1d9c274bb?source=rss----a1cd8e0d60b2---4</link>
            <guid isPermaLink="false">https://medium.com/p/b8e1d9c274bb</guid>
            <category><![CDATA[mongodb]]></category>
            <category><![CDATA[optimization]]></category>
            <category><![CDATA[mongo]]></category>
            <category><![CDATA[rails]]></category>
            <category><![CDATA[aggregation]]></category>
            <dc:creator><![CDATA[Joshua Kelly]]></dc:creator>
            <pubDate>Tue, 30 May 2017 15:40:49 GMT</pubDate>
            <atom:updated>2017-05-30T15:39:54.478Z</atom:updated>
            <content:encoded><![CDATA[<p>At Universe, we use Mongo as a primary data store for our events, users, sessions, and more. The documents we store in our collections tend to be mostly uniform attribute-wise. We model relationships using the standard relationship_id approach implemented by most ORM/ODMs like Mongoid (a Ruby library).</p><p>To date, we’ve relied on the domain modelled in our application — a consumer-facing Rails app — to implement traversals of relationships between documents. Mongoid provides us with easy-to-use relationship helpers that makes our best code concise and expressive.</p><p>Alas, the tale often told about ORM/ODMs is true: sometimes you just need to write a query. But wait, isn’t querying for relationships going to be a pain? Aha! It doesn’t have to be if you use the Mongo Aggregation Pipeline.</p><h3>What are aggregations?</h3><blockquote>Aggregation process documents and return computed results. Aggregation operations group values from multiple documents together, and can perform a variety of operations on the grouped data to return a single result.</blockquote><p>Aggregations can be used to apply a sequence of query-operations to the documents in a collection, reducing and transforming them. With aggregations, we can perform queries that offer similar functionality to the behaviors we might expect to see in a relational database query.</p><p>In Mongo, aggregations work as a <a href="https://docs.mongodb.com/manual/core/aggregation-pipeline/"><strong>pipeline</strong></a>, or a list of operators/filters applied to the data. Operators come in three varieties: <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/match/#stage-operators">stages</a>, <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/match/#expression-operators">expressions</a>, and <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/match/#accumulators">accumulators</a>. When calling aggregate on a collection, we pass a list of stage operators. Documents are processed through the stages in sequence, with each stage applying to each document individually.</p><h4>$match (where)</h4><p>$match is a stage operator with this definition: { $match: { &lt;query&gt; } }</p><p>The syntax for query is identical <a href="https://docs.mongodb.com/manual/tutorial/query-documents/#read-operations-query-argument">read operation query</a> syntax. Ideally, you will use $match as early in the pipeline as possible:</p><blockquote>Because <em>$match</em> limits the total number of documents in the aggregation pipeline, earlier <em>$match</em> operations minimize the amount of processing down the pipe.</blockquote><p>In effect, with $match Mongo will filter the collection accoring to the query parameters, and only pass through the <em>documents matching the query</em>, to the next stage of the pipeline. Take this example of “article” documents in a collection, with an author — and then filtering by the name of the author:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bc05886b36ce555514db1f4b25340324/href">https://medium.com/media/bc05886b36ce555514db1f4b25340324/href</a></iframe><p>We can apply other conditions and constraints by using an expression operator (one of the two other operators in addition to stage, mentioned above). For example, we can pass $or to $match and then provide a list of matchable queries, one of which must be true in order for the document to be included by the filter. This is an example of a <a href="https://docs.mongodb.com/manual/reference/operator/aggregation-boolean/">boolean aggregation</a>.</p><p>We can also apply additional constraints such as <a href="https://docs.mongodb.com/manual/reference/operator/aggregation-comparison/">comparisons</a>, where document values must be greater than, less than, or equal to some other value. Here we use both $or and $gte in a $match stage:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2080439fb38899d27e2f592300ad8c7b/href">https://medium.com/media/2080439fb38899d27e2f592300ad8c7b/href</a></iframe><h4>$skip + $limit</h4><p>$skip and $limit both accept a positive integer — and do what you expect them to do: skip documents, and limit the number returned. For the purposes of developing rich APIs on top of Mongo, this is obviously quite useful. A simple example building on the above example of an articles collection might look like:</p><pre>db.articles.aggregate([<br>  { $match: { score: { $gt: 60 }}},<br>  { $limit: 2},<br>  { $skip: 1 }<br>])</pre><p>First we’re filtering the collection for documents with a field called score with a value greater than 60. Next, we limit the number of documents to two, and then return the second one by skipping the first.</p><p>$skip and $limit are also subject to <a href="https://docs.mongodb.com/manual/core/aggregation-pipeline-optimization/">pipeline optimizations</a>.</p><h4>$lookup (join)</h4><p>Ok, so far — so good. But what about modelling relationships?</p><p>$lookup kind of functions the way a join does — it matches each document in the pipeline stage to a set of documents from another collection, and then returns those documents <em>as an attribute on the current one</em> to the next stage of the pipeline. <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/#use-lookup-with-an-array">You can even match arrays</a>.</p><p>The syntax of this operation looks like:</p><pre>{ $lookup: {<br>  from: &lt;collection to join&gt;,<br>  localField: &lt;field from the input documents&gt;,<br>  foreignField: &lt;field from the documents of the &quot;from&quot; collection&gt;,<br>  as: &lt;output array field&gt;<br>} }</pre><p>As a full fledged example inside of database modelling an inventory system:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/017124f64282bad14ef5010e1f22117d/href">https://medium.com/media/017124f64282bad14ef5010e1f22117d/href</a></iframe><h4>$project (select)</h4><p>Naturally, we’re going to want to reduce the documents into smaller objects — returning just the fields we want, or aliasing their names. In the SQL paradigm, this sounds like a SELECT , for Mongo it’s $project .</p><p>The structure you pass to $project is a field mapping:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RJUdEa25wIdkjs_P2trZ-g.png" /></figure><p>We can actually compose very complex serialization routines using $project with &lt;expression&gt; . Otherwise, including a field is as easy as passing &lt;field&gt;: &lt;1 or true&gt; . The documents returned to the next stage of the pipeline <em>will only contain</em> the values specified by $project.</p><p>Hopefully you’re excited to try out aggregations yourself! The current version of the Mongo manual (3.4) provides some excellent example datasets to experiment with:</p><ul><li><a href="https://docs.mongodb.com/manual/tutorial/aggregation-zip-code-data-set/">Aggregation with the Zip Code Data Set</a></li><li><a href="https://docs.mongodb.com/manual/tutorial/aggregation-with-user-preference-data/">Aggregation with User Preference Data - MongoDB Manual 3.4</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b8e1d9c274bb" width="1" height="1" alt=""><hr><p><a href="https://engineering.universe.com/mongo-aggregations-in-5-minutes-b8e1d9c274bb">Mongo Aggregations in 5 Minutes</a> was originally published in <a href="https://engineering.universe.com">Universe Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Reading Redux]]></title>
            <link>https://engineering.universe.com/reading-redux-ca160163867e?source=rss----a1cd8e0d60b2---4</link>
            <guid isPermaLink="false">https://medium.com/p/ca160163867e</guid>
            <category><![CDATA[redux]]></category>
            <category><![CDATA[react]]></category>
            <dc:creator><![CDATA[Joshua Kelly]]></dc:creator>
            <pubDate>Mon, 01 May 2017 01:08:28 GMT</pubDate>
            <atom:updated>2017-08-27T23:53:44.758Z</atom:updated>
            <content:encoded><![CDATA[<h3>Reading Redux: createStore</h3><p>An essential skill every developer must learn is <strong>reading the source</strong>. By the reading source, we eliminate FUD (Fear, Uncertainty, Doubt) about our tools. This is especially critical as teams grow in size, as folklore explanations calcify and shortcut deep dives.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/127af0f802edac7d1f3af65c903f75a2/href">https://medium.com/media/127af0f802edac7d1f3af65c903f75a2/href</a></iframe><p>Many of us have worked on projects whose dependencies undergo a kind of “community rot”. Yes, our code still works, builds still run, tests still pass, but public documentation of our dependencies starts to disappear. Soon enough, we’re left a major version of two behind without anything but… the source.</p><p>So being comfortable with navigating the source of our tools becomes critical.</p><p>Let’s apply that idea and try to read through the popular state management tool, Redux.</p><blockquote><a href="http://redux.js.org/">Redux library</a> itself is only a set of helpers to “mount” reducers to a single global store object.</blockquote><p>Redux is composed of the following modules: createStore, combineReducers, bindActionCreators, applyMiddleware, and compose. We’re going to narrow in on createStore today, and cover the others in subsequent followups.</p><h3>createStore</h3><p>This module exports a function of the same name by default.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9d53126b6aa1025766453ea96423d7c3/href">https://medium.com/media/9d53126b6aa1025766453ea96423d7c3/href</a></iframe><p>The createStore function accepts a reducer, preloadedState, and an enhancer.</p><p>We call the value returned by the createStore function a Store. As you can see, Stores have a number of properties: dispatch, subscribe, getState, and replaceReducer (plus the private $$observable).</p><p>At a high level, that’s the entire structure of this module.</p><h3>(reducer, preloadedState, enhancer)</h3><p>To start, createStore accepts a reducer, and we see it applies a typeof check to ensure that this parameter is a function: <a href="https://github.com/reactjs/redux/blob/552f1f84ab803d19bd79a785b2c89d94f11c405c/src/createStore.js#L53-L55">https://github.com/reactjs/redux/blob/552f1f84ab803d19bd79a785b2c89d94f11c405c/src/createStore.js#L53-L55</a></p><p>The inline docs describe a reducer as:</p><blockquote>A function that returns the next state tree, given the current state tree and the action to handle.</blockquote><p>And we see this reflected in the flow types provided by the distribution at redux/flow-types/redux.js:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c02aae1cd1215f720420af5ac8d8109d/href">https://medium.com/media/c02aae1cd1215f720420af5ac8d8109d/href</a></iframe><p>To synthesize: the reducer we pass to createStore is a function that takes the current state and action, and the transforms them into the next state. We’ll return to the details of the action parameter under <strong>#dispatch.</strong></p><p>But already, we know enough to be able to make a minimal Redux app:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/407088c6848c49f18d2c7e857aeacce1/href">https://medium.com/media/407088c6848c49f18d2c7e857aeacce1/href</a></iframe><p>That’s it. Critically, notice that we can run this in contexts outside of a browser, like in the Node REPL (using the commented out CommonJS version of the module import). Our app object is working — but useless — Store.</p><p>What about preloadedState and enhancer?</p><blockquote><strong>preloadedState</strong> The initial state. You may optionally specify it to hydrate the state from the server in universal apps, or to restore a previously serialized user session.</blockquote><blockquote><strong>enhancer</strong> The store enhancer. You may optionally specify it to enhance the store with third-party capabilities such as middleware, time travel, persistence, etc. The only store enhancer that ships with Redux is `applyMiddleware()`.</blockquote><p>Let’s add a preloadedState:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b23510d8a1792c8de997848e73a73bd4/href">https://medium.com/media/b23510d8a1792c8de997848e73a73bd4/href</a></iframe><p>In this example, the initial starting state of the app is simply { booted_at: new Date() }. The reducer we pass as the first argument simply returns the existing state, no matter what action is passed to it.</p><p>We’ll return to include an enhancer to this example later.</p><p>As mentioned, the return value of createStore is an object with a number of functions as properties: getState, dispatch, subscribe, replaceReducer.</p><h3>#getState</h3><p>This function returns the currentState — a value first set to the value of preloadedState. That’s how the initial state bootstrapping works.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/162ffee9aa228af083cfe886e009df89/href">https://medium.com/media/162ffee9aa228af083cfe886e009df89/href</a></iframe><h3>#dispatch</h3><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a4eccf25e56972371ed812cea0d27185/href">https://medium.com/media/a4eccf25e56972371ed812cea0d27185/href</a></iframe><p>#dispatch invokes two conditions on its parameter: an Object type check, and a value of anything <em>but</em> undefined for the property type. So basically, dispatch must be called with an object passing a type prop — an object which it returns. This is captured in the Flow type provided by the project:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5541ea71aa638939a1ec239c99c48786/href">https://medium.com/media/5541ea71aa638939a1ec239c99c48786/href</a></iframe><p>What other constraints does Redux impose on actions? None! They can have any other data. “Uh, won’t that get messy?” Maybe. The <a href="https://github.com/acdlite/flux-standard-action">flux-standard-action</a> project has recommendations on how you might structure actions inside of an application.</p><p>The function otherwise consists of two critical operations: (1) calling the reducer with the currentState and action then assign the return to currentState, and (2) invoking all of the listeners after the reducer has been called.</p><h3>#subscribe</h3><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bd2b4a52f2f38a81346f1685d396d4db/href">https://medium.com/media/bd2b4a52f2f38a81346f1685d396d4db/href</a></iframe><p>First, do a function type check on the listener parameter. Set a value isSubscribed to true, and then push the listener function onto the existing array of nextListeners. Return a function, unsubscribe, which removes the listener from the nextListeners array. Recall that the listeners are actually invoked at the end of the dispatch call. In effect, every dispatch ends with all of the listeners being invoked. Ultimately, the react-redux bridge is an abstraction built on extending dispatch listeners.</p><p>Putting this together, we can understand everything that’s going on in the example calculator app from Redux’s own README:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1e561c8384adc2a6af5d25f1b1d1067d/href">https://medium.com/media/1e561c8384adc2a6af5d25f1b1d1067d/href</a></iframe><ol><li>Importing the createStore function</li><li>Defining a reducer function (counter) which accepts the state, an action Object with a type property, and transforms those inputs into the next state</li><li>Calling createStore with the reducer</li><li>Passing a listener to subscribe, to log the state on each dispatch</li><li>Calling dispatch with different values for the type property in the action object</li></ol><p>This all seems less daunting than it did initially, right? Redux is a clean, simple state management library with highly readable modern JavaScript. Who’d’ve thought?</p><p>We only need to cover a few other modules before we’ve finished reading through Redux: combineReducers, bindActionCreators, applyMiddleware. We’ll return to these soon!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ca160163867e" width="1" height="1" alt=""><hr><p><a href="https://engineering.universe.com/reading-redux-ca160163867e">Reading Redux</a> was originally published in <a href="https://engineering.universe.com">Universe Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>