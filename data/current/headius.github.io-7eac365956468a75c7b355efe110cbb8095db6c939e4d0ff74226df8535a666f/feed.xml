<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://headius.github.io/feed.xml" rel="self" type="application/atom+xml"><link href="https://headius.github.io/" rel="alternate" type="text/html"><updated>2019-09-19T16:23:58+00:00</updated><id>https://headius.github.io/feed.xml</id><title type="html">Charles Oliver Nutter</title><subtitle>Java, Ruby, and JVM guy trying to make sense of it all</subtitle><entry><title type="html">Start It Up: Improving JRuby’s Startup Time</title><link href="https://headius.github.io/2019/09/jruby-startup-time-exploration.html" rel="alternate" type="text/html" title="Start It Up: Improving JRuby's Startup Time"><published>2019-09-18T16:09:00+00:00</published><updated>2019-09-18T16:09:00+00:00</updated><id>https://headius.github.io/2019/09/jruby-startup-time-exploration</id><content type="html" xml:base="https://headius.github.io/2019/09/jruby-startup-time-exploration.html">&lt;p&gt;Hello friends! How long has it been? Too long! This post marks the beginning of a return to blogging about JRuby, the JVM, and everything in between. Today we’ll cover a topic that never gets old: JRuby’s startup performance.&lt;/p&gt;

&lt;h2 id=&quot;startup-time-is-important&quot;&gt;Startup time is important!&lt;/h2&gt;

&lt;p&gt;Before we get started running numbers and tweaking flags, you may be wondering why startup time is such a big
deal. Most applications are deployed pretty rarely. right? Even the most aggressive organizations usually won’t
redeploy more than once a day, right?&lt;/p&gt;

&lt;p&gt;While this is usually true (ignoring &lt;a href=&quot;https://www.agilealliance.org/glossary/continuous-deployment&quot;&gt;continuous deployment&lt;/a&gt; for now),
the actual development experience of different languages varies greatly. Java, for example, managed to dodge the
startup time issue for years because most developers built their apps using an always-on IDE that transparently
compiled code in the background and often restarted or redeployed to local development servers automatically. Only
recently with the rise of small services and cloud “functions” has the issue of Java startup time really become
a concern again.&lt;/p&gt;

&lt;p&gt;In the Ruby world, things are very different. Most non-coding development tasks involve running Ruby at a command
line, be they installing libraries, generating code, or starting up interactive consoles and application
servers. Rubyists do the great majority of their work using two tools: an editor, and a terminal. And because of
this, JRuby needs to have acceptable startup time.&lt;/p&gt;

&lt;h2 id=&quot;why-is-this-hard&quot;&gt;Why is this hard?&lt;/h2&gt;

&lt;p&gt;JRuby is an implementation of Ruby that runs on the JVM, and as you’d expect a large part of our codebase starts
running as JVM bytecode. Most JVMs initially execute that bytecode using an interpreter, similar to how CRuby
executes its “instruction sequences”. As that code gets “hot”, the JVM will usually compile it to native
instructions that the system CPU can execute directly. The idea is that since the interpreter can start executing
immediately, we save some startup time by not running an expensive JIT compiler before execution.&lt;/p&gt;

&lt;p&gt;This works…sort of. Modern JVMs do start executing very quickly, but if your application needs to run lots of
code at boot, before it can do useful work, startup time will reflect the fact that the bytecode interpreter
is much slower than native code.&lt;/p&gt;

&lt;p&gt;Anyone familiar with deploying JVM-based applications will know this as the dreaded “warmup curve”. It hits
JRuby particularly hard.&lt;/p&gt;

&lt;p&gt;Compare JRuby with CRuby at startup. Ruby apps are distributed as source code, which means they must first
first pass through a parser and compiler before being executed with an interpreter – all of which start out as
“cold” JVM bytecode in JRuby. Contrast this to CRuby, where all of these stages
are written in C and precompiled to native code long before the runtime is launched. Both JRuby and CRuby
run lots of Ruby code on every startup, mostly to boot up the RubyGems subsystem&lt;/p&gt;

&lt;p&gt;Making matters worse, all of the logic to set up the core classes (defining String or Array and their
methods, for example) as well as the actual core method implementations &lt;em&gt;also&lt;/em&gt; start out “cold”. Everything
running during the first few seconds after launching JRuby is running in the JVM interpreter or the JRuby
interpreter…which itself is also running in the JVM interpreter!&lt;/p&gt;

&lt;h2 id=&quot;how-bad-is-it&quot;&gt;How bad is it?&lt;/h2&gt;

&lt;p&gt;Let’s take a look at some worst-case numbers.&lt;/p&gt;

&lt;p&gt;This first example compares &lt;code class=&quot;highlighter-rouge&quot;&gt;ruby -e 1&lt;/code&gt; times on CRuby 2.6.4 versus JRuby 9.2.9 (from master) on OpenJDK 8.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ruby_dash_e.png&quot; alt=&quot;ruby -e 1 startup times&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Yikes! Both implementations are doing roughly the same work here: setting up the core classes and booting the
RubyGems subsystem. But JRuby’s taking 16x longer to do it!&lt;/p&gt;

&lt;p&gt;I say this is a worst case because it really gives us no time at all to optimize any code. JRuby doesn’t try to
JIT anything until &lt;em&gt;after&lt;/em&gt; baseline startup. The JVM does optimize some of our code, but it’s too little, too
late.&lt;/p&gt;

&lt;h2 id=&quot;optimizing-for-development&quot;&gt;Optimizing for development&lt;/h2&gt;

&lt;p&gt;In fact, the JVM is actually a little too aggressive, spending many CPU cycles during this 1.6 seconds optimizing
and emitting code that will only be used briefly. We pay a large cost at startup in trade for reducing
longer-term warmup times.&lt;/p&gt;

&lt;p&gt;We can actually tweak OpenJDK to be less aggressive by forcing it to only use the simplest part of its JIT,
rather than working hard to create optimized native code we won’t use.&lt;/p&gt;

&lt;p&gt;We do this by forcing the Hotspot “tiered” compiler to only use its first tier by passing
&lt;code class=&quot;highlighter-rouge&quot;&gt;-XX:TieredStopAtLevel=1&lt;/code&gt; to the JVM.&lt;/p&gt;

&lt;p&gt;In addition, we know JRuby’s compiler won’t help us much during these first few seconds, so we can turn that
off too using the &lt;code class=&quot;highlighter-rouge&quot;&gt;-X-C&lt;/code&gt; JRuby flag.&lt;/p&gt;

&lt;p&gt;Finally, we also turn off the JVM’s &lt;em&gt;bytecode verification&lt;/em&gt; since all the bytecode we’ll run has been verified
to death in JRuby’s continuous integration server. We do this by passing &lt;code class=&quot;highlighter-rouge&quot;&gt;-Xverify:none&lt;/code&gt; to the JVM.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/jruby_dev_flag.png&quot; alt=&quot;jruby --dev -e 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see here, this combination of flags trims a good bit of time off startup. As a service to JRuby users
everywhere, we include these flags (and a few others) as the &lt;code class=&quot;highlighter-rouge&quot;&gt;--dev&lt;/code&gt; flag. This is your first,
best startup time tip!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/jruby_dev_flag_chart.png&quot; alt=&quot;jruby --dev -e 1 comparison&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We recommend JRuby users add this flag to the &lt;code class=&quot;highlighter-rouge&quot;&gt;JRUBY_OPTS&lt;/code&gt; environment variable, so all JRuby processes and
subprocesses see it.&lt;/p&gt;

&lt;p&gt;(Edit: Keep in mind that this flag turns off a number of optimizations, so don’t try to benchmark any code with
it enabled. Note below that I have dedicated part of my bash prompt to showing JRUBY_OPTS so I don’t forget.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/jruby_opts_dev_flag.png&quot; alt=&quot;JRUBY_OPTS --dev flag&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s move on to some examples that actually do some work. I promise things get better from here!&lt;/p&gt;

&lt;h2 id=&quot;less-contrived-examples&quot;&gt;Less contrived examples&lt;/h2&gt;

&lt;p&gt;Of course most Rubyists won’t stop at evaluating the number &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;. They’re probably going to run some actual Ruby
commands, Let’s look at a common one: installing a gem.&lt;/p&gt;

&lt;p&gt;These numbers compare CRuby and JRuby installing a single gem with no dependencies from a local gem file. I’m
using Rake for this example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/gem_install_rake.png&quot; alt=&quot;gem install rake from local file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ok, now we can see that our ratio has improved from 16x slower to a mere 5x slower. We’re now giving the JVM
a chance to “warm up” and optimize JRuby itself (but not too much!) so the numbers improve.&lt;/p&gt;

&lt;p&gt;Here’s a comparison of the &lt;code class=&quot;highlighter-rouge&quot;&gt;gem list&lt;/code&gt; command, listing all the gems I have installed locally (about 640 of them).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/gem_list.png&quot; alt=&quot;gem list with 641 gems installed&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Even better: our slowdown is only 3x compared to CRuby!&lt;/p&gt;

&lt;p&gt;Finally, let’s measure the cost of starting up a Rails interactive console. This example is actually aggravated
by a Rails design choice: in order to guarantee only the required libraries get loaded, most Rails commands
you run will spawn a subprocess to actually execute. So we pay twice the cost and startup time becomes an even
bigger challenge.&lt;/p&gt;

&lt;p&gt;For this example, I’m piping the function call &lt;code class=&quot;highlighter-rouge&quot;&gt;exit&lt;/code&gt; to the console so it terminates immediately after it starts
up.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/rails_console.png&quot; alt=&quot;rails console startup and exit&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, finally, we have an example where JRuby’s “best” startup time is less than 2x slower than CRuby. We are
closing the gap!&lt;/p&gt;

&lt;p&gt;We have some baseline numbers now, based on the &lt;code class=&quot;highlighter-rouge&quot;&gt;-e 1&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;gem list&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;gem install&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;rails console&lt;/code&gt; command
lines. Let’s explore a few other scenarios for speeding up these commands.&lt;/p&gt;

&lt;h2 id=&quot;ahead-of-time-compilation&quot;&gt;Ahead-of-time compilation&lt;/h2&gt;

&lt;p&gt;The new hotness in the JVM world is the re-emergence of &lt;em&gt;ahead-of-time&lt;/em&gt; compilation (AOT), which precompiles all
your application’s JVM bytecode to native code before you ever run it. The most recent and arguably most
successful example of this comes from GraalVM, an enhanced OpenJDK-based runtime that examines and optimizes
your whole application at once (so-called “closed-world” optimization) to produce small, fast, efficient native
binaries.&lt;/p&gt;

&lt;p&gt;Sounds like just the magic we need, right? For some cases, it may be! An alternative Ruby implementation called
TruffleRuby – part of the same GraalVM project – uses AOT and a prebooted image of the heap to improve their
baseline startup substantially.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ruby_dash_e_truffleruby.png&quot; alt=&quot;rails -e 1 truffleruby comparison&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Wow! Not only has this eliminated the startup gap, it’s actually &lt;em&gt;better&lt;/em&gt; startup time than CRuby.&lt;/p&gt;

&lt;p&gt;There’s a number of techniques at play here in addition to AOT, which you can read about in Benoit Daloze’s blog post
&lt;a href=&quot;https://eregon.me/blog/2019/04/24/how-truffleruby-startup-became-faster-than-mri.html&quot;&gt;How TruffleRuby’s Startup Became Faster than MRI’s&lt;/a&gt;.
There’s a lot of clever, exciting work going on there.&lt;/p&gt;

&lt;p&gt;So how does TruffleRuby fare on running our three common Ruby commands above? Things get a little murky here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ruby_commands_truffleruby.png&quot; alt=&quot;common ruby commands truffleruby comparison&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, since TruffleRuby still parses, compiles, and executes Ruby code from source, they still see poor
startup time in comparison to CRuby. I’m sure they’re continuing to work on this, so watch this space.&lt;/p&gt;

&lt;p&gt;Ahead-of-time compilation may still be an option for JRuby, however. Our interpreter is much simpler than the one
found in TruffleRuby, so precompiling JRuby to native code should run pretty well. And since JRuby already can
precompile Ruby code to JVM bytecode, it’s possible we could skip right past the parse, compile, and JIT phases. We
hope to explore this option in the next few months.&lt;/p&gt;

&lt;h2 id=&quot;alternative-jvms-eclipse-openj9&quot;&gt;Alternative JVMs: Eclipse OpenJ9&lt;/h2&gt;

&lt;p&gt;One of the most exciting developments of the past year was the official open source release of IBM’s J9 JVM as OpenJ9.
J9 is one of the few world-class, fully-compliant JVM implementations out there, with a completely different array
of optimizations, garbage collectors, and supported platforms. Having OpenJ9 available gives JRubyists another way
to run, scale, and deploy Ruby applications.&lt;/p&gt;

&lt;p&gt;One of the cooler features of OpenJ9 is its ability to share pre-processed class data across runs. When you pass the
&lt;code class=&quot;highlighter-rouge&quot;&gt;-Xshareclasses&lt;/code&gt; flag, OpenJ9 will create a shared archive containing pre-parsed, pre-verified JVM bytecode and class
data. In addition, it will dynamically save native code output from the JIT, allowing those methods to start up a bit
“hotter” and skipping the interpreter and some optimization stages.&lt;/p&gt;

&lt;p&gt;An additional flag &lt;code class=&quot;highlighter-rouge&quot;&gt;-Xquickstart&lt;/code&gt; reduces how much optimization OpenJ9 does (similar to the Hotspot &lt;code class=&quot;highlighter-rouge&quot;&gt;TieredStopAtLevel&lt;/code&gt;
flag shown above) to allow short-running commands to get up and going more quickly.&lt;/p&gt;

&lt;p&gt;And as of JRuby 9.2.9, we include these flags in our &lt;code class=&quot;highlighter-rouge&quot;&gt;--dev&lt;/code&gt; mode!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ruby_commands_openj9.png&quot; alt=&quot;common ruby commands openj9 comparison&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we’re talking! By combining the “quickstart” and “shareclasses” flags, we’ve improved on the Hotspot startup
times in two out of three cases. The third, &lt;code class=&quot;highlighter-rouge&quot;&gt;rails console&lt;/code&gt; is oddly slower…we look forward to working with the
OpenJ9 team to get that one optimized as well.&lt;/p&gt;

&lt;p&gt;If you’re on an unusual platform (AIX? PowerPC?) or just want to try something new, you should definitely start
playing with JRuby on OpenJ9. Expect to see JRuby make better use of OpenJ9’s unique features very soon.&lt;/p&gt;

&lt;h2 id=&quot;bleeding-edge-openjdk-13&quot;&gt;Bleeding edge: OpenJDK 13&lt;/h2&gt;

&lt;p&gt;The last example I want to show is from the just-released OpenJDK 13, which brings to the table improved support for
what they call “class data sharing” (CDS).&lt;/p&gt;

&lt;p&gt;The CDS feature started out as a paid option from Sun Microsystems and later Oracle, but as of OpenJDK 10 it is both
free and Free for all uses. Since that time, it has been improved to cache more data, more efficiently, and most recently it is
now possible to generate the CDS “archive” dynamically based on a given run of the JVM.&lt;/p&gt;

&lt;p&gt;We can use the new &lt;code class=&quot;highlighter-rouge&quot;&gt;-XX:ArchiveClassesAtExit=filename.jsa&lt;/code&gt; flag to produce one of these archives, and
&lt;code class=&quot;highlighter-rouge&quot;&gt;-XX:SharedArchiveFile=filename.jsa&lt;/code&gt; flag to use it at runtime.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/cds_command_line.png&quot; alt=&quot;jdk13 cds command line&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can specify different files for different commands, of course, to have fine grained control over what’s getting
cached for you. With our simple example, we see some very nice improvements:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ruby_commands_cds13.png&quot; alt=&quot;common ruby commands on JDK13 CDS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Another win over plain &lt;code class=&quot;highlighter-rouge&quot;&gt;--dev&lt;/code&gt; on OpenJDK 8’s version of Hotspot! Both of the gem commands are the fastest ever, and
the &lt;code class=&quot;highlighter-rouge&quot;&gt;-e 1&lt;/code&gt; time has dropped to a merge 10x CRuby’s time. Frustratingly, the &lt;code class=&quot;highlighter-rouge&quot;&gt;rails console&lt;/code&gt; is again slower than on
Hotspot 8…what is it about Rails that continues to confound optimizing VMs?&lt;/p&gt;

&lt;p&gt;We will be exploring how best to take advantage of these improvements. Currently, JRuby will automatically use the
CDS archive in &lt;code class=&quot;highlighter-rouge&quot;&gt;lib/jruby.jsa&lt;/code&gt; if it exists, and if you’re not using JDK 13 we provide the &lt;a href=&quot;https://github.com/jruby/jruby-startup&quot;&gt;jruby-startup&lt;/a&gt;
gem with its &lt;code class=&quot;highlighter-rouge&quot;&gt;generate-appcds&lt;/code&gt; command to regenerate this archive for you. Give it a try and let us know how it works
for you!&lt;/p&gt;

&lt;h2 id=&quot;the-bottom-line&quot;&gt;The bottom line&lt;/h2&gt;

&lt;p&gt;Startup time is crucial to the development process for most Rubyists, and we continue to improve how JRuby boots and
executes to reduce startup time. Meanwhile, there’s an army of VM engineers bringing startup optimizations to OpenJDK,
GraalVM, and OpenJ9…that you as a JRubyist will benefit from. Work continues, but the future looks bright!&lt;/p&gt;

&lt;p&gt;Here’s a short summary of what we’ve learned today:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;JRuby provides a &lt;code class=&quot;highlighter-rouge&quot;&gt;--dev&lt;/code&gt; flag to reduce startup time by reducing how much optimization happens.&lt;/li&gt;
  &lt;li&gt;GraalVM may provide a future path toward pre-compiling JRuby and key Ruby libraries to improve startup.&lt;/li&gt;
  &lt;li&gt;OpenJ9 includes &lt;a href=&quot;https://developer.ibm.com/articles/optimize-jvm-startup-with-eclipse-openjj9/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;-Xquickstart&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;-Xshareclasses&lt;/code&gt;&lt;/a&gt;
flags today to help commands start up more quickly.&lt;/li&gt;
  &lt;li&gt;Hotspot’s Class Data Sharing &lt;a href=&quot;https://bugs.openjdk.java.net/browse/JDK-8221706&quot;&gt;continues to improve&lt;/a&gt; and already
provides the best JRuby startup for many commands.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can try out JRuby 9.2.9 from our &lt;a href=&quot;https://oss.sonatype.org/content/repositories/snapshots/org/jruby/jruby-dist/9.2.9.0-SNAPSHOT/&quot;&gt;nightly builds&lt;/a&gt;
if you’d want the fastest-starting version of JRuby yet. We’ll be putting out a formal release within the next
couple weeks.&lt;/p&gt;

&lt;p&gt;Have fun!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/ruby/comments/d6444b/start_it_up_improving_jrubys_startup_time/&quot;&gt;Discuss this post on Reddit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Edit: An earlier version of this pots stated that Class Data Sharing was made free/Free in OpenJDK 8. It has been amended
to indicate this happened in OpenJDK 10.)&lt;/p&gt;</content><author><name>Charles Oliver Nutter</name></author><summary type="html">Hello friends! How long has it been? Too long! This post marks the beginning of a return to blogging about JRuby, the JVM, and everything in between. Today we’ll cover a topic that never gets old: JRuby’s startup performance.</summary></entry><entry><title type="html">Running JRuby on the Graal JIT</title><link href="https://headius.github.io/2018/07/running-jruby-on-graal-jit.html" rel="alternate" type="text/html" title="Running JRuby on the Graal JIT"><published>2018-07-03T21:45:00+00:00</published><updated>2018-07-03T21:45:00+00:00</updated><id>https://headius.github.io/2018/07/running-jruby-on-graal-jit</id><content type="html" xml:base="https://headius.github.io/2018/07/running-jruby-on-graal-jit.html">&lt;div dir=&quot;ltr&quot; style=&quot;text-align: left;&quot; trbidi=&quot;on&quot;&gt;Hello, friends! Long time no blog!&lt;br /&gt;&lt;br /&gt;I'm still here hacking away on JRuby for the benefit of Rubyists everywhere, usually slogging through compatibility fixes and new Ruby features. However with the release of JRuby 9.2, we've caught up to Ruby 2.5 (the current release) and I'm spending a little time on performance.&lt;br /&gt;&lt;br /&gt;I thought today would be a good opportunity to show you how to start exploring some next-generation JRuby performance by running on top of the Graal JIT.&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Graal&lt;/h2&gt;&lt;div style=&quot;text-align: left;&quot;&gt;The &lt;a href=&quot;https://github.com/oracle/graal&quot;&gt;Graal JIT&lt;/a&gt; is a project that grew out of a Sun Microsystem Labs project called MaxineVM. Maxine was an attempt to implement a JVM entirely in Java...via various specialized Java dialects, annotations, and compilation strategies. Graal is the latest iteration of the JIT work done for Maxine, and provides an optimizing JVM JIT implemented entirely in Java. Maxine only lives on as a research project, but Graal is rapidly shaping up to become the preferred JIT in OpenJDK.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;The majority of optimizations that Graal does to code are no different from the &quot;classic&quot; OpenJDK JIT, Hotspot's &quot;C2&quot; compiler. You get all the usual dead code, loop unrolling, method inlining, branch profiling, and so on. However Graal goes beyond Hotspot in a few key ways. Most interesting to JRuby (and other dynamic languages) is the fact that Graal finally brings good Escape Analysis to the JVM.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Escape Analysis&lt;/h2&gt;&lt;div style=&quot;text-align: left;&quot;&gt;The biggest gains JRuby sees running on Graal are from Escape Analysis (EA). The basic idea behind escape analysis goes like this: if you allocate an object, use it and its contents, and then abandon that object all within the same thread-local piece of code (i.e. not storing that object in a globally-visible location) then there's no reason to allocate the object. Think about this in terms of Java's autoboxing or varargs: if you box arguments or numbers, pass them to a method, and then unbox them again...the intermediate box was not really necessary.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;The basic idea of EA is not complicated to understand, but implementing well can be devilishly hard. For example, the Hotspot JIT has had a form of escape analysis for years, optional in Java 7 and I believe turned on by default during Java 8 maintenance releases. However this EA was very limited...if any wrapper object was used across a branch (even a simple if/else) or if it might under any circumstance leave the compiled code being optimized, it would be forced to allocate every time.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;The Graal EA improves on this with a technique called Partial Escape Analysis (PEA). In PEA, branches and loops do not interfere with the process of eliminating objects because all paths are considered. In addition, if there are boxed values eventually passed out of the compiled code, their allocation can be deferred until needed.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;JRuby on Graal&lt;/h2&gt;&lt;div style=&quot;text-align: left;&quot;&gt;By now you've likely heard about TruffleRuby, a Graal-based Ruby implementation that uses the Truffle framework for implementing all core language features and classes. Truffle provides many cool features for specializing data structures and making sure code inlines, but many of the optimizations TR users see is due to Graal doing such a great job of eliminating transient objects.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;And because Graal is also open source and available in Java 10 and higher, JRuby can see some of those benefits!&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;There's two easy ways for you to test out JRuby on the Graal JIT&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Using Java 10 with Graal &lt;/h3&gt;&lt;div style=&quot;text-align: left;&quot;&gt;OpenJDK 9 included a new feature to pre-compile Java code to native (with the &quot;jaotc&quot; command), and this compiler made use of Graal. In OpenJDK 10, Graal is now included even on platforms where the ahead-of-time compiler is not supported.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;http://www.oracle.com/technetwork/java/javase/downloads/jdk10-downloads-4416644.html&quot;&gt;Download and install any OpenJDK 10&lt;/a&gt; (or higher) release, and pass these flags to the JVM (either with -J flags to JRuby or using the JAVA_OPTS environment variable):&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;-XX:+UnlockExperimentalVMOptions&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;-XX:+EnableJVMCI&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;-XX:+UseJVMCICompiler&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Using GraalVM&lt;/h3&gt;&lt;div style=&quot;text-align: left;&quot;&gt;GraalVM is a new build of OpenJDK that includes Graal and Truffle by default. Depending on which one you use (community or enterprise edition) you may also have access to additional proprietary optimizations.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;GraalVM can be downloaded in community form for Linux and enterprise form for Linux and MacOS from &lt;a href=&quot;https://www.graalvm.org/&quot;&gt;graalvm.org&lt;/a&gt;. Install it and set it up as the JVM JRuby will run with (JAVA_HOME and/or PATH, as usual). It will use the Graal JIT by default.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Additional JRuby Flags&lt;/h3&gt;&lt;div style=&quot;text-align: left;&quot;&gt;You will also want to include some JRuby flags that help us optimize Ruby code in ways that work better on Graal:&lt;/div&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;-Xcompile.invokedynamic enables our use of the InvokeDynamic feature, which lets dynamic calls inline and optimize like static Java calls.&lt;/li&gt;&lt;li&gt;-Xfixnum.cache=false disables our cache of small Fixnum objects. Using the cache helps on Hotspot, which has no reliable escape analysis, but having those objects floating around sometimes confuses Graal's partial escape analysis. Try your code with the cache on and off and let us know how it affects performance.&lt;/li&gt;&lt;/ul&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;What to Expect&lt;/h2&gt;&lt;div style=&quot;text-align: left;&quot;&gt;We have only been exploring how JRuby can make use of Graal over the past few months, but we're already seeing positive results on some benchmarks. However the optimizations we want to see are heavily dependent on inlining all code, an area where we need some work. I present two results here, one where Graal is working like we expect, and another where we're not yet seeing our best results.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Numeric Algorithms Look Good&lt;/h3&gt;&lt;div style=&quot;text-align: left;&quot;&gt;One of the biggest areas JRuby performance suffers is in numeric algorithms. On a typical Hotspot-based JVM, all Fixnum and Float objects are actually object boxes that must be allocated and garbage collected like any other object. As you'd expect, this means that numeric algorithms pay a very high cost. This also means that Graal's partial escape analysis gives us huge gains, because all those boxes get swept away.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-NwkSJ4sYN24/WzvkhwwrEWI/AAAAAAABfFA/jB9mui7_Cn07FSozea_StktVS76kB4wiACLcBGAs/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B3.58.48%2BPM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;852&quot; data-original-width=&quot;1600&quot; height=&quot;211&quot; src=&quot;https://3.bp.blogspot.com/-NwkSJ4sYN24/WzvkhwwrEWI/AAAAAAABfFA/jB9mui7_Cn07FSozea_StktVS76kB4wiACLcBGAs/s400/Screen%2BShot%2B2018-07-03%2Bat%2B3.58.48%2BPM.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;div style=&quot;text-align: left;&quot;&gt;This first result is from a pure-Ruby Mandelbrot fractal-generating algorithm that makes the rounds periodically. The math here is almost all floating-point, with a few integers thrown in, but the entire algorithm fits in a single method. With JRuby using invokedynamic and running on Graal, all the code inlines and optimizes like a native numeric algorithm! Hooray for partial escape analysis!&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;We also have anecdotal reports of other numeric benchmarks performing significantly better with JRuby on Graal than JRuby on Hotspot...and in some cases, JRuby on Graal is the fastest result available!&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Data Structures Need More Work&lt;/h3&gt;&lt;div style=&quot;text-align: left;&quot;&gt;Of course most applications don't just work with numbers. They usually have a graph of objects in memory they need to traverse, search, create and destroy. In many cases, those objects include Array and Hash instances rather than user-defined objects, and frequently these structures are homogeneous: they contain only numbers, for example.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;JRuby currently does not do everything it could to inline object creation and access. We also are not doing any numeric specialization of structures like Array, which means a list of Fixnums actually has to allocate all those Fixnum objects and hold them in memory. These are areas we intend to work on; I am currently looking at doing some minimal specialization of algorithmic code and numeric data structures, and we will release some specialization code for instance variables (right-sizing the object rather than using a separate array to hold instance variables) in JRuby 9.2.1.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-FQswQpqF_64/WzvqWhCRc_I/AAAAAAABfFM/ebRE01fIBVssNW3SDXpGR09ecfGRLyM9gCLcBGAs/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B4.27.39%2BPM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;853&quot; data-original-width=&quot;1600&quot; height=&quot;212&quot; src=&quot;https://4.bp.blogspot.com/-FQswQpqF_64/WzvqWhCRc_I/AAAAAAABfFM/ebRE01fIBVssNW3SDXpGR09ecfGRLyM9gCLcBGAs/s400/Screen%2BShot%2B2018-07-03%2Bat%2B4.27.39%2BPM.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div style=&quot;text-align: left;&quot;&gt;The red/black benchmark tests the performance of a pure-Ruby red/black tree implementation. It creates a large graph, traverses it, searches it, and clears it. JRuby with InvokeDynamic on Hotspot still provides the best result here, perhaps because the extra magic of Graal is not utilized well.&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-FyekceTtJZQ/Wzvq03CN24I/AAAAAAABfFU/Y9uZ_PE8VWw4_sn0vWyyfosl2So5If4rgCLcBGAs/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B4.29.35%2BPM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;840&quot; data-original-width=&quot;1600&quot; height=&quot;208&quot; src=&quot;https://1.bp.blogspot.com/-FyekceTtJZQ/Wzvq03CN24I/AAAAAAABfFU/Y9uZ_PE8VWw4_sn0vWyyfosl2So5If4rgCLcBGAs/s400/Screen%2BShot%2B2018-07-03%2Bat%2B4.29.35%2BPM.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;This benchmark of ActiveRecord shows the opposite result: JRuby on Graal gets the best performance. Without having dug into the details, I'd guess there's some hot loop code in the middle of the &quot;find all&quot; logic, and that loop is optimizing well on Graal. But ultimately the objects all need to be created and the gain from Graal is fairly limited. I also have examples of other read, write, and update operations and only about half of them are faster with Graal.&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Your Turn&lt;/h2&gt;&lt;div style=&quot;text-align: left;&quot;&gt;The JRuby team (of which only two of us are actually employed to work on JRuby) has always managed resources with an eye for compatibility first. This has meant that JRuby performance -- while usually solid and usually faster than CRuby -- has received much less attention than some other Ruby implementations. But it turns out we can get really solid performance simply by inlining all appropriate code, specializing appropriate data structures, and running atop a JIT with good escape analysis.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;We will be working over the next few months to better leverage Graal. In the mean time, we'd love to hear from JRuby users about their experiences with JRuby on Graal! If your code runs faster, let us know so we can tell others. If your code runs slower, let us know so we can try to improve it. And if you're interested in comparing with other Ruby implementations, just make sure your benchmark reflects a real-world case and doesn't simply optimize down to nothing.&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;text-align: left;&quot;&gt;JRuby on Graal has great promise for the future. Try it out and let us know how it goes!&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;</content><author><name>headius</name></author><summary type="html">Hello, friends! Long time no blog!I'm still here hacking away on JRuby for the benefit of Rubyists everywhere, usually slogging through compatibility fixes and new Ruby features. However with the release of JRuby 9.2, we've caught up to Ruby 2.5 (the current release) and I'm spending a little time on performance.I thought today would be a good opportunity to show you how to start exploring some next-generation JRuby performance by running on top of the Graal JIT.GraalThe Graal JIT is a project that grew out of a Sun Microsystem Labs project called MaxineVM. Maxine was an attempt to implement a JVM entirely in Java...via various specialized Java dialects, annotations, and compilation strategies. Graal is the latest iteration of the JIT work done for Maxine, and provides an optimizing JVM JIT implemented entirely in Java. Maxine only lives on as a research project, but Graal is rapidly shaping up to become the preferred JIT in OpenJDK.The majority of optimizations that Graal does to code are no different from the &quot;classic&quot; OpenJDK JIT, Hotspot's &quot;C2&quot; compiler. You get all the usual dead code, loop unrolling, method inlining, branch profiling, and so on. However Graal goes beyond Hotspot in a few key ways. Most interesting to JRuby (and other dynamic languages) is the fact that Graal finally brings good Escape Analysis to the JVM.Escape AnalysisThe biggest gains JRuby sees running on Graal are from Escape Analysis (EA). The basic idea behind escape analysis goes like this: if you allocate an object, use it and its contents, and then abandon that object all within the same thread-local piece of code (i.e. not storing that object in a globally-visible location) then there's no reason to allocate the object. Think about this in terms of Java's autoboxing or varargs: if you box arguments or numbers, pass them to a method, and then unbox them again...the intermediate box was not really necessary.The basic idea of EA is not complicated to understand, but implementing well can be devilishly hard. For example, the Hotspot JIT has had a form of escape analysis for years, optional in Java 7 and I believe turned on by default during Java 8 maintenance releases. However this EA was very limited...if any wrapper object was used across a branch (even a simple if/else) or if it might under any circumstance leave the compiled code being optimized, it would be forced to allocate every time.The Graal EA improves on this with a technique called Partial Escape Analysis (PEA). In PEA, branches and loops do not interfere with the process of eliminating objects because all paths are considered. In addition, if there are boxed values eventually passed out of the compiled code, their allocation can be deferred until needed.JRuby on GraalBy now you've likely heard about TruffleRuby, a Graal-based Ruby implementation that uses the Truffle framework for implementing all core language features and classes. Truffle provides many cool features for specializing data structures and making sure code inlines, but many of the optimizations TR users see is due to Graal doing such a great job of eliminating transient objects.And because Graal is also open source and available in Java 10 and higher, JRuby can see some of those benefits!There's two easy ways for you to test out JRuby on the Graal JITUsing Java 10 with Graal OpenJDK 9 included a new feature to pre-compile Java code to native (with the &quot;jaotc&quot; command), and this compiler made use of Graal. In OpenJDK 10, Graal is now included even on platforms where the ahead-of-time compiler is not supported.Download and install any OpenJDK 10 (or higher) release, and pass these flags to the JVM (either with -J flags to JRuby or using the JAVA_OPTS environment variable):-XX:+UnlockExperimentalVMOptions-XX:+EnableJVMCI-XX:+UseJVMCICompilerUsing GraalVMGraalVM is a new build of OpenJDK that includes Graal and Truffle by default. Depending on which one you use (community or enterprise edition) you may also have access to additional proprietary optimizations.GraalVM can be downloaded in community form for Linux and enterprise form for Linux and MacOS from graalvm.org. Install it and set it up as the JVM JRuby will run with (JAVA_HOME and/or PATH, as usual). It will use the Graal JIT by default.Additional JRuby FlagsYou will also want to include some JRuby flags that help us optimize Ruby code in ways that work better on Graal:-Xcompile.invokedynamic enables our use of the InvokeDynamic feature, which lets dynamic calls inline and optimize like static Java calls.-Xfixnum.cache=false disables our cache of small Fixnum objects. Using the cache helps on Hotspot, which has no reliable escape analysis, but having those objects floating around sometimes confuses Graal's partial escape analysis. Try your code with the cache on and off and let us know how it affects performance.What to ExpectWe have only been exploring how JRuby can make use of Graal over the past few months, but we're already seeing positive results on some benchmarks. However the optimizations we want to see are heavily dependent on inlining all code, an area where we need some work. I present two results here, one where Graal is working like we expect, and another where we're not yet seeing our best results.Numeric Algorithms Look GoodOne of the biggest areas JRuby performance suffers is in numeric algorithms. On a typical Hotspot-based JVM, all Fixnum and Float objects are actually object boxes that must be allocated and garbage collected like any other object. As you'd expect, this means that numeric algorithms pay a very high cost. This also means that Graal's partial escape analysis gives us huge gains, because all those boxes get swept away.This first result is from a pure-Ruby Mandelbrot fractal-generating algorithm that makes the rounds periodically. The math here is almost all floating-point, with a few integers thrown in, but the entire algorithm fits in a single method. With JRuby using invokedynamic and running on Graal, all the code inlines and optimizes like a native numeric algorithm! Hooray for partial escape analysis!We also have anecdotal reports of other numeric benchmarks performing significantly better with JRuby on Graal than JRuby on Hotspot...and in some cases, JRuby on Graal is the fastest result available!Data Structures Need More WorkOf course most applications don't just work with numbers. They usually have a graph of objects in memory they need to traverse, search, create and destroy. In many cases, those objects include Array and Hash instances rather than user-defined objects, and frequently these structures are homogeneous: they contain only numbers, for example.JRuby currently does not do everything it could to inline object creation and access. We also are not doing any numeric specialization of structures like Array, which means a list of Fixnums actually has to allocate all those Fixnum objects and hold them in memory. These are areas we intend to work on; I am currently looking at doing some minimal specialization of algorithmic code and numeric data structures, and we will release some specialization code for instance variables (right-sizing the object rather than using a separate array to hold instance variables) in JRuby 9.2.1.The red/black benchmark tests the performance of a pure-Ruby red/black tree implementation. It creates a large graph, traverses it, searches it, and clears it. JRuby with InvokeDynamic on Hotspot still provides the best result here, perhaps because the extra magic of Graal is not utilized well.This benchmark of ActiveRecord shows the opposite result: JRuby on Graal gets the best performance. Without having dug into the details, I'd guess there's some hot loop code in the middle of the &quot;find all&quot; logic, and that loop is optimizing well on Graal. But ultimately the objects all need to be created and the gain from Graal is fairly limited. I also have examples of other read, write, and update operations and only about half of them are faster with Graal.Your TurnThe JRuby team (of which only two of us are actually employed to work on JRuby) has always managed resources with an eye for compatibility first. This has meant that JRuby performance -- while usually solid and usually faster than CRuby -- has received much less attention than some other Ruby implementations. But it turns out we can get really solid performance simply by inlining all appropriate code, specializing appropriate data structures, and running atop a JIT with good escape analysis.We will be working over the next few months to better leverage Graal. In the mean time, we'd love to hear from JRuby users about their experiences with JRuby on Graal! If your code runs faster, let us know so we can tell others. If your code runs slower, let us know so we can try to improve it. And if you're interested in comparing with other Ruby implementations, just make sure your benchmark reflects a real-world case and doesn't simply optimize down to nothing.JRuby on Graal has great promise for the future. Try it out and let us know how it goes!</summary></entry><entry><title type="html">Migrating to Java 9: Modules, Maven, OSGI, Travis CI</title><link href="https://headius.github.io/2017/10/migrating-to-java-9-modules-maven-osgi.html" rel="alternate" type="text/html" title="Migrating to Java 9: Modules, Maven, OSGI, Travis CI"><published>2017-10-24T15:55:00+00:00</published><updated>2017-10-24T15:55:00+00:00</updated><id>https://headius.github.io/2017/10/migrating-to-java-9-modules-maven-osgi</id><content type="html" xml:base="https://headius.github.io/2017/10/migrating-to-java-9-modules-maven-osgi.html">&lt;div dir=&quot;ltr&quot; style=&quot;text-align: left;&quot; trbidi=&quot;on&quot;&gt;Hello friends! It has been too long!&lt;br /&gt;&lt;br /&gt;Today, after many years, I've got something I wanted to blog rather than tweet out in code snippits and screenshots: I'm starting to get my projects working with Java 9.&lt;br /&gt;&lt;br /&gt;I hope to cover all of the challenges and solutions I've come up with, but today I'll be focusing on something &quot;simple&quot;: getting a straightforward Maven project to export a Java 9 module while still working on Java 8 and lower.&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;InvokeBinder&lt;/h2&gt;&lt;div&gt;Some years ago I started work on a library called &lt;a href=&quot;https://github.com/headius/invokebinder&quot;&gt;InvokeBinder&lt;/a&gt; (com.headius:invokebinder), a fluent wrapper around the Java 7 &quot;MethodHandles&quot; API in java.lang.invoke. InvokeBinder provides a more straightforward way to manipulate method handles, juggle arguments, and debug problems.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;It's also a nice, simple library to try to get into Java 9.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;module-info.java&lt;/h2&gt;&lt;div&gt;The main stumbling block for exposing a module is this new file format in module-info.java:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/headius/259bdf9493ac1f03596d517898391b66.js?file=module-info.java&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;Obviously this is not something we can compile on Java 8, so we will need to use a Java 9-compatible toolchain to build at least this one file.&lt;br /&gt;&lt;br /&gt;&lt;script src=&quot;https://gist.github.com/headius/259bdf9493ac1f03596d517898391b66.js?file=module_java_7_error.txt&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;In my case, I also want the rest of the library to work on Java 8, and since compiling module-info.java requires the target class files to be Java 9 format or higher, I'll need to compile everything &lt;b&gt;except&lt;/b&gt;&amp;nbsp;module-info.java separately.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Tool Support for Java 9 is Still Weak&lt;/h2&gt;&lt;div&gt;I deferred exploring Java 9 until its release because of weak tool support. Now that I'm forced to deal with Java 9 I'm constantly stymied by weak tool support.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Most IDEs have some support for Java 9's syntactic changes, and of course accessing libraries from a Java 9 install is as easy as it was on previous versions. But the structural changes for Java 9: modules, multi-release jars, linking, and ahead-of-time compilation (which is admittedly experimental) generally do not exist.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;How bad is it? Even NetBeans, Oracle's flagship IDE, usually the fastest free way to access the latest and greatest Java features...doesn't support these Java 9 features well at all (NetBeans 9 is &lt;a href=&quot;http://services.netbeans.org/dashboard/web/&quot;&gt;still in development&lt;/a&gt;)&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;I generally use IntelliJ IDEA, which has always been ahead of the curve on supporting new Java features, and things are somewhat better here, You can define module definitions, but generally can't split JDK versions for a single source tree, the standard layout for a single module.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;(Full disclosure: because I'm just trying to support Java 9 now, and I'm mostly migrating additional projects, I've only just started to figure out what features are supported in which IDs and how well; corrections and updates in comments are welcome.)&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Because the IDE space is moving quickly, I won't go into how to get your IDE of choice working nicely with Java 9 structural features. As I cross that bridge, I'll try to post about it.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Maven?&lt;/h2&gt;&lt;div&gt;Yes, you guessed it: like a majority of projects in the Java world, InvokeBinder still builds with Maven. For my purposes, it's the simplest way to get an artifact build and deployed, and Maven central is still the canonical place people look for libraries.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Maven is made up of thousands of little plugins and libraries, which makes updating any Maven project for a new JDK version an exercise in pain. When I first looked into Java 9 support some months ago, I basically had to give up; too many plugins I use hadn't updated, and in most cases there was no way to work around the incompatibilities.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Thankfully, most core Maven plugins now appear to work properly with Java 9, though most have not started to expose those structural features I discuss above.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;To build invokebinder's module targeting Java 9 and build everything else targeting Java 8 required a newer version of the maven-compiler-plugin and some manual configuration, roughly described on this &lt;a href=&quot;https://maven.apache.org/plugins/maven-compiler-plugin/examples/module-info.html&quot;&gt;example page for module-info.java&lt;/a&gt;. In my case, I did not need to support Java versions earlier than Java 7, so the split toolkit configuration was unnecessary.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/headius/259bdf9493ac1f03596d517898391b66.js?file=maven-compiler-plugin.xml&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;With this change, I was able to get my jar to export a module, and it still works properly when run on Java 7 and 8.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;OSGI&lt;/h2&gt;&lt;div&gt;Before Java 9 modules, the typical fine-grained runtime dependency system of choice has been OSGI, and InvokeBinder does indeed export an OSGI package.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Unfortunately, the plugin I'm using for doing this (the &lt;a href=&quot;http://felix.apache.org/documentation/subprojects/apache-felix-maven-bundle-plugin-bnd.html&quot;&gt;maven-bundle-plugin from the Felix project&lt;/a&gt;) has not been released in some time, and the version of the &quot;bnd&quot; library it uses does not work properly on Java 9.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;script src=&quot;https://gist.github.com/headius/259bdfc1f03596d517898391b66.js?file=bnd_error.txt&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;A bit more manual configuration lets us force the plugin to use a newer version of the library, and everything now works: Java 8 or 9 with OSGI and Java 9 with modules!&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;script src=&quot;https://gist.github.com/headius/259bdf9493ac1f03596d517898391b66.js?file=maven-bundle-plugin.xml&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Update: Travis CI&lt;/h2&gt;&lt;div&gt;After finishing the changes needed above, I realized I needed to get this to build and work on Travis CI, which I use to test InvokeBinder.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Luckily, though &lt;a href=&quot;https://github.com/travis-ci/travis-ci/issues/5520&quot;&gt;Java 9 support in Travis&lt;/a&gt; is still a bit manual, it's not a difficult change:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;script src=&quot;https://gist.github.com/headius/259bdf9493ac1f03596d517898391b66.js?file=.travis.yml.diff&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;What's Next?&lt;/h2&gt;&lt;div&gt;I am both excited about Java 9 features and required to make JRuby and my other projects work with them, so this will be one of my primary projects over the next few months. I look forward to hearing how you are supporting Java 9, so please comment with corrections, tips, and updates on anything I've posted here.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;In future posts I'll talk about getting multi-release jars, ahead-of-time compilation and linking, and JRuby itself running on Java 9. Stay tuned!&lt;/div&gt;&lt;/div&gt;</content><author><name>headius</name></author><summary type="html">Hello friends! It has been too long!Today, after many years, I've got something I wanted to blog rather than tweet out in code snippits and screenshots: I'm starting to get my projects working with Java 9.I hope to cover all of the challenges and solutions I've come up with, but today I'll be focusing on something &quot;simple&quot;: getting a straightforward Maven project to export a Java 9 module while still working on Java 8 and lower.InvokeBinderSome years ago I started work on a library called InvokeBinder (com.headius:invokebinder), a fluent wrapper around the Java 7 &quot;MethodHandles&quot; API in java.lang.invoke. InvokeBinder provides a more straightforward way to manipulate method handles, juggle arguments, and debug problems.It's also a nice, simple library to try to get into Java 9.module-info.javaThe main stumbling block for exposing a module is this new file format in module-info.java:Obviously this is not something we can compile on Java 8, so we will need to use a Java 9-compatible toolchain to build at least this one file.In my case, I also want the rest of the library to work on Java 8, and since compiling module-info.java requires the target class files to be Java 9 format or higher, I'll need to compile everything except&amp;nbsp;module-info.java separately.Tool Support for Java 9 is Still WeakI deferred exploring Java 9 until its release because of weak tool support. Now that I'm forced to deal with Java 9 I'm constantly stymied by weak tool support.Most IDEs have some support for Java 9's syntactic changes, and of course accessing libraries from a Java 9 install is as easy as it was on previous versions. But the structural changes for Java 9: modules, multi-release jars, linking, and ahead-of-time compilation (which is admittedly experimental) generally do not exist.How bad is it? Even NetBeans, Oracle's flagship IDE, usually the fastest free way to access the latest and greatest Java features...doesn't support these Java 9 features well at all (NetBeans 9 is still in development)I generally use IntelliJ IDEA, which has always been ahead of the curve on supporting new Java features, and things are somewhat better here, You can define module definitions, but generally can't split JDK versions for a single source tree, the standard layout for a single module.(Full disclosure: because I'm just trying to support Java 9 now, and I'm mostly migrating additional projects, I've only just started to figure out what features are supported in which IDs and how well; corrections and updates in comments are welcome.)Because the IDE space is moving quickly, I won't go into how to get your IDE of choice working nicely with Java 9 structural features. As I cross that bridge, I'll try to post about it.Maven?Yes, you guessed it: like a majority of projects in the Java world, InvokeBinder still builds with Maven. For my purposes, it's the simplest way to get an artifact build and deployed, and Maven central is still the canonical place people look for libraries.Maven is made up of thousands of little plugins and libraries, which makes updating any Maven project for a new JDK version an exercise in pain. When I first looked into Java 9 support some months ago, I basically had to give up; too many plugins I use hadn't updated, and in most cases there was no way to work around the incompatibilities.Thankfully, most core Maven plugins now appear to work properly with Java 9, though most have not started to expose those structural features I discuss above.To build invokebinder's module targeting Java 9 and build everything else targeting Java 8 required a newer version of the maven-compiler-plugin and some manual configuration, roughly described on this example page for module-info.java. In my case, I did not need to support Java versions earlier than Java 7, so the split toolkit configuration was unnecessary.With this change, I was able to get my jar to export a module, and it still works properly when run on Java 7 and 8.OSGIBefore Java 9 modules, the typical fine-grained runtime dependency system of choice has been OSGI, and InvokeBinder does indeed export an OSGI package.Unfortunately, the plugin I'm using for doing this (the maven-bundle-plugin from the Felix project) has not been released in some time, and the version of the &quot;bnd&quot; library it uses does not work properly on Java 9.A bit more manual configuration lets us force the plugin to use a newer version of the library, and everything now works: Java 8 or 9 with OSGI and Java 9 with modules!Update: Travis CIAfter finishing the changes needed above, I realized I needed to get this to build and work on Travis CI, which I use to test InvokeBinder.Luckily, though Java 9 support in Travis is still a bit manual, it's not a difficult change:What's Next?I am both excited about Java 9 features and required to make JRuby and my other projects work with them, so this will be one of my primary projects over the next few months. I look forward to hearing how you are supporting Java 9, so please comment with corrections, tips, and updates on anything I've posted here.In future posts I'll talk about getting multi-release jars, ahead-of-time compilation and linking, and JRuby itself running on Java 9. Stay tuned!</summary></entry><entry><title type="html">JRubyConf.eu 2014!</title><link href="https://headius.github.io/2014/05/jrubyconfeu-2014.html" rel="alternate" type="text/html" title="JRubyConf.eu 2014!"><published>2014-05-21T17:44:00+00:00</published><updated>2014-05-21T17:44:00+00:00</updated><id>https://headius.github.io/2014/05/jrubyconfeu-2014</id><content type="html" xml:base="https://headius.github.io/2014/05/jrubyconfeu-2014.html">&lt;div dir=&quot;ltr&quot; style=&quot;text-align: left;&quot; trbidi=&quot;on&quot;&gt;I'm thrilled to announce that we'll have another edition of &lt;a href=&quot;http://2014.jrubyconf.eu/&quot;&gt;JRubyConf.eu&lt;/a&gt;&amp;nbsp;this year!&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://2013.jrubyconf.eu/&quot;&gt;Last year's event&lt;/a&gt; was a great success. We had a two-day conference in Berlin immediately before &lt;a href=&quot;http://2013.eurucamp.org/&quot;&gt;Eurucamp 2013&lt;/a&gt;, with two speakers from the core team (myself and &lt;a href=&quot;http://twitter.com/tom_enebo&quot;&gt;Tom Enebo&lt;/a&gt;) and a whopping &lt;b&gt;fifteen&lt;/b&gt;&amp;nbsp;non-core speakers. A great event was had by all.&lt;br /&gt;&lt;br /&gt;This year, we've decided to pull the event back to its roots, as part of &lt;a href=&quot;http://2014.eurucamp.org/&quot;&gt;Eurucamp 2014&lt;/a&gt;. We'll return to the single-track, single-day event co-located with and immediately preceding Eurucamp on 1st August. We really wanted to bring JRuby back to Rubyists, and we're looking forward to hanging out at Eurucamp the whole weekend!&lt;br /&gt;&lt;br /&gt;Why not visit Eurucamp early and spend a day learning about JRuby with the best JRubyists in Europe?&lt;br /&gt;&lt;br /&gt;If you're interested in attending, tickets are available for only €99 at the &lt;a href=&quot;http://tickets.eurucamp.org/&quot;&gt;Eurucamp ticket site&lt;/a&gt; now!&lt;br /&gt;&lt;br /&gt;We're also looking for speakers from the JRuby community. You can submit to the CFP (which ends Sunday 28 May) using the &lt;a href=&quot;http://cfp.eurucamp.org/&quot;&gt;Eurucamp CFP app&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Looking forward to seeing you at JRubyConf and Eurucamp this summer!&lt;/div&gt;</content><author><name>headius</name></author><summary type="html">I'm thrilled to announce that we'll have another edition of JRubyConf.eu&amp;nbsp;this year!Last year's event was a great success. We had a two-day conference in Berlin immediately before Eurucamp 2013, with two speakers from the core team (myself and Tom Enebo) and a whopping fifteen&amp;nbsp;non-core speakers. A great event was had by all.This year, we've decided to pull the event back to its roots, as part of Eurucamp 2014. We'll return to the single-track, single-day event co-located with and immediately preceding Eurucamp on 1st August. We really wanted to bring JRuby back to Rubyists, and we're looking forward to hanging out at Eurucamp the whole weekend!Why not visit Eurucamp early and spend a day learning about JRuby with the best JRubyists in Europe?If you're interested in attending, tickets are available for only €99 at the Eurucamp ticket site now!We're also looking for speakers from the JRuby community. You can submit to the CFP (which ends Sunday 28 May) using the Eurucamp CFP app.Looking forward to seeing you at JRubyConf and Eurucamp this summer!</summary></entry><entry><title type="html">The Pain of Broken Subprocess Management on JDK</title><link href="https://headius.github.io/2013/06/the-pain-of-broken-subprocess.html" rel="alternate" type="text/html" title="The Pain of Broken Subprocess Management on JDK"><published>2013-06-07T08:58:00+00:00</published><updated>2013-06-07T08:58:00+00:00</updated><id>https://headius.github.io/2013/06/the-pain-of-broken-subprocess</id><content type="html" xml:base="https://headius.github.io/2013/06/the-pain-of-broken-subprocess.html">&lt;script type=&quot;text/javascript&quot;&gt;SyntaxHighlighter.defaults.gutter = false;&lt;/script&gt;&lt;div dir=&quot;ltr&quot; style=&quot;text-align: left;&quot; trbidi=&quot;on&quot;&gt;I prefer to write happy posts...I really do. But tonight I'm completely defeated by the JDK's implementation of subprocess launching, and I need to tell the world why.&lt;br /&gt;&lt;br /&gt;JRuby has always strived to mimic MRI's behavior as much as possible, which in many cases has meant we need to route around the JDK to get at true POSIX APIs and behaviors.&lt;br /&gt;&lt;br /&gt;For example, JRuby has provided the ability to manipulate symbolic links since well before Java 7 provided that capability, using a native POSIX subsystem built atop jnr-ffi, our Java-to-C FFI layer (courtesy of Wayne Meissner). Everyone in the Java world knew for years the lack of symlink support was a gross omission, but most folks just sucked it up and went about their business. We could not afford to do that.&lt;br /&gt;&lt;br /&gt;We've repeated this process for many other Ruby features: UNIX sockets, libc-like IO, selectable stdin, filesystem attributes...on and on. And we've been able to provide the best POSIX runtime on the JVM &lt;b&gt;bar none&lt;/b&gt;. Nobody has gone as far or done as much as JRuby has.&lt;br /&gt;&lt;br /&gt;Another area where we've had to route around the JDK is in subprocess launching and management. The JDK provides java.lang.ProcessBuilder, an API for assembling the appropriate pieces of a subprocess launch, producing a java.lang.Process object. Process in turn provides methods to wait for the subprocess, get access to its streams, and destroy it forcibly. It works great, on the surface.&lt;br /&gt;&lt;br /&gt;Unfortunately, the cake is a lie.&lt;br /&gt;&lt;br /&gt;Under the covers, the JDK implements Process through a complicated series of tricks. We want to be able to interactively control the child process, monitor it for writes, govern its lifecycle exactly. The JDK attempts to provide a consistent experience across all platforms. Unfortunately, those two worlds are not currently compatible, and the resulting experience is consistently awful.&lt;br /&gt;&lt;br /&gt;We'll start at the bottom to see where things go wrong.&lt;br /&gt;&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;POSIX, POSIX, Everywhere&lt;/h4&gt;&lt;br /&gt;At the core of ProcessBuilder, inside the native code behind UNIXProcess, we do find somewhat standard POSIX calls to fork and exec, wrapped up in a native downcall forkAndExec:&lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;script class=&quot;brush: java;&quot; type=&quot;syntaxhighlighter&quot;&gt;&lt;![CDATA[     /**      * Create a process using fork(2) and exec(2).      *      * @param std_fds array of file descriptors.  Indexes 0, 1, and      *        2 correspond to standard input, standard output and      *        standard error, respectively.  On input, a value of -1      *        means to create a pipe to connect child and parent      *        processes.  On output, a value which is not -1 is the      *        parent pipe fd corresponding to the pipe which has      *        been created.  An element of this array is -1 on input      *        if and only if it is &lt;em&gt;not&lt;/em&gt; -1 on output.      * @return the pid of the subprocess      */     private native int forkAndExec(byte[] prog,                                    byte[] argBlock, int argc,                                    byte[] envBlock, int envc,                                    byte[] dir,                                    int[] std_fds,                                    boolean redirectErrorStream) ]]&gt;&lt;/script&gt;&lt;br /&gt;&lt;/div&gt;The C code behind this is a bit involved, so I'll summarize what it does.&lt;br /&gt;&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Sets up pipes for in, out, err, and fail to communicate with the eventual child process.&lt;/li&gt;&lt;li&gt;Copies the parent's descriptors from the pipes into the &quot;fds&quot; array.&lt;/li&gt;&lt;li&gt;Launches the child through a fairly standard fork+exec sequence.&lt;/li&gt;&lt;li&gt;Waits for the child to write a byte to the fail pipe indicating success or failure.&lt;/li&gt;&lt;li&gt;Scrubs the unused sides of the pipes in parent and child.&lt;/li&gt;&lt;li&gt;Returns the child process ID.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;This is all pretty standard for subprocess launching, and if it proceeded to put those file descriptors into direct, selectable channels we'd have no issues. Unfortunately, things immediately go awry once we return to the Java code.&lt;br /&gt;&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Interactive?&lt;/h4&gt;&lt;br /&gt;The call to forkAndExec occurs inside the UNIXProcess constructor, as the very first thing it does. At that point, it has in hand the three standard file descriptors and the subprocess pid, and it knows that the subprocess has at least been successfully forked. The next step is to wrap the file descriptors in appropriate InputStream and OutputStream objects, and this is where we find the first flaw.&lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;script class=&quot;brush: java;&quot; type=&quot;syntaxhighlighter&quot;&gt;&lt;![CDATA[             if (std_fds[0] == -1)                 stdin_stream = ProcessBuilder.NullOutputStream.INSTANCE;             else {                 FileDescriptor stdin_fd = new FileDescriptor();                 fdAccess.set(stdin_fd, std_fds[0]);                 stdin_stream = new BufferedOutputStream(                     new FileOutputStream(stdin_fd));             } ]]&gt;&lt;/script&gt;&lt;br /&gt;&lt;/div&gt;This is the code to set up an OutputStream for the input channel of the child process, so we can write to it. Now we know the operating system is going to funnel those written bytes directly to the subprocess's input stream, and ideally if we're launching a subprocess we intend to control it...perhaps by sending it interactive commands. Why, then, do we wrap the file descriptor with a BufferedOutputStream? &lt;br /&gt;This is where JRuby's hacks begin. In our process subsystem, we have the following piece of code, which attempts to unwrap buffering from any stream it is given. &lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;script class=&quot;brush: java;&quot; type=&quot;syntaxhighlighter&quot;&gt;&lt;![CDATA[     /**      * Unwrap all filtering streams between the given stream and its actual      * unfiltered stream. This is primarily to unwrap streams that have      * buffers that would interfere with interactivity.      *      * @param filteredStream The stream to unwrap      * @return An unwrapped stream, presumably unbuffered      */     public static OutputStream unwrapBufferedStream(OutputStream filteredStream) {         if (RubyInstanceConfig.NO_UNWRAP_PROCESS_STREAMS) return filteredStream;         while (filteredStream instanceof FilterOutputStream) {             try {                 filteredStream = (OutputStream)                     FieldAccess.getProtectedFieldValue(FilterOutputStream.class,                         &quot;out&quot;, filteredStream);             } catch (Exception e) {                 break; // break out if we've dug as deep as we can             }         }         return filteredStream;     } ]]&gt;&lt;/script&gt;&lt;/div&gt;&lt;br /&gt;The FieldAccess.getProtectedFieldValue call there does what you think it does...attempt to read the &quot;out&quot; field from within FilteredOutputStream, which in this case will be the FileOutputStream from above. Unwrapping the stream in this way allows us to do two things:&lt;br /&gt;&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;We can do unbuffered writes to (or reads from, in the case of the child's out and err streams) the child process.&lt;/li&gt;&lt;li&gt;We can get access to the more direct FileChannel for the stream, to do direct ByteBuffer reads and writes or low-level stream copying.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;So we're in good shape, right? It's a bit of hackery, but we've got our unbuffered Channel and can interact directly with the subprocess. Is this good enough?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;I wish it were.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Selectable?&lt;/h4&gt;&lt;div&gt;&lt;br /&gt;The second problem we run into is that users very often would like to select against the output streams of the child process, to perform nonblocking IO operations until the child has actually written some data. It gets reported as a JRuby bug over and over again because there's simply no way for us to implement it. Why? Because FileChannel is not selectable. &lt;br /&gt;&lt;br /&gt;&lt;script class=&quot;brush: java;&quot; type=&quot;syntaxhighlighter&quot;&gt;&lt;![CDATA[ public abstract class FileChannel     extends AbstractInterruptibleChannel     implements SeekableByteChannel, GatheringByteChannel, ScatteringByteChannel ]]&gt;&lt;/script&gt;&lt;br /&gt;FileChannel implements methods for random-access reads and writes (positioning) and blocking IO interruption (which NIO implements by closing the stream...that's a rant for another day), but it does not implement any of the logic necessary for doing nonblocking IO using an NIO Selector. This comes up in at least one other place: the JVM's own standard IO streams are also not selectable, which means you can't select for user input at the console. Consistent experience indeed...it seems that all interaction with the user or with processes must be treated as file IO, with no selection capabilities. &lt;br /&gt;&lt;br /&gt;(It is interesting to note that the JVM's standard IO streams are *also* wrapped in buffers, which we dutifully unwrap to provide a truly interactive console.) &lt;br /&gt;&lt;br /&gt;Why are inter-proces file descriptors, which would support selector operations just wonderfully, wrapped in an unselectable channel? I have no idea, and it's impossible for us to hack around. &lt;br /&gt;&lt;br /&gt;Let's not dwell on this item, since there's more to cover. &lt;br /&gt;&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Fear the Reaper&lt;/h4&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;You may recall I also wanted to have direct control over the lifecycle of the subprocess, to be able to wait for it or kill it at my own discretion. And on the surface, Process appears to provide these capabilities via the waitFor() and destroy() methods. Again it's all smoke and mirrors.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Further down in the UNIXProcess constructor, you'll find this curious piece of code:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script class=&quot;brush: java;&quot; type=&quot;syntaxhighlighter&quot;&gt;&lt;![CDATA[         /*          * For each subprocess forked a corresponding reaper thread          * is started.  That thread is the only thread which waits          * for the subprocess to terminate and it doesn't hold any          * locks while doing so.  This design allows waitFor() and          * exitStatus() to be safely executed in parallel (and they          * need no native code).          */          java.security.AccessController.doPrivileged(             new java.security.PrivilegedAction&lt;void&gt;() { public Void run() {                 Thread t = new Thread(&quot;process reaper&quot;) {                     public void run() {                         int res = waitForProcessExit(pid);                         synchronized (UNIXProcess.this) {                             hasExited = true;                             exitcode = res;                             UNIXProcess.this.notifyAll();                         }                     }                 };                 t.setDaemon(true);                 t.start();                 return null; }}); ]]&gt;&lt;/script&gt; &lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;For each subprocess started through this API, the JVM will spin up a &quot;process reaper&quot; thread. This thread is designed to monitor the subprocess for liveness and notify the parent UNIXProcess object when that process has died, so it can pass on that information to the user via the waitFor() and exitValue() API calls.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The interesting bit here is the waitForProcessExit(pid) call, which is another native downcall into C land:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;script class=&quot;brush: c;&quot; type=&quot;syntaxhighlighter&quot;&gt;&lt;![CDATA[ /* Block until a child process exits and return its exit code.    Note, can only be called once for any given pid. */ JNIEXPORT jint JNICALL Java_java_lang_UNIXProcess_waitForProcessExit(JNIEnv* env,                                               jobject junk,                                               jint pid) {     /* We used to use waitid() on Solaris, waitpid() on Linux, but      * waitpid() is more standard, so use it on all POSIX platforms. */     int status;     /* Wait for the child process to exit.  This returns immediately if        the child has already exited. */     while (waitpid(pid, &amp;status, 0) &lt; 0) {         switch (errno) {         case ECHILD: return 0;         case EINTR: break;         default: return -1;         }     }      if (WIFEXITED(status)) {         /*          * The child exited normally; get its exit code.          */         return WEXITSTATUS(status);     } else if (WIFSIGNALED(status)) {         /* The child exited because of a signal.          * The best value to return is 0x80 + signal number,          * because that is what all Unix shells do, and because          * it allows callers to distinguish between process exit and          * process death by signal.          * Unfortunately, the historical behavior on Solaris is to return          * the signal number, and we preserve this for compatibility. */ #ifdef __solaris__         return WTERMSIG(status); #else         return 0x80 + WTERMSIG(status); #endif     } else {         /*          * Unknown exit code; pass it through.          */         return status;     } } ]]&gt;&lt;/script&gt;&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;There's nothing too peculiar here; this is how you'd wait for the child process to exit if you were writing plain old C code. But there's a sinister detail you can't see just by looking at this code: waitpid can be called &lt;b&gt;exactly once&lt;/b&gt;&amp;nbsp;by the parent process.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Part of the Ruby Process API is the ability to get a subprocess PID and wait for it. The concept of a process ID has been around for a long time, and Rubyists (even amateur Rubyists who've never written a line of C code) don't seem to have any problem calling Process.waitpid when they want to wait for a child to exit. JRuby is an implementation of Ruby, and we would ideally like to be able to run all Ruby code that exists, so we also must implement Process.waitpid in some reasonable way. Our choice was to literally call the C function waitpid(2) via our FFI layer.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here's the subtle language from the wait(2) manpage (which includes waitpid):&lt;/div&gt;&lt;div&gt;&lt;pre&gt;&lt;br /&gt;RETURN VALUES&lt;br /&gt;     If wait() returns due to a stopped or terminated child&lt;br /&gt;     process, the process ID of the child is returned to the&lt;br /&gt;     calling process.  Otherwise, a value of -1 is returned&lt;br /&gt;     and errno is set to indicate the error.&lt;br /&gt;&lt;br /&gt;     If wait3(), wait4(), or waitpid() returns due to a&lt;br /&gt;     stopped or terminated child process, the process ID of&lt;br /&gt;     the child is returned to the calling process.  If there&lt;br /&gt;     are no children not previously awaited, -1 is returned&lt;br /&gt;     with errno set to [ECHILD].  Otherwise, if WNOHANG is&lt;br /&gt;     specified and there are no stopped or exited children,&lt;br /&gt;     0 is returned. If an error is detected or a caught&lt;br /&gt;     signal aborts the call, a value of -1 is returned and&lt;br /&gt;     errno is set to indicate the error.&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div&gt;There's a lot of negatives and passives and conditions there, so I'll spell it out for you more directly: If you call waitpid for a given child PID and someone else in your process has already done so...bad things happen.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We effectively have to race the JDK to the waitpid call. If we get there first, the reaper thread bails out immediately and does no further work. If we don't get their first, it becomes impossible for a Ruby user to waitpid for that child process.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Now you may be saying &quot;why don't you just wait on the Process object and let the JDK do its job, old man? The problem here is that Ruby's Process API behaves like a POSIX process API: you get a PID back, and you wait on that PID. We can't mimic that API without returning a PID and implementing Process.waitpid appropriately.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;(Interesting note: we also use reflection tricks to get the real PID out of the java.lang.Process object, since it is not normally exposed.)&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Could we have some internal lookup table mapping PIDs to Process objects, and make our wait logic just call Process.waitFor? In order to do so, we'd need to manage a weak-valued map from integers to Process objects...which is certainly doable, but it breaks if someone uses a native library or FFI call to launch a process themselves. Oh, but if it's not in our table we could do waitpid. And so the onion grows more layers, all because we can't simply launch a process, get a PID, and wait on it.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;It doesn't end here, though.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Keep Boiling That Ocean&lt;/h4&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;At this point we've managed to at least get interactive streams to the child process, and even if they're not selectable that's a big improvement over the standard API. We've managed to dig out a process ID and sometimes we can successfully wait for it with a normal waitpid function call. So out of our three goals (interactivity, selectability, lifecycle control) we're maybe close to halfway there.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Then the JDK engineers go and pull the rug out from under us.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The logic for UNIXProcess has changed over time. Here's the notable differences in the current JDK 7 codebase:&lt;/div&gt;&lt;div&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;An Executor is now used to avoid spinning up a new thread for each child process. I'd&amp;nbsp;+1 this, if the reaping logic weren't already causing me headaches.&lt;/li&gt;&lt;li&gt;The streams are now instances of UNIXProcess.ProcessPipeOutputStream and ProcessPipeInputStream. Don't get excited...they're still just buffered wrappers around File streams.&lt;/li&gt;&lt;li&gt;The logic run when the child process exist has changed...with catastrophic consequences.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;Here's the new stream setup and reaper logic:&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;script class=&quot;brush: java;&quot; type=&quot;syntaxhighlighter&quot;&gt;&lt;![CDATA[      void initStreams(int[] fds) throws IOException {         stdin = (fds[0] == -1) ?             ProcessBuilder.NullOutputStream.INSTANCE :             new ProcessPipeOutputStream(fds[0]);          stdout = (fds[1] == -1) ?             ProcessBuilder.NullInputStream.INSTANCE :             new ProcessPipeInputStream(fds[1]);          stderr = (fds[2] == -1) ?             ProcessBuilder.NullInputStream.INSTANCE :             new ProcessPipeInputStream(fds[2]);          processReaperExecutor.execute(new Runnable() {             public void run() {                 int exitcode = waitForProcessExit(pid);                 UNIXProcess.this.processExited(exitcode);             }});     } ]]&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Now instead of simply notifying the UNIXProcess that the child has died, there's a call to processExited().&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script class=&quot;brush: java;&quot; type=&quot;syntaxhighlighter&quot;&gt;&lt;![CDATA[     void processExited(int exitcode) {         synchronized (this) {             this.exitcode = exitcode;             hasExited = true;             notifyAll();         }          if (stdout instanceof ProcessPipeInputStream)             ((ProcessPipeInputStream) stdout).processExited();          if (stderr instanceof ProcessPipeInputStream)             ((ProcessPipeInputStream) stderr).processExited();          if (stdin instanceof ProcessPipeOutputStream)             ((ProcessPipeOutputStream) stdin).processExited();     } ]]&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Ok, doesn't look bad so far. Let's look at ProcessPipeInputStream, which handles output from the child process.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script class=&quot;brush: java;&quot; type=&quot;syntaxhighlighter&quot;&gt;&lt;![CDATA[     /**      * A buffered input stream for a subprocess pipe file descriptor      * that allows the underlying file descriptor to be reclaimed when      * the process exits, via the processExited hook.      *      * This is tricky because we do not want the user-level InputStream to be      * closed until the user invokes close(), and we need to continue to be      * able to read any buffered data lingering in the OS pipe buffer.      */     static class ProcessPipeInputStream extends BufferedInputStream {         ProcessPipeInputStream(int fd) {             super(new FileInputStream(newFileDescriptor(fd)));         }          private static byte[] drainInputStream(InputStream in)                 throws IOException {             if (in == null) return null;             int n = 0;             int j;             byte[] a = null;             while ((j = in.available()) &gt; 0) {                 a = (a == null) ? new byte[j] : Arrays.copyOf(a, n + j);                 n += in.read(a, n, j);             }             return (a == null || n == a.length) ? a : Arrays.copyOf(a, n);         }          /** Called by the process reaper thread when the process exits. */         synchronized void processExited() {             // Most BufferedInputStream methods are synchronized, but close()             // is not, and so we have to handle concurrent racing close().             try {                 InputStream in = this.in;                 if (in != null) {                     byte[] stragglers = drainInputStream(in);                     in.close();                     this.in = (stragglers == null) ?                         ProcessBuilder.NullInputStream.INSTANCE :                         new ByteArrayInputStream(stragglers);                     if (buf == null) // asynchronous close()?                         this.in = null;                 }             } catch (IOException ignored) {                 // probably an asynchronous close().             }         }     } ]]&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;So when the child process exits, the any data waiting to be read from its output stream is drained into a buffer. &lt;b&gt;All of it. In memory.&lt;/b&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Did you launch a process that writes a gigabyte of data to its output stream and then terminates? Well, friend, I sure hope you have a gigabyte of memory, because the JDK is going to read that sucker in and there's nothing you can do about it. And let's hope there's not more than 2GB of data, since this code basically just grows a byte[], which in Java can only grow to 2GB. If there's more than 2GB of data on that stream, this logic errors out and the data is lost forever.  Oh, and by the way...if you happened to be devlishly clever and managed to dig down to the real FileChannel attached to the child process, all the data from that stream has suddenly disappeared, and the channel itself is closed, even if you never got a chance to read from it. Thanks for the help, JDK.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The JDK has managed to both break our clever workarounds (for its previously broken logic) an break itself even more badly. It's almost like they want to make subprocess launching so dreadfully bad you just don't use it anymore.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Never Surrender&lt;/h4&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Of course I could cry into my beer over this, but these sorts of problems and challenges are exactly why I'm involved in JRuby and OpenJDK. Obviously this API has gone off the deep end and can't be saved, so what's a hacker to do? In our case, we make our own API.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;At this point, that's our only option. The ProcessBuilder and Process APIs are so terribly broken that we can't rely on them anymore. Thankfully, JRuby ships with a solid, fast FFI layer called the Java Native Runtime (JNR) that should make it possible for us to write our own process API entirely in Java. We will of course do that in the open, and we are hoping you will help us.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;What's the moral of the story? I don't really know. Perhaps it's that lowest-common-denominator APIs usually trend toward uselessness. Perhaps it's that ignoring POSIX is an expressway to failure. Perhaps it's that I don't know when to quit. In any case, you can count on the JRuby team to continue bringing you the only true POSIX experience on the JVM, and you can count on me to keep pushing OpenJDK to follow our lead.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>headius</name></author><summary type="html">I prefer to write happy posts...I really do. But tonight I'm completely defeated by the JDK's implementation of subprocess launching, and I need to tell the world why.JRuby has always strived to mimic MRI's behavior as much as possible, which in many cases has meant we need to route around the JDK to get at true POSIX APIs and behaviors.For example, JRuby has provided the ability to manipulate symbolic links since well before Java 7 provided that capability, using a native POSIX subsystem built atop jnr-ffi, our Java-to-C FFI layer (courtesy of Wayne Meissner). Everyone in the Java world knew for years the lack of symlink support was a gross omission, but most folks just sucked it up and went about their business. We could not afford to do that.We've repeated this process for many other Ruby features: UNIX sockets, libc-like IO, selectable stdin, filesystem attributes...on and on. And we've been able to provide the best POSIX runtime on the JVM bar none. Nobody has gone as far or done as much as JRuby has.Another area where we've had to route around the JDK is in subprocess launching and management. The JDK provides java.lang.ProcessBuilder, an API for assembling the appropriate pieces of a subprocess launch, producing a java.lang.Process object. Process in turn provides methods to wait for the subprocess, get access to its streams, and destroy it forcibly. It works great, on the surface.Unfortunately, the cake is a lie.Under the covers, the JDK implements Process through a complicated series of tricks. We want to be able to interactively control the child process, monitor it for writes, govern its lifecycle exactly. The JDK attempts to provide a consistent experience across all platforms. Unfortunately, those two worlds are not currently compatible, and the resulting experience is consistently awful.We'll start at the bottom to see where things go wrong.POSIX, POSIX, EverywhereAt the core of ProcessBuilder, inside the native code behind UNIXProcess, we do find somewhat standard POSIX calls to fork and exec, wrapped up in a native downcall forkAndExec:The C code behind this is a bit involved, so I'll summarize what it does.Sets up pipes for in, out, err, and fail to communicate with the eventual child process.Copies the parent's descriptors from the pipes into the &quot;fds&quot; array.Launches the child through a fairly standard fork+exec sequence.Waits for the child to write a byte to the fail pipe indicating success or failure.Scrubs the unused sides of the pipes in parent and child.Returns the child process ID.This is all pretty standard for subprocess launching, and if it proceeded to put those file descriptors into direct, selectable channels we'd have no issues. Unfortunately, things immediately go awry once we return to the Java code.Interactive?The call to forkAndExec occurs inside the UNIXProcess constructor, as the very first thing it does. At that point, it has in hand the three standard file descriptors and the subprocess pid, and it knows that the subprocess has at least been successfully forked. The next step is to wrap the file descriptors in appropriate InputStream and OutputStream objects, and this is where we find the first flaw.This is the code to set up an OutputStream for the input channel of the child process, so we can write to it. Now we know the operating system is going to funnel those written bytes directly to the subprocess's input stream, and ideally if we're launching a subprocess we intend to control it...perhaps by sending it interactive commands. Why, then, do we wrap the file descriptor with a BufferedOutputStream? This is where JRuby's hacks begin. In our process subsystem, we have the following piece of code, which attempts to unwrap buffering from any stream it is given. The FieldAccess.getProtectedFieldValue call there does what you think it does...attempt to read the &quot;out&quot; field from within FilteredOutputStream, which in this case will be the FileOutputStream from above. Unwrapping the stream in this way allows us to do two things:We can do unbuffered writes to (or reads from, in the case of the child's out and err streams) the child process.We can get access to the more direct FileChannel for the stream, to do direct ByteBuffer reads and writes or low-level stream copying.So we're in good shape, right? It's a bit of hackery, but we've got our unbuffered Channel and can interact directly with the subprocess. Is this good enough?I wish it were.Selectable?The second problem we run into is that users very often would like to select against the output streams of the child process, to perform nonblocking IO operations until the child has actually written some data. It gets reported as a JRuby bug over and over again because there's simply no way for us to implement it. Why? Because FileChannel is not selectable. FileChannel implements methods for random-access reads and writes (positioning) and blocking IO interruption (which NIO implements by closing the stream...that's a rant for another day), but it does not implement any of the logic necessary for doing nonblocking IO using an NIO Selector. This comes up in at least one other place: the JVM's own standard IO streams are also not selectable, which means you can't select for user input at the console. Consistent experience indeed...it seems that all interaction with the user or with processes must be treated as file IO, with no selection capabilities. (It is interesting to note that the JVM's standard IO streams are *also* wrapped in buffers, which we dutifully unwrap to provide a truly interactive console.) Why are inter-proces file descriptors, which would support selector operations just wonderfully, wrapped in an unselectable channel? I have no idea, and it's impossible for us to hack around. Let's not dwell on this item, since there's more to cover. Fear the ReaperYou may recall I also wanted to have direct control over the lifecycle of the subprocess, to be able to wait for it or kill it at my own discretion. And on the surface, Process appears to provide these capabilities via the waitFor() and destroy() methods. Again it's all smoke and mirrors.Further down in the UNIXProcess constructor, you'll find this curious piece of code: For each subprocess started through this API, the JVM will spin up a &quot;process reaper&quot; thread. This thread is designed to monitor the subprocess for liveness and notify the parent UNIXProcess object when that process has died, so it can pass on that information to the user via the waitFor() and exitValue() API calls.The interesting bit here is the waitForProcessExit(pid) call, which is another native downcall into C land:There's nothing too peculiar here; this is how you'd wait for the child process to exit if you were writing plain old C code. But there's a sinister detail you can't see just by looking at this code: waitpid can be called exactly once&amp;nbsp;by the parent process.Part of the Ruby Process API is the ability to get a subprocess PID and wait for it. The concept of a process ID has been around for a long time, and Rubyists (even amateur Rubyists who've never written a line of C code) don't seem to have any problem calling Process.waitpid when they want to wait for a child to exit. JRuby is an implementation of Ruby, and we would ideally like to be able to run all Ruby code that exists, so we also must implement Process.waitpid in some reasonable way. Our choice was to literally call the C function waitpid(2) via our FFI layer.Here's the subtle language from the wait(2) manpage (which includes waitpid):RETURN VALUES If wait() returns due to a stopped or terminated child process, the process ID of the child is returned to the calling process. Otherwise, a value of -1 is returned and errno is set to indicate the error. If wait3(), wait4(), or waitpid() returns due to a stopped or terminated child process, the process ID of the child is returned to the calling process. If there are no children not previously awaited, -1 is returned with errno set to [ECHILD]. Otherwise, if WNOHANG is specified and there are no stopped or exited children, 0 is returned. If an error is detected or a caught signal aborts the call, a value of -1 is returned and errno is set to indicate the error.There's a lot of negatives and passives and conditions there, so I'll spell it out for you more directly: If you call waitpid for a given child PID and someone else in your process has already done so...bad things happen.We effectively have to race the JDK to the waitpid call. If we get there first, the reaper thread bails out immediately and does no further work. If we don't get their first, it becomes impossible for a Ruby user to waitpid for that child process.Now you may be saying &quot;why don't you just wait on the Process object and let the JDK do its job, old man? The problem here is that Ruby's Process API behaves like a POSIX process API: you get a PID back, and you wait on that PID. We can't mimic that API without returning a PID and implementing Process.waitpid appropriately.(Interesting note: we also use reflection tricks to get the real PID out of the java.lang.Process object, since it is not normally exposed.)Could we have some internal lookup table mapping PIDs to Process objects, and make our wait logic just call Process.waitFor? In order to do so, we'd need to manage a weak-valued map from integers to Process objects...which is certainly doable, but it breaks if someone uses a native library or FFI call to launch a process themselves. Oh, but if it's not in our table we could do waitpid. And so the onion grows more layers, all because we can't simply launch a process, get a PID, and wait on it.It doesn't end here, though.Keep Boiling That OceanAt this point we've managed to at least get interactive streams to the child process, and even if they're not selectable that's a big improvement over the standard API. We've managed to dig out a process ID and sometimes we can successfully wait for it with a normal waitpid function call. So out of our three goals (interactivity, selectability, lifecycle control) we're maybe close to halfway there.Then the JDK engineers go and pull the rug out from under us.The logic for UNIXProcess has changed over time. Here's the notable differences in the current JDK 7 codebase:An Executor is now used to avoid spinning up a new thread for each child process. I'd&amp;nbsp;+1 this, if the reaping logic weren't already causing me headaches.The streams are now instances of UNIXProcess.ProcessPipeOutputStream and ProcessPipeInputStream. Don't get excited...they're still just buffered wrappers around File streams.The logic run when the child process exist has changed...with catastrophic consequences.Here's the new stream setup and reaper logic:Now instead of simply notifying the UNIXProcess that the child has died, there's a call to processExited().Ok, doesn't look bad so far. Let's look at ProcessPipeInputStream, which handles output from the child process.So when the child process exits, the any data waiting to be read from its output stream is drained into a buffer. All of it. In memory.Did you launch a process that writes a gigabyte of data to its output stream and then terminates? Well, friend, I sure hope you have a gigabyte of memory, because the JDK is going to read that sucker in and there's nothing you can do about it. And let's hope there's not more than 2GB of data, since this code basically just grows a byte[], which in Java can only grow to 2GB. If there's more than 2GB of data on that stream, this logic errors out and the data is lost forever. Oh, and by the way...if you happened to be devlishly clever and managed to dig down to the real FileChannel attached to the child process, all the data from that stream has suddenly disappeared, and the channel itself is closed, even if you never got a chance to read from it. Thanks for the help, JDK.The JDK has managed to both break our clever workarounds (for its previously broken logic) an break itself even more badly. It's almost like they want to make subprocess launching so dreadfully bad you just don't use it anymore.Never SurrenderOf course I could cry into my beer over this, but these sorts of problems and challenges are exactly why I'm involved in JRuby and OpenJDK. Obviously this API has gone off the deep end and can't be saved, so what's a hacker to do? In our case, we make our own API.At this point, that's our only option. The ProcessBuilder and Process APIs are so terribly broken that we can't rely on them anymore. Thankfully, JRuby ships with a solid, fast FFI layer called the Java Native Runtime (JNR) that should make it possible for us to write our own process API entirely in Java. We will of course do that in the open, and we are hoping you will help us.What's the moral of the story? I don't really know. Perhaps it's that lowest-common-denominator APIs usually trend toward uselessness. Perhaps it's that ignoring POSIX is an expressway to failure. Perhaps it's that I don't know when to quit. In any case, you can count on the JRuby team to continue bringing you the only true POSIX experience on the JVM, and you can count on me to keep pushing OpenJDK to follow our lead.</summary></entry><entry><title type="html">On Languages, VMs, Optimization, and the Way of the World</title><link href="https://headius.github.io/2013/05/on-languages-vms-optimization-and-way.html" rel="alternate" type="text/html" title="On Languages, VMs, Optimization, and the Way of the World"><published>2013-05-11T10:05:00+00:00</published><updated>2013-05-11T10:05:00+00:00</updated><id>https://headius.github.io/2013/05/on-languages-vms-optimization-and-way</id><content type="html" xml:base="https://headius.github.io/2013/05/on-languages-vms-optimization-and-way.html">&lt;div dir=&quot;ltr&quot; style=&quot;text-align: left;&quot; trbidi=&quot;on&quot;&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;I shouldn't be up this late, but I've been doing lots of thinking and exploring tonight.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;In studying various VMs over the past few years, I've come up with a list of do's and don't that make things optimize right. These apply to languages, the structures that back them, and the VMs that optimize those languages, and from what I've seen there's a lot of immutable truths here given current optimization technology.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;Let's dive in.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;#1: Types don't have to be static&lt;/h3&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;JVM and other dynamic-optimizing runtimes have proven this out. At runtime, it's possible to gather the same information static types would provide you at compile time, leading to optimizations at least as good as fully statically-typed, statically-optimized code. In some cases, it may be possible to do a better job, since runtime profiling is based on real execution, real branch percentages, real behavior, rather than a guess at what a program might do. You could probably make the claim that static optimization is a halting problem, and dynamic optimization eventually can beat it by definition since it can optimize what the program is actually doing.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;However, this requires one key thing to really work well.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;#2: Types need to be predictable&lt;/h3&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;In order for runtime optimization to happen, objects need to have predictable types and those types need to have a predictable structure. This isn't to say that types must be statically declared...they just need to look the same on repeat visits. If objects can change type (smalltalk's become, perl's and C's weak typing) you're forced to include more guards against those changes, or you're forced to invalidate more code whenever something changes (or in the case of C, you just completely shit the bed when things aren't as expected). If change is possible and exposed at a language level, there may be nothing you can do to cope with all those different type shapes, and optimization can only go so far.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;This applies both to the shape of a type's method table (methods remaining consistent once encountered) and the shape of the type's instances (predictable object layout). Many dynamically-typed languages impose dynamic type shape and object shape on VMs that run them, preventing those VMs from making useful predictions about how to optimize code. Optimistic predictions (generating synthetic types for known type shapes or preemptively allocating objects based on previously-seen shapes) still have to include fallback logic to maintain the mutable behavior, should it ever be needed. Again, optimization potential is limited, because the shape of the world can change on a whim and the VM has to be vigilent&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;The alternative summation of #1 and #2 is that types don't have to be statically declared, but they need to be statically defined. Most popular dynamic languages do neither, but all they really need to do is the latter.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;#3: You can't cheat the CPU&lt;/h3&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;Regardless of how clever you'd like to be in your code or language or VM or JIT, the limiting factor is how modern CPUs actually run your code. There's a long list of expectations you must meet to squeeze every last drop of speed out of a system, and diverging from those guidelines will always impose a penalty. This is the end...the bottom turtle...the unifying theory. It is, at the end of the day, the CPU you must appease to get the best performance. All other considerations fall out of that, and anywhere performance does not live up to expectations you are guaranteed to discover that someone tried to cheat the CPU.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;Traditionally, static typing was the best way to guarantee we produced good CPU instructions. It gave us a clear picture of the world we could ponder and meditate over, eventually boiling out the secrets of the universe and producing the fastest possible code. But that always assumed a narrow vision of a world with unlimited resources. It assumed we could make all the right decisions for a program ahead of time and that no limitations outside our target instruction set would ever affect us. In the real world, however, CPUs have limited cache sizes, multiple threads, bottlenecked memory pipelines, and basic physics to contend with (you can only push so many electrons through a given piece of matter without blowing it up). Language and VM authors ignore the expectations of their target systems only at great peril.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;Let's look at a few languages and where they fit.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Language Scorecard&lt;/h3&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;Java is statically typed and types are of a fixed shape. This is the ideal situation mostly because of the type structure being predictable. Once encountered, a rose is just a rose. Given appropriate dynamic optimizations, there's no reason Java code can't compete with or surpass statically-typed and statically-compiled C/++, and in theory there's nothing preventing Java code from becoming optimal CPU instructions.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;Dart is dynamically typed (or at least, types are optional and the VM doesn't care about them), but types are of a fixed shape. If programmers can tolerate fixed-shape types, Dart provides a very nice dynamic language that still can achieve the same optimizations as statically-typed Java or statically-compiled C/++.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;Groovy is dynamically typed with some inference and optimization if you specify static types, but most (all?) types defined in Groovy are not guaranteed to be a fixed shape. As a result, even when specifying static types, guards must be inserted to check that those types' shapes have not changed. Groovy does, however, guarantee object shape is consistent over time, which avoids overhead from being able to reshape objects at runtime.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;Ruby and JavaScript are dynamically typed and types and objects can change shape at runtime. This is a confluence of all the hardest-to-optimize language characteristics. In both cases, the best we can do is to attempt to predict common type and object shapes and insert guards for when we're wrong, but it's not possible to achieve the performance of a system with fully-predictable type and object shapes. Prove me wrong.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;Now of course when I say it's not possible, I mean it's not possible for the general case. Specific cases of a known closed-world application can indeed be optimized as though the types and objects involved had static shapes. I do something along these lines in my RubyFlux compiler, which statically analyzes incoming Ruby code and assumes the methods it sees defined and the fields it sees accessed will be the only methods and fields it ever needs to worry about. But that requires omitting features that can mutate type and object structure, or else you have to have a way to know which types and objects those features will affect. Sufficiently smart compiler indeed.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;Python has similar structural complexities to Ruby and adds in the additional complexity of an introspectable call stack. Under those circumstances, even on-stack execution state is not safe; a VM can't even make guarantees about the values it has in hand or the shape of a given call's activation. PyPy does an admirable job of attacking this problem by rewriting currently-running code and lifting on-stack state to the heap when it is accessed, but this approach prevents dropping unused local state (since you can't predict who might want to see it) and also fails to work under parallel execution (since you can't rewrite code another thread might be executing). Again, the dynamicity of a &quot;cool&quot; feature brings with it intrinsic penalties that are reducible but not removable.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Get to the Damn Point, Already&lt;/h3&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;So what am I trying to say in all this? I started the evening by exploring a benchmark post comparing Dart's VM with JVM on the same benchmark. The numbers were not actually very exciting...with a line-by-line port from Dart to Java, Java came out slightly behind Dart. With a few modifications to the Java code, Java pulled slightly ahead. With additional modifications to the Dart code, it might leapfrog Java again. But this isn't interesting because Dart and Java can both rely on type and object shapes remaining consistent, and as a result the optimizations they perform can basically accomplish the same thing. Where it matters, they're similar enough that VMs don't care about the differences.&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;Where does this put languages I love, like Ruby? It's probably fair to concede that Ruby can't ever achieve the raw, straight-line performance of type-static (not statically-typed) languages like Dart or Java, regardless of the VM technologies involved. We'll be able to get close; JRuby can, with the help of invokedynamic, make method calls *nearly* as fast as Java calls, and by generating type shapes we can make object state *nearly* as predictable as Java types, but we can't go all the way. Regardless of how great the underlying VM is, if you can't hold to its immutable truths, you're walking against the wind. Ruby on Dart would probably not be any faster than Ruby on JVM, because you'd still have to implement mutable types and growable objects in pretty much the same way. Ruby on PyPy might be able to go farther, since the VM is designed for mutable types and growable objects, but you might have to sacrifice parallelism or accept that straight-line object-manipulating performance won't go all the way to a Java or Dart. Conversely, languages that make those type-static guarantees might be able to beat dynamic languages when running on dynamic language VMs (e.g. dart2js) for exactly the same reasons that they excel on their own VMs: they provide a more consistent view of the world, and offer no surprises to the VM that would hinder optimization. You trade dynamicity at the language level for predictability at the VM level.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;The Actual Lesson&lt;/h3&gt;&lt;div style=&quot;color: #222222; font-family: arial; font-size: small;&quot;&gt;&lt;br /&gt;I guess the bottom line for me is realizing that there's always going to be a conflict between what programmers want out of programming languages and what's actually possible to give them. There's no magical fairy world where every language can be as fast as every other language, because there's no way to predict how every program is going to execute (or in truth, how a given program is going to execute given a general strategy). And that's ok; most of these languages can still get very close to each other in performance, and over time the dynamic type/object-shaped languages may offer ways to ratchet down some of that dynamism...or they might not care and just accept what limitations result. The important thing is for language users to recognize that nothing is free, and to understand the implications of language features and design decisions they make in their own programs.&lt;/div&gt;&lt;/div&gt;</content><author><name>headius</name></author><summary type="html">I shouldn't be up this late, but I've been doing lots of thinking and exploring tonight.In studying various VMs over the past few years, I've come up with a list of do's and don't that make things optimize right. These apply to languages, the structures that back them, and the VMs that optimize those languages, and from what I've seen there's a lot of immutable truths here given current optimization technology.Let's dive in.#1: Types don't have to be staticJVM and other dynamic-optimizing runtimes have proven this out. At runtime, it's possible to gather the same information static types would provide you at compile time, leading to optimizations at least as good as fully statically-typed, statically-optimized code. In some cases, it may be possible to do a better job, since runtime profiling is based on real execution, real branch percentages, real behavior, rather than a guess at what a program might do. You could probably make the claim that static optimization is a halting problem, and dynamic optimization eventually can beat it by definition since it can optimize what the program is actually doing.However, this requires one key thing to really work well.#2: Types need to be predictableIn order for runtime optimization to happen, objects need to have predictable types and those types need to have a predictable structure. This isn't to say that types must be statically declared...they just need to look the same on repeat visits. If objects can change type (smalltalk's become, perl's and C's weak typing) you're forced to include more guards against those changes, or you're forced to invalidate more code whenever something changes (or in the case of C, you just completely shit the bed when things aren't as expected). If change is possible and exposed at a language level, there may be nothing you can do to cope with all those different type shapes, and optimization can only go so far.This applies both to the shape of a type's method table (methods remaining consistent once encountered) and the shape of the type's instances (predictable object layout). Many dynamically-typed languages impose dynamic type shape and object shape on VMs that run them, preventing those VMs from making useful predictions about how to optimize code. Optimistic predictions (generating synthetic types for known type shapes or preemptively allocating objects based on previously-seen shapes) still have to include fallback logic to maintain the mutable behavior, should it ever be needed. Again, optimization potential is limited, because the shape of the world can change on a whim and the VM has to be vigilentThe alternative summation of #1 and #2 is that types don't have to be statically declared, but they need to be statically defined. Most popular dynamic languages do neither, but all they really need to do is the latter.#3: You can't cheat the CPURegardless of how clever you'd like to be in your code or language or VM or JIT, the limiting factor is how modern CPUs actually run your code. There's a long list of expectations you must meet to squeeze every last drop of speed out of a system, and diverging from those guidelines will always impose a penalty. This is the end...the bottom turtle...the unifying theory. It is, at the end of the day, the CPU you must appease to get the best performance. All other considerations fall out of that, and anywhere performance does not live up to expectations you are guaranteed to discover that someone tried to cheat the CPU.Traditionally, static typing was the best way to guarantee we produced good CPU instructions. It gave us a clear picture of the world we could ponder and meditate over, eventually boiling out the secrets of the universe and producing the fastest possible code. But that always assumed a narrow vision of a world with unlimited resources. It assumed we could make all the right decisions for a program ahead of time and that no limitations outside our target instruction set would ever affect us. In the real world, however, CPUs have limited cache sizes, multiple threads, bottlenecked memory pipelines, and basic physics to contend with (you can only push so many electrons through a given piece of matter without blowing it up). Language and VM authors ignore the expectations of their target systems only at great peril.Let's look at a few languages and where they fit.Language ScorecardJava is statically typed and types are of a fixed shape. This is the ideal situation mostly because of the type structure being predictable. Once encountered, a rose is just a rose. Given appropriate dynamic optimizations, there's no reason Java code can't compete with or surpass statically-typed and statically-compiled C/++, and in theory there's nothing preventing Java code from becoming optimal CPU instructions.Dart is dynamically typed (or at least, types are optional and the VM doesn't care about them), but types are of a fixed shape. If programmers can tolerate fixed-shape types, Dart provides a very nice dynamic language that still can achieve the same optimizations as statically-typed Java or statically-compiled C/++.Groovy is dynamically typed with some inference and optimization if you specify static types, but most (all?) types defined in Groovy are not guaranteed to be a fixed shape. As a result, even when specifying static types, guards must be inserted to check that those types' shapes have not changed. Groovy does, however, guarantee object shape is consistent over time, which avoids overhead from being able to reshape objects at runtime.Ruby and JavaScript are dynamically typed and types and objects can change shape at runtime. This is a confluence of all the hardest-to-optimize language characteristics. In both cases, the best we can do is to attempt to predict common type and object shapes and insert guards for when we're wrong, but it's not possible to achieve the performance of a system with fully-predictable type and object shapes. Prove me wrong.Now of course when I say it's not possible, I mean it's not possible for the general case. Specific cases of a known closed-world application can indeed be optimized as though the types and objects involved had static shapes. I do something along these lines in my RubyFlux compiler, which statically analyzes incoming Ruby code and assumes the methods it sees defined and the fields it sees accessed will be the only methods and fields it ever needs to worry about. But that requires omitting features that can mutate type and object structure, or else you have to have a way to know which types and objects those features will affect. Sufficiently smart compiler indeed.Python has similar structural complexities to Ruby and adds in the additional complexity of an introspectable call stack. Under those circumstances, even on-stack execution state is not safe; a VM can't even make guarantees about the values it has in hand or the shape of a given call's activation. PyPy does an admirable job of attacking this problem by rewriting currently-running code and lifting on-stack state to the heap when it is accessed, but this approach prevents dropping unused local state (since you can't predict who might want to see it) and also fails to work under parallel execution (since you can't rewrite code another thread might be executing). Again, the dynamicity of a &quot;cool&quot; feature brings with it intrinsic penalties that are reducible but not removable.Get to the Damn Point, AlreadySo what am I trying to say in all this? I started the evening by exploring a benchmark post comparing Dart's VM with JVM on the same benchmark. The numbers were not actually very exciting...with a line-by-line port from Dart to Java, Java came out slightly behind Dart. With a few modifications to the Java code, Java pulled slightly ahead. With additional modifications to the Dart code, it might leapfrog Java again. But this isn't interesting because Dart and Java can both rely on type and object shapes remaining consistent, and as a result the optimizations they perform can basically accomplish the same thing. Where it matters, they're similar enough that VMs don't care about the differences.Where does this put languages I love, like Ruby? It's probably fair to concede that Ruby can't ever achieve the raw, straight-line performance of type-static (not statically-typed) languages like Dart or Java, regardless of the VM technologies involved. We'll be able to get close; JRuby can, with the help of invokedynamic, make method calls *nearly* as fast as Java calls, and by generating type shapes we can make object state *nearly* as predictable as Java types, but we can't go all the way. Regardless of how great the underlying VM is, if you can't hold to its immutable truths, you're walking against the wind. Ruby on Dart would probably not be any faster than Ruby on JVM, because you'd still have to implement mutable types and growable objects in pretty much the same way. Ruby on PyPy might be able to go farther, since the VM is designed for mutable types and growable objects, but you might have to sacrifice parallelism or accept that straight-line object-manipulating performance won't go all the way to a Java or Dart. Conversely, languages that make those type-static guarantees might be able to beat dynamic languages when running on dynamic language VMs (e.g. dart2js) for exactly the same reasons that they excel on their own VMs: they provide a more consistent view of the world, and offer no surprises to the VM that would hinder optimization. You trade dynamicity at the language level for predictability at the VM level.The Actual LessonI guess the bottom line for me is realizing that there's always going to be a conflict between what programmers want out of programming languages and what's actually possible to give them. There's no magical fairy world where every language can be as fast as every other language, because there's no way to predict how every program is going to execute (or in truth, how a given program is going to execute given a general strategy). And that's ok; most of these languages can still get very close to each other in performance, and over time the dynamic type/object-shaped languages may offer ways to ratchet down some of that dynamism...or they might not care and just accept what limitations result. The important thing is for language users to recognize that nothing is free, and to understand the implications of language features and design decisions they make in their own programs.</summary></entry><entry><title type="html">Constant and Global Optimization in JRuby 1.7.1 and 1.7.2</title><link href="https://headius.github.io/2013/01/constant-and-global-optimization-in.html" rel="alternate" type="text/html" title="Constant and Global Optimization in JRuby 1.7.1 and 1.7.2"><published>2013-01-05T16:47:00+00:00</published><updated>2013-01-05T16:47:00+00:00</updated><id>https://headius.github.io/2013/01/constant-and-global-optimization-in</id><content type="html" xml:base="https://headius.github.io/2013/01/constant-and-global-optimization-in.html">&lt;div dir=&quot;ltr&quot; style=&quot;text-align: left;&quot; trbidi=&quot;on&quot;&gt;With every JRuby release, there's always at least a handful of optimizations. They range from tiny improvements in the compiler to perf-aware rewrites of core class methods, but they're almost always driven by real-world cases.&lt;br /&gt;&lt;br /&gt;In JRuby 1.7.1 and 1.7.2, I made several improvements to the performance of Ruby constants and global variables that might be of some interest to you, dear reader.&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Constants&lt;/h2&gt;&lt;div&gt;In Ruby, a constant is a lexically and hierarchically accessed variable that starts with a capital letter. Class and module names like Object, Kernel, String, are all constants defined under the Object class. When I say constants are both lexical and hierarchically accessed, what I mean is that at access time we first search outward through lexically-enclosing scopes, and failing that we search through the class hierarchy of the innermost scope. For example:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/4459891.js?file=file1.rb&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here, the first two constant accesses inside class B are successful; the first (IN_FOO) is located lexically in Foo, because it encloses the body of class B. The second (IN_A) is located hierarchically by searching B's ancestors. The third access fails, because the IN_BAR constant is only available within the Bar module's scope, so B can't see it.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Constants also...aren't. It is possible to redefine a constant, or define new constants deeper in a lexical or hierarchical strcture that mask earlier ones. However in most code (i.e. &quot;good&quot; code) constants eventually stabilize. This makes it possible to perform a variety of optimizations against them, even though they're not necessarily static.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Constants are used heavily throughout Ruby, both for constant values like Float::MAX and for classes like Array or Hash. It is therefore especially important that they be as fast as possible.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Global Variables&lt;/h2&gt;&lt;div&gt;Globals in Ruby are about like you'd expect...name/value pairs in a global namespace. They start with &amp;nbsp;$ character. Several global variables are &quot;special&quot; and exist in a more localized source, like $~ (last regular expression match in this call frame), $! (last exception raised in this thread), and so on. Use of these &quot;local globals&quot; mostly just amounts to special variable names that are always available; they're not really true global variables.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Everyone knows global variables should be discouraged, but that's largely referring to global variable use in normal program flow. Using global state across your application – potentially across threads – is a pretty nasty thing to do to yourself and your coworkers. But there are some valid uses of globals, like for logging state and levels, debugging flags, and truly global constructs like standard IO.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/4459891.js?file=file2.rb&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here, we're using the global $DEBUG to specify whether logging should occur in MyApp#log. Those log messages are written to the stderr stream accessed via $stderr. Note also that $DEBUG can be set to true by passing -d at the JRuby command line.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Optimizing Constant Access (pre-1.7.1)&lt;/h2&gt;&lt;div&gt;I've posted in the past about how JRuby optimizes constant access, so I'll just quickly review that here.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;At a given access point, constant values are looked up from the current lexical scope and cached. Because constants can be modified, or new constants can be introduce that mask earlier ones, the JRuby runtime (org.jruby.Ruby) holds a global constant invalidator checked on each access to ensure the previous value is still valid.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;On non-invokedynamic JVMs, verifying the cache involves an object identity comparison every time, which means a non-final value must be accessed via a couple levels of indirection. This adds a certain amount of overhead to constant access, and also makes it impossible for the JVM to fold multiple constant accesses away, or make static decisions based on a constant's value.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;On an invokedynamic JVM, the cache verification is in the form of a SwitchPoint. SwitchPoint is a type of on/off guard used at invokedynamic call sites to represent a hard failure. Because it can only be switched off, the JVM is able to optimize the SwitchPoint logic down to what's called a &quot;safe point&quot;, a very inexpensive ping back into the VM. As a result, constant accesses under invokedynamic can be folded away, and repeat access or unused accesses are not made at all.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;However, there's a problem. In JRuby 1.7.0 and earlier, the only way we could access the current lexical scope (in a StaticScope object) was via the current call frame's DynamicScope, a heap-based object created on each activation of a given body of code. In order to reduce the performance hit to methods containing constants, we introduced a one-time DynamicScope called the &quot;dummy scope&quot;, attached to the lexical scope and only created once. This avoided the huge hit of constructing a DynamicScope for every call, but caused constant-containing methods to be considerably slower than those without constants.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Lifting Lexical Scope Into Code&lt;/h2&gt;&lt;div&gt;In JRuby 1.7.1, I decided to finally bite the bullet and make the lexical scope available to all method bodies, without requiring a DynamicScope intermediate. This was a&amp;nbsp;&lt;a href=&quot;https://github.com/jruby/jruby/compare/fb65c539a9b4f52d1d063dbe36de69217ab6a896...ad5d07291d09f57849f873d405607fbb6fed1544&quot;&gt;nontrivial piece of work&lt;/a&gt;&amp;nbsp;that took several days to get right, so although most of the work occurred before JRuby 1.7.0 was released, we opted to let it bake a bit before release.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The changes made it possible for all class, module, method, and block bodies to access their lexical scope essentially for free. It also helped us finally deliver on the promise of truly free constant access when running under invokedynamic.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;So, does it work?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/4459891.js?file=file3.rb&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Assuming constant access is free, the three loops here should perform identically. The non-expression calls to foo and bar should disappear, since they both return a constant value that's never used. The calls for decrementing the 'a' variable should produce a constant value '1' and perform the same as the literal decrement in the control loop.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here's Ruby (MRI) 2.0.0 performance on this benchmark.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/4459891.js?file=file4.rb&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The method call itself adds a significant amount of overhead here, and the constant access adds another 50% of that overhead. Ruby 2.0.0 has done a lot of work on performance, but the cost of invoking Ruby methods and accessing constants remains high, and constant accesses do not fold away as you would like.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here's JRuby 1.7.2 performance on the same benchmark.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/4459891.js?file=file5.rb&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We obviously run all cases significantly faster than Ruby 2.0.0, but the important detail is that the method call adds only about 11% overhead to the control case, and constant access adds almost nothing.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;For comparison, here's JRuby 1.7.0, which did not have free access to lexical scopes.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/4459891.js?file=file6.rb&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;So by avoiding the intermediate DynamicScope, methods containing constant accesses are somewhere around 7x faster than before. Not bad.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Optimizing Global Variables&lt;/h2&gt;&lt;div&gt;Because global variables have a much simpler structure than constants, they're pretty easy to optimize. I had not done so up to JRuby 1.7.1 mostly because I didn't see a compelling use case and didn't want to encourage their use. However, after Tony Arcieri pointed out that invokedynamic-optimized global variables could be used to add logging and profiling to an application with zero impact when disabled, I was convinced. Let's look at the example from above again.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/4459891.js?file=file2.rb&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;In this example, we would ideally like there to be no overhead at all when $DEBUG is untrue, so we're free to add optional logging throughout the application with no penalty. In order to support this, two improvements were needed.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;First, I modified our invokedynamic logic to cache global variables using a per-variable SwitchPoint. This makes access to mostly-static global variables as free as constant access, with the same performance improvements.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Second, I added some smarts into the compiler for conditional forms like &quot;if $DEBUG&quot; that would avoid re-checking the $DEBUG value at all if it were false the first time (and start checking it again if it were modified).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;It's worth noting I also made this second optimization for constants; code like &quot;if DEBUG_ENABLED&quot; will also have the same performance characteristics.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Let's see how it performs.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/4459891.js?file=file7.rb&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;In this case, we should again expect that all three forms have identical performance. Both the constant and the global resolve to an untrue value, so they should ideally not introduce any overhead compared to the bare method.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here's Ruby (MRI) 2.0.0:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/4459891.js?file=file8.rb&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Both the global and the constant add overhead here in the neighborhood of 25% over an empty method. This means you can't freely add globally-conditional logic to your application without accepting a performance hit.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;JRuby 1.7.2:&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/4459891.js?file=file9.rb&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Again we see JRuby +&amp;nbsp;invokedynamic optimizing method calls considerably better than MRI, but additionally we see that the untrue global conditions add no overhead compared to the empty method. You can freely use globals as conditions for logging, profiling, and other code you'd like to have disabled most of the time.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;And finally, JRuby 1.7.1, which optimized constants, did not optimize globals, and did not have specialized conditional logic for either:&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;script src=&quot;https://gist.github.com/4459891.js?file=file10.rb&quot;&gt;&lt;/script&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Where Do We Go From Here?&lt;/h2&gt;&lt;div&gt;Hopefully I've helped show that we're really just seeing the tip of the iceberg as far as optimizing JRuby using invokedynamic. More than anything we want you to report real-world use cases that could benefit from additional optimization, so we can target our work effectively. And as always, please try out your apps on JRuby, enable JRuby testing in Travis CI, and let us know what we can do to make your JRuby experience better!&lt;/div&gt;&lt;/div&gt;</content><author><name>headius</name></author><category term="ruby"/><category term="invokedynamic"/><category term="optimization"/><category term="jvm"/><category term="jruby"/><summary type="html">With every JRuby release, there's always at least a handful of optimizations. They range from tiny improvements in the compiler to perf-aware rewrites of core class methods, but they're almost always driven by real-world cases.In JRuby 1.7.1 and 1.7.2, I made several improvements to the performance of Ruby constants and global variables that might be of some interest to you, dear reader.ConstantsIn Ruby, a constant is a lexically and hierarchically accessed variable that starts with a capital letter. Class and module names like Object, Kernel, String, are all constants defined under the Object class. When I say constants are both lexical and hierarchically accessed, what I mean is that at access time we first search outward through lexically-enclosing scopes, and failing that we search through the class hierarchy of the innermost scope. For example:Here, the first two constant accesses inside class B are successful; the first (IN_FOO) is located lexically in Foo, because it encloses the body of class B. The second (IN_A) is located hierarchically by searching B's ancestors. The third access fails, because the IN_BAR constant is only available within the Bar module's scope, so B can't see it.Constants also...aren't. It is possible to redefine a constant, or define new constants deeper in a lexical or hierarchical strcture that mask earlier ones. However in most code (i.e. &quot;good&quot; code) constants eventually stabilize. This makes it possible to perform a variety of optimizations against them, even though they're not necessarily static.Constants are used heavily throughout Ruby, both for constant values like Float::MAX and for classes like Array or Hash. It is therefore especially important that they be as fast as possible.Global VariablesGlobals in Ruby are about like you'd expect...name/value pairs in a global namespace. They start with &amp;nbsp;$ character. Several global variables are &quot;special&quot; and exist in a more localized source, like $~ (last regular expression match in this call frame), $! (last exception raised in this thread), and so on. Use of these &quot;local globals&quot; mostly just amounts to special variable names that are always available; they're not really true global variables.Everyone knows global variables should be discouraged, but that's largely referring to global variable use in normal program flow. Using global state across your application – potentially across threads – is a pretty nasty thing to do to yourself and your coworkers. But there are some valid uses of globals, like for logging state and levels, debugging flags, and truly global constructs like standard IO.Here, we're using the global $DEBUG to specify whether logging should occur in MyApp#log. Those log messages are written to the stderr stream accessed via $stderr. Note also that $DEBUG can be set to true by passing -d at the JRuby command line.Optimizing Constant Access (pre-1.7.1)I've posted in the past about how JRuby optimizes constant access, so I'll just quickly review that here.At a given access point, constant values are looked up from the current lexical scope and cached. Because constants can be modified, or new constants can be introduce that mask earlier ones, the JRuby runtime (org.jruby.Ruby) holds a global constant invalidator checked on each access to ensure the previous value is still valid.On non-invokedynamic JVMs, verifying the cache involves an object identity comparison every time, which means a non-final value must be accessed via a couple levels of indirection. This adds a certain amount of overhead to constant access, and also makes it impossible for the JVM to fold multiple constant accesses away, or make static decisions based on a constant's value.On an invokedynamic JVM, the cache verification is in the form of a SwitchPoint. SwitchPoint is a type of on/off guard used at invokedynamic call sites to represent a hard failure. Because it can only be switched off, the JVM is able to optimize the SwitchPoint logic down to what's called a &quot;safe point&quot;, a very inexpensive ping back into the VM. As a result, constant accesses under invokedynamic can be folded away, and repeat access or unused accesses are not made at all.However, there's a problem. In JRuby 1.7.0 and earlier, the only way we could access the current lexical scope (in a StaticScope object) was via the current call frame's DynamicScope, a heap-based object created on each activation of a given body of code. In order to reduce the performance hit to methods containing constants, we introduced a one-time DynamicScope called the &quot;dummy scope&quot;, attached to the lexical scope and only created once. This avoided the huge hit of constructing a DynamicScope for every call, but caused constant-containing methods to be considerably slower than those without constants.Lifting Lexical Scope Into CodeIn JRuby 1.7.1, I decided to finally bite the bullet and make the lexical scope available to all method bodies, without requiring a DynamicScope intermediate. This was a&amp;nbsp;nontrivial piece of work&amp;nbsp;that took several days to get right, so although most of the work occurred before JRuby 1.7.0 was released, we opted to let it bake a bit before release.The changes made it possible for all class, module, method, and block bodies to access their lexical scope essentially for free. It also helped us finally deliver on the promise of truly free constant access when running under invokedynamic.So, does it work?Assuming constant access is free, the three loops here should perform identically. The non-expression calls to foo and bar should disappear, since they both return a constant value that's never used. The calls for decrementing the 'a' variable should produce a constant value '1' and perform the same as the literal decrement in the control loop.Here's Ruby (MRI) 2.0.0 performance on this benchmark.The method call itself adds a significant amount of overhead here, and the constant access adds another 50% of that overhead. Ruby 2.0.0 has done a lot of work on performance, but the cost of invoking Ruby methods and accessing constants remains high, and constant accesses do not fold away as you would like.Here's JRuby 1.7.2 performance on the same benchmark.We obviously run all cases significantly faster than Ruby 2.0.0, but the important detail is that the method call adds only about 11% overhead to the control case, and constant access adds almost nothing.For comparison, here's JRuby 1.7.0, which did not have free access to lexical scopes.So by avoiding the intermediate DynamicScope, methods containing constant accesses are somewhere around 7x faster than before. Not bad.Optimizing Global VariablesBecause global variables have a much simpler structure than constants, they're pretty easy to optimize. I had not done so up to JRuby 1.7.1 mostly because I didn't see a compelling use case and didn't want to encourage their use. However, after Tony Arcieri pointed out that invokedynamic-optimized global variables could be used to add logging and profiling to an application with zero impact when disabled, I was convinced. Let's look at the example from above again.In this example, we would ideally like there to be no overhead at all when $DEBUG is untrue, so we're free to add optional logging throughout the application with no penalty. In order to support this, two improvements were needed.First, I modified our invokedynamic logic to cache global variables using a per-variable SwitchPoint. This makes access to mostly-static global variables as free as constant access, with the same performance improvements.Second, I added some smarts into the compiler for conditional forms like &quot;if $DEBUG&quot; that would avoid re-checking the $DEBUG value at all if it were false the first time (and start checking it again if it were modified).It's worth noting I also made this second optimization for constants; code like &quot;if DEBUG_ENABLED&quot; will also have the same performance characteristics.Let's see how it performs.In this case, we should again expect that all three forms have identical performance. Both the constant and the global resolve to an untrue value, so they should ideally not introduce any overhead compared to the bare method.Here's Ruby (MRI) 2.0.0:Both the global and the constant add overhead here in the neighborhood of 25% over an empty method. This means you can't freely add globally-conditional logic to your application without accepting a performance hit.JRuby 1.7.2:Again we see JRuby +&amp;nbsp;invokedynamic optimizing method calls considerably better than MRI, but additionally we see that the untrue global conditions add no overhead compared to the empty method. You can freely use globals as conditions for logging, profiling, and other code you'd like to have disabled most of the time.And finally, JRuby 1.7.1, which optimized constants, did not optimize globals, and did not have specialized conditional logic for either:Where Do We Go From Here?Hopefully I've helped show that we're really just seeing the tip of the iceberg as far as optimizing JRuby using invokedynamic. More than anything we want you to report real-world use cases that could benefit from additional optimization, so we can target our work effectively. And as always, please try out your apps on JRuby, enable JRuby testing in Travis CI, and let us know what we can do to make your JRuby experience better!</summary></entry><entry><title type="html">Refining Ruby</title><link href="https://headius.github.io/2012/11/refining-ruby.html" rel="alternate" type="text/html" title="Refining Ruby"><published>2012-11-19T14:38:00+00:00</published><updated>2012-11-19T14:38:00+00:00</updated><id>https://headius.github.io/2012/11/refining-ruby</id><content type="html" xml:base="https://headius.github.io/2012/11/refining-ruby.html">&lt;div dir=&quot;ltr&quot; style=&quot;text-align: left;&quot; trbidi=&quot;on&quot;&gt;What does the following code do?&lt;br /&gt;&lt;br /&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref_1.rb&quot;&gt;&lt;/script&gt; If you answered &quot;it upcases two strings and adds them together, returning the result&quot; you might be wrong because of a new Ruby feature called &quot;refinements&quot;.&lt;br /&gt;&lt;br /&gt;Let's start with the problem refinements are supposed to solve: monkey-patching.&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Monkey-patching&lt;/h2&gt;&lt;div&gt;In Ruby, all classes are mutable. Indeed, when you define a new class, you're really just creating an empty class and filling it with methods. The ability to mutate classes at runtime has been used (or abused) by many libraries and frameworks to decorate Ruby's core classes with additional (or replacement) behavior. For example, you might add a &quot;camelize&quot; method to String that knows how to convert under_score_names to camelCaseNames. This is lovingly called &quot;monkey-patching&quot; by the Ruby community.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Monkey-patching can be very useful, and many patterns in Ruby are built around the ability to modify classes. It can also cause problems if a library patches code in a way the user does not expect (or want), or if two libraries try to apply conflicting patches. Sometimes, you simply don't want patches to apply globally, and this is where refinements come in.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Localizing Monkeypatches&lt;/h2&gt;&lt;div&gt;Refinements have been discussed as a feature for several years, sometimes under the name &quot;selector namespaces&quot;. In essence, refinements are intended to allow monkey-patching only within certain limited scopes, like within a library that wants to use altered or enhanced versions of core Ruby types without affecting code outside the library. This is the case within the ActiveSupport library that forms part of the core of Rails.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;ActiveSupport provides a number of extensions (patches) to the core Ruby classes like String#pluralize, Range#overlaps?, and Array#second. Some of these extensions are intended for use by Ruby developers, as conveniences that improve the readability or conciseness of code. Others exist mostly to support Rails itself. In both cases, it would be nice if we could prevent those extensions from leaking out of ActiveSupport into code that does not want or need them.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Refinements&lt;/h2&gt;&lt;div&gt;In short, refinements provide a way to make class modifications that are only seen from within certain scopes. In the following example, I add a &quot;camelize&quot; method to the String class that's only seen from code within the Foo class.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref_2.rb&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;With the Foo class refined, we can see that the &quot;camelize&quot; method is indeed available within the &quot;camelize_string&quot; method but not outside of the Foo class.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref_3.txt&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;On the surface, this seems like exactly what we want. Unfortunately, there's a lot more complexity here than meets the eye.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Ruby Method Dispatch&lt;/h2&gt;&lt;div&gt;In order to do a method call in Ruby, a runtime simply looks at the target object's class hierarchy, searches for the method from bottom to top, and upon finding it performs the call. A smart runtime will cache the method to avoid performing this search every time, but in general the mechanics of looking up a method body are rather simple.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;In an implementation like JRuby, we might cache the method at what's called the &quot;call site&quot;—the point in Ruby code where a method call is actually performed. In order to know that the method is valid for future calls, we perform two checks at the call site: that the incoming object is of the same type as for previous calls; and that the type's hierarchy has not been mutated since the method was cached.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Up to now, method dispatch in Ruby has depended solely on the type of the target object. The calling context has not been important to the method lookup process, other than to confirm that visibility restrictions are enforced (primarily for protected methods, since private methods are rejected for non–self calls). That simplicity has allowed Ruby implementations to optimize method calls and Ruby programmers to understand code by simply determining the target object and methods available on it.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Refinements change everything.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Refinements Basics&lt;/h2&gt;&lt;div&gt;Let's revisit the camelize example again.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref_2.rb&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The visible manifestation of refinements comes via the &quot;refine&quot; and &quot;using&quot; methods.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The &quot;refine&quot; method takes a class or module (the String class, in this case) and a block. Within the block, methods defined (camelize) are added to what might be called a patch set (a la monkey-patching) that can be applied to specific scopes in the future. The methods are not actually added to the refined class (String) except in a &quot;virtual&quot; sense when a body of code activates the refinement via the &quot;using&quot; method.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The &quot;using&quot; method takes a refinement-containing module and applies it to the current scope. Methods within that scope should see the refined version of the class, while methods outside that scope do not.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Where things get a little weird is in defining exactly what that scope should be and in implementing refined method lookup in such a way that does not negatively impact the performance of unrefined method lookup. In the current implementation of refinements, a &quot;using&quot; call affects all of the following scopes related to where it is called:&lt;/div&gt;&lt;div&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;The direct scope, such as the top-level of a script, the body of a class, or the body of a method or block&lt;/li&gt;&lt;li&gt;Classes down-hierarchy from a refined class or module body&lt;/li&gt;&lt;li&gt;Bodies of code run via eval forms that change the &quot;self&quot; of the code, such as module_eval&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;It's worth emphasizing at this point that refinements can affect code far away from the original &quot;using&quot; call site. It goes without saying that refined method calls must now be aware of both the target type and the calling scope, but what of unrefined calls?&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Dynamic Scoping of Method Lookup&lt;/h2&gt;&lt;div&gt;Refinements (in their current form) basically cause method lookup to be dynamically scoped. In order to properly do a refined call, we need to know what refinements are active for the context in which the call is occurring and the type of the object we're calling against. The latter is simple, obviously, but determining the former turns out to be rather tricky.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Locally-applied refinements&lt;/h3&gt;&lt;div&gt;In the simple case, where a &quot;using&quot; call appears alongside the methods we want to affect, the immediate calling scope contains everything we need. Calls in that scope (or in child scopes like method bodies) would perform method lookup based on the target class, a method name, and the hierarchy of scopes that surrounds them. The key for method lookup expands from a simple name to a name plus a call context.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Hierarchically-applied refinements&lt;/h3&gt;&lt;div&gt;Refinements applied to a class must also affect subclasses, so even when we don't have a &quot;using&quot; call present we still may need to do refined dispatch. The following example illustrates this with a subclass of Foo (building off the previous example).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref_4.rb&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here, the camelize method is used within a &quot;map&quot; call, showing that refinements used by the Foo class apply to Bar, its method definitions, and any subscopes like blocks within those methods. It should be apparent now why my first example might not do what you expect. Here's my first example again, this time with the Quux class visible.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref_5.rb&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The Quux class uses refinements from the BadRefinement module, effectively changing String#upcase to actually do String#reverse. By looking at the Baz class alone you can't tell what's supposed to happen, even if you are certain that str1 and str2 are always going to be String. Refinements have effectively localized the changes applied by the BadRefinement module, but they've also made the code more difficult to understand; the programmer (or the reader of the code) must know everything about the calling hierarchy to reason about method calls and expected results.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Dynamically-applied refinements&lt;/h3&gt;&lt;div&gt;One of the key features of refinements is to allow block-based DSLs (domain-specific languages) to decorate various types of objects without affecting code outside the DSL. For example, an RSpec spec.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref_6.rb&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;There's several calls here that we'd like to refine.&lt;/div&gt;&lt;div&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;The &quot;describe&quot; method is called at the top of the script against the &quot;toplevel&quot; object (essentially a singleton Object instance). We'd like to apply a refinement at this level so &quot;describe&quot; does not have to be defined on Object itself.&lt;/li&gt;&lt;li&gt;The &quot;it&quot; method is called within the block passed to &quot;describe&quot;. We'd like whatever self object is live inside that block to have an &quot;it&quot; method without modifying self's type directly.&lt;/li&gt;&lt;li&gt;The &quot;should&quot; method is called against an instance of MyClass, presumably a user-created class that does not define such a method. We would like to refine MyClass to have the &quot;should&quot; method only within the context of the block we pass to &quot;it&quot;.&lt;/li&gt;&lt;li&gt;Finally, the &quot;be_awesome&quot; method—which RSpec translates into a call to MyClass#awesome?—should be available on the self object active in the &quot;it&quot; block without actually adding be_awesome to self's type.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;In order to do this without having a &quot;using&quot; present in the spec file itself, we need to be able to dynamically apply refinements to code that might otherwise not be refined. The current implementation does this via Module#module_eval (or its argument-receiving brother, Module#module_exec).&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;A block of code passed to &quot;module_eval&quot; or &quot;instance_eval&quot; will see its self object changed from that of the original surrounding scope (the self at block creation time) to the target class or module. This is frequently used in Ruby to run a block of code as if it were within the body of the target class, so that method definitions affect the &quot;module_eval&quot; target rather than the code surrounding the block.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We can leverage this behavior to apply refinements to any block of code in the system. Because refined calls must look at the hierarchy of classes in the surrounding scope, every call in every block in every piece of code can potentially become refined in the future, if the block is passed via module_eval to a refined hierarchy. The following simple case might not do what you expect, even if the String class has not been modified directly.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref_7.rb&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Because the &quot;+&quot; method is called within a block, all bets are off. The str_ary passed in might not be a simple Array; it could be any user class that implements the &quot;inject&quot; method. If that implementation chooses, it can force the incoming block of code to be refined. Here's a longer version with such an implementation visible.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref_8.rb&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Suddenly, what looks like a simple addition of two strings produces a distinctly different result.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref_9.txt&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Now that you know how refinements work, let's discuss the problems they create.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Implementation Challenges&lt;/h2&gt;&lt;div&gt;Because I know that most users don't care if a new, useful feature makes my life as a Ruby implementer harder, I'm not going to spend a great deal of time here.&amp;nbsp;My concerns revolve around the complexities of knowing when to do a refined call and how to discover those refinements.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Current Ruby implementations are all built around method dispatch depending solely on the target object's type, and much of the caching and optimization we do depends on that. With refinements in play, we must also search and guard against types in the caller's context, which makes lookup much more complicated. Ideally we'd be able to limit this complexity to only refined calls, but because &quot;using&quot; can affect code far away from where it is called, we often have no way to know whether a given call might be refined in the future. This is especially pronounced in the &quot;module_eval&quot; case, where code that isn't even in the same class hierarchy as a refinement must still observe it.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;There are numerous ways to address the implementation challenges.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Eliminate the &quot;module_eval&quot; Feature&lt;/h3&gt;&lt;div&gt;At present, nobody knows of an easy way to implement the &quot;module_eval&quot; aspect of refinements. The current implementation in MRI does it in a brute-force way, flushing the global method cache on every execution and generating a new, refined, anonymous module for every call. Obviously this is not a feasible direction to go; block dispatch will happen very frequently at runtime, and we can't allow refined blocks to destroy performance for code elsewhere in the system.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The basic problem here is that in order for &quot;module_eval&quot; to work, every block in the system must be treated as a refined body of code all the time. That means that calls inside blocks throughout the system need to search and guard against the calling context even if no refinements are ever applied to them. The end result is that those calls suffer complexity and performance hits across the board.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;At the moment, I do not see (nor does anyone else see) an efficient way to handle the &quot;module_eval&quot; case. It should be removed.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Localize the &quot;using&quot; Call&lt;/h3&gt;&lt;div&gt;No new Ruby feature should cause across-the-board performance hits; one solution is for refinements to be recognized at parse time. This makes it easy to keep existing calls the way they are and only impose refinement complexity upon method calls that are actually refined.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The simplest way to do this is also the most limiting and the most cumbersome: force &quot;using&quot; to only apply to the immediate scope. This would require every body of code to &quot;using&quot; a refinement if method calls in that body should be refined. Here's a couple of our previous examples with this modification.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref10.rb&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;This is obviously pretty ugly, but it makes implementation much simpler. In every scope where we see a &quot;using&quot; call, we simply force all future calls to honor refinements. Calls appearing outside &quot;using&quot; scopes do not get refined and perform calls as normal.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We can improve this by making &quot;using&quot; apply to child scopes as well. This still provides the same parse-time &quot;pseudo-keyword&quot; benefit without the repetition.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref11.rb&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Even better would be to officially make &quot;using&quot; a keyword and have it open a refined scope; that results in a clear delineation between refined and unrefined code. I show two forms of this below; the first opens a scope like &quot;class&quot; or &quot;module&quot;, and the second uses a &quot;do...end&quot; block form.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref12.rb&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;It would be fair to say that requiring more explicit scoping of &quot;using&quot; would address my concern about knowing when to do a refined call. It does not, however, address the issues of locating active refinements at call time.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;text-align: left;&quot;&gt;Locating Refinements&lt;/h3&gt;&lt;div&gt;In each of the above examples, we still must pass some state from the calling context through to the method dispatch logic. Ideally we'd only need to pass in the calling object, which is already passed through for visibility checking. This works for refined class hierarchies, but it does not work for the RSpec case, since the calling object in some cases is just the top-level Object instance (and remember we don't want to decorate Object).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;It turns out that there's already a feature in Ruby that follows lexical scoping: constant lookup. When Ruby code accesses a constant, the runtime must first search all enclosing scopes for a definition of that constant. Failing that, the runtime will walk the self object's class hierarchy. This is similar to what we want for the simplified version of refinements.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If we assume we've localized refinements to only calls within &quot;using&quot; scopes, then at parse time we can emit something like a RefinedCall for every method call in the code. A RefinedCall would be special in that it uses both the containing scope and the target class to look up a target method. The lookup process would proceed as follows:&lt;/div&gt;&lt;div&gt;&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Search the call's context for refinements, walking lexical scopes only&lt;/li&gt;&lt;li&gt;If refinements are found, search for the target method&lt;/li&gt;&lt;li&gt;If a refined method is found, use it for the call&lt;/li&gt;&lt;li&gt;Otherwise, proceed with normal lookup against the target object's class&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;Because the parser has already isolated refinement logic to specific calls, the only change needed is to pass the caller's context through to method dispatch.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Usability Concerns&lt;/h2&gt;&lt;div&gt;There are indeed flavors of refinements that can be implemented reasonably efficiently, or at least implemented in such a way that unrefined code will not pay a price. I believe this is a requirement of any new feature: do no harm. But harm can come in a different form if a new feature makes Ruby code harder to reason about. I have some concerns here.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Let's go back to our &quot;module_eval&quot; case.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref_7.rb&quot;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Because there's no &quot;using&quot; anywhere in the code, and we're not extending some other class, most folks will assume we're simply concatenating strings here. After all, why would I expect my &quot;+&quot; call to do something else? Why &lt;b&gt;should&lt;/b&gt;&amp;nbsp;my &quot;+&quot; call ever do something else here?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Ruby has many features that might be considered a little &quot;magical&quot;. In most cases, they're only magic because the programmer doesn't have a good understanding of how they work. Constant lookup, for example, is actually rather simple...but if you don't know it searches both lexical and hierarchical contexts, you may be confused where values are coming from.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The &quot;module_eval&quot; behavior of refinements simply goes too far. It forces every Ruby programmer to second-guess every block of code they pass into someone else's library or someone else's method call. The guarantees of standard method dispatch no longer apply; you need to know if the method you're calling will change what calls your code makes. You need to understand the internal details of the target method. That's a terrible, terrible thing to do to Rubyists.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The same goes for refinements that are active down a class hierarchy. You can no longer extend a class and know that methods you call actually do what you expect. Instead, you have to know whether your parent classes or their ancestors refine some call you intend to make. I would argue this is considerably &lt;b&gt;worse&lt;/b&gt;&amp;nbsp;than directly monkey-patching some class, since at least in that case every piece of code has a uniform view.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The problems are compounded over time, too. As libraries you use change, you need to again review them to see if refinements are in play. You need to understand all those refinements just to be able to reason about your own code. And you need to hope and pray two libraries you're using don't define different refinements, causing one half of your application to behave one way and the other half of your application to behave another way.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;I believe the current implementation of refinements introduces more complexity than it solves, mostly due to the lack of a strict lexical &quot;using&quot;. Rubyists should be able to look at a piece of code and know what it does based solely on the types of objects it calls. Refinements make that impossible.&lt;br /&gt;&lt;br /&gt;&lt;i style=&quot;font-weight: bold;&quot;&gt;Update:&lt;/i&gt;&amp;nbsp;Josh Ballanco points out another usability problem: &quot;using&quot; only affects method bodies defined temporally after it is called. For example, the following code only refines the &quot;bar&quot; method, not the &quot;foo&quot; method.&lt;br /&gt;&lt;br /&gt;&lt;script src=&quot;https://gist.github.com/4110634.js?file=ref13.rb&quot;&gt;&lt;/script&gt;&lt;br /&gt;This may simply be an artifact of the current implementation, or it may be specified behavior; it's hard to tell since there's no specification of any kind other than the implementation and a handful of tests. In any case, it's yet another confusing aspect, since it means the order in which code is loaded can actually change which refinements are active.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;tl;dr&lt;/h2&gt;&lt;div&gt;My point here is not to beat down refinements. I agree there are cases where they'd be very useful, especially given the sort of monkey-patching I've seen in the wild. But the current implementation overreaches; it provides several features of questionable value, while simultaneously making both performance and understandability harder to achieve. Hopefully we'll be able to work with Matz and ruby-core to come up with a more reasonable, limited version of refinements...or else convince them not to include refinements in Ruby 2.0.&lt;/div&gt;&lt;/div&gt;</content><author><name>headius</name></author><summary type="html">What does the following code do? If you answered &quot;it upcases two strings and adds them together, returning the result&quot; you might be wrong because of a new Ruby feature called &quot;refinements&quot;.Let's start with the problem refinements are supposed to solve: monkey-patching.Monkey-patchingIn Ruby, all classes are mutable. Indeed, when you define a new class, you're really just creating an empty class and filling it with methods. The ability to mutate classes at runtime has been used (or abused) by many libraries and frameworks to decorate Ruby's core classes with additional (or replacement) behavior. For example, you might add a &quot;camelize&quot; method to String that knows how to convert under_score_names to camelCaseNames. This is lovingly called &quot;monkey-patching&quot; by the Ruby community.Monkey-patching can be very useful, and many patterns in Ruby are built around the ability to modify classes. It can also cause problems if a library patches code in a way the user does not expect (or want), or if two libraries try to apply conflicting patches. Sometimes, you simply don't want patches to apply globally, and this is where refinements come in.Localizing MonkeypatchesRefinements have been discussed as a feature for several years, sometimes under the name &quot;selector namespaces&quot;. In essence, refinements are intended to allow monkey-patching only within certain limited scopes, like within a library that wants to use altered or enhanced versions of core Ruby types without affecting code outside the library. This is the case within the ActiveSupport library that forms part of the core of Rails.ActiveSupport provides a number of extensions (patches) to the core Ruby classes like String#pluralize, Range#overlaps?, and Array#second. Some of these extensions are intended for use by Ruby developers, as conveniences that improve the readability or conciseness of code. Others exist mostly to support Rails itself. In both cases, it would be nice if we could prevent those extensions from leaking out of ActiveSupport into code that does not want or need them.RefinementsIn short, refinements provide a way to make class modifications that are only seen from within certain scopes. In the following example, I add a &quot;camelize&quot; method to the String class that's only seen from code within the Foo class. With the Foo class refined, we can see that the &quot;camelize&quot; method is indeed available within the &quot;camelize_string&quot; method but not outside of the Foo class. On the surface, this seems like exactly what we want. Unfortunately, there's a lot more complexity here than meets the eye.Ruby Method DispatchIn order to do a method call in Ruby, a runtime simply looks at the target object's class hierarchy, searches for the method from bottom to top, and upon finding it performs the call. A smart runtime will cache the method to avoid performing this search every time, but in general the mechanics of looking up a method body are rather simple.In an implementation like JRuby, we might cache the method at what's called the &quot;call site&quot;—the point in Ruby code where a method call is actually performed. In order to know that the method is valid for future calls, we perform two checks at the call site: that the incoming object is of the same type as for previous calls; and that the type's hierarchy has not been mutated since the method was cached.Up to now, method dispatch in Ruby has depended solely on the type of the target object. The calling context has not been important to the method lookup process, other than to confirm that visibility restrictions are enforced (primarily for protected methods, since private methods are rejected for non–self calls). That simplicity has allowed Ruby implementations to optimize method calls and Ruby programmers to understand code by simply determining the target object and methods available on it.Refinements change everything.Refinements BasicsLet's revisit the camelize example again. The visible manifestation of refinements comes via the &quot;refine&quot; and &quot;using&quot; methods.The &quot;refine&quot; method takes a class or module (the String class, in this case) and a block. Within the block, methods defined (camelize) are added to what might be called a patch set (a la monkey-patching) that can be applied to specific scopes in the future. The methods are not actually added to the refined class (String) except in a &quot;virtual&quot; sense when a body of code activates the refinement via the &quot;using&quot; method.The &quot;using&quot; method takes a refinement-containing module and applies it to the current scope. Methods within that scope should see the refined version of the class, while methods outside that scope do not.Where things get a little weird is in defining exactly what that scope should be and in implementing refined method lookup in such a way that does not negatively impact the performance of unrefined method lookup. In the current implementation of refinements, a &quot;using&quot; call affects all of the following scopes related to where it is called:The direct scope, such as the top-level of a script, the body of a class, or the body of a method or blockClasses down-hierarchy from a refined class or module bodyBodies of code run via eval forms that change the &quot;self&quot; of the code, such as module_evalIt's worth emphasizing at this point that refinements can affect code far away from the original &quot;using&quot; call site. It goes without saying that refined method calls must now be aware of both the target type and the calling scope, but what of unrefined calls?Dynamic Scoping of Method LookupRefinements (in their current form) basically cause method lookup to be dynamically scoped. In order to properly do a refined call, we need to know what refinements are active for the context in which the call is occurring and the type of the object we're calling against. The latter is simple, obviously, but determining the former turns out to be rather tricky.Locally-applied refinementsIn the simple case, where a &quot;using&quot; call appears alongside the methods we want to affect, the immediate calling scope contains everything we need. Calls in that scope (or in child scopes like method bodies) would perform method lookup based on the target class, a method name, and the hierarchy of scopes that surrounds them. The key for method lookup expands from a simple name to a name plus a call context.Hierarchically-applied refinementsRefinements applied to a class must also affect subclasses, so even when we don't have a &quot;using&quot; call present we still may need to do refined dispatch. The following example illustrates this with a subclass of Foo (building off the previous example). Here, the camelize method is used within a &quot;map&quot; call, showing that refinements used by the Foo class apply to Bar, its method definitions, and any subscopes like blocks within those methods. It should be apparent now why my first example might not do what you expect. Here's my first example again, this time with the Quux class visible. The Quux class uses refinements from the BadRefinement module, effectively changing String#upcase to actually do String#reverse. By looking at the Baz class alone you can't tell what's supposed to happen, even if you are certain that str1 and str2 are always going to be String. Refinements have effectively localized the changes applied by the BadRefinement module, but they've also made the code more difficult to understand; the programmer (or the reader of the code) must know everything about the calling hierarchy to reason about method calls and expected results.Dynamically-applied refinementsOne of the key features of refinements is to allow block-based DSLs (domain-specific languages) to decorate various types of objects without affecting code outside the DSL. For example, an RSpec spec. There's several calls here that we'd like to refine.The &quot;describe&quot; method is called at the top of the script against the &quot;toplevel&quot; object (essentially a singleton Object instance). We'd like to apply a refinement at this level so &quot;describe&quot; does not have to be defined on Object itself.The &quot;it&quot; method is called within the block passed to &quot;describe&quot;. We'd like whatever self object is live inside that block to have an &quot;it&quot; method without modifying self's type directly.The &quot;should&quot; method is called against an instance of MyClass, presumably a user-created class that does not define such a method. We would like to refine MyClass to have the &quot;should&quot; method only within the context of the block we pass to &quot;it&quot;.Finally, the &quot;be_awesome&quot; method—which RSpec translates into a call to MyClass#awesome?—should be available on the self object active in the &quot;it&quot; block without actually adding be_awesome to self's type.In order to do this without having a &quot;using&quot; present in the spec file itself, we need to be able to dynamically apply refinements to code that might otherwise not be refined. The current implementation does this via Module#module_eval (or its argument-receiving brother, Module#module_exec).A block of code passed to &quot;module_eval&quot; or &quot;instance_eval&quot; will see its self object changed from that of the original surrounding scope (the self at block creation time) to the target class or module. This is frequently used in Ruby to run a block of code as if it were within the body of the target class, so that method definitions affect the &quot;module_eval&quot; target rather than the code surrounding the block.We can leverage this behavior to apply refinements to any block of code in the system. Because refined calls must look at the hierarchy of classes in the surrounding scope, every call in every block in every piece of code can potentially become refined in the future, if the block is passed via module_eval to a refined hierarchy. The following simple case might not do what you expect, even if the String class has not been modified directly. Because the &quot;+&quot; method is called within a block, all bets are off. The str_ary passed in might not be a simple Array; it could be any user class that implements the &quot;inject&quot; method. If that implementation chooses, it can force the incoming block of code to be refined. Here's a longer version with such an implementation visible. Suddenly, what looks like a simple addition of two strings produces a distinctly different result. Now that you know how refinements work, let's discuss the problems they create.Implementation ChallengesBecause I know that most users don't care if a new, useful feature makes my life as a Ruby implementer harder, I'm not going to spend a great deal of time here.&amp;nbsp;My concerns revolve around the complexities of knowing when to do a refined call and how to discover those refinements.Current Ruby implementations are all built around method dispatch depending solely on the target object's type, and much of the caching and optimization we do depends on that. With refinements in play, we must also search and guard against types in the caller's context, which makes lookup much more complicated. Ideally we'd be able to limit this complexity to only refined calls, but because &quot;using&quot; can affect code far away from where it is called, we often have no way to know whether a given call might be refined in the future. This is especially pronounced in the &quot;module_eval&quot; case, where code that isn't even in the same class hierarchy as a refinement must still observe it.There are numerous ways to address the implementation challenges.Eliminate the &quot;module_eval&quot; FeatureAt present, nobody knows of an easy way to implement the &quot;module_eval&quot; aspect of refinements. The current implementation in MRI does it in a brute-force way, flushing the global method cache on every execution and generating a new, refined, anonymous module for every call. Obviously this is not a feasible direction to go; block dispatch will happen very frequently at runtime, and we can't allow refined blocks to destroy performance for code elsewhere in the system.The basic problem here is that in order for &quot;module_eval&quot; to work, every block in the system must be treated as a refined body of code all the time. That means that calls inside blocks throughout the system need to search and guard against the calling context even if no refinements are ever applied to them. The end result is that those calls suffer complexity and performance hits across the board.At the moment, I do not see (nor does anyone else see) an efficient way to handle the &quot;module_eval&quot; case. It should be removed.Localize the &quot;using&quot; CallNo new Ruby feature should cause across-the-board performance hits; one solution is for refinements to be recognized at parse time. This makes it easy to keep existing calls the way they are and only impose refinement complexity upon method calls that are actually refined.The simplest way to do this is also the most limiting and the most cumbersome: force &quot;using&quot; to only apply to the immediate scope. This would require every body of code to &quot;using&quot; a refinement if method calls in that body should be refined. Here's a couple of our previous examples with this modification. This is obviously pretty ugly, but it makes implementation much simpler. In every scope where we see a &quot;using&quot; call, we simply force all future calls to honor refinements. Calls appearing outside &quot;using&quot; scopes do not get refined and perform calls as normal.We can improve this by making &quot;using&quot; apply to child scopes as well. This still provides the same parse-time &quot;pseudo-keyword&quot; benefit without the repetition. Even better would be to officially make &quot;using&quot; a keyword and have it open a refined scope; that results in a clear delineation between refined and unrefined code. I show two forms of this below; the first opens a scope like &quot;class&quot; or &quot;module&quot;, and the second uses a &quot;do...end&quot; block form. It would be fair to say that requiring more explicit scoping of &quot;using&quot; would address my concern about knowing when to do a refined call. It does not, however, address the issues of locating active refinements at call time.Locating RefinementsIn each of the above examples, we still must pass some state from the calling context through to the method dispatch logic. Ideally we'd only need to pass in the calling object, which is already passed through for visibility checking. This works for refined class hierarchies, but it does not work for the RSpec case, since the calling object in some cases is just the top-level Object instance (and remember we don't want to decorate Object).It turns out that there's already a feature in Ruby that follows lexical scoping: constant lookup. When Ruby code accesses a constant, the runtime must first search all enclosing scopes for a definition of that constant. Failing that, the runtime will walk the self object's class hierarchy. This is similar to what we want for the simplified version of refinements.If we assume we've localized refinements to only calls within &quot;using&quot; scopes, then at parse time we can emit something like a RefinedCall for every method call in the code. A RefinedCall would be special in that it uses both the containing scope and the target class to look up a target method. The lookup process would proceed as follows:Search the call's context for refinements, walking lexical scopes onlyIf refinements are found, search for the target methodIf a refined method is found, use it for the callOtherwise, proceed with normal lookup against the target object's classBecause the parser has already isolated refinement logic to specific calls, the only change needed is to pass the caller's context through to method dispatch.Usability ConcernsThere are indeed flavors of refinements that can be implemented reasonably efficiently, or at least implemented in such a way that unrefined code will not pay a price. I believe this is a requirement of any new feature: do no harm. But harm can come in a different form if a new feature makes Ruby code harder to reason about. I have some concerns here.Let's go back to our &quot;module_eval&quot; case. Because there's no &quot;using&quot; anywhere in the code, and we're not extending some other class, most folks will assume we're simply concatenating strings here. After all, why would I expect my &quot;+&quot; call to do something else? Why should&amp;nbsp;my &quot;+&quot; call ever do something else here?Ruby has many features that might be considered a little &quot;magical&quot;. In most cases, they're only magic because the programmer doesn't have a good understanding of how they work. Constant lookup, for example, is actually rather simple...but if you don't know it searches both lexical and hierarchical contexts, you may be confused where values are coming from.The &quot;module_eval&quot; behavior of refinements simply goes too far. It forces every Ruby programmer to second-guess every block of code they pass into someone else's library or someone else's method call. The guarantees of standard method dispatch no longer apply; you need to know if the method you're calling will change what calls your code makes. You need to understand the internal details of the target method. That's a terrible, terrible thing to do to Rubyists.The same goes for refinements that are active down a class hierarchy. You can no longer extend a class and know that methods you call actually do what you expect. Instead, you have to know whether your parent classes or their ancestors refine some call you intend to make. I would argue this is considerably worse&amp;nbsp;than directly monkey-patching some class, since at least in that case every piece of code has a uniform view.The problems are compounded over time, too. As libraries you use change, you need to again review them to see if refinements are in play. You need to understand all those refinements just to be able to reason about your own code. And you need to hope and pray two libraries you're using don't define different refinements, causing one half of your application to behave one way and the other half of your application to behave another way.I believe the current implementation of refinements introduces more complexity than it solves, mostly due to the lack of a strict lexical &quot;using&quot;. Rubyists should be able to look at a piece of code and know what it does based solely on the types of objects it calls. Refinements make that impossible.Update:&amp;nbsp;Josh Ballanco points out another usability problem: &quot;using&quot; only affects method bodies defined temporally after it is called. For example, the following code only refines the &quot;bar&quot; method, not the &quot;foo&quot; method.This may simply be an artifact of the current implementation, or it may be specified behavior; it's hard to tell since there's no specification of any kind other than the implementation and a handful of tests. In any case, it's yet another confusing aspect, since it means the order in which code is loaded can actually change which refinements are active.tl;drMy point here is not to beat down refinements. I agree there are cases where they'd be very useful, especially given the sort of monkey-patching I've seen in the wild. But the current implementation overreaches; it provides several features of questionable value, while simultaneously making both performance and understandability harder to achieve. Hopefully we'll be able to work with Matz and ruby-core to come up with a more reasonable, limited version of refinements...or else convince them not to include refinements in Ruby 2.0.</summary></entry><entry><title type="html">So You Want To Optimize Ruby</title><link href="https://headius.github.io/2012/10/so-you-want-to-optimize-ruby.html" rel="alternate" type="text/html" title="So You Want To Optimize Ruby"><published>2012-10-15T18:26:00+00:00</published><updated>2012-10-15T18:26:00+00:00</updated><id>https://headius.github.io/2012/10/so-you-want-to-optimize-ruby</id><content type="html" xml:base="https://headius.github.io/2012/10/so-you-want-to-optimize-ruby.html">&lt;div dir=&quot;ltr&quot; style=&quot;text-align: left;&quot; trbidi=&quot;on&quot;&gt;I was recently asked for a list of &quot;hard problems&quot; a Ruby implementation really needs to solve before reporting benchmark numbers. You know...the sort of problems that might invalidate early perf numbers because they impact how you optimize Ruby. This post is a rework of my response...I hope you find it informative!&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Fixnum to Bignum promotion&lt;/h4&gt;In Ruby, Fixnum math can promote to Bignum when the result is out of Fixnum's range. On implementations that use tagged pointers to represent Fixnum (MRI, Rubinius, MacRuby), the Fixnum range is somewhat less than the base CPU bits (32/64). On JRuby, Fixnum is always a straight 64-bit signed value.&lt;br /&gt;&lt;br /&gt;This promotion is a performance concern for a couple reasons:&lt;br /&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Every math operation that returns a new Fixnum must be range-checked. This slows all Fixnum operations.&lt;/li&gt;&lt;li&gt;It is difficult (if not impossible) to predict whether a Fixnum math operation will return a Fixnum or a Bignum. Since Bignum is always represented as a full object (not a primitive or a tagged pointer) this impacts optimizing Fixnum math call sites.&lt;/li&gt;&lt;/ul&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Floating-point performance&lt;/h4&gt;A similar concern is the performance of floating point values. Most of&amp;nbsp;the native implementations have tagged values for Fixnum but only one&amp;nbsp;I know of (Macruby) uses tagged values for Float. This can skew&amp;nbsp;expectations because an implementation may perform very well on integer math and&amp;nbsp;considerably worse on floating-point math due to the objects created (and collected). JRuby uses objects for both Fixnum and Float, so performance is roughly equivalent (and slower than I'd like).&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Closures&lt;/h4&gt;&lt;div&gt;Any language that supports closures (&quot;blocks&quot; in Ruby) has to deal with efficiently accessing frame-local data from calls down-stack. In Java, both anonymous inner classes and the upcoming lambda feature treat frame-local values (local variables, basically) as immutable...so their values can simply be copied into the closure object or carried along in some other way. In Ruby, local variables are always mutable, so an eventual activation of a closure body needs to be able to write into its containing frame. If a runtime does not support arbitrary frame access (as is the case on the JVM) it may have to allocate a separate data structure to represent those frame locals...and that impacts performance.&lt;/div&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Bindings and eval&lt;/h4&gt;The eval methods in Ruby can usually accept an optional binding under which to run. This means any call to binding must return a fully-functional execution environment, and in JRuby this means both eval and binding force a full deoptimization of the surrounding method body.&lt;br /&gt;&lt;br /&gt;There's an even more unpleasant aspect to this, however: every block can be used as a binding too.&lt;br /&gt;&lt;br /&gt;All blocks can be&amp;nbsp;turned into Proc and used as bindings, which means every block in the&amp;nbsp;system has to have full access to values in the containing call frame. Most implementers hate this feature, since it means that optimizing call frames in the presence of blocks is much more difficult. Because they can be used as a binding, that of course means&amp;nbsp;literally all frame data must be accessible: local variables;&amp;nbsp;frame-local $ variables like $~; constants lookup environment; method visibility; and so on.&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;callcc and Continuation&lt;/h4&gt;JRuby doesn't implement callcc since the JVM doesn't support continuations, but any implementation hoping to optimize Ruby will have to take a stance here. Continuations obviously make optimization more difficult since you can branch into and out of execution contexts in rather unusual ways.&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Fiber implementation&lt;/h4&gt;In JRuby, each Fiber runs on its own thread (though we pool the native thread to reduce Fiber spin-up costs). Other than that they&amp;nbsp;operate pretty much like closures.&lt;br /&gt;&lt;br /&gt;A Ruby implementer needs to decide whether it will use C-style native stack juggling (which makes optimizations like frame elimination trickier to implement) or give Fibers their own stacks in which to execute independently.&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Thread/frame/etc local $globals&lt;/h4&gt;Thread globals are easy, obviously. All(?) host systems already have some repesentation of thread-local values.&amp;nbsp;The tricky ones are explicit frame&amp;nbsp;globals like $~ and $_ and implicit frame-local values like&amp;nbsp;visibility, etc.&lt;br /&gt;&lt;br /&gt;In the case of $~ and $_, the challenge is not in representing accesses of them directly but in handling implicit reads and writes of them that cross call boundaries. For example, calling [] on a String and passing a Regexp will cause the caller's frame-local $~ (and related values) to be updated to the MatchData for the pattern match that happens inside []. There are a number of core Ruby methods like this that can reach back into the caller's frame and read or write these values. This obviously makes reducing or eliminating call frames very tricky.&lt;br /&gt;&lt;br /&gt;In JRuby, we track all core methods that read or write these values, and if we see those methods called in a body of code (the names, mind you...this is a static inspection), we will stand up a call frame for that body. This is not ideal. We would like to move these values into a separate stack that's lazily allocated only when actually needed, since methods that cross frames like String#[] force other methods like Array#[] to deoptimize too.&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;C extension support&lt;/h4&gt;If a given Ruby implementation is likely to fit into the &quot;native&quot; side of Ruby&amp;nbsp;implementations (as opposed to implementations like JRuby or IronRuby that target an existing managed runtime), it will need to have a C extension story.&lt;br /&gt;&lt;br /&gt;Ruby's C&amp;nbsp;extension API is easier to support than some languages' native APIs (e.g. no reference-counting as in Python)&amp;nbsp;but it still very much impacts how a runtime optimizes. Because the API needs to return forever-valid object references, implementations that don't give out pointers will have to maintain a handle table. The API includes a number of macros that provide access to object internals; they'll need to be simulated or explicitly unsupported. And the API makes no guarantees about concurrency and provides few primitives for controlling concurrent execution, so most implementations will need to lock around native downcalls.&lt;br /&gt;&lt;br /&gt;An alternative for a new Ruby implementation is to expect extensions to be written in the host runtime's native language (Java or other JVM languages for JRuby; C# or other .NET languages for IronRuby, etc). However this imposes a burden on folks implementing language extensions, since they'll have to support yet another language to cover all Ruby implementations.&lt;br /&gt;&lt;br /&gt;Ultimately, though, the unfortunate fact for most &quot;native&quot; impls is that regardless of how fast&amp;nbsp;you can run Ruby code, the choke point is often going to be the C API&amp;nbsp;emulation, since it will require a lot of handle-juggling and indirection&amp;nbsp;compared to MRI. So without supporting the C API, there's a very large&amp;nbsp;part of the story missing...a part of the story that accesses frame&amp;nbsp;locals, closure bodies, bindings, and so on.&lt;br /&gt;&lt;br /&gt;Of course if you can run Ruby code as fast as C, maybe it won't&amp;nbsp;matter. :) Users can just implement their extensions in Ruby.&amp;nbsp;JRuby is starting to approach that kind of performance for non-numeric,&amp;nbsp;non-closure cases, but that sort of perf is not yet widespread enough to&amp;nbsp;bank on.&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Ruby 1.9 encoding support&lt;/h4&gt;Any benchmark that touches anything relating to binary text&amp;nbsp;data must have encoding support, or you're really fudging the&amp;nbsp;numbers. Encoding touches damn near everything, and can add a significant amount of overhead to String-manipulating benchmarks.&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Garbage collection and object allocation&lt;/h4&gt;It's easy for a new impl to show good performance on benchmarks that&amp;nbsp;do no allocation (or little allocation) and require no GC, like raw numerics (fib, tak, etc).&amp;nbsp;Macruby and Rubinius, for example, really shine here. But many impls&amp;nbsp;have drastically different performance when an algorithm starts&amp;nbsp;allocating objects.&amp;nbsp;Very&amp;nbsp;few applications are doing pure integer numeric algorithms, so object&lt;br /&gt;allocation and GC performance are an absolutely critical part of the performance story.&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Concurrency / Parallelism&lt;/h4&gt;If you intend to be an impl that supports parallel thread execution,&amp;nbsp;you're going to have to deal with various issues before publishing&amp;nbsp;numbers. For example, threads can #kill or #raise each other, which in&lt;br /&gt;a truly parallel runtime requires periodic safepoints/pings to know&amp;nbsp;whether a cross-thread event has fired. If you're not handling those&amp;nbsp;safepoints, you're not telling the whole story, since they impact execution.&lt;br /&gt;&lt;br /&gt;There's also the thread-safety of runtime structures to be considered. As an example,&amp;nbsp;Rubinius until recently had a hard lock around a data structure responsible for invalidating call sites, which&amp;nbsp;meant that its simple inline cache could see a severe performance&amp;nbsp;degradation at polymorphic call sites (they've since added polymorphic caching to ameliorate this case). The thread-safety of a Ruby implementation's core runtime structures can drastically impact even straight-line, non-concurrent performance.&lt;br /&gt;&lt;br /&gt;Of course, for an impl that doesn't support parallel execution (which&amp;nbsp;would put it in the somewhat more limited realm of MRI), you can get away with GIL&amp;nbsp;scheduling tricks. You just won't have a very good in-process scaling story.&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Tracing/debugging&lt;/h4&gt;All current impls support tracing or debugging APIs, though some (like&lt;br /&gt;JRuby) require you to enable support for them via command-line or compile-time flags. A Ruby implementation needs to have an answer for&amp;nbsp;this, since the runtime-level hooks required will have an impact...and may&amp;nbsp;require users to opt-in.&lt;br /&gt;&lt;h4&gt;ObjectSpace&lt;/h4&gt;ObjectSpace#each_object needs to be addressed before talking about&amp;nbsp;performance. In JRuby, supporting each_object over arbitrary types was&amp;nbsp;a major performance issue, since we had to track all objects in a&amp;nbsp;separate data structure in case they were needed. We ultimately&amp;nbsp;decided each_object would only work with Class and Module, since those&amp;nbsp;were the major practical use cases (and tracking Class/Module hierarchies is far easier than tracking all objects in the system).&lt;br /&gt;&lt;br /&gt;Depending on how a Ruby implementation tracks in-memory objects (and depending on the level of accuracy expected from ObjectSpace#each_object) this can impact how allocation logic and GC are optimized.&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Method invalidation&lt;/h4&gt;Several implementations can see severe global effects due to methods like Object#extend&amp;nbsp;blowing all global caches (or at least several caches), so you need to be&amp;nbsp;able to support #extend in a reasonable way before talking about&amp;nbsp;performance. Singleton objects also have a similar effect, since they&amp;nbsp;alter the character of method caches by introducing new anonymous types at&amp;nbsp;any time (and sometimes, in rapid succession).&lt;br /&gt;&lt;br /&gt;In JRuby, singleton and #extend effects are limited to the call sites that see them. I also have an experimental branch that's smarter about type identity, so simple anonymous types (that have only had modules included or extended into them) will not damage caches at all. Hopefully we'll land that in a future release.&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Constant lookup and invalidation&lt;/h4&gt;I believe all implementations have implemented constant cache&amp;nbsp;invalidation as a global invalidation, though there are other more&amp;nbsp;complicated ways to do it. The main challenge is the fact that constant lookup is tied to both lexical scope and class hiearchy, so invalidating individual constant lookup sites is usually infeasible. Constant lookup is also rather tricky&amp;nbsp;and must be implemented correctly&amp;nbsp;before talking about the performance of any benchmark that references&amp;nbsp;constants.&lt;br /&gt;&lt;h4 style=&quot;text-align: left;&quot;&gt;Rails&lt;/h4&gt;&lt;div&gt;Finally, regardless of how awesome a new Ruby implementation claims to be, most users will simply ask &quot;but does it run Rails?&quot; You can substitute your favorite framework or library, if you like...the bottom line is that an awesome Ruby implementation that doesn't run any Ruby applications is basically useless. Beware of crowing about your victory over Ruby performance before you can run code people actually care about.&lt;/div&gt;&lt;/div&gt;</content><author><name>headius</name></author><summary type="html">I was recently asked for a list of &quot;hard problems&quot; a Ruby implementation really needs to solve before reporting benchmark numbers. You know...the sort of problems that might invalidate early perf numbers because they impact how you optimize Ruby. This post is a rework of my response...I hope you find it informative!Fixnum to Bignum promotionIn Ruby, Fixnum math can promote to Bignum when the result is out of Fixnum's range. On implementations that use tagged pointers to represent Fixnum (MRI, Rubinius, MacRuby), the Fixnum range is somewhat less than the base CPU bits (32/64). On JRuby, Fixnum is always a straight 64-bit signed value.This promotion is a performance concern for a couple reasons:Every math operation that returns a new Fixnum must be range-checked. This slows all Fixnum operations.It is difficult (if not impossible) to predict whether a Fixnum math operation will return a Fixnum or a Bignum. Since Bignum is always represented as a full object (not a primitive or a tagged pointer) this impacts optimizing Fixnum math call sites.Floating-point performanceA similar concern is the performance of floating point values. Most of&amp;nbsp;the native implementations have tagged values for Fixnum but only one&amp;nbsp;I know of (Macruby) uses tagged values for Float. This can skew&amp;nbsp;expectations because an implementation may perform very well on integer math and&amp;nbsp;considerably worse on floating-point math due to the objects created (and collected). JRuby uses objects for both Fixnum and Float, so performance is roughly equivalent (and slower than I'd like).ClosuresAny language that supports closures (&quot;blocks&quot; in Ruby) has to deal with efficiently accessing frame-local data from calls down-stack. In Java, both anonymous inner classes and the upcoming lambda feature treat frame-local values (local variables, basically) as immutable...so their values can simply be copied into the closure object or carried along in some other way. In Ruby, local variables are always mutable, so an eventual activation of a closure body needs to be able to write into its containing frame. If a runtime does not support arbitrary frame access (as is the case on the JVM) it may have to allocate a separate data structure to represent those frame locals...and that impacts performance.Bindings and evalThe eval methods in Ruby can usually accept an optional binding under which to run. This means any call to binding must return a fully-functional execution environment, and in JRuby this means both eval and binding force a full deoptimization of the surrounding method body.There's an even more unpleasant aspect to this, however: every block can be used as a binding too.All blocks can be&amp;nbsp;turned into Proc and used as bindings, which means every block in the&amp;nbsp;system has to have full access to values in the containing call frame. Most implementers hate this feature, since it means that optimizing call frames in the presence of blocks is much more difficult. Because they can be used as a binding, that of course means&amp;nbsp;literally all frame data must be accessible: local variables;&amp;nbsp;frame-local $ variables like $~; constants lookup environment; method visibility; and so on.callcc and ContinuationJRuby doesn't implement callcc since the JVM doesn't support continuations, but any implementation hoping to optimize Ruby will have to take a stance here. Continuations obviously make optimization more difficult since you can branch into and out of execution contexts in rather unusual ways.Fiber implementationIn JRuby, each Fiber runs on its own thread (though we pool the native thread to reduce Fiber spin-up costs). Other than that they&amp;nbsp;operate pretty much like closures.A Ruby implementer needs to decide whether it will use C-style native stack juggling (which makes optimizations like frame elimination trickier to implement) or give Fibers their own stacks in which to execute independently.Thread/frame/etc local $globalsThread globals are easy, obviously. All(?) host systems already have some repesentation of thread-local values.&amp;nbsp;The tricky ones are explicit frame&amp;nbsp;globals like $~ and $_ and implicit frame-local values like&amp;nbsp;visibility, etc.In the case of $~ and $_, the challenge is not in representing accesses of them directly but in handling implicit reads and writes of them that cross call boundaries. For example, calling [] on a String and passing a Regexp will cause the caller's frame-local $~ (and related values) to be updated to the MatchData for the pattern match that happens inside []. There are a number of core Ruby methods like this that can reach back into the caller's frame and read or write these values. This obviously makes reducing or eliminating call frames very tricky.In JRuby, we track all core methods that read or write these values, and if we see those methods called in a body of code (the names, mind you...this is a static inspection), we will stand up a call frame for that body. This is not ideal. We would like to move these values into a separate stack that's lazily allocated only when actually needed, since methods that cross frames like String#[] force other methods like Array#[] to deoptimize too.C extension supportIf a given Ruby implementation is likely to fit into the &quot;native&quot; side of Ruby&amp;nbsp;implementations (as opposed to implementations like JRuby or IronRuby that target an existing managed runtime), it will need to have a C extension story.Ruby's C&amp;nbsp;extension API is easier to support than some languages' native APIs (e.g. no reference-counting as in Python)&amp;nbsp;but it still very much impacts how a runtime optimizes. Because the API needs to return forever-valid object references, implementations that don't give out pointers will have to maintain a handle table. The API includes a number of macros that provide access to object internals; they'll need to be simulated or explicitly unsupported. And the API makes no guarantees about concurrency and provides few primitives for controlling concurrent execution, so most implementations will need to lock around native downcalls.An alternative for a new Ruby implementation is to expect extensions to be written in the host runtime's native language (Java or other JVM languages for JRuby; C# or other .NET languages for IronRuby, etc). However this imposes a burden on folks implementing language extensions, since they'll have to support yet another language to cover all Ruby implementations.Ultimately, though, the unfortunate fact for most &quot;native&quot; impls is that regardless of how fast&amp;nbsp;you can run Ruby code, the choke point is often going to be the C API&amp;nbsp;emulation, since it will require a lot of handle-juggling and indirection&amp;nbsp;compared to MRI. So without supporting the C API, there's a very large&amp;nbsp;part of the story missing...a part of the story that accesses frame&amp;nbsp;locals, closure bodies, bindings, and so on.Of course if you can run Ruby code as fast as C, maybe it won't&amp;nbsp;matter. :) Users can just implement their extensions in Ruby.&amp;nbsp;JRuby is starting to approach that kind of performance for non-numeric,&amp;nbsp;non-closure cases, but that sort of perf is not yet widespread enough to&amp;nbsp;bank on.Ruby 1.9 encoding supportAny benchmark that touches anything relating to binary text&amp;nbsp;data must have encoding support, or you're really fudging the&amp;nbsp;numbers. Encoding touches damn near everything, and can add a significant amount of overhead to String-manipulating benchmarks.Garbage collection and object allocationIt's easy for a new impl to show good performance on benchmarks that&amp;nbsp;do no allocation (or little allocation) and require no GC, like raw numerics (fib, tak, etc).&amp;nbsp;Macruby and Rubinius, for example, really shine here. But many impls&amp;nbsp;have drastically different performance when an algorithm starts&amp;nbsp;allocating objects.&amp;nbsp;Very&amp;nbsp;few applications are doing pure integer numeric algorithms, so objectallocation and GC performance are an absolutely critical part of the performance story.Concurrency / ParallelismIf you intend to be an impl that supports parallel thread execution,&amp;nbsp;you're going to have to deal with various issues before publishing&amp;nbsp;numbers. For example, threads can #kill or #raise each other, which ina truly parallel runtime requires periodic safepoints/pings to know&amp;nbsp;whether a cross-thread event has fired. If you're not handling those&amp;nbsp;safepoints, you're not telling the whole story, since they impact execution.There's also the thread-safety of runtime structures to be considered. As an example,&amp;nbsp;Rubinius until recently had a hard lock around a data structure responsible for invalidating call sites, which&amp;nbsp;meant that its simple inline cache could see a severe performance&amp;nbsp;degradation at polymorphic call sites (they've since added polymorphic caching to ameliorate this case). The thread-safety of a Ruby implementation's core runtime structures can drastically impact even straight-line, non-concurrent performance.Of course, for an impl that doesn't support parallel execution (which&amp;nbsp;would put it in the somewhat more limited realm of MRI), you can get away with GIL&amp;nbsp;scheduling tricks. You just won't have a very good in-process scaling story.Tracing/debuggingAll current impls support tracing or debugging APIs, though some (likeJRuby) require you to enable support for them via command-line or compile-time flags. A Ruby implementation needs to have an answer for&amp;nbsp;this, since the runtime-level hooks required will have an impact...and may&amp;nbsp;require users to opt-in.ObjectSpaceObjectSpace#each_object needs to be addressed before talking about&amp;nbsp;performance. In JRuby, supporting each_object over arbitrary types was&amp;nbsp;a major performance issue, since we had to track all objects in a&amp;nbsp;separate data structure in case they were needed. We ultimately&amp;nbsp;decided each_object would only work with Class and Module, since those&amp;nbsp;were the major practical use cases (and tracking Class/Module hierarchies is far easier than tracking all objects in the system).Depending on how a Ruby implementation tracks in-memory objects (and depending on the level of accuracy expected from ObjectSpace#each_object) this can impact how allocation logic and GC are optimized.Method invalidationSeveral implementations can see severe global effects due to methods like Object#extend&amp;nbsp;blowing all global caches (or at least several caches), so you need to be&amp;nbsp;able to support #extend in a reasonable way before talking about&amp;nbsp;performance. Singleton objects also have a similar effect, since they&amp;nbsp;alter the character of method caches by introducing new anonymous types at&amp;nbsp;any time (and sometimes, in rapid succession).In JRuby, singleton and #extend effects are limited to the call sites that see them. I also have an experimental branch that's smarter about type identity, so simple anonymous types (that have only had modules included or extended into them) will not damage caches at all. Hopefully we'll land that in a future release.Constant lookup and invalidationI believe all implementations have implemented constant cache&amp;nbsp;invalidation as a global invalidation, though there are other more&amp;nbsp;complicated ways to do it. The main challenge is the fact that constant lookup is tied to both lexical scope and class hiearchy, so invalidating individual constant lookup sites is usually infeasible. Constant lookup is also rather tricky&amp;nbsp;and must be implemented correctly&amp;nbsp;before talking about the performance of any benchmark that references&amp;nbsp;constants.RailsFinally, regardless of how awesome a new Ruby implementation claims to be, most users will simply ask &quot;but does it run Rails?&quot; You can substitute your favorite framework or library, if you like...the bottom line is that an awesome Ruby implementation that doesn't run any Ruby applications is basically useless. Beware of crowing about your victory over Ruby performance before you can run code people actually care about.</summary></entry><entry><title type="html">Explanation of Warnings From MRI’s Test Suite</title><link href="https://headius.github.io/2012/09/explanation-of-warnings-from-mris-test.html" rel="alternate" type="text/html" title="Explanation of Warnings From MRI's Test Suite"><published>2012-09-26T21:03:00+00:00</published><updated>2012-09-26T21:03:00+00:00</updated><id>https://headius.github.io/2012/09/explanation-of-warnings-from-mris-test</id><content type="html" xml:base="https://headius.github.io/2012/09/explanation-of-warnings-from-mris-test.html">&lt;div dir=&quot;ltr&quot; style=&quot;text-align: left;&quot; trbidi=&quot;on&quot;&gt;JRuby has, for some time now, run the same &lt;a href=&quot;https://github.com/jruby/jruby/tree/master/test/externals/ruby1.9&quot;&gt;test suite as MRI&lt;/a&gt; (C Ruby, Matz's Ruby). Because not all tests pass, we use &lt;a href=&quot;https://github.com/seattlerb/minitest-excludes&quot;&gt;minitest-excludes&lt;/a&gt; to mask out the failures, and over time we unmask stuff as we fix it.&lt;br /&gt;&lt;br /&gt;However, there's a number of warnings we get from the suite that are nonfatal and unmaskable. I thought I'd show them to you and tell their stories.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;JRuby 1.9 mode only supports the `psych` YAML engine; ignoring `syck`&lt;/b&gt;&lt;br /&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;div&gt;When we started implementing support for the new &quot;psych&quot; YAML engine that Aaron Patterson created (atop libyaml) for Ruby 1.9, we decided that we would not support the broken &quot;syck&quot; engine anymore. The libyaml version is strictly YAML spec compliant, and this is our contribution to ridding the world of &quot;syck&quot;'s broken YAML forever.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;GC.stress= does nothing on JRuby&lt;/b&gt;&lt;br /&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div&gt;JRuby does not have direct control over the JVM's GC, and so we can't implement things like GC.stress=, which MRI uses to put the GC into &quot;stress&quot; mode (GCing much more frequently to better test GC stability and behavior). There are flags for the JVM to do this sort of testing, but since we don't really need to test the JVM's GC for correctness and stability, we have not exposed those flags directly.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;This flag is used in a number of MRI tests to force GC to happen more often and/or to actually test GC behaviors.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;SAFE levels are not supported in JRuby&lt;/b&gt;&lt;br /&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div&gt;JRuby does not support standard Ruby's security model, &quot;safe levels&quot;, because we believe safe levels are a flawed, too-coarse mechanism. On JRuby, you can use standard Java security policies.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We have debated mapping the various Ruby safe levels to equivalent sets of Java security permissions, but have never gotten around to it.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;GC.enable does nothing on JRuby / GC.disable does nothing on JRuby&lt;/b&gt;&lt;br /&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div&gt;There's no standard API on the JVM to disable the garbage collector completely, so GC.enable and GC.disable do nothing in JRuby.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;It's also interesting to note that while you &lt;b&gt;can&lt;/b&gt;&amp;nbsp;request a GC run from the JVM by calling System.gc, JRuby also stubs out Ruby's GC.start. We opted to do this because GC.start is used in some Ruby libraries as a band-aid around Ruby's sometimes-slow GC, but the same call on JRuby is both unnecessary (because GC overhead is rarely a problem) and a major performance hit (because it triggers a full GC over the entire heap).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>headius</name></author><summary type="html">JRuby has, for some time now, run the same test suite as MRI (C Ruby, Matz's Ruby). Because not all tests pass, we use minitest-excludes to mask out the failures, and over time we unmask stuff as we fix it.However, there's a number of warnings we get from the suite that are nonfatal and unmaskable. I thought I'd show them to you and tell their stories.JRuby 1.9 mode only supports the `psych` YAML engine; ignoring `syck`When we started implementing support for the new &quot;psych&quot; YAML engine that Aaron Patterson created (atop libyaml) for Ruby 1.9, we decided that we would not support the broken &quot;syck&quot; engine anymore. The libyaml version is strictly YAML spec compliant, and this is our contribution to ridding the world of &quot;syck&quot;'s broken YAML forever.GC.stress= does nothing on JRubyJRuby does not have direct control over the JVM's GC, and so we can't implement things like GC.stress=, which MRI uses to put the GC into &quot;stress&quot; mode (GCing much more frequently to better test GC stability and behavior). There are flags for the JVM to do this sort of testing, but since we don't really need to test the JVM's GC for correctness and stability, we have not exposed those flags directly.This flag is used in a number of MRI tests to force GC to happen more often and/or to actually test GC behaviors.SAFE levels are not supported in JRubyJRuby does not support standard Ruby's security model, &quot;safe levels&quot;, because we believe safe levels are a flawed, too-coarse mechanism. On JRuby, you can use standard Java security policies.We have debated mapping the various Ruby safe levels to equivalent sets of Java security permissions, but have never gotten around to it.GC.enable does nothing on JRuby / GC.disable does nothing on JRubyThere's no standard API on the JVM to disable the garbage collector completely, so GC.enable and GC.disable do nothing in JRuby.It's also interesting to note that while you can&amp;nbsp;request a GC run from the JVM by calling System.gc, JRuby also stubs out Ruby's GC.start. We opted to do this because GC.start is used in some Ruby libraries as a band-aid around Ruby's sometimes-slow GC, but the same call on JRuby is both unnecessary (because GC overhead is rarely a problem) and a major performance hit (because it triggers a full GC over the entire heap).</summary></entry></feed>