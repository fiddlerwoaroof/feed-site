<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>The ryg blog</title>
	<atom:link href="https://fgiesen.wordpress.com/feed/" rel="self" type="application/rss+xml"/>
	<link>https://fgiesen.wordpress.com</link>
	<description>When I grow up I'll be an inventor.</description>
	<lastBuildDate>Mon, 30 Aug 2021 08:49:26 +0000</lastBuildDate>
	<language>en</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
<cloud domain="fgiesen.wordpress.com" port="80" path="/?rsscloud=notify" registerProcedure="" protocol="http-post"/>
<image>
		<url>https://s0.wp.com/i/buttonw-com.png</url>
		<title>The ryg blog</title>
		<link>https://fgiesen.wordpress.com</link>
	</image>
	<atom:link rel="search" type="application/opensearchdescription+xml" href="https://fgiesen.wordpress.com/osd.xml" title="The ryg blog"/>
	<atom:link rel="hub" href="https://fgiesen.wordpress.com/?pushpress=hub"/>
	<item>
		<title>Entropy coding in Oodle Data: Huffman coding</title>
		<link>https://fgiesen.wordpress.com/2021/08/30/entropy-coding-in-oodle-data-huffman-coding/</link>
					<comments>https://fgiesen.wordpress.com/2021/08/30/entropy-coding-in-oodle-data-huffman-coding/#respond</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Mon, 30 Aug 2021 08:49:26 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<category><![CDATA[Compression]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7171</guid>

					<description><![CDATA[Last time I covered the big picture, so we know the ground rules for the modular entropy coding layer in Oodle Data: bytestream consisting of several independent streams, pluggable algorithms, bytes in and bytes out, and entropy decoding is done as a separate pass, not inlined into the code that consumes the decoded data. There [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>Last time I covered the big picture, so we know the ground rules for the modular entropy coding layer in Oodle Data: bytestream consisting of several independent streams, pluggable algorithms, bytes in and bytes out, and entropy decoding is done as a separate pass, not inlined into the code that consumes the decoded data.</p>
<p>There are good and bad news here: the good news is that the encoders and decoders need to do just one thing, they have homogeneous data to work with, and they ultimately boil down to very simple operations in extremely predictable loops so they can pull out all the stops when it comes to optimization. The bad news is that since they are running on their own, if they&#8217;re inefficient, there&#8217;s no other useful work being accomplished when the decoders are stalled. Careful design is a necessity if we want to ensure that things go smoothly come decode time.</p>
<p>In this post, I&#8217;ll start by looking at Huffman coding, or rather, Huffman decoding; encoding, we won&#8217;t have to worry about much, since it&#8217;s an inherently easier problem to begin with and the design decisions we&#8217;ll make to enable fast decoding will also make encoding faster. Required reading for everything that follows will be my earlier posts <a href="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/">&#8220;A whirlwind introduction to dataflow graphs&#8221;</a> as well as my series <a href="https://fgiesen.wordpress.com/2018/02/19/reading-bits-in-far-too-many-ways-part-1/">&#8220;Reading bits in far too many ways&#8221;</a> (<a href="https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/">part 2</a>, <a href="https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/">part 3</a>). I will assume the contents of these posts as known, so if something is unclear and has to do with bit IO, it&#8217;s in there.</p>
<h3>Huffman (de)coding basics</h3>
<p>Huffman coding is a form of variable-length coding that in its usual presentation is given a finite symbol alphabet <em>Î£</em> along with a positive count <em>f<sub>s</sub></em> (the <em>frequency</em>) of how often each symbol <em>s</em> occurs in the source. Huffman&#8217;s classic algorithm then assigns variable-length binary codes (meaning strings of 0s and 1s) of length <em>c<sub>s</sub></em> to each symbol that guarantee unique decodability and minimize the number of bits in the coded version of the source; we want to send the symbols from the source in fewer bits, and the way to do so is to assign shorter codes to more frequent symbols. To be precise, it solves the optimization problem</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin+%5Csum_%7Bs+%5Cin+%5CSigma%7D+f_s+c_s&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;displaystyle &#92;min &#92;sum_{s &#92;in &#92;Sigma} f_s c_s" class="latex" /></p>
<p>(i.e. minimize the number of bits sent for the payload, because each <em>c<sub>s</sub></em>-bit code word occurs <em>f<sub>s</sub></em> times in the source) for integer <em>c<sub>s</sub></em> subject to the constraints</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+c_s+%5Cin+%5Cmathbb%7BZ%7D%2C+1+%5Cle+c_s+%5Cforall+s+%5Cin+%5CSigma%2C+%5Csum_%7Bs+%5Cin+%5CSigma%7D+2%5E%7B-c_s%7D+%3D+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;displaystyle c_s &#92;in &#92;mathbb{Z}, 1 &#92;le c_s &#92;forall s &#92;in &#92;Sigma, &#92;sum_{s &#92;in &#92;Sigma} 2^{-c_s} = 1" class="latex" /></p>
<p>The first of these constraints just requires that code lengths be positive integers and is fairly self-explanatory, the second requires equality in the Kraft-McMillan inequality which guarantees that the resulting code is both complete and uniquely decodable; in fact, let&#8217;s just call the quantity <img src="https://s0.wp.com/latex.php?latex=K+%3D+%28%5Csum_%7Bs+%5Cin+%5CSigma%7D+2%5E%7B-c_s%7D%29+-+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="K = (&#92;sum_{s &#92;in &#92;Sigma} 2^{-c_s}) - 1" class="latex" /> the &#8220;Kraft defect&#8221;. When <em>K</em>&lt;0, the code has redundancy (i.e. there free code space left for assignment), for <em>K</em>=0 it is complete, and <em>K</em>&gt;0 is over-complete (not uniquely decodable or using more code space than is available).</p>
<p>Huffman&#8217;s algorithm to perform this construction is a computer science classic, intuitive, a literal textbook example for greedy algorithms and matroids, and it even gives us not just the sequence of code lengths, but an actual code assignment that achieves those code lengths. Nevertheless, actual Huffman codes are of limited use in applications, primarily due to one big problem: the above problem places no upper bound on the code lengths, and indeed a <em>n</em>-symbol alphabet can reach a maximum code length of up to <em>n</em>-1 given the right (adversarial) frequency distribution that produces a fully left- or right-leaning tree.<sup><a href="#fn1">1</a></sup> This is bad for both encoders and decoders, which generally <em>have</em> to adhere at the very least to limits derived from the choice of target platform (mostly register width) and bit I/O refill strategy (see my long series on the topic mentioned earlier), although it is often desirable to enforce much tighter limits (more on that below). One way to accomplish this is by taking the results of the standard Huffman construction and using heuristics and local rewrite rules on either the tree or the code lengths to make the longest codes shorter at the expense of making some other (more frequent) codes longer than they would otherwise have to be. Another easy heuristic option is to just use the regular Huffman construction and if codes end up over-long, divide all symbol frequencies by 2 while rounding up (so that non-zero symbol frequencies never drop to zero) and keep trying again until success. This works because it keeps the frequencies of more popular symbols approximately correct and in the right relation to each other, while it flattens out the distribution of the less popular symbols, which all end up eventually having symbol frequencies of 1, resulting in a uniform symbol distribution at the bottom end. A non-heuristic, actually optimal option is to directly solve a constrained version of the above optimized problem that adds a maximum codeword length constraint. The standard solution to the resulting combinatorial optimization problem is called the <a href="https://en.wikipedia.org/wiki/Package-merge_algorithm">package-merge algorithm</a>.</p>
<p>A related observation is that there is really no particular reason to prefer the code assignment produced by Huffman&#8217;s algorithm. Even for cases where no length-limiting is required to fall within usual limits, there are usually many possible code assignments that all result in the exact same bit count for the payload portion; and given an assignment of code length to symbols, it is easy to come up with a codeword assignment that matches those code lengths. In short, the per-symbol code lengths make a difference to how efficient a given code is, but we can freely choose which codes we assign subject to the length constraint, and can do so in a way that is convenient for us.</p>
<p>This has the effect of deciding on a single <em>canonical</em> Huffman tree for each valid sequence of code lengths (valid here meaning <em>K</em>=0), even in cases where Huffman&#8217;s algorithm generates distinct trees and code assignments that happen to have the same code lengths. The actual coded bits are just part of the puzzle; an actual codec needs to transmit not just the coded bits, but also enough information for the decoder to work out what codes were assigned to which symbols, and specifying a canonical Huffman code requires less information than its non-canonical counterpart would.<sup><a href="#fn2">2</a></sup> There are several options as to how exactly we assign codes from code lengths, the most common of which is to assign code words sequentially to symbols in lexicographic order for the (<em>c<sub>s</sub></em>, <em>s</em>) pairsâthat is, code lengths paired up with symbol values, ordered by ascending code length and then symbol value.<sup><a href="#fn3">3</a></sup></p>
<p>Using canonical, length-limited codes is a common choice. For example, <a href="https://datatracker.ietf.org/doc/html/rfc1951">Deflate</a> (the algorithm used in zlib/ZIP) uses canonical codes limited to a maximum length of 15 bits, and  JPEG limits the maximum code length to 16 bits.</p>
<p>In principle, such a data stream can be decoded by reading a bit at a time and walking the Huffman tree accordingly, starting from the root. Whenever a leaf is reached, the corresponding symbol is emitted and the tree walk resets back to the root. However, processing data one bit at a time in this fashion is very slow, and implementations generally avoid it. Instead, decoders try to decode many bits at once. At the extreme, when all code lengths are below some upper bound <em>N</em>, a decoder can just &#8220;peek ahead&#8221; <em>N</em> bits into the bitstream, and prepare a table that contains the results of the tree walks for all 2<sup><em>N</em></sup> possible bit patterns; table entries store which symbol was eventually reached and how many bits were actually consumed along the way. The decoder emits the symbol, advances its read cursor by the given number of bits, and reads the next symbol.</p>
<p>This method can be much faster and is easy to implement; however, when <em>N</em> is large, the resulting tables will be too, and there are steep penalties whenever table sizes start exceeding the sizes of successive cache levels, since the table lookups themselves are essentially random.<sup><a href="#fn4">4</a></sup> Furthermore, most sources being compressed aren&#8217;t static, so Huffman tables aren&#8217;t used forever. A typical Huffman table will<br />
last somewhere between 5 and 100 kilobytes of data. For larger <em>N</em>, it&#8217;s easy for table initialization to take as long, or longer, than actual bitstream decoding with that table, certainly for shorter-lived Huffman tables. The compromise most commonly used in practical implementations is to use a multi-level table scheme, where individual tables cover something like 9 to 12 bits worth of input. All the shorter symbols are decoded in a single table access, but the entries for longer symbols don&#8217;t contain a symbol value; instead, they reference a secondary table that resolves the remaining bits (there can be many such secondary tables). The point of Huffman coding is to assign shorter codes to more common symbols, so having to take a secondary table walk is relative uncommon. And of course there are many possible variants of this technique: table walks can do more than 2 levels, and in the other direction, if we have many shorter codes (say at most 4 bits long), then a single access to a 512-entry, 9-bit table can resolve two symbols (or even more) at once. This can accelerate decoding of very frequent symbols, at the expense of a more complicated decoder inner loop and slower table build step.<sup><a href="#fn5">5</a></sup></p>
<h3>Huffman coding in Oodle</h3>
<p>Our older codecs, namely Oodle LZH and LZHLW, used a 15-bit code length limit and multi-level tables, both common choices. The problem with this design is that every symbol decode needs to check whether a second-level table lookup is necessary; a related problem is that efficient bit IO implementations maintain a small buffer that is periodically refilled to contain somewhere between 23 and 57 bits of data (details depend on the choice of bit IO implementation and whether we&#8217;re on a 32-bit or 64-bit platform, see the bit-IO series I referenced earlier). When we know that codes are guaranteed to be short enough, we can decode multiple symbols per refill. This is significant because refilling the bit buffer is about as expensive as decoding a symbol itself; being able to always decode two symbols per refill, as opposed to one, has the potential to reduce refill overhead from around 50% of decode time to about 33% on 32-bit targets!</p>
<p>When Charles first prototyped what later turned into Kraken, he was using our standard bit IO implementation, which uses byte-wise refill so post-refill, the 32-bit version guarantees at least 25 usable bits, while for in 64-bit the number is 57 bits, all typical limits.</p>
<p>Therefore, if codes are limited to at most 12 bits, the 32-bit decoder can sustain a guaranteed two symbols per refill; for 64-bit, we can manage 4 decodes per refill (using 48 out of 57 bits). As a side effect, the tables with a 12-bit limit are small enough that even without a multi-level table scheme, everything fits in the L1D cache. And if codes are limited to 11 bits, the 32-bit decoders still manage 2 symbols per refill, but the 64-bit decoders now go up to 5 decodes per refill, which is a nice win.  Therefore, the newer Oodle codecs use Huffman codes length-limited to only 11 bits, which allows branch-free decoding of 2 symbols per refill on 32-bit platforms, and 5 symbols per refill on 64-bit targets.<sup><a href="#fn6">6</a></sup></p>
<p>The other question is, of course, how much limiting the code lengths aggressively in this way costs in terms of compression ratio, to which the answer is: not much, although I don&#8217;t have good numbers for the 11-bit limit we eventually ended up with. For a 12-bit limit, the numbers in our tests were below a 0.1% hit vs. the original 15-bit limit (albeit with a note that this was when using proper package-merge, and that the hit was appreciably worse with some of the hackier heuristics). We ended up getting a fairly significant speed-up from this choice, so that slight hit is well worth it.</p>
<p>With these simplifications applied, our core Huffman decoder uses pre-baked table entries like this:</p>
<pre>struct HuffTableEntry {
    uint8_t len; // length of code in bits
    uint8_t sym; // symbol value
};</pre>
<p>and the decoder loop looks something like this (32-bit version):</p>
<pre>while (!done) {
    bitbuf.refill();
    // can decode up to 25 bits without refills!

    // Decode first symbol
    {
        intptr_t index = bitbuf.peek(11);
        HuffTableEntry e = table[index];
        bitbuf.consume(e.len);
        output.append(e.sym);
    }

    // Decode second symbol
    {
        intptr_t index = bitbuf.peek(11);
        HuffTableEntry e = table[index];
        bitbuf.consume(e.len);
        output.append(e.sym);
    }
}</pre>
<p>Note that with a typical bit buffer implementation, the <code>peek</code> operation uses a single bit-wise AND or shift instruction; the table access is a 16-bit load (and load address calculation, which depending on the target CPU might or might not be an addressing mode). The bit buffer <code>consume</code> operation usually takes an integer shift and an add, and finally the &#8220;append&#8221; step boils down to an integer store, possibly an integer shift to get the symbol value in the right place prior to the store (the details of this very much depend on the target ISA, I&#8217;ll get into such details in later posts in this series), and some pointer updating that are easily amortized if the loop is unrolled more than just twice.</p>
<p>That means that in terms of the work we&#8217;re doing, we&#8217;re already starting out with something around 5-7 machine instructions per symbol, and if we want good performance, it matters a lot what exactly those instructions are and how they relate. But before we get there, there&#8217;s a few other details about the resulting dataflow to understand first.</p>
<p>The first and most important thing to note here is what the critical path for the entire decoder operation is: everything is in serialized through the bit buffer access. The <code>peek</code> operation depends on the bit buffer state after the previous <code>consume</code>, the table access depends on the result of the <code>peek</code> operation, and the <code>consume</code> depends on the result of the table access (namely the <code>len</code> field) which completes our cycle. This is the critical dependency chain for the entire decoder, and the actual symbol decoding or output isn&#8217;t even part of it. Nothing in this loop depends on or cares about <code>e.sym</code> at all; its load and eventual store to the output buffer should be computed eventually, but there&#8217;s no rush. The code length determination, on the other hand, is our primary bottleneck. If the critical computation for <code>peek</code> and <code>consume</code> works out to a single 1-cycle latency integer instruction each (which it will turn out it does on most of the targets we&#8217;ll see later in this series), then everything hinges on how fast we can do that table access.</p>
<p>On current out-of-order CPUs (no matter the ISA), the answer usually turns out to be 4 or 5 clock cycles, making the critical path from one symbol decoded to the next either 6 or 7 clock cycles. This is very bad news, because as just pointed out, we&#8217;re executing around 5-7 instructions per symbol decoded, total. This means that, purely from the dependency structure, we&#8217;re limited to around 1 instruction per cycle average on CPUs that can easily sustain more than 3 instructions per cycle given the right code. Viewed another way, in those 7 cycles per symbol decoded, we could easily run 21 independent instructions (or even more) if we had them, but all we do is something like 6 or 7.<sup><a href="#fn7">7</a></sup></p>
<p>The obvious solution here is to not have a single bit buffer (or indeed bitstream), but several at once. Once again we were not the first or only ones to realize this, and e.g. Yann Collet&#8217;s Huff0 uses the same idea. If the symbols we want to decode are evenly distributed between multiple bitstreams, we can have multiple one-symbol-at-a-time dependency chains in flight at once, and hopefully get to a state where we&#8217;re mostly (or even completely) limited by instruction throughput (i.e. total instruction count) and not latency.</p>
<p>Our high-level plan of attack will be exactly that. Huff0 uses 4 parallel bitstreams. For reasons that will become clearer in subsequent parts of this series, Huffman coding in the newer Oodle codecs has two different modes, a 3-bitstream mode and a 6-bitstream mode. Using more bitstreams enables more parallelism but the flipside is that it also requires more registers to store the per-bitstream state and is more sensitive to other micro-architectural issues; the actual counts were chosen to be a good compromise between the desires of several different targets.<sup><a href="#fn8">8</a></sup></p>
<p>For the rest of this post, I&#8217;ll go over other considerations; in the next part of this series, I plan to start looking at some actual decoder inner loops.</p>
<h3>Aside: tANS vs. Huffman</h3>
<p>The ANS family of algorithms is the new kid on the block among entropy coders. Originally we meant to use tANS in the original Kraken release back in 2016, but the shipping version of Kraken doesn&#8217;t contain it; it was later added as a new entropy coding variant in 2018 along with Leviathan, but is rarely used.</p>
<p>However, one myth that was repeatedly claimed around the appearance of tANS was that it is &#8220;faster than Huffman&#8221;. While it is true that some tANS decoders are faster than some Huffman decoders, this is mostly an artifact of other implementation choices and the resulting dependency structure of the decoder, so I want to spend a few paragraphs explaining when and why this effect appears, and why despite it, it&#8217;s generally not a good idea to replace Huffman decoders with tANS decoders.</p>
<p>I&#8217;m not going to explain any of the theory or construction of tANS here; all I&#8217;ll note is that the individual per-symbol decode step uses tables similar to a Huffman decode table, and paraphrasing <a href="http://cbloomrants.blogspot.com/2015/10/huffman-performance.html">old code from Charles</a> here, the key decode step ends up being:</p>
<pre>HuffTableEntry e = table[state];
state = next_state[state] | bitbuffer.get(e.len); 
output.append(e.sym);</pre>
<p>In essence, this is a &#8220;lookahead Huffman&#8221; decoder, where each stream keeps <em>N</em> bits of lookahead at all times. Instead of looking at the next few bits in the bit buffer to figure out the next code length, we already have enough bits to figure out what the next code is stored in our <code>state</code>, and after decoding a symbol we shift out a number of bits corresponding to the length of the code and replenish the bottom of our state from the bitstream.<sup><a href="#fn9">9</a></sup></p>
<p>The bits come from a different place, and we now have the extra <code>state</code> de-facto shift register in the mix, so why would this run faster, at least sometimes? Again we need to look at the data dependencies.</p>
<p>In the regular Huffman decode, we had a single critical dependency chain with the peek/table lookup/consume loop, as well as the bit-buffer refill every few symbols. In this version of the loop, we get not one but two separate dependency chains:</p>
<ol>
<li>The <code>state</code> update has a table lookup (two in fact, but they&#8217;re independent and can overlap), the bit buffer <code>get</code> operation to get the next set of low bits for the next state (usually something like an integer add and a shift), and an OR to combine them. This is not any better than the regular Huffman decoder (in fact it&#8217;s somewhat worse in multiple ways), but note that the high-latency memory loads happen <em>before</em> any interactions with the bit buffer.</li>
<li>The bit buffer update has the periodic refill, and then the <code>get</code> operations after the <code>state</code> table lookups (which are a combined <code>peek</code> and <code>consume</code>).</li>
</ol>
<p>The crucial difference here is that in the regular Huffman decoder, most of the &#8220;refill&#8221; operation (which needs to know how many bits were consumed, load extra bytes and shift them into place, then do some accounting) is on the same dependency chain as the symbol decoding, and decoding the next symbol after a refill can make no meaningful progress until that refill is complete.  In the TANS-style decoder, the refill can overlap with the state-dependent table lookups; and then once those table lookups complete, the updates to the bit buffer state involve only ALU operations. In throughput terms the amount of work per symbol in the TANS-style decoder is not better (in fact it&#8217;s appreciably worse), but the critical-path latency is improved, and when not using enough streams to keep the machine busy, that latency is all that matters.</p>
<p>A related technique that muddles this further is that ANS-based coders can use interleaving, where (in essence) we just keep multiple <code>state</code> values around, all of which use the same underlying bitstream. This gives us twice the parallelism: two TANS decoders with their own <code>state</code> are independent, and even making them share the same underlying bitstream keeps much of the advantage, because the high-latency portion (the table lookup) is fully overlapped, and individual reads from the bit buffer (once all the lengths are known) can usually be staggered just 1 cycle apart from each other.</p>
<p>A 2Ã interleaved tANS decoder is substantially faster than a single-stream Huffman decoder, and has lower incremental cost (a single extra integer state is easier to add and track than the registers required for a second bitstream state). However it also does more work per symbol than a 2-stream Huffman decoder does, so asymptotically it is slowerâif you ever get close to the asymptote, that is.</p>
<p>A major disadvantage of tANS-style decoding is that interleaving multiple states into the same bitstream means that there is essentially only a single viable decoder implementation; bits need to be read in the same way and at the same time by all decoders to produce the same results, fast implementations need larger and more expensive to set up tables than a straight Huffman decoder, plus other overheads in the bitstream. In contrast, as we&#8217;ll see later, the 6-stream Huffman<br />
coding variant is decoded on some platforms as two 3-stream blocks, which would not easily map to a tANS-style implementation. In the end, the lower asymptotic performance and decreased flexibility is what made us stick with a regular Huffman decoder in Oodle Data, because as it turns out we can get decent wins from decoding the bitstreams somewhat differently on different machines.</p>
<h3>Huffman table initialization</h3>
<p>As mentioned several times throughout this post, it&#8217;s easy for Huffman table initialization to become a major time sink, especially when tables are large, used for just a short time, or the table builder is unoptimized. Luckily, optimizing the table builder isn&#8217;t too hard, and it also gives a nice and intuitive (to me anyway) view of how canonical Huffman codes work in practice.</p>
<p>For now, suppose that decoding uses a MSB-first bit packing convention, i.e. a bit-at-a-time Huffman decoder would read the MSB of a codeword first and keep reading additional less significant bits until the symbol is determined. Although our 11-bit code length limit is quite short, that&#8217;s still a large number of bits to work through in an example, so let&#8217;s instead consider at a toy scenario with a code length limit of 4 bits and four symbols a, b, c, d with code lengths 1, 3, 2, and 3 bits, respectively.</p>
<p>The code length limit of 4 bits means we have a total space of 16 possible 4-bit patterns to cover; I&#8217;ll refer to each such bit pattern as a &#8220;slot&#8221;. Our 1-bit symbol &#8220;a&#8221; is the first and shortest, which means it gets assigned first, and as such gets a 1-bit code of &#8220;0&#8221;, with the next 3 bits irrelevant. In short, all slots with a leading 0 bit correspond to the symbol &#8216;a&#8217;; there&#8217;s 2<sup>4-1</sup> = 8 of them. The next-shortest symbol is &#8216;c&#8217;, which get the next available 2-bit code. All codes starting with 0 are taken; the next available slot starts with a 2-bit pattern of &#8220;10&#8221;, so that&#8217;s what we use. There are 2<sup>4-2</sup> = 4 slots corresponding to a bit pattern starting &#8220;10&#8221;, so that&#8217;s what we assign to c. The two remaining symbols &#8216;b&#8217; and &#8216;d&#8217; get 3-bit codes each. Anything starting with &#8220;0&#8221; or &#8220;10&#8221; is taken, so they need to start with &#8220;11&#8221;, which means our choice is forced: &#8216;b&#8217; being earlier in the alphabet (since we break ties in code length by which symbol is lower-valued) gets the codes starting &#8220;110&#8221;, &#8216;d&#8217; gets &#8220;111&#8221;, and both get 2<sup>4-3</sup>=2 table slots each. If we write up the results, we get:</p>
<table>
<tbody>
<tr>
<th>Code</th>
<th>Sym</th>
<th>Len</th>
</tr>
<tr>
<td><b>0</b>000</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>001</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>010</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>011</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>100</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>101</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>110</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>111</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>10</b>00</td>
<td>c</td>
<td>2</td>
</tr>
<tr>
<td><b>10</b>01</td>
<td>c</td>
<td>2</td>
</tr>
<tr>
<td><b>10</b>10</td>
<td>c</td>
<td>2</td>
</tr>
<tr>
<td><b>10</b>11</td>
<td>c</td>
<td>2</td>
</tr>
<tr>
<td><b>110</b>0</td>
<td>b</td>
<td>3</td>
</tr>
<tr>
<td><b>110</b>1</td>
<td>b</td>
<td>3</td>
</tr>
<tr>
<td><b>111</b>0</td>
<td>d</td>
<td>3</td>
</tr>
<tr>
<td><b>111</b>1</td>
<td>d</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Note that we simply assigned all of the code space in linear order to symbols sorted by their code length and breaking ties by symbol value, where each symbol gets 2<sup><em>N</em>&#8211;<em>k</em></sup> slots, where <em>N</em> is the overall code length limit and <em>k</em> is the code length for the symbol in question. At any step, the next code we assign simply maps to the first few bits of the next available table entry. It&#8217;s trivial to discover malformed codes in this form: we need to assign exactly 2<sup><em>N</em></sup> slots total. If any symbol has a code length larger than <em>N</em> or we end up assigning more or less than that number of slots total, the code is, respectively, over- or under-complete; the current running total of slots assigned is a scaled version of the number in the Kraft inequality.</p>
<p>Moreover, it should also be obvious how the canonical table initialization for a MSB-first code works out in practice: the table is simply written top to bottom in linear order, with each entry repeated 2<sup><em>N</em>&#8211;<em>k</em></sup> times; in essence, a slightly more complicated <code>memset</code>. Eventually, each slot is just duplicated a small number of times, like 4, 2 or just 1, so as code lengths get closer to the limit it&#8217;s better to use specialized loops and not treat it as a big memset anymore, but the basic idea remains the same.</p>
<p>This approach requires a list of symbols sorted by code length; for Oodle, the code that decodes Huffman table descriptions already produces the symbol list in this form. Other than that, it is straightforward.</p>
<p>The only wrinkle is that, for reasons to do with the details of the decoder implementation (which will be covered in future posts), the fast Huffman decoders in Oodle do not actually work MSB-first; they work LSB-first. In effect, we produce the table as above, but then reverse all the bit strings. While it is possible to directly build a canonical Huffman decode table in a LSB-first fashion, all the nice (and SIMD-friendly) sequential memory writes we had before now turn into scatters, which is a lot messier. It turns out to be easier and faster to first build a MSB-first table as described above and then apply a separate bit-reverse permutation to the elements to obtain the corresponding LSB-first decoding table. This is extra work, but bit-reverse permutations have a very regular structure and also admit a fast SIMD implementation that ends up being essentially a variant of a matrix transpose operation. In our experiments, the SIMD-friendly (and memset-able) MSB-first table initialization followed by a MSBâLSB bit-reverse was much faster than a direct LSB-first canonical table initialization, and LSB-first decoding saved enough work in the core decode loops to be worth the extra step during table building for the vast majority of our Huffman tables, so that&#8217;s what we settled on.</p>
<p>And that&#8217;s it for the high-level overview of Huffman decoding in Oodle Data. For the next few upcoming posts, I plan to get into detail about a bunch of the decoder loops we use and the CPUs they are meant for, which should keep us busy for a while!</p>
<h3>Footnotes</h3>
<p><span id="fn1">[1]</span> An easy way to construct a pathological frequency distribution is to pick the symbol frequencies as Fibonacci numbers. They grow exponentially with <em>n</em>, but not so quickly as to make the symbol frequencies required to exceed common practical code length limits impossible to occur for typical block sizes. The existence of such cases for at least certain input blocks means that implementations need to handle them.</p>
<p><span id="fn2">[2]</span> There are strictly more Huffman trees than there are canonical Huffman trees. For example, if symbol &#8216;a&#8217; has a 1-bit code and symbols &#8216;b&#8217; through &#8216;e&#8217; all get 3-bit codes, then our corresponding canonical tree for that length distribution might be <code>(a ((b c) (d e)))</code>, but it is equally possible (with the right symbol frequencies) to get say the tree <code>(a ((b d) (c e)))</code> or one of many other possible trees that all result in the same code lengths. The encoded size depends only on the chosen code lengths, and spending extra bits on distinguishing between these equivalent trees is wasteful unless the tree shape carries other useful information, which it can in some applications, but not in usual Huffman coding. Along the same lines, for any given Huffman tree there are in general many possible sets of symbol frequencies that produce it, so sending an encoding of the original symbol frequencies rather than the tree shape they result in is even more wasteful.</p>
<p><span id="fn3">[3]</span> When assigning codewords this way and using a MSB-first bit packing convention, the blocks of codes assigned to a given code length form intervals. When codes are limited to <em>N</em> bits, that means the length of a variable-length code word in the bitstream can be determined by figuring out which interval a code falls into, which takes at most <em>N</em>-1 comparisons against constants of width â¤<em>N</em>-1 bits. This allows for different, and often more efficient, implementation options for the latency-critical length determination step.</p>
<p><span id="fn4">[4]</span> This is inherent: the entire point of entropy coding is to make bits in the output bitstream approach an actual information content around 1 bit. In the process of Huffman coding, we are purposefully making the output bitstream &#8220;more random&#8221;, so we can&#8217;t rely on locality of reference to give us better cache hit rates here.</p>
<p><span id="fn5">[5]</span> Decoding multiple symbols at once may sound like an easy win, but it does add extra work and complications to decoding symbols that are long enough not to participate in pair-at-a-time decoding, and once again the table build time needs to be carefully watched; generally speaking multi-symbol decoding tends to help on extremely redundant input but slows down decoding of data that achieves maybe 7 bits per byte from Huffman coding, and the latter tends to be more common than the former. The actual evaluation ends up far from straightforward and ends up highly dependent on other implementation details. Multi-symbol decoding tends to look good when plugged into otherwise inefficient decoders, but as these inefficiencies are eliminated the trade-offs get more complicated.</p>
<p><span id="fn6">[6]</span> No doubt we were neither the first nor last to arrive at that particular design point; with an 8-bit alphabet, the set of viable options for at least two decodes per refill if 32b targets matter essentially boils down to 9b to 12b, with 11b being the first where 5 decodes/refill on 64b targets are possible.</p>
<p><span id="fn7">[7]</span> This gap is one of the things that makes multi-symbol decoding look great when plugged into a simple toy decoder. Decoding multiple symbols at once requires extra bookkeeping and instructions, and also introduces extra dependencies between the stores since the number of symbols decoded in each step is no longer known at compile time and folded into an addressing mode, but when we&#8217;re only using about a third of the capacity of the machine, inserting extra instructions into the symbol decode is very cheap.</p>
<p><span id="fn8">[8]</span> Oodle has the same bitstream format for everyone; while we can (and do) make choices to avoid pathologies on all of the targets we care about, any choice we make that ends up in the bitstream needs to work reasonably well for all targets.</p>
<p><span id="fn9">[9]</span> <code>next_state</code> here essentially just tabulates a shift and mask operation; in a &#8220;real&#8221; tANS decoder this can be more complicated, but when we&#8217;re looking at a case corresponding to a Huffman table and symbol distribution, that&#8217;s all it does.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2021/08/30/entropy-coding-in-oodle-data-huffman-coding/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>Entropy coding in Oodle Data: the big picture</title>
		<link>https://fgiesen.wordpress.com/2021/07/09/entropy-coding-in-oodle-data-the-big-picture/</link>
					<comments>https://fgiesen.wordpress.com/2021/07/09/entropy-coding-in-oodle-data-the-big-picture/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Sat, 10 Jul 2021 02:41:52 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<category><![CDATA[Compression]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7161</guid>

					<description><![CDATA[April 26, 2016 was the release date of Oodle 2.1.5 which introduced Kraken, so it celebrated its 5-year anniversary recently. A few months later we launched Selkie and Mermaid, which wetre already deep in development at the time of the Kraken release but not quite finished yet, and in early 2018 we added Leviathan, adding [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>April 26, 2016 was the release date of Oodle 2.1.5 which introduced Kraken, so it celebrated its 5-year anniversary recently. A few months later we launched Selkie and Mermaid, which wetre already deep in development at the time of the Kraken release but not quite finished yet, and in early 2018 we added Leviathan, adding a higher-compression option to our current suite of codecs, the &#8220;sea beastiary&#8221;.</p>
<p>In those 5 years, these codecs have seen wide adoption in their intended market (which is video games).</p>
<p>There&#8217;s a few interesting things to talk about, and the 5-year anniversary seems as good a reason as any to get started; this is the first in what will be a series of yet to be determined length, in which I&#8217;ll do a deep-dive on one interesting aspect of these codecs, namely the way they handle entropy coding.</p>
<p>Before I can get into any details, first some general notes on how things fit together. Kraken, Mermaid, Selkie, and Leviathan are lossless data compression algorithms, all variations on the basic LZ77 + entropy coding formula that has been the de facto standard in general-purpose compressors since the late 80s, because such codecs can achieve a good balance of compression ratio and compression/decompression speed for practical applications. Other codecs belonging to this family include Deflate (ZIP/gzip), LZX (Amiga LZX/CAB), LZMA (7zip/xz), Zstd, Brotli, LZHAM, and many others, including most of the older Oodle Data codecs (Oodle LZH/LZHLW, LZA, LZNA, and BitKnit).</p>
<p>The &#8220;LZ77&#8221; portion here refers to the LZ77 algorithm, which, broadly speaking, compresses data by replacing repeated byte sequences in a stream with references to prior occurrences; as long as the back-reference is smaller than the bytes themselves, this will result in compression. Nobody actually uses the original LZ77 algorithm per se (which has a very inefficient encoding by today&#8217;s standards), but the general idea remains the same. Some codecs, especially ones designed for faster decoding or encoding, use just this byte/string matching approach without any entropy decoding; this includes many well-known faster codecs such as LZ4, Snappy, LZO, and again many others, including the remaining older Oodle Data<br />
codecs (Oodle LZB/LZBLW and LZNib).</p>
<p>I won&#8217;t talk about the LZ77 portion here (that&#8217;s its whole own thing), and instead focus on the &#8220;entropy coding&#8221; portion of things. In essence, what all these codecs do is identify repeated strings, resulting in some mixture of back-references (&#8220;matches&#8221;) and bytes that couldn&#8217;t be profitably matched to anything earlier in the data stream (&#8220;literals&#8221;). All the codecs mentioned differ in how exactly these things are encoded, but all of them eventually boil it down to one<br />
or more data streams that each have symbols from a finite-size and not too large alphabet (typical sizes include a few dozen to a few hundred distinct symbols in the alphabet), plus maybe a few extra streams that contain raw bits for data that is essentially random.</p>
<p>Entropy coding, then, processes these streams of symbols from a finite alphabet and tries to turn them into an equivalent form that is smaller. There are numerous ways to do this; one well-known method, and indeed a mainstay in data compression to this day (with some caveats), is Huffman coding, which counts how often individual symbols occur and assigns short bit codes to more likely symbols and longer symbols to less likely symbols, in a way that remains unambiguous and uniquely decodable.<sup><a href="#fn1">1</a></sup></p>
<p>The second major class of techniques is arithmetic coding, which compresses slightly better than Huffman for typical data and can give significant improvements on either small alphabets or highly skewed distributions, and is also more suitable for applications when there is not an a priori known fixed symbol distribution, but rather an on-line model that is being updated as new data is seen (&#8220;adaptive coding&#8221;).</p>
<p>Finally, the new kid on the block is the ANS (&#8220;Asymmetric Numeral System&#8221;) family of methods, which is conceptually related to arithmetic coding but works differently, and has different trade-offs. Kraken and Mermaid use ANS sometimes and Leviathan frequently; I&#8217;m sure I&#8217;ll cover its usage in those codecs at some point, and for now there are plenty of my older writings on the topic from around 2014-2016.</p>
<p>Anyway, it is common for LZ77-derived algorithms with a Huffman or arithmetic coding backend to use odd alphabet sizes; when you&#8217;re assigning variable-length codes to everything anyway, it doesn&#8217;t really matter whether your alphabet has just 256 distinct byte values in it, or if the limit is 290 or 400 or something else non-power-of-2. It&#8217;s likewise common to interleave all kinds of data into a single contiguous bitstream. The sea beastiary family does things a bit differently than most codecs.</p>
<p>Namely, the input (uncompressed) data is processed in 128KiB chunks, which all have self-contained bytestreams with an explicitly signaled size.<sup><a href="#fn2">2</a></sup> These chunk bytestreams are themselves made up out of multiple independent bitstreams for different types of data, and most of these streams (except for a few &#8220;misc&#8221;/leftover data streams) use a single shared entropy coding layer to handle what we now call &#8220;primary streams&#8221;.<sup><a href="#fn3">3</a></sup></p>
<p>This layer works on streams that always use a byte alphabet, i.e. unsigned values between 0 and 255. That means that everything that goes through this layer needs to be presented in a byte-packed format. This kind of restriction is common in fast byte-aligned LZ77 coders without a final entropy coding stage (like LZ4 or Snappy), but atypical outside of that space.</p>
<p>Working in bytes needs some care to be taken in other parts of the codec design, but comes with a massive advantage, because it allows us to have an uncompressed mode that just stores streams as uncompressed bytestreams and doesn&#8217;t need any decoding at all. That means that streams that are nearly random, or just short, don&#8217;t have to waste CPU time on setting up a more complicated decoder and then decoding bytes individually through a more complicated algorithm when just grabbing uncompressed bytes straight from the bytestream is good enough.</p>
<p>In short, for most codecs, the choice of entropy coding scheme is a design-time decision. Algorithms like Deflate which uses LZ+Huffman <em>always</em> perform Huffman coding, even when doing so isn&#8217;t actually worthwhile, because they use extended alphabets with more than 256 symbols in them that mean pass-through coding would still need to use larger-than-8-bit units, both introducing extra waste and making it more expensive in terms of CPU time. Sticking with a byte alphabet means that the Oodle sea beastiary codecs can either use a conventional entropy coding stage or choose to send data uncompressed and act like a faster byte-aligned LZ codec. In fact, that is exactly how Selkie works: under the hood, Selkie is the same codec as Mermaid, but with all streams in uncompressed mode at all times (and some other features disabled).</p>
<p>The final big departure from most codecs is that the sea beastiary codecs run all entropy decoding as separate passes that consume an input bytestream with a common header and output an array of bytes. Uncompressed streams can skip the decoding step and consume data straight from the input bytestream. These passes just handle whatever entropy coding scheme is used and nothing else. This means that instead of the choice of entropy coders being fundamentally built into the algorithm, it&#8217;s a collection of small loops that can easily be interchanged or added to, and indeed that&#8217;s what we&#8217;ve been doing; the original version of Kraken shipped in Oodle 2.1.5 with a choice between just uncompressed streams and one particular version of Huffman coding. We&#8217;ve added several more options since then, including an alternative Huffman coding variant, an optional RLE pre-pass, TANS, and the ability to mix-and-match all of these within a single stream, all without it being a big redesign, and we&#8217;re quite happy with the results.</p>
<p>There&#8217;s many interesting things in here that I plan to cover in small installments, starting with the way we do Huffman (de)coding, which is worth several posts on its own. I&#8217;ll try to keep updating this at a reasonable cadence, but no promises since my blogging time is pretty limited these days!</p>
<h3>Footnotes</h3>
<p><span id="fn1">[1]</span> Huffman coding technically refers to such variable-length<br />
coding (meaning not all symbols need to be coded with the same number of bits) only when the code in question is actually generated via Huffman&#8217;s algorithm (which minimizes the length of the coded stream given a certain number of assumptions), but in practice most implementations don&#8217;t actually use Huffman&#8217;s original algorithm which can assign awkwardly long code lengths to rare symbols. Instead the maximum code length is usually capped, which is a different problem and yields codes that are not actually Huffman codes (and are very slightly less efficient), but just general variable-length codes. Having said that, approximately everyone calls it Huffman coding even when the codes are length-limited anyway, and I won&#8217;t sweat it either in the following.</p>
<p><span id="fn2">[2]</span> The chunks being independent is how we implement what is called &#8220;thread-phased&#8221; decoding. As is explained further down in the article, Oodle decodes the bitstreams to temporary memory buffers in a first phase, and the chunks can be processed completely independently for this part of the process, making it a prime candidate to move off to a separate thread (or threads). After phase 1 completes, the actual LZ decoding takes place in &#8220;phase 2&#8221;; this portion can make references to any bytes earlier in the stream, so it must be run in order and can&#8217;t as easily be threaded. Customers that want wider parallelism (or easy seeking) usually chop data into fixed-size blocks, typically between 64KiB and 512KiB, that are encoded independently with no back-references permitted across block boundaries. This decreases compression ratio but makes it trivial to have many workers decoding different blocks at the same time.</p>
<p><span id="fn3">[3]</span> In the original code, these streams of bytes are, very non-distinctly, just called &#8220;arrays&#8221;. When specifying the bitstream more formally, a lot of things suddenly needed proper names to avoid confusion, and &#8220;arrays&#8221; was just too indistinct and overloaded, so &#8220;primary streams&#8221; was what we settled on. Sometimes additional data streams exist within a chunk for information not contained in the primary streams, and these are therefore called &#8220;secondary streams&#8221;.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2021/07/09/entropy-coding-in-oodle-data-the-big-picture/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>Frequency responses of half-pel filters</title>
		<link>https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/</link>
					<comments>https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/#respond</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Sat, 20 Jul 2019 21:58:12 +0000</pubDate>
				<category><![CDATA[Maths]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7125</guid>

					<description><![CDATA[In the previous post, I looked at repeated application of FIR filters on the same signal and laid out why their frequency response is the thing to look at if we&#8217;re wondering about their long-term stability under repeated application, because the Discrete-Time Fourier Transform (DTFT) of the filter coefficients corresponds to the eigenvalues of the [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>In the <a href="https://fgiesen.wordpress.com/2019/04/08/what-happens-when-iterating-filters/">previous post</a>, I looked at repeated application of FIR filters on the same signal and laid out why their frequency response is the thing to look at if we&#8217;re wondering about their long-term stability under repeated application, because the Discrete-Time Fourier Transform (DTFT) of the filter coefficients corresponds to the eigenvalues of the linear map that convolves a signal with the filter once.</p>
<p>Now, let&#8217;s go back to the <a href="https://caseymuratori.com/blog_0035">original problem</a> that Casey asked about: half-pixel (or half-pel) interpolation filters for motion compensation. We know from Casey&#8217;s post which of the filters he tried &#8220;blow up&#8221; under repeated application, and which ones don&#8217;t, and we now know some of the theory. The question is, does it track? Let&#8217;s find out!</p>
<h3>A small filter zoo: 6 taps</h3>
<p>We&#8217;ll be looking at the frequency responses of various filters here. As a reminder, what we&#8217;re looking at are the values of the function</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%28%5Comega%29+%3D+%5Csum_%7Bk%3D0%7D%5E%7Bm-1%7D+f_k+%5Cexp%28-i+%5Comega+k%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;displaystyle &#92;hat{f}(&#92;omega) = &#92;sum_{k=0}^{m-1} f_k &#92;exp(-i &#92;omega k)" class="latex" /></p>
<p>where the <img src="https://s0.wp.com/latex.php?latex=f_k&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="f_k" class="latex" /> are our filter coefficients, which are real numbers. It&#8217;s easy to prove from this form that the function <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;hat{f}" class="latex" /> is continuous, 2Ï-periodic, and has <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28-%5Comega%29+%3D+%5Coverline%7B%5Chat%7Bf%7D%28%5Comega%29%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;hat{f}(-&#92;omega) = &#92;overline{&#92;hat{f}(&#92;omega)}" class="latex" /> (where the bar denotes complex conjugation, as usual). Therefore the &#8220;interesting&#8221; part of the frequency response (for our purposes anyway) is contained in the values of <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;hat{f}(&#92;omega)" class="latex" /> for Ï in [0,Ï]. Furthermore, we only care about the &#8220;magnitude response&#8221;, the absolute values of the (complex) frequency response, which is conventionally plotted in decibels (i.e. effectively on a logarithmic scale); the angle denotes phase response, which is not relevant to our question (and also happens to be relatively boring in this case, because all filters under consideration are symmetric and thus linear-phase FIR filters). Finally I&#8217;ll crop the plots I&#8217;m showing to only show frequencies from 0 to 0.8Ï (up to 80% <a href="https://en.wikipedia.org/wiki/Nyquist_frequency">Nyquist frequency</a>) because all filters under consideration sharply decay past that point (if not much earlier), so the portion from 0.8Ï to Ï contributes very little information and makes the y axis scaling awkward.</p>
<p>With all that out of the way, let&#8217;s start with our first filter: the 2-tap linear interpolation filter [0.5, 0.5] (it&#8217;s &#8220;bilinear&#8221; if you do it in both the X and Y directions, but all filters under consideration are separable and we&#8217;re only looking at 1D versions of them, so it&#8217;s just straight-up linear interpolation).</p>
<p><img data-attachment-id="7126" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_linear/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of linear filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png?w=497" alt="Magnitude response of linear interpolation filter"   class="aligncenter size-full wp-image-7126" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png?w=497 497w, https://fgiesen.files.wordpress.com/2019/04/mag_linear.png?w=150 150w, https://fgiesen.files.wordpress.com/2019/04/mag_linear.png?w=300 300w, https://fgiesen.files.wordpress.com/2019/04/mag_linear.png 614w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>The dotted line at 0dB is the line we&#8217;re not allowed to cross if we want repeated application to be stable. It&#8217;s also the line we want to be <em>exactly on</em> for an ideal interpolation filter. As we can see from the diagram, linear interpolation is good at the first part, not so good at the second part: it&#8217;s unit gain at a frequency of 0 but immediately rolls off, which is what causes it to over-blur. Not great.</p>
<p>The next filter that Casey looks at jumps from 2 taps all the way to 6: it&#8217;s the half-pixel interpolation filter from H.264/AVC. Here&#8217;s its magnitude response:</p>
<p><img data-attachment-id="7127" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_h-264/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of H.264 filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png?w=497" alt="Magnitude response of H.264 filter"   class="aligncenter size-full wp-image-7127" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png?w=497 497w, https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png?w=150 150w, https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png?w=300 300w, https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png 614w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>This one is quite different. We can immediately see that it has above-unit gain for a significant fraction of its spectrum, peaking around 0.5Ï. That tells us it should blow up in Casey&#8217;s repeated-filtering test, and indeed it does. On the plus side, it has a much wider passband, and is in general much closer to an ideal interpolation filter than basic linear interpolation is.</p>
<p>The image for the 6-tap Lanczos filter is not that different:</p>
<p><img data-attachment-id="7129" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_lanczos6/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of 6-tap Lancozs filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png?w=497" alt="Magnitude response of 6-tap Lancozs filter"   class="aligncenter size-full wp-image-7129" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png?w=497 497w, https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png?w=150 150w, https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png?w=300 300w, https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png 614w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>In fact, this one&#8217;s qualitatively so similar that it seems fair to assume that the H.264 filter was designed as an approximation to Lanczos6 with reduced-precision coefficients. (It would certainly be plausible, but I haven&#8217;t checked.)</p>
<p>Next up, let&#8217;s look at Casey&#8217;s filter with quantized coefficients (numerator of 32):</p>
<p><img data-attachment-id="7128" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_muratori6_32/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of quantized Muratori6" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png?w=497" alt="Magnitude response of quantized Muratori6"   class="aligncenter size-full wp-image-7128" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png?w=497 497w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png?w=150 150w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png?w=300 300w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png 614w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>Casey&#8217;s filter stays at or below unit gain; it has noticeably below-unit gain for much of its passband which is not ideal, but at least its passband is significantly wider than basic linear interpolation, and looks decent out to about 0.5Ï before it really starts to cut off.</p>
<p>And while we&#8217;re at it, here&#8217;s Casey&#8217;s other 6-tap filter from the second part of his series:</p>
<p><img data-attachment-id="7130" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_muratori6/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of Muratori6 filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png?w=497" alt="Magnitude response of Muratori6 filter"   class="aligncenter size-full wp-image-7130" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png?w=497 497w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png?w=150 150w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png?w=300 300w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png 614w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>This one <em>very</em> slightly pokes above unit gain around 0.5Ï, but evidently little enough not to blow up when the output signal is quantized to 8 bits after every step, as Casey&#8217;s test does. Other than that, we have some passband ripple, but are really quite close to unit gain until at least half-Nyquist, after which our performance starts to deteriorate, as it does for all 6-tap filters.</p>
<p>Our final 6-tap filter in this round is a 6-tap Lagrange interpolator. This one&#8217;s not in Casey&#8217;s series; its coefficients are [ 0.011719, -0.097656, 0.585938, 0.585938, -0.097656, 0.011719 ]. Lagrange interpolators are a classic family of interpolating filters (closely related to Lagrange polynomials) and have extremely flat passbands:</p>
<p><img data-attachment-id="7131" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_lagrange6/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of 6-tap Lagrange interpolator" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png?w=497" alt="Magnitude response of 6-tap Lagrange interpolator"   class="aligncenter size-full wp-image-7131" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png?w=497 497w, https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png?w=150 150w, https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png?w=300 300w, https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png 614w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>On the good side, yes, the passband really is flat. The trade-off is that the filter response visibly starts to dip around 0.4Ï: the price we pay for that flat passband is losing more of the high frequencies than we would with other filters. On the other hand, this type of filter is not going to explode with repeated application. (Really, this is the second Lagrange-type filter I&#8217;ve shown, since the initial linear interpolation filter coincides with a 2-tap Lagrange interpolator.)</p>
<h3>8-tap filters</h3>
<p>I&#8217;m putting these in their own category: the two extra taps make a significant difference in the attainable filter quality, so they&#8217;re not on even footing with the 6-tap contenders. Let&#8217;s start with the filter from H.265/HEVC:</p>
<p><img data-attachment-id="7132" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_hevc/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of HEVC filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png?w=497" alt="Magnitude response of HEVC filter"   class="aligncenter size-full wp-image-7132" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png?w=497 497w, https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png?w=150 150w, https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png?w=300 300w, https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png 614w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>Some ripple in the passband and an above-unit peak around 0.6Ï. Looking at the frequency response, this filter should blow up in Casey&#8217;s tests, and indeed it does. However it also manages to pass through frequencies all the way out to nearly 0.7Ï, not something we&#8217;ve seen so far.</p>
<p>Here&#8217;s 8-tap Lanczos:</p>
<p><img data-attachment-id="7133" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_lanczos8/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of Lanczos 8-tap" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png?w=497" alt="Magnitude response of Lanczos 8-tap"   class="aligncenter size-full wp-image-7133" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png?w=497 497w, https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png?w=150 150w, https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png?w=300 300w, https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png 614w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>Again qualitatively similar to what we see from the HEVC filter, although I overall like the &#8220;real&#8221; HEVC filter a bit better than this one. Another filter that we would expect to blow up (and Casey&#8217;s testing confirms it does).</p>
<p>At the other extreme, 8-tap Lagrange:</p>
<p><img data-attachment-id="7134" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_lagrange8/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of 8-tap Lagrange filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png?w=497" alt="Magnitude response of 8-tap Lagrange filter"   class="aligncenter size-full wp-image-7134" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png?w=497 497w, https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png?w=150 150w, https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png?w=300 300w, https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png 614w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>Passband straight as a ruler, but not much use past 0.5Ï. The good news is that, once again, it&#8217;s perfectly stable.</p>
<p>Here&#8217;s Casey&#8217;s contender:</p>
<p><img data-attachment-id="7135" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_muratori8/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of Muratori8 filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png?w=497" alt="Magnitude response of Muratori8 filter"   class="aligncenter size-full wp-image-7135" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png?w=497 497w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png?w=150 150w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png?w=300 300w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png 614w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>Again some passband ripple and it&#8217;s poking slightly above unity around 0.55Ï, but it manages good results out to about 0.6Ï before it starts to cut.</p>
<h3>Conclusions</h3>
<p>That&#8217;s a couple of different filter types, and at this point we&#8217;ve seen enough to start drawing some conclusions, namely:</p>
<p>First, adding more taps gives us frequency responses closer to our ideal, which is not exactly surprising. The trade-offs are that filters with more taps are more expensive to evaluate, and that codecs also care about things other than frequency response. Among other things, we generally want reduced-precision fixed-point approximations to the filter coefficients both for efficiency (particularly in hardware decoders) and to ensure different codec implementations agree (which, for various reasons, gets significantly nastier once floating-point is in the mix).</p>
<p>Second, among the filters I showed, there&#8217;s a fairly clear spectrum: at one end, we have the Lagrange interpolators with strictly unit-or-below gain. They are completely stable under repeated application but also tend to lose high frequencies fairly quickly. In the other direction, we have filters like the Lanczos-derived ones or those used in H.264 or HEVC that have wider pass bands but also clearly above-unit gain for at least some frequencies, meaning that frequency content in those regions will grow and ultimately explode under repeated application. Finally, Casey&#8217;s filters are somewhere in between; they have wider pass bands than pure Lagrange interpolators but keep their maximum gain close enough to 1 to avoid blow-ups when applied to data that is quantized to 8 bit fixed point.</p>
<p>This last part is something I originally intended to do a more in-depth analysis of, which is the reason this post is so delayed. But I just never felt inspired to actually write this part and didn&#8217;t make any headway the 3 times I tried to just sit down and write it <em>without</em> inspiration either, so, well, sorry. No proper analysis here. I figured it&#8217;s better to at least publish the rest of the article without it.</p>
<p>The gist of it is this: if your samples are quantized to say 8-bit fixed point after every step, it seems plausible that you should be able to get away with slightly above unit gain at some frequencies. Essentially, all the inputs (being quantized) are integers, which means they need to change by at least 0.5 steps in either direction to actually round to a different value. If the gain at a given frequency isn&#8217;t high enough to accomplish this change, nothing will happen, and even a filter with above-unit gain for some frequencies that should in theory blow up eventually, won&#8217;t. Experimentally this seems to be true and I meant to do a proper proof but as said, you&#8217;ll have to live without it for the time being. (I might do another post in the future if I do come up with a nice proof.)</p>
<p>Finally: how much does it matter? Casey&#8217;s original posts framed not diverging over repeated application as an essential requirement for a motion interpolation filter in a video codec, but it&#8217;s really not that simple.</p>
<p>There&#8217;s no question that stability under iteration is desirable, the same way that a perfectly flat response over all frequencies is desirable: ideally we&#8217;d like interpolation filters to behave like a perfect all-pass filter. But we care about computational cost and memory bandwidth too, so we don&#8217;t get to use infinitely wide filters, which means we can&#8217;t get a perfect frequency response. This is somewhat hidden by the cropping I did, but all filters shown decay very sharply above around 80% Nyquist. The stability issue is similar: if we care about stability above all else, there is a known answer, which is to use Lagrange interpolators, which give us perfect stability but have subpar response at the high frequencies.</p>
<p>There is another thing to consider here: in a video codec, motion compensation is part of the coding loop. The encoder knows exactly what the decoder will do with a given frame and set of motion vectors, and will use that result to code further updates against. In short, the interpolation filter is not left to its own devices in a feedback loop. The encoder is continuously monitoring what the current state of the frame in a compliant decoder will be. If there is a build-up of high-frequency energy over time, all that really happens is that at some point, the error will become high enough for the encoder to decide to do something about it, which means sending not just a pure motion vector, but instead a motion vector with a residual (a correction applied to the image data after motion compensation). Said residual is also coded in frequency space (essentially), most commonly using a DCT variant. In short, the end result of using a filter that has resonances in the high frequencies is that a few times per second, the encoder will have to send residuals to cancel out these resonances. This costs extra bits, and correcting errors in the high frequencies tends to be more expensive than for the lower frequencies (image and video codecs can generally code lower frequencies more cheaply than high frequencies).</p>
<p>But suppose we didn&#8217;t do that, and instead used say a perfectly stable Lagrange interpolator. It doesn&#8217;t have any resonances that would cause the image to blow up, but it does act like a bit of a low-pass filter. In short, instead of steadily gaining energy in the high frequencies, we end up steadily losing energy in the high frequencies. And this too ends up with the encoder having to send high-frequency heavy residuals periodically, this time to add in the missing high frequencies (instead of removing excess ones).</p>
<p>Neither of these is obviously preferable to, or cheaper than, the other. Sending high-frequency heavy residuals is relatively expensive, but we end up having to do it periodically regardless of which type we choose.</p>
<p>That&#8217;s not to say that it doesn&#8217;t matter at all; it&#8217;s just to point out that the actual decision of which type of filter to use in any real codec is not made in a vacuum and depends on other factors. For example, H.264/AVC and HEVC at low bit rates rely aggressively on their deblocking filters, which are essentially low-pass filters applied over block edges. In that context, a somewhat sharpening motion interpolation filter can help mitigate some of the damage, whereas a Lagrange interpolator would make things even more one-sided.</p>
<p>In short, there is no such thing as a single &#8220;best&#8221; interpolation filter. The decision is made in context and depends on what the rest of the codec does.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png" medium="image">
			<media:title type="html">Magnitude response of linear interpolation filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png" medium="image">
			<media:title type="html">Magnitude response of H.264 filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png" medium="image">
			<media:title type="html">Magnitude response of 6-tap Lancozs filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png" medium="image">
			<media:title type="html">Magnitude response of quantized Muratori6</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png" medium="image">
			<media:title type="html">Magnitude response of Muratori6 filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png" medium="image">
			<media:title type="html">Magnitude response of 6-tap Lagrange interpolator</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png" medium="image">
			<media:title type="html">Magnitude response of HEVC filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png" medium="image">
			<media:title type="html">Magnitude response of Lanczos 8-tap</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png" medium="image">
			<media:title type="html">Magnitude response of 8-tap Lagrange filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png" medium="image">
			<media:title type="html">Magnitude response of Muratori8 filter</media:title>
		</media:content>
	</item>
		<item>
		<title>What happens when iterating filters?</title>
		<link>https://fgiesen.wordpress.com/2019/04/08/what-happens-when-iterating-filters/</link>
					<comments>https://fgiesen.wordpress.com/2019/04/08/what-happens-when-iterating-filters/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Mon, 08 Apr 2019 07:56:51 +0000</pubDate>
				<category><![CDATA[Maths]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7122</guid>

					<description><![CDATA[Casey Muratori posted on his blog about half-pixel interpolation filters this week, where he ends up focusing on a particular criterion: whether the filter in question is stable under repeated application or not. There are many things about filters that are more an art than a science, especially where perceptual factors are concerned, but this [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>Casey Muratori posted on his blog about <a href="https://caseymuratori.com/blog_0035">half-pixel interpolation filters</a> this week, where he ends up focusing on a particular criterion: whether the filter in question is stable under repeated application or not.</p>
<p>There are many things about filters that are more an art than a science, especially where perceptual factors are concerned, but this particular question is both free of tricky perceptual evaluations and firmly in the realm of things we have excellent theory for, albeit one that will require me to start with a linear algebra infodump. So let&#8217;s get into it!</p>
<h3>Analyzing iterated linear maps</h3>
<p>Take any vector space V over some field <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;mathbb{F}" class="latex" /> and any linear map <img src="https://s0.wp.com/latex.php?latex=T+%3A+V+%5Crightarrow+V&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="T : V &#92;rightarrow V" class="latex" /> from that space to itself. An <em>eigenvector</em> v of T is a nonzero element of V such that <img src="https://s0.wp.com/latex.php?latex=T%28v%29+%3D+Tv+%3D+%5Clambda+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="T(v) = Tv = &#92;lambda v" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cin+%5Cmathbb%7BF%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;lambda &#92;in &#92;mathbb{F}" class="latex" /> &#8211; that is, the result of applying the map T to v is a scaled version of v itself. The scale factor Î» is the corresponding <em>eigenvalue</em>.</p>
<p>Now when we&#8217;re iterating the map T multiple times, eigenvectors of T behave in a very simple way under the iterated map: we know that applying T to v gives back a scaled version of v, and then linearity of T allows us to conclude that:<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T%5E2%28v%29+%3D+T%28T%28v%29%29+%3D+T%28Tv%29+%3D+T%28%5Clambda+v%29+%3D+%5Clambda+T%28v%29+%3D+%5Clambda%5E2+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;displaystyle T^2(v) = T(T(v)) = T(Tv) = T(&#92;lambda v) = &#92;lambda T(v) = &#92;lambda^2 v" class="latex" /><br />
and more generally <img src="https://s0.wp.com/latex.php?latex=T%5Ek%28v%29+%3D+%5Clambda%5Ek+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="T^k(v) = &#92;lambda^k v" class="latex" /> for any <img src="https://s0.wp.com/latex.php?latex=k+%5Cin+%5Cmathbb%7BN%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="k &#92;in &#92;mathbb{N}" class="latex" />.</p>
<p>The best possible case is that we find lots of eigenvectors &#8211; enough to fully characterize the map purely by what it does on its eigenvectors. For example, if V is a finite-dimensional vector space with <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdim%7D%28V%29%3Dn&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;mathrm{dim}(V)=n" class="latex" />, then if we can find n linearly independent eigenvectors, we&#8217;re golden: we can select a basis entirely made of eigenvectors, and then written in that basis, T will have a very simple form: we will have <img src="https://s0.wp.com/latex.php?latex=T+%3D+Q+%5CLambda+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="T = Q &#92;Lambda Q^{-1}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5Cmathrm%7Bdiag%7D%28%5Clambda_1%2C+%5Cdots%2C+%5Clambda_n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;Lambda = &#92;mathrm{diag}(&#92;lambda_1, &#92;dots, &#92;lambda_n)" class="latex" /> for some Q (whose columns contain n linearly independent eigenvectors of T).</p>
<p>That is, in the right basis (made of eigenvectors), T is just a diagonal matrix, which is to say, a (non-uniform) scale. This makes analysis of repeated applications of T easy, since:<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T%5E2+%3D+Q+%5CLambda+Q%5E%7B-1%7D+Q+%5CLambda+Q%5E%7B-1%7D+%3D+Q+%5CLambda%5E2+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;displaystyle T^2 = Q &#92;Lambda Q^{-1} Q &#92;Lambda Q^{-1} = Q &#92;Lambda^2 Q^{-1}" class="latex" /><br />
and in general<br />
<img src="https://s0.wp.com/latex.php?latex=T%5Ek+%3D+Q+%5CLambda%5Ek+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="T^k = Q &#92;Lambda^k Q^{-1}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5CLambda%5Ek+%3D+%5Cmathrm%7Bdiag%7D%28%5Clambda_1%5Ek%2C+%5Cdots%2C+%5Clambda_n%5Ek%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;Lambda^k = &#92;mathrm{diag}(&#92;lambda_1^k, &#92;dots, &#92;lambda_n^k)" class="latex" />: viewed in the basis made of eigenvectors, repeated application of T is just repeated scaling, and behaviour over lots of iterations ultimately just hinges on what the eigenvalues are.</p>
<p>Not every matrix can be written that way; ones that can are called <em>diagonalizable</em>. But there is a very important class of transforms (and now we allow infinite-dimensional spaces again) that is guaranteed to be diagonalizable: so called self-adjoint transforms. In the finite-dimensional real case, these correspond to symmetric matrices (matrices A such that <img src="https://s0.wp.com/latex.php?latex=A+%3D+A%5ET&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="A = A^T" class="latex" />). Such transforms are guaranteed to be diagonalizable, and even better, their eigenvectors are guaranteed to be pairwise orthogonal to each other, meaning the transform Q is an orthogonal matrix (a rotation or reflection), which among other things makes the whole process numerically quite well-behaved.</p>
<p>As a small aside, if you&#8217;ve ever wondered why iterative solvers for linear systems usually require symmetric (or, in the complex case, Hermitian) matrices: this is why. If a matrix is symmetric, it it diagonalizable, which allows us to build an iterative process to solve linear equations that we can analyze easily and <em>know</em> will converge (if we do it right). It&#8217;s not that we can&#8217;t possibly do anything iterative on non-symmetric linear systems; it just becomes a lot trickier to make any guarantees, especially if we allow arbitrary matrices. (Which could be quite pathological.)</p>
<p>Anyway, that&#8217;s a bit of background on eigendecompositions of linear maps. But what does any of this have to do with filtering?</p>
<h3>Enter convolution</h3>
<p>Convolution itself is a <em>bilinear map</em>, meaning it&#8217;s linear in both arguments. That means that if we fix either of the arguments, we get a linear map. Suppose we have a FIR filter f given by its coefficients <img src="https://s0.wp.com/latex.php?latex=%28f_0%2C+f_1%2C+%5Cdots%2C+f_%7Bm-1%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="(f_0, f_1, &#92;dots, f_{m-1})" class="latex" />. Then we can define an associated linear map <img src="https://s0.wp.com/latex.php?latex=T_f&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="T_f" class="latex" /> on a suitable space, say something like <img src="https://s0.wp.com/latex.php?latex=T_f+%3A+%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29+%5Crightarrow+%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="T_f : &#92;ell^&#92;infty(&#92;mathbb{C}) &#92;rightarrow &#92;ell^&#92;infty(&#92;mathbb{C})" class="latex" /> (writing <img src="https://s0.wp.com/latex.php?latex=%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;ell^&#92;infty(&#92;mathbb{C})" class="latex" /> for the set of bounded sequences of complex numbers) by setting<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T_f%28x%29+%3D+T_f+x+%3A%3D+f+%2A+x&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;displaystyle T_f(x) = T_f x := f * x" class="latex" />.</p>
<p>If this is all a bit dense on notation for you, all I&#8217;m doing here is holding one of the two arguments to the convolution operator constant, and trying to at least specify what set our map is working on (in this case, bounded sequences of real numbers).</p>
<p>And now we&#8217;re just about ready for the punchline: we now have a linear map from a set to itself, although in this case we&#8217;re dealing with infinite sequences, not finite ones. Luckily the notions of eigenvectors (eigensequences in this case) and eigenvalues generalize just fine. What&#8217;s even better is that for <em>all</em> discrete convolutions, we get a full complement of eigensequences, and we know exactly what they are. Namely, define the family of sequences <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="e_&#92;omega" class="latex" /> by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+e_%5Comega%5Bn%5D+%3D+%5Cexp%28i+%5Comega+n%29+%3D+%5Ccos%28%5Comega+n%29+%2B+i+%5Csin%28%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;displaystyle e_&#92;omega[n] = &#92;exp(i &#92;omega n) = &#92;cos(&#92;omega n) + i &#92;sin(&#92;omega n)" class="latex" /></p>
<p>That&#8217;s a cosine wave with frequency &omega; in the real part and the corresponding sine wave in the imaginary part, if you are so inclined, although I much prefer to stick with the complex exponentials, especially when doing algebra (it makes things easier). Anyway, if we apply our FIR filter f to that signal, we get (this is just expanding out the definition of discrete convolution for our filter and input signal, using the convention that unqualified summation is over all values of k where the sum is well-defined)</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28T_f+e_%5Comega%29%5Bn%5D+%3D+%5Csum_k+f_k+%5Cexp%28i+%5Comega+%28n-k%29%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;displaystyle (T_f e_&#92;omega)[n] = &#92;sum_k f_k &#92;exp(i &#92;omega (n-k))" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+%5Cexp%28i+%5Comega+n%29+%5Cunderbrace%7B%5Csum_k+f_k+%5Cexp%28-i+%5Comega+k%29%7D_%7B%3D%3A%5Chat%7Bf%7D%28%5Comega%29%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;displaystyle = &#92;exp(i &#92;omega n) &#92;underbrace{&#92;sum_k f_k &#92;exp(-i &#92;omega k)}_{=:&#92;hat{f}(&#92;omega)}" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+%5Chat%7Bf%7D%28%5Comega%29+%5Cexp%28i+%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;displaystyle = &#92;hat{f}(&#92;omega) &#92;exp(i &#92;omega n)" class="latex" /></p>
<p>There&#8217;s very little that happens here. The first line is just expanding the definition; then in the second line we use the properties of the exponential function (and the linearity of sums) to pull out the constant factor of <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28i+%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;exp(i &#92;omega n)" class="latex" />. And it turns out the entire rest of the formula doesn&#8217;t depend on n at all, so it turns into a constant factor for the whole sequence. It does depend on f and &omega;, so we label it <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;hat{f}(&#92;omega)" class="latex" />. The final line states exactly what we wanted, namely that the result of applying <img src="https://s0.wp.com/latex.php?latex=T_f&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="T_f" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="e_&#92;omega" class="latex" /> is just a scaled copy of <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="e_&#92;omega" class="latex" /> itself&mdash;we have an eigensequence (with eigenvalue <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;hat{f}(&#92;omega)" class="latex" />).</p>
<p>Also note that the formula for the eigenvalue isn&#8217;t particularly scary either in our case, since we&#8217;re dealing with a FIR filter f, meaning it&#8217;s a regular finite sum:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%28%5Comega%29+%3D+%5Csum_%7Bk%3D0%7D%5E%7Bm-1%7D+f_k+%5Cexp%28-i+%5Comega+k%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;displaystyle &#92;hat{f}(&#92;omega) = &#92;sum_{k=0}^{m-1} f_k &#92;exp(-i &#92;omega k)" class="latex" /></p>
<p>Oh, and there&#8217;s one more minor detail I&#8217;ve neglected to mention so far: that&#8217;s just the <a href="https://en.wikipedia.org/wiki/Discrete-time_Fourier_transform">discrete-time Fourier transform</a> (DTFT, not to be confused with the DFT, although they&#8217;re related) of f. Yup, we started out with a digital FIR filter, asked what happens when we iterate it a bunch, did a brief detour into linear algebra, and ended up in Fourier theory.</p>
<p>Long story short, if you want to know whether a linear digital filter is stable under repeated application, you want to look at its eigenvalues, which in turn are just given by its frequency response. In particular, for any given frequency &omega;, we have exactly three options:</p>
<ul>
<li><img src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3D+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="|&#92;hat{f}(&#92;omega)| = 1" class="latex" />. In this case, the amplitude at that frequency is preserved exactly under repeated application.</li>
<li><img src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3C+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="|&#92;hat{f}(&#92;omega)| &lt; 1" class="latex" />. If the filter dampens a given frequency, no matter how little, then the amplitude of the signal at that frequency will eventually be driven to zero. This is stable but causes the signal to degrade. Typical interpolation filters tend to do this for the higher frequencies, which is why signals tend to lose such frequencies (in visual terms, get blurrier) over time.</li>
<li><img src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3E+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="|&#92;hat{f}(&#92;omega)| &gt; 1" class="latex" />. If a filter amplifies any frequency by more than 1, even by just a tiny bit, then any signal containing a nonzero amount of that frequency will eventually blow up.</li>
</ul>
<p>The proof for all three cases is simply observing that k-fold application of the filter f to the signal <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="e_&#92;omega" class="latex" /> will result in the signal <img src="https://s0.wp.com/latex.php?latex=%28%5Chat%7Bf%7D%28%5Comega%29%29%5Ek+e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="(&#92;hat{f}(&#92;omega))^k e_&#92;omega" class="latex" />. To generalize this to a wider class of signals (not just complex exponentials) we would need to represent said signals as sum of complex exponentials, which is exactly what Fourier series are all about; so it can be done, but I won&#8217;t bother with the details here, since they&#8217;re out of the scope of this post.</p>
<p>Therefore, all you need to know about the stability of a given filter under repeated application is contained in its Fourier transform. I&#8217;ll try to do another post soon that shows the Fourier transforms of the filters Casey mentioned (or their magnitude response anyway, which is what we care about) and touches on other aspects such as the effect of rounding and quantization, but we&#8217;re at a good stopping point right now, so I&#8217;ll end this post here.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2019/04/08/what-happens-when-iterating-filters/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>Cache tables</title>
		<link>https://fgiesen.wordpress.com/2019/02/11/cache-tables/</link>
					<comments>https://fgiesen.wordpress.com/2019/02/11/cache-tables/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Tue, 12 Feb 2019 01:39:25 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7120</guid>

					<description><![CDATA[Hash tables are the most popular probabilistic data structure, by quite a margin. You compute some hash code then compute an array index from that hash code. If you&#8217;re using open addressing-class schemes, you then have a rule to find other array indices that a value with the given hash code might be found at; [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>Hash tables are the most popular probabilistic data structure, by quite a margin. You compute some hash code then compute an array index from that hash code. If you&#8217;re using open addressing-class schemes, you then have a rule to find other array indices that a value with the given hash code might be found at; with separate chaining, your array entries point to the head of a linked list, or the root of a binary tree, or whatever other data structure you prefer for the given use case.</p>
<p>No matter what exactly you do, this entire class of schemes always gives you a <a href="https://en.wikipedia.org/wiki/Las_Vegas_algorithm">Las Vegas algorithm</a>, meaning this type of hash table will always retain values for all the keys you inserted, but you&#8217;re gambling on how long a lookup takes.</p>
<p>An alternative is to enforce a strict limit P&ge;1 on the number of probes performed (for open addressing) or the size of any of the secondary data structures (for separate chaining). The result is a &#8220;forgetful dictionary&#8221;: keys are allowed to disappear, or at least become unretrievable. That&#8217;s quite a limitation. In return, the worst-case lookup cost becomes bounded. In short, we now have a <a href="https://en.wikipedia.org/wiki/Monte_Carlo_algorithm">Monte Carlo</a> algorithm: we&#8217;re now allowed to fail lookups (keys can disappear over time), but runtime variability is significantly reduced.</p>
<p>Let&#8217;s call this data structure a &#8220;cache table&#8221;, both because it&#8217;s useful for caching the results of queries and because it matches the most common organization of caches in hardware. We&#8217;ve been using that name at RAD for a while now, and I&#8217;ve also seen it used elsewhere, so it&#8217;s likely someone independently came up with the same name for the same thing, which I take to be a good sign.</p>
<p>In conventional hash tables, we have different probing schemes, or separate chaining with different data structures. For a cache table, we have our strict bound P on the number of entries we allow in the same bucket, and practical choices of P are relatively small, in the single or low double digits. That makes the most natural representation a simple 2D array: N rows of hash buckets with P columns, each with space for one entry, forming a N&times;P grid. Having storage for all entries in the same row right next to each other leads to favorable memory access patterns, better than most probe sequences used in open addressing or the pointer-heavy data structures typically used in separate chaining.</p>
<p>To look up a key, we calculate the row index from the hash code, using something like <code>row = hash % N</code>. Then we check whether there is a matching entry in that row, by looping over all P columns. That&#8217;s also why you don&#8217;t want P to get too large. What I&#8217;ve just described matches the operation of a P-way <a href="https://en.wikipedia.org/wiki/Cache_placement_policies#Set_Associative_Cache">set associative cache</a> in hardware exactly, and we will sometimes call P the number of &#8220;ways&#8221; in the cache table.</p>
<p>P=1 is the simplest case and corresponds to a direct-mapped cache in hardware. Each entry has exactly one location in the cache array where it can go; it&#8217;s either there or it&#8217;s not present at all. Inserting an entry means replacing the previous entry at that location.</p>
<p>For Pâ 1, there are multiple choices of which item to replace on insertion; which one is picked is determined by the <em>replacement policy</em>. There are many candidates to choose from, with different implementation trade-offs; a decent option that doesn&#8217;t require any extra metadata stored alongside each row is to use random replacement, i.e. just picking a (pseudo-)random column within the given row to evict on every insertion. &#8220;Hang on&#8221;, you might say, &#8220;aren&#8217;t hash codes pseudo-random already?&#8221;. Yes, they are, but you want to use a random number generator independent of your hash function here, or else you&#8217;re really just building a direct-mapped cache with N&times;P rows. One of the benefits of keeping P entries per row is that it allows us to have two different keys with identical hash values (i.e. a hash collision) in the same table; as per the <a href="https://en.wikipedia.org/wiki/Birthday_problem">birthday problem</a>, even with a well-distributed hash, you are likely to see collisions.</p>
<p>So what is this type of data structure useful for? I&#8217;ve come across two classes of use cases so far:</p>
<ul>
<li>Caching queries, as noted above. It&#8217;s used extensively to build (memory) caches in hardware, but it&#8217;s useful in software too. If you&#8217;re trying to cache results of operations in software, there is often a desire to keep the size of the cache bounded and have lookups take a predictable time; cache tables deliver both and are generally simpler than trying to manually limit the number of live entries in a regular hash table.</li>
<li>Approximate searching tasks. For example, they&#8217;re quite useful in LZ77 match finding &#8211; and have been used that way since at least the late 80s (a 1986 patent covering the case P=1, now expired, was the subject of a rather <a href="https://en.wikipedia.org/wiki/Stac_Electronics#Microsoft_lawsuit">famous lawsuit</a>) and <a href="http://www.ross.net/compression/lzrw3a.html">early 90s</a> (Pâ 1), respectively.</li>
</ul>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2019/02/11/cache-tables/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>Rotating a single vector using a quaternion</title>
		<link>https://fgiesen.wordpress.com/2019/02/09/rotating-a-single-vector-using-a-quaternion/</link>
					<comments>https://fgiesen.wordpress.com/2019/02/09/rotating-a-single-vector-using-a-quaternion/#respond</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Sat, 09 Feb 2019 10:15:21 +0000</pubDate>
				<category><![CDATA[Maths]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7118</guid>

					<description><![CDATA[This is a rehash of something I wrote in a forum post something like 10 years ago. It turns out that forum prohibited archiving in its robots.txt so it&#8217;s not on the Internet Archive. The original operator of said forum hosted a copy (with broken formatting) of the original posts for a while, but at [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>This is a rehash of something I wrote in a forum post something like 10 years ago. It turns out that forum prohibited archiving in its <code>robots.txt</code> so it&#8217;s not on the Internet Archive. The original operator of said forum hosted a copy (with broken formatting) of the original posts for a while, but at a different URL, breaking all links. And now that copy&#8217;s gone too, again with archiving disabled apparently. Sigh.</p>
<p>I got asked about this yesterday; I don&#8217;t have a copy of my original derivation anymore either, but here&#8217;s my best attempt at reconstructing what I probably wrote back then, and hopefully it won&#8217;t get lost again this time.</p>
<p>Suppose you&#8217;re given a unit quaternion <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q" class="latex" /> and a vector <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="v" class="latex" />. Quaternions are a common rotation representation in several fields (including computer graphics and numerical rigid-body dynamics) for reasons beyond the scope of this post. To apply a rotation to a vector, one computes the quaternion product <img src="https://s0.wp.com/latex.php?latex=q+v+q%5E%2A&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q v q^*" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="v" class="latex" /> is implicitly identified with the quaternion with real (scalar) part 0 and <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="v" class="latex" /> as its imaginary part, and <img src="https://s0.wp.com/latex.php?latex=q%5E%2A&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q^*" class="latex" /> denotes the conjugate of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q" class="latex" />. Such quaternions with a real part of 0 are also referred to as &#8220;pure imaginary&#8221; quaternions. Anyway, the result of the above product is another pure imaginary quaternion, corresponding to the rotated vector.</p>
<p>This is all explained and motivated elsewhere, and I won&#8217;t bother doing so here. Now generally, you often want to apply the same transformation to many vectors. In that case, you&#8217;re better off turning the quaternion into the equivalent 3&#215;3 rotation matrix first. You can look up the formula elsewhere or work it out from the expression above with some algebra. That&#8217;s not the topic of this post either.</p>
<p>But sometimes you really only want to transform a single vector with a quaternion, or have other reasons for not wanting to expand to an explicit 3&#215;3 (or larger) matrix first, like for example trying to minimize live register count in a shader program. So let&#8217;s look at ways of doing that.</p>
<h3>The direct way</h3>
<p>First, I&#8217;m going to split quaternions in their real (scalar) and imaginary (vector) parts, writing <img src="https://s0.wp.com/latex.php?latex=q%3D%28q_r%2C+q_%7Bxyz%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q=(q_r, q_{xyz})" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=q_r&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q_r" class="latex" /> is the real part and <img src="https://s0.wp.com/latex.php?latex=q_%7Bxyz%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q_{xyz}" class="latex" /> imaginary. The conjugate of <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is given by <img src="https://s0.wp.com/latex.php?latex=q%5E%2A%3D%28q_r%2C+-q_%7Bxyz%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q^*=(q_r, -q_{xyz})" class="latex" />.</p>
<p>The product of two quaternions <img src="https://s0.wp.com/latex.php?latex=a&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="a" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=b&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="b" class="latex" /> is given by</p>
<p><img src="https://s0.wp.com/latex.php?latex=ab+%3D+%28a_r+b_r+-+a_%7Bxyz%7D+%5Ccdot+b_%7Bxyz%7D%2C+a_r+b_%7Bxyz%7D+%2B+b_r+a_%7Bxyz%7D+%2B+a_%7Bxyz%7D+%5Ctimes+b_%7Bxyz%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="ab = (a_r b_r - a_{xyz} &#92;cdot b_{xyz}, a_r b_{xyz} + b_r a_{xyz} + a_{xyz} &#92;times b_{xyz})" class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Ccdot&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;cdot" class="latex" /> denotes the usual dot product and <img src="https://s0.wp.com/latex.php?latex=%5Ctimes&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;times" class="latex" /> the cross product. If you&#8217;re not used to seeing this in vector notation, I recommend using this (or something more abstract like geometric algebra) for derivations; writing everything in terms of scalars and the <img src="https://s0.wp.com/latex.php?latex=i%2C+j%2C+k&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="i, j, k" class="latex" /> basis elements makes you miss the forest for the trees.</p>
<p>Anyway, armed with this formula, we can compute our final product without too much trouble. Let&#8217;s start with the <img src="https://s0.wp.com/latex.php?latex=qv&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="qv" class="latex" /> part:</p>
<p><img src="https://s0.wp.com/latex.php?latex=qv+%3D+%28-q_%7Bxyz%7D+%5Ccdot+v%2C+q_r+v+%2B+q_%7Bxyz%7D+%5Ctimes+v%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="qv = (-q_{xyz} &#92;cdot v, q_r v + q_{xyz} &#92;times v)" class="latex" /></p>
<p>Not so bad. Now we have to multiply it from the right by <img src="https://s0.wp.com/latex.php?latex=q%5E%2A&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q^*" class="latex" />, which I&#8217;ll do in multiple steps. First, let&#8217;s take care of the real part, by multiplying our just-computed values for <img src="https://s0.wp.com/latex.php?latex=qv&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="qv" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=q%5E%2A&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q^*" class="latex" /> using the general multiplication formula above:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28qvq%5E%2A%29_r+%3D+-q_r+%28q_%7Bxyz%7D+%5Ccdot+v%29+-+%28%28q_r+v+%2B+q_%7Bxyz%7D+%5Ctimes+v%29+%5Ccdot+%28-q_%7Bxyz%7D%29%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="(qvq^*)_r = -q_r (q_{xyz} &#92;cdot v) - ((q_r v + q_{xyz} &#92;times v) &#92;cdot (-q_{xyz}))" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%3D+-q_r+%28q_%7Bxyz%7D+%5Ccdot+v%29+%2B+q_r+%28v+%5Ccdot+q_%7Bxyz%7D%29+%2B+%28%28q_%7Bxyz%7D+%5Ctimes+v%29+%5Ccdot+q_%7Bxyz%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="= -q_r (q_{xyz} &#92;cdot v) + q_r (v &#92;cdot q_{xyz}) + ((q_{xyz} &#92;times v) &#92;cdot q_{xyz})" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%3D+q_%7Bxyz%7D+%5Ccdot+%28q_%7Bxyz%7D+%5Ctimes+v%29+%3D+0&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="= q_{xyz} &#92;cdot (q_{xyz} &#92;times v) = 0" class="latex" /></p>
<p>because the first two dot products are identical and the cross product of <img src="https://s0.wp.com/latex.php?latex=q_%7Bxyz%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q_{xyz}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="v" class="latex" /> is perpendicular to <img src="https://s0.wp.com/latex.php?latex=q_%7Bxyz%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q_{xyz}" class="latex" />. This proves that <img src="https://s0.wp.com/latex.php?latex=qvq%5E%2A&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="qvq^*" class="latex" /> is indeed a pure imaginary quaternion again, just like the <img src="https://s0.wp.com/latex.php?latex=v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="v" class="latex" /> we started out with.</p>
<p>Nice to know, but of course we&#8217;re actually here for the vector part:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28qvq%5E%2A%29_%7Bxyz%7D+%3D+%28-q_%7Bxyz%7D+%5Ccdot+v%29+%28-q_%7Bxyz%7D%29+%2B+q_r+%28q_r+v+%2B+q_%7Bxyz%7D+%5Ctimes+v%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="(qvq^*)_{xyz} = (-q_{xyz} &#92;cdot v) (-q_{xyz}) + q_r (q_r v + q_{xyz} &#92;times v)" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%2B+%28q_r+v+%2B+q_%7Bxyz%7D+%5Ctimes+v%29+%5Ctimes+%28-q_%7Bxyz%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="+ (q_r v + q_{xyz} &#92;times v) &#92;times (-q_{xyz})" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%3D+%28q_%7Bxyz%7D+%5Ccdot+v%29+q_%7Bxyz%7D+%2B+q_r%5E2+v+%2B+q_r+%28q_%7Bxyz%7D+%5Ctimes+v%29+%2B+q_%7Bxyz%7D+%5Ctimes+%28q_r+v+%2B+q_%7Bxyz%7D+%5Ctimes+v%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="= (q_{xyz} &#92;cdot v) q_{xyz} + q_r^2 v + q_r (q_{xyz} &#92;times v) + q_{xyz} &#92;times (q_r v + q_{xyz} &#92;times v)" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%3D+%28q_%7Bxyz%7D+%5Ccdot+v%29+q_%7Bxyz%7D+%2B+q_r%5E2+v+%2B+2+q_r+%28q_%7Bxyz%7D+%5Ctimes+v%29+%2B+q_%7Bxyz%7D+%5Ctimes+%28q_%7Bxyz%7D+%5Ctimes+v%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="= (q_{xyz} &#92;cdot v) q_{xyz} + q_r^2 v + 2 q_r (q_{xyz} &#92;times v) + q_{xyz} &#92;times (q_{xyz} &#92;times v)" class="latex" /></p>
<p>which so far has used nothing fancier than the antisymmetry of the cross product <img src="https://s0.wp.com/latex.php?latex=a+%5Ctimes+b+%3D+-b+%5Ctimes+a&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="a &#92;times b = -b &#92;times a" class="latex" />.</p>
<p>If we pull out and name the shared cross product, we get</p>
<p><img src="https://s0.wp.com/latex.php?latex=u+%3D+q_%7Bxyz%7D+%5Ctimes+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="u = q_{xyz} &#92;times v" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%28qvq%5E%2A%29_%7Bxyz%7D+%3D+%28q_%7Bxyz%7D+%5Ccdot+v%29+q_%7Bxyz%7D+%2B+q_r%5E2+v+%2B+2+q_r+u+%2B+q_%7Bxyz%7D+%5Ctimes+u&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="(qvq^*)_{xyz} = (q_{xyz} &#92;cdot v) q_{xyz} + q_r^2 v + 2 q_r u + q_{xyz} &#92;times u" class="latex" /></p>
<p>which is the direct expression for the transformed vector from the formula. This is what you get if you just plug everything into the formulas and apply basic algebraic simplifications until you run out of obvious things to try (which is exactly what we did).</p>
<p>In terms of scalar operation count, this boils down to two cross products at 6 multiplies and 3 additions (well, subtractions) each; one 3D dot product at 3 multiplies and 2 additions; 3 vector-by-scalar multiplications at 3 multiplies each; two scalar multiplies (to form <img src="https://s0.wp.com/latex.php?latex=q_r%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q_r^2" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=2q_r&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="2q_r" class="latex" />, although the latter can also be computed via addition if you prefer); and finally 3 vector additions at 3 adds each. The total operation count is thus 26 scalar multiplies and 17 additions, unless I miscounted. For GPUs, a multiply-add &#8220;a*b+c&#8221; generally counts as a single operation, and in that case the scalar operation count is 9 scalar multiplies and 17 scalar multiply-adds.</p>
<h3>Stand back, I&#8217;m going to try algebra!</h3>
<p>We can do better than that. So far, we haven&#8217;t used that <img src="https://s0.wp.com/latex.php?latex=q&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q" class="latex" /> is a unit quaternion, meaning that <img src="https://s0.wp.com/latex.php?latex=q_r%5E2+%2B+q_%7Bxyz%7D+%5Ccdot+q_%7Bxyz%7D+%3D+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q_r^2 + q_{xyz} &#92;cdot q_{xyz} = 1" class="latex" />. We can thus plug in <img src="https://s0.wp.com/latex.php?latex=%281+-+q_%7Bxyz%7D+%5Ccdot+q_%7Bxyz%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="(1 - q_{xyz} &#92;cdot q_{xyz})" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=q_r%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="q_r^2" class="latex" />, which yields:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28qvq%5E%2A%29_%7Bxyz%7D+%3D+%28q_%7Bxyz%7D+%5Ccdot+v%29+q_%7Bxyz%7D+%2B+%281+-+q_%7Bxyz%7D+%5Ccdot+q_%7Bxyz%7D%29+v+%2B+2+q_r+u+%2B+q_%7Bxyz%7D+%5Ctimes+u&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="(qvq^*)_{xyz} = (q_{xyz} &#92;cdot v) q_{xyz} + (1 - q_{xyz} &#92;cdot q_{xyz}) v + 2 q_r u + q_{xyz} &#92;times u" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%3D+v+%2B+%28q_%7Bxyz%7D+%5Ccdot+v%29+q_%7Bxyz%7D+-+%28q_%7Bxyz%7D+%5Ccdot+q_%7Bxyz%7D%29+v+%2B+2+q_r+u+%2B+q_%7Bxyz%7D+%5Ctimes+u&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="= v + (q_{xyz} &#92;cdot v) q_{xyz} - (q_{xyz} &#92;cdot q_{xyz}) v + 2 q_r u + q_{xyz} &#92;times u" class="latex" /></p>
<p>This might look worse, but it&#8217;s progress, because we can now apply the <a href="https://en.wikipedia.org/wiki/Triple_product#Vector_triple_product">vector triple product</a> identity</p>
<p><img src="https://s0.wp.com/latex.php?latex=a+%5Ctimes+%28b+%5Ctimes+c%29+%3D+%28a+%5Ccdot+c%29b+-+%28a+%5Ccdot+b%29c&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="a &#92;times (b &#92;times c) = (a &#92;cdot c)b - (a &#92;cdot b)c" class="latex" /></p>
<p>with <img src="https://s0.wp.com/latex.php?latex=a%3Dq_%7Bxyz%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="a=q_{xyz}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=b%3Dq_%7Bxyz%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="b=q_{xyz}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=c%3Dv&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="c=v" class="latex" />, leaving us with:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28qvq%5E%2A%29_%7Bxyz%7D+%3D+v+%2B+q_%7Bxyz%7D+%5Ctimes+%28q_%7Bxyz%7D+%5Ctimes+v%29+%2B+2+q_r+u+%2B+q_%7Bxyz%7D+%5Ctimes+u&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="(qvq^*)_{xyz} = v + q_{xyz} &#92;times (q_{xyz} &#92;times v) + 2 q_r u + q_{xyz} &#92;times u" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%3D+v+%2B+q_%7Bxyz%7D+%5Ctimes+u+%2B+2+q_r+u+%2B+q_%7Bxyz%7D+%5Ctimes+u&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="= v + q_{xyz} &#92;times u + 2 q_r u + q_{xyz} &#92;times u" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%3D+v+%2B+2+q_r+u+%2B+2+q_%7Bxyz%7D+%5Ctimes+u&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="= v + 2 q_r u + 2 q_{xyz} &#92;times u" class="latex" /></p>
<p>The two remaining terms involving <img src="https://s0.wp.com/latex.php?latex=u&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="u" class="latex" /> both multiply by two, so we use a slightly different shared temporary for the final version of our formula:</p>
<p><img src="https://s0.wp.com/latex.php?latex=t+%3D+2+q_%7Bxyz%7D+%5Ctimes+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="t = 2 q_{xyz} &#92;times v" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%28qvq%5E%2A%29_%7Bxyz%7D+%3D+v+%2B+q_r+t+%2B+q_%7Bxyz%7D+%5Ctimes+t&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="(qvq^*)_{xyz} = v + q_r t + q_{xyz} &#92;times t" class="latex" /></p>
<p>Final scalar operation count: without multiply-adds, the two cross products sum to 12 multiplies and 6 adds, scaling t by two takes 3 multiplies (or adds, your choice), and the final vector-by-scalar multiply and summation take 3 multiplies and 6 adds, respectively. That&#8217;s 18 multiplies and 12 adds total, or 15 multiplies and 15 adds if you did the doubling using addition.</p>
<p>Either way, that&#8217;s a significant reduction on both counts.</p>
<p>With multiply-adds, I count a total of 3 multiplies and 9 multiply-adds for the cross products (the first cross product has nothing to sum to, but the second does); either 3 multiplies or 3 adds (your choice again) for the doubling; and another 3 multiply-adds for the <img src="https://s0.wp.com/latex.php?latex=v+%2B+q_r+t&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="v + q_r t" class="latex" /> portion. That&#8217;s either 6 multiplies and 12 multiply-adds or 3 multiplies, 3 adds and 12 multiply-adds. Furthermore some GPUs can fold a doubling straight into the operand without counting as another operation at all; in that case we get 3 multiplies and 12 multiply-adds.</p>
<p>For comparison, multiplying a vector by a 3&#215;3 rotation matrix takes 9 multiplies and 6 additions (or 3 multiplies plus 6 multiply-adds). So even though this is much better than we started out with, it&#8217;s generally still worthwhile to form that rotation matrix explicitly if you plan on transforming lots of vectors by the same quaternion, and aren&#8217;t worried about GPU vector register counts or similar.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2019/02/09/rotating-a-single-vector-using-a-quaternion/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>Rate-distortion optimization</title>
		<link>https://fgiesen.wordpress.com/2018/12/10/rate-distortion-optimization/</link>
					<comments>https://fgiesen.wordpress.com/2018/12/10/rate-distortion-optimization/#respond</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Mon, 10 Dec 2018 08:44:01 +0000</pubDate>
				<category><![CDATA[Compression]]></category>
		<category><![CDATA[Maths]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7109</guid>

					<description><![CDATA[&#8220;Rate-distortion optimization&#8221; is a term in lossy compression that sounds way more complicated and advanced than it actually is. There&#8217;s an excellent chance that by the end of this post you&#8217;ll go &#8220;wait, that&#8217;s it?&#8221;. If so, great! My goal here is just to explain the basic idea, show how it plays out in practice, [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>&#8220;Rate-distortion optimization&#8221; is a term in lossy compression that sounds way more complicated and advanced than it actually is. There&#8217;s an excellent chance that by the end of this post you&#8217;ll go &#8220;wait, that&#8217;s it?&#8221;. If so, great! My goal here is just to explain the basic idea, show how it plays out in practice, and maybe get you thinking about other applications. So let&#8217;s get started!</p>
<h3>What does &#8220;rate-distortion optimization&#8221; actually mean?</h3>
<p>The mission statement for lossless data compression is pretty clear. A lossless compressor transforms one blob of data into another one, ideally (but not always) smaller, in a way that is reversible: there&#8217;s another transform (the decoder) that, when applied to the second blob, will return an <em>exact</em> copy of the original data.</p>
<p>Lossy compression is another matter. The output of the decoder is usually at least slightly different from the original data, and sometimes very different; and generally, it&#8217;s almost always possible to take an existing lossily-compressed piece of data, say a short video or MP3 file, make a slightly smaller copy by literally deleting a hundred bytes from the middle of the file with a hex editor, and get another version of the original video (or audio) file that is still clearly recognizable yet degraded. Seriously, if you&#8217;ve never done this before, try it, especially with MP3s: it&#8217;s quite hard to mangle a MP3 file in a way that will make it not play back anymore, because MP3s have no required file-level headers, tolerate arbitrary junk in the middle of the bitstream, and are self-synchronizing.</p>
<p>With lossless compression, it makes sense to ask &#8220;what is the smallest I can make this file?&#8221;. With lossy compression, less so; you can generally get files far smaller than you would ever want to use, because the data is degraded so much it&#8217;s barely recognizable (if that). Minimizing file size alone isn&#8217;t interesting; we want to minimize size while simultaneously maximizing the quality of the result. The way we do this is by coming up with some error metric that measures the distance between the original image and the result the decoder will actually produce given a candidate bitstream. Now our bitstream has two associated values: its length in bits, the (bit) <em>rate</em>, usually called R in formulas, and a measure of how much error was introduced as a result of the lossy coding process, the <em>distortion</em>, or D in formulas. R is almost always measured in bits or bytes; D can be in one of many units, depending on what type of error metric is used.</p>
<p>Rate-distortion optimization (RDO for short) then means that an encoder considers several possible candidate bit streams, evaluates their rate and distortion, and tries to make an optimal choice; if possible, we&#8217;d like it to be <em>globally</em> optimal (i.e. returning a true best-possible solution), but at the very least optimal among the candidates that were considered. Sounds great, but what does &#8220;better&#8221; mean here? Given a pair <img src="https://s0.wp.com/latex.php?latex=%28R_1%2CD_1%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="(R_1,D_1)" class="latex" /> and another pair <img src="https://s0.wp.com/latex.php?latex=%28R_2%2CD_2%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="(R_2,D_2)" class="latex" />, how do we tell which one is better?</p>
<h3>The pareto frontier</h3>
<p>Suppose what we have 8 possible candidate solutions we are considering, each with their own rate and distortion scores. If we prepare a scatter plot of rate on the x-axis versus distortion on the y-axis, we might get something like this:</p>
<p><img data-attachment-id="7110" data-permalink="https://fgiesen.wordpress.com/2018/12/10/rate-distortion-optimization/rdo_scatter/" data-orig-file="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter.png" data-orig-size="452,280" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Rate vs. Distortion scatterplot" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter.png?w=452" src="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter.png?w=497" alt="Rate vs. Distortion scatterplot"   class="alignnone size-full wp-image-7110" srcset="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter.png 452w, https://fgiesen.files.wordpress.com/2018/12/rdo_scatter.png?w=150 150w, https://fgiesen.files.wordpress.com/2018/12/rdo_scatter.png?w=300 300w" sizes="(max-width: 452px) 100vw, 452px" /></p>
<p>The thin, light-gray candidates aren&#8217;t very interesting, because what they have in common is that there is at least one other candidate solution that is strictly better than them in every way. That is, some other candidate point has the same (or lower) rate, and also the same (or lower) distortion as the grayed-out points. There&#8217;s just no reason to ever pick any of those points, based on the criteria we&#8217;re considering, anyway. In the scatterplot, this means that there is at least one other point that is both to the left (lower rate) and below (lower distortion).</p>
<p>For any of the points in the diagram, imagine a horizontal and a vertical line going through it, partitioning the plane into four quadrants. Any point that ends up in the lower-left quadrant (lower rate and lower distortion) is clearly superior. Likewise, any point in the upper-right quadrant (higher rate and higher distortion) is clearly inferior. The situation with the other two quadrants isn&#8217;t as clear.</p>
<p>Which brings us to the other four points: the three fat black points, and the red point. These are all points that have no other points to the left and below them, meaning they are <a href="https://en.wikipedia.org/wiki/Pareto_efficiency">pareto efficient</a>. This means that, unlike the situation with the light gray points, we can&#8217;t pick another candidate that improves one of the metrics without making the other worse. The set of points that are pareto efficient constitutes the <em>pareto frontier</em>.</p>
<p>These points are not all the same, though. The three fat black points are not just pareto efficient, but are also on the convex hull of the point set (the lower left contour of the convex hull here is drawn using blue lines), whereas the red point is not. The points that are both pareto and on the convex hull of the pareto frontier are particularly important, and can be characterized in a different way.</p>
<p>Namely, imagine taking a straightedge, angling it so that it&#8217;s either horizontal, &#8220;descending&#8221; (with reference to our graph) from left to right, or vertical, and then sliding it slowly &#8220;upwards&#8221; from the origin without changing its orientation until it hits one of the points in our set. It will look something like this:</p>
<p><img data-attachment-id="7111" data-permalink="https://fgiesen.wordpress.com/2018/12/10/rate-distortion-optimization/rdo_scatter2/" data-orig-file="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter2.png" data-orig-size="452,280" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Rate vs. Distortion scatterplot with lines" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter2.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter2.png?w=452" src="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter2.png?w=497" alt="Rate vs. Distortion scatterplot with lines"   class="alignnone size-full wp-image-7111" srcset="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter2.png 452w, https://fgiesen.files.wordpress.com/2018/12/rdo_scatter2.png?w=150 150w, https://fgiesen.files.wordpress.com/2018/12/rdo_scatter2.png?w=300 300w" sizes="(max-width: 452px) 100vw, 452px" /></p>
<p>The shading here is meant to suggest the motion of the green line; we keep sliding it up until it eventually catches on the middle of our three fat black points. If we change the angle of our line to something else, we can manage to hit the other two black points, but not the red one. This has nothing to do with this particular problem and is a general property of convex sets: any vertices of the set must be extremal in some direction.</p>
<p>Getting a bit more precise here, the various copies of the green line I&#8217;ve drawn correspond to lines</p>
<p><img src="https://s0.wp.com/latex.php?latex=w_1+R+%2B+w_2+D+%3D+%5Cmathrm%7Bconst.%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="w_1 R + w_2 D = &#92;mathrm{const.}" class="latex" /></p>
<p>and the constraints I gave on the orientation of the line boil down to <img src="https://s0.wp.com/latex.php?latex=w_1%2C+w_2+%5Cge+0&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="w_1, w_2 &#92;ge 0" class="latex" /> (with at least one being nonzero). Sliding our straightedge until we hit a point corresponds to the minimization problem</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmin_i+w_1+R_i+%2B+w_2+D_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="&#92;min_i w_1 R_i + w_2 D_i" class="latex" /></p>
<p>for a given choice of w<sub>1</sub> and w<sub>2</sub>, and the three black points are the three possible solutions we might get for our set of candidate points. So we&#8217;ve switched from purely minimizing rate or purely minimizing distortion (both of which, in general, tend to give somewhat pathological results) towards minimizing some linear combination of the two with non-negative weights; and doing so with various choices of the weights w<sub>1</sub> and w<sub>2</sub> will allow us to sweep out the lower left convex hull of the pareto frontier, which is often the &#8220;interesting&#8221; part (although, as the red point in our example illustrates, this process excludes some of the points on the pareto frontier).</p>
<p>This does not seem particularly impressive so far: we don&#8217;t want to purely minimize one quantity or the other, so instead we&#8217;re minimizing a linear combination of the two. That seems it would&#8217;ve been the obvious next thing to try. But looking at the characterization above does at least give us some idea on what looking at these linear combinations ends up doing, and exactly what we end up giving up (namely, the pareto points not on the convex hull). And there&#8217;s another massively important aspect to consider here.</p>
<h3>The Lagrangian connection</h3>
<p>If we take our linear combination above and divide through by w<sub>1</sub> or w<sub>2</sub> (assuming they are non-zero; dividing our objective by a scalar constant does not change the results of the optimization problem), respectively, we get:</p>
<p><img src="https://s0.wp.com/latex.php?latex=L_1+%3D+R+%2B+%5Cfrac%7Bw_2%7D%7Bw_1%7D+D+%3D%3A+R+%2B+%5Clambda_1+D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="L_1 = R + &#92;frac{w_2}{w_1} D =: R + &#92;lambda_1 D" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=L_2+%3D+D+%2B+%5Cfrac%7Bw_1%7D%7Bw_2%7D+R+%3D%3A+D+%2B+%5Clambda_2+R&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="L_2 = D + &#92;frac{w_1}{w_2} R =: D + &#92;lambda_2 R" class="latex" /></p>
<p>which are essentially the <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrangians</a> we would get for continuous optimization problems of the form &#8220;minimize R subject to D=const.&#8221; (L<sub>1</sub>) and &#8220;minimize D subject to R=const.&#8221; (L<sub>2</sub>); that is, if we were in a continuously differentiable setting (for data compression we usually aren&#8217;t), trying to solve the problems of either minimizing bit rate while hitting a set distortion target or minimizing distortion within a given bit rate limit woud lead us to study the same type of expression. Generalizing one step further, allowing not just equality but also inequality constraints (i.e. rate or distortion <em>at most</em> a certain value, rather then requiring exact match) still leads to essentially the same functions, this time by way of the <a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">KKT conditions</a>.</p>
<p>In this post, I intentionally went &#8220;backwards&#8221; by starting with the Lagrangian-esque expressions and then mentioning the connection to continuous optimization problems because I want to emphasize that this type of linear combination of the different metrics arises naturally, even in a fully discrete setting; starting out with Lagrange or KKT multipliers would get us into technical difficulties with discrete decisions that do not necessary admit continuously differentiable objectives or constraint functions. Since the whole process makes sense even without explicitly mentioning Lagrange multipliers at all, that seemed like the most natural way to handle it.</p>
<h3>What this means in practice</h3>
<p>I hopefully now have you convinced that looking at the minima of the linear combination</p>
<p><img src="https://s0.wp.com/latex.php?latex=w_1+R+%2B+w_2+D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="w_1 R + w_2 D" class="latex" /></p>
<p>is a sensible thing to do, and both our direct derivation and the two Lagrange multiplier formulations for continuous problems I mentioned lead us towards it. Neither our direct derivation nor the Lagrange multiplier version tells us what to set our mystery weight parameters to, however. In fact, the Lagrange multiplier method flat-out tells you that for every instance of your optimization problem, there <em>exist</em> the right values for the Lagrange multipliers that correspond to an optimum of the problem you&#8217;re interested in, but it doesn&#8217;t tell you how to get them.</p>
<p>However, what&#8217;s nice is that the same thing also works in reverse, as we saw earlier with our line-sweeping procedure: picking the angle of the line we&#8217;re sliding around corresponds to picking a Lagrange multiplier. No matter which one we pick, we are going to end up finding an optimal point for <em>some</em> trade-off, namely one that is pareto; it just might not be the exact trade-off we wanted.</p>
<p>For example, suppose we decide that a single-variable parameterization like in the Lagrange multiplier scenario is convenient, and we pick something like L<sub>1</sub>, namely w<sub>1</sub> fixed at 1, w<sub>2</sub> = Î». We were assuming from the beginning that we have some method of generating candidate solutions; all that&#8217;s left to do is to rank them and pick a winner. Let&#8217;s start with Î»=1, which leads to a solution with some (R,D) pair that minimizes <img src="https://s0.wp.com/latex.php?latex=R+%2B+D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="R + D" class="latex" />. Note it&#8217;s often useful to think of these quantities with units attached; we typically measure R in bits and [D] is whatever it is, so the unit of Î» must be [Î»] = bits/[D], i.e. Î» is effectively an exchange rate that tells us how many units of distortion are worth as much as a single bit. We can then look at the R and D values of the solution we got back. If say we&#8217;re trying to hit a certain bit rate, then if R is close to that target, we might be happy and just stop. If R is way above the target bit rate, we overshot, and clearly need to penalize high distortions less; we might try another round with Î»=0.1 next. Conversely, if say R is a few percent below the target rate, we might try another round with slightly higher lambda, say Î»=1.02, trying to penalize distortion a bit more so we spend more bits to reduce it, and see if that gets us even closer.</p>
<p>With this kind of process, even knowing nothing else about the problem, you can systematically explore the options along the pareto frontier and try to find a better fit. What&#8217;s nice is that finding the minimum for a given choice of parameters (Î» in our case) doesn&#8217;t require us to store all candidate options considered and rank them later; storing all candidates is not a big deal in our original example, where we were only trying to decide between a handful of options, but in practice you often end up with huge search spaces (exponential in the problem size is not unheard of), and being able to bake it down to a single linear function is convenient in other ways; for example, it tends to work well with efficient dynamic programming solutions to problems that would be infeasible to handle with brute force.</p>
<h3>Wait, that&#8217;s it?</h3>
<p>Yes, pretty much. Instead of trying to purely minimize bit rate or distortion, you use a combination <img src="https://s0.wp.com/latex.php?latex=R+%2B+%5Clambda+D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="R + &#92;lambda D" class="latex" /> and vary Î» to taste to hit your targets. Often, you know enough about your problem space to have a pretty good idea of what values Î» should have; for example, in video compression, it&#8217;s pretty common to tie the Î» used when coding residuals to quantizer step size, based on the (known) behavior of the quantizer and the (expected) characteristics of real-world signals. But even when you don&#8217;t know anything about your data, you can always use a search process for Î» as outlined above (which is, of course, slower).</p>
<p>Now the one thing to note in practice is that you rarely use just a single distortion metric; for example, in video coding, it&#8217;s pretty common to use one distortion metric when quantizing residuals, a different one for motion search, and a third for overall block mode decision. In general, the more frequently something is done (or the bigger the search space is), the more willing codecs are to make compromises with their distortion measure to enable more efficient searches. In general, good results require both decent metrics and doing a good job exploring the search space, and accepting some defects in the metrics in exchange for a significant increase in search space covered in the same amount of time is often a good deal.</p>
<p>But the basic process is just this: measure bit rate and distortion (in your unit of choice) for every option you&#8217;re considering, and then rank your options based on their combined <img src="https://s0.wp.com/latex.php?latex=R+%2B+%5Clambda+D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="R + &#92;lambda D" class="latex" /> (or <img src="https://s0.wp.com/latex.php?latex=D+%2B+%5Clambda%27+R&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="D + &#92;lambda&#039; R" class="latex" />, which is a different but equivalent parameterization) scores. This gives you points on the lower left convex hull of the pareto frontier, which is what you want.</p>
<p>This applies in other settings as well. For example, the various lossless compressors in Oodle are, well, lossless, but they still have a bunch of decisions to make, some of which take more time in the decoder than others. For a lossless codec, measuring &#8220;distortion&#8221; doesn&#8217;t make any sense (it&#8217;s always 0), but measuring decode time does; so the Oodle encoders optimize for a trade-off between compressed size and decode time.</p>
<p>Of course, you can have more parameters too; for example, you might want to combine these two ideas and do joint optimization over bit rate, distortion, and decode time, leading to an expression of the type <img src="https://s0.wp.com/latex.php?latex=R+%2B+%5Clambda+D+%2B+%5Cmu+T&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" alt="R + &#92;lambda D + &#92;mu T" class="latex" /> with two Lagrange multipliers, with Î» as before, and a second multiplier Î¼ that encodes the exchange rate from time units into bits.</p>
<p>Either way, the details of this quickly get complicated, but the basic idea is really quite simple. I hope this post de-mystifies it a bit.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2018/12/10/rate-distortion-optimization/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter.png" medium="image">
			<media:title type="html">Rate vs. Distortion scatterplot</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/12/rdo_scatter2.png" medium="image">
			<media:title type="html">Rate vs. Distortion scatterplot with lines</media:title>
		</media:content>
	</item>
		<item>
		<title>Reading bits in far too many ways (part 3)</title>
		<link>https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/</link>
					<comments>https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/#respond</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Thu, 27 Sep 2018 11:50:42 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<category><![CDATA[Compression]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7102</guid>

					<description><![CDATA[(Continued from part 2. &#8220;A whirlwind introduction to dataflow graphs&#8221; is required reading.) Last time, we saw a whole bunch of different bit reader implementations. This time, I&#8217;ll continue with a few more variants, implementation considerations on pipelined superscalar CPUs, and some ways to use the various degrees of freedom to our advantage. Dependency structure [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>(Continued from <a href="https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/">part 2</a>. &#8220;<a href="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/">A whirlwind introduction to dataflow graphs</a>&#8221; is required reading.)</p>
<p>Last time, we saw a whole bunch of different bit reader implementations. This time, I&#8217;ll continue with a few more variants, implementation considerations on pipelined superscalar CPUs, and some ways to use the various degrees of freedom to our advantage.</p>
<h3>Dependency structure of bit readers</h3>
<p>To get a better feel for where the bottlenecks in bit decoding are, let me restate some of the bit reading approaches we&#8217;ve covered in the previous parts again in our pseudo-assembly language, and then we can have a look at the corresponding dependency graphs.</p>
<p>Let&#8217;s start with variant 3 from last time, but I&#8217;ll do a LSB-first version this time:</p>
<pre>
refill3_lsb:
    rBytesConsumed = lsr(rBitPos, 3);
    rBitPtr = rBitPtr + rBytesConsumed;
    rBitBuf = load64LE(rBitPtr);
    rBitPos = rBitPos &amp; 7;

peekbits3_lsb(count):
    rBits = lsr(rBitBuf, rBitPos);
    rBitMask = lsl(1, count);
    rBitMask = rBitMask - 1;
    rBits = rBits &amp; rBitMask; // result

consume3_lsb(count):
    rBitPos = rBitPos + count;
</pre>
<p>Note that if <code>count</code> is a compile-time constant, the computation for <code>rBitMask</code> can be entirely constant-folded. Peeking ahead by a constant, fixed number of bits then working out from the result how many bits to actually consume is quite common in practice, so that&#8217;s what we&#8217;ll do. If we do a refill followed by two peek/consume cycles with the consume count being determined from the read bits &#8220;somehow&#8221;, followed by another refill (for the next loop iteration), the resulting pseudo-asm is like this:</p>
<pre>
    // Initial refill
    rBytesConsumed = lsr(rBitPos, 3);   // Consumed 0
    rBitPtr = rBitPtr + rBytesConsumed; // Advance 0
    rBitBuf = load64LE(rBitPtr);        // Load 0
    rBitPos = rBitPos &amp; 7;              // LeftoverBits 0

    // First decode (peek count==19)
    rBits = lsr(rBitBuf, rBitPos);      // BitsRemaining 0
    rBits = rBits &amp; 0x7ffff;            // BitsMasked 0
    rCount = determineCount(rBits);     // DetermineCount 0
    rBitPos = rBitPos + rCount;         // PosInc 0
    
    // Second decode
    rBits = lsr(rBitBuf, rBitPos);      // BitsRemaining 1
    rBits = rBits &amp; 0x7ffff;            // BitsMasked 1
    rCount = determineCount(rBits);     // DetermineCount 1
    rBitPos = rBitPos + rCount;         // PosInc 1

    // Second refill
    rBytesConsumed = lsr(rBitPos, 3);   // Consumed 1
    rBitPtr = rBitPtr + rBytesConsumed; // Advance 1
    rBitBuf = load64LE(rBitPtr);        // Load 1
    rBitPos = rBitPos &amp; 7;              // LeftoverBits 1
</pre>
<p>And the dependency graph looks dishearteningly long and skinny:</p>
<p><img data-attachment-id="7103" data-permalink="https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/bitread_var3_lsb-gv/" data-orig-file="https://fgiesen.files.wordpress.com/2018/09/bitread_var3_lsb-gv.png" data-orig-size="424,827" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Dependency graph for bit reading, variant 3" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/09/bitread_var3_lsb-gv.png?w=154" data-large-file="https://fgiesen.files.wordpress.com/2018/09/bitread_var3_lsb-gv.png?w=424" src="https://fgiesen.files.wordpress.com/2018/09/bitread_var3_lsb-gv.png?w=497" alt="Dependency graph for bit reading, variant 3"   class="aligncenter size-full wp-image-7103" srcset="https://fgiesen.files.wordpress.com/2018/09/bitread_var3_lsb-gv.png 424w, https://fgiesen.files.wordpress.com/2018/09/bitread_var3_lsb-gv.png?w=77 77w, https://fgiesen.files.wordpress.com/2018/09/bitread_var3_lsb-gv.png?w=154 154w" sizes="(max-width: 424px) 100vw, 424px" /></p>
<p>Ouch. That&#8217;s averaging less than one instruction per cycle, and it&#8217;s all in one big, serial dependency chain. Not depicted in this graph but also worth noting is that the 4-cycle latency edge from &#8220;Load&#8221; to &#8220;BitsRemaining&#8221; is a recurring delay that will occur on every refill, because the computation of the updated rBitPtr depends on the decode prior to the refill having been completed. Now this is not a full decoder, since I&#8217;m showing only the parts to do with the bitstream IO (presumably a real decoder also contains code to, you know, actually decode the bits and store the result somewhere), but it&#8217;s still somewhat disappointing. Note that the DetermineCount step is a placeholder: if the count is known in advance, for example because we&#8217;re reading a fixed-length field, you can ignore it completely. The single cycle depicted in the graph is OK for very simple cases; more complicated cases will often need multiple cycles here, for example because they perform a table lookup to figure out the count. Either way, even with our optimistic single-cycle single-operation DetermineCount step, the critical path through this graph is pretty long, and there&#8217;s very little latent parallelism in it.</p>
<p>Does variant 4 fare any better? The primitives look like this in pseudo-ASM:</p>
<pre>
refill4_lsb:
    rNext = load64LE(rBitPtr);
    rNextSh = lsl(rNext, rBitCount);
    rBitBuf = rBitBuf | rNextSh;

    // Most instruction sets don't have a subtract-from-immediate
    // but do have xor-by-immediate, so this is an advantageous
    // way to write 63 - rBitCount. (This works since we know that
    // rBitCount is in [0,63]).
    rBitsAdvance = rBitCount ^ 63;
    rBytesAdvance = lsr(rBitsAdvance, 3);
    rBitPtr = rBitPtr + rBytesAdvance;

    rBitCount = rBitCount | 56;

peekbits4_lsb(count):
    rBitMask = lsl(1, count);
    rBitMask = rBitMask - 1;
    rBits = rBitBuf &amp; rBitMask; // result

consume4_lsb(count):
    rBitBuf = lsr(rBitBuf, count);
    rBitCount = rBitCount - count;
</pre>
<p>the pseudo-code for our &#8220;refill, do two peek/consume cycles, then refill again&#8221; scenario looks like this:</p>
<pre>
    // Initial refill
    rNext = load64LE(rBitPtr);          // LoadNext 0
    rNextSh = lsl(rNext, rBitCount);    // NextShift 0
    rBitBuf = rBitBuf | rNextSh;        // BitInsert 0
    rBitsAdv = rBitCount ^ 63;          // AdvanceBits 0
    rBytesAdv = lsr(rBitsAdv, 3);       // AdvanceBytes 0
    rBitPtr = rBitPtr + rBytesAdv;      // AdvancePtr 0
    rBitCount = rBitCount | 56;         // RefillCount 0

    // First decode (peek count==19)
    rBits = rBitBuf &amp; 0x7ffff;          // BitsMasked 0
    rCount = determineCount(rBits);     // DetermineCount 0
    rBitBuf = lsr(rBitBuf, rCount);     // ConsumeShift 0
    rBitCount = rBitCount - rCount;     // ConsumeSub 0

    // Second decode
    rBits = rBitBuf &amp; 0x7ffff;          // BitsMasked 1
    rCount = determineCount(rBits);     // DetermineCount 1
    rBitBuf = lsr(rBitBuf, rCount);     // ConsumeShift 1
    rBitCount = rBitCount - rCount;     // ConsumeSub 1

    // Second refill
    rNext = load64LE(rBitPtr);          // LoadNext 1
    rNextSh = lsl(rNext, rBitCount);    // NextShift 1
    rBitBuf = rBitBuf | rNextSh;        // BitInsert 1
    rBitsAdv = rBitCount ^ 63;          // AdvanceBits 1
    rBytesAdv = lsr(rBitsAdv, 3);       // AdvanceBytes 1
    rBitPtr = rBitPtr + rBytesAdv;      // AdvancePtr 1
    rBitCount = rBitCount | 56;         // RefillCount 1
</pre>
<p>with this dependency graph:</p>
<p><img data-attachment-id="7104" data-permalink="https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/bitread_var4_lsb-gv/" data-orig-file="https://fgiesen.files.wordpress.com/2018/09/bitread_var4_lsb-gv.png" data-orig-size="661,747" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Dependency graph for bit reading, variant 4" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/09/bitread_var4_lsb-gv.png?w=265" data-large-file="https://fgiesen.files.wordpress.com/2018/09/bitread_var4_lsb-gv.png?w=497" src="https://fgiesen.files.wordpress.com/2018/09/bitread_var4_lsb-gv.png?w=497" alt="Dependency graph for bit reading, variant 4"   class="aligncenter size-full wp-image-7104" srcset="https://fgiesen.files.wordpress.com/2018/09/bitread_var4_lsb-gv.png?w=497 497w, https://fgiesen.files.wordpress.com/2018/09/bitread_var4_lsb-gv.png?w=133 133w, https://fgiesen.files.wordpress.com/2018/09/bitread_var4_lsb-gv.png?w=265 265w, https://fgiesen.files.wordpress.com/2018/09/bitread_var4_lsb-gv.png 661w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>That&#8217;s a bunch of differences, and you might want to look at variant 3 and 4 in different windows side-by-side. The variant 4 refill does take 3 extra instructions, but we can immediately see that we get more latent instruction-level parallelism (ILP) in return:</p>
<ol>
<li>The variant 4 refill splits into three dependency chains, not two.</li>
<li>The LoadNext for the second refill can start immediately after the AdvancePtr for the first refill, moving the load off the critical path for the second and subsequent iterations. Variant 3 has a 6-cycle latency from the determination of the final <code>rBitPos</code> in the first iteration to a refilled <code>rBitBuf</code>; in variant 4, that latency shrinks to 2 cycles (one shift and an OR). In other words, while the refill takes more instructions, most of them are off the critical path.</li>
<li>The consume step in variant 4 has two parallel computations; in variant 3, the <code>rBitPos</code> update is critical and feeds into the shift in the next &#8220;peek&#8221; operation. Variant 4 has a single shift (to consume bits) on the critical path to the next peek; as a result, the latency between two subsequent decodes is one cycle less in variant 4: 3 cycles instead of 4.</li>
</ol>
<p>In short, this version trades a slight increase in refill complexity for a noticeable latency reduction of several key steps, provided it&#8217;s running on a superscalar CPU. That&#8217;s definitely nice. On the other hand, the key decode steps are still very linear. We&#8217;re limited by the latency of a long chain of serial computations, which is a bad place to be: <em>if possible, it&#8217;s generally preferable to be limited by throughput (how many instructions we can execute), not latency (how fast we can complete them)</em>. Especially so if most of the latency in question comes from integer instructions that already have a single cycle of latency. Over the past 30 years, the number of executions units and instructions per cycle in mainstream CPU parts have steadily, if slowly, increased. But if we want to see any benefit from this, we need to write code that has a use for these extra execution resources.</p>
<h3>Multiple streams</h3>
<p>As is often the case, the best solution to this problem is the straightforward one: if decoding from a single bitstream is too serial, then why not decode from multiple bitstreams at once? And indeed, this is much better; there&#8217;s not much point to showing a graph here, since it&#8217;s literally just two copies of a single-stream graph next to each other. Even with a very serial decoder like variant 3 above, you can come a lot closer to filling up a wide out-of-order machine as long as you use enough streams. To a first-order approximation, using N streams will also give you N times the latent ILP&mdash;and given how serial a lot of the direct decoders are, this will translate into a substantial (usually not quite N-times, but still very noticeable) speed-up in the decoder on wide-enough processors. So what&#8217;s the catch? There are several:</p>
<ol>
<li>Using multiple streams is a change to the bitstream format, not just an implementation detail. In particular, in any long-term storage format, any change in the number of bitstreams is effectively a change in the protocol or file format.</li>
<li>You need to define how to turn the multiple streams into a single output bytestream. This can be simple concatenation along with a header, it can be some form of interleaving or a sophisticated framing format, but no matter what it ends up being, it&#8217;s an increase in complexity (and usually also in storage overhead) relative to producing a single bitstream that contains everything in the order it&#8217;s read.</li>
<li>For anything with short packets and low latency requirements (e.g. game packets or voice chat), you either have to interleave streams fairly finely-grained (increasing size overhead), or suffer latency increases.</li>
<li>Decoding from N streams in parallel increases the amount of internal state in the decoder. In the decoder variants shown above, a N-wide variant needs N copies of <code>rBitBuf</code>, <code>rBitPos</code>/<code>rBitCount</code> and <code>rBitPtr</code>, at the very least, plus several temporary registers. For N=2 this is usually not a big deal, but for large counts you will start to run out of registers at least on some targets. There&#8217;s relatively little work being done on any given individual data item; if values get spilled from registers, the resulting loads and stores tend to have a very noticeable cost and will easily negate the benefit from using more streams.</li>
</ol>
<p>In short, it&#8217;s not a panacea, but one of the usual engineering trade-offs. So how many streams should you use? It depends. At this point, for <em>anything</em> that is even remotely performance-sensitive, I would recommend trying at least N=2 streams. Even if your decoder has a lot of other stuff going on (computations with the decoded values etc.), bitstream decoding tends to be serial enough that there&#8217;s many wasted cycles otherwise, even on something relatively narrow like a dual-issue in-order machine. Having two streams adds a relatively small amount of overhead to the bitstream format (to signal the start of the data for stream 2 in every coding unit, or something equivalent), needs a modest amount of extra state for the second bit decoder, and tends to result in sizeable wins on pretty much any current CPU. </p>
<p>Using more than 2 streams can be a significant win in tight loops that do nothing but bitstream decoding, but is overkill in most other cases. Before you commit to a specific (high) number, you ideally want to try implementations on at least a few different target devices; a good number on one device may be past a big performance cliff on another, and having that kind of thing enshrined in a protocol or file format is unfortunate.</p>
<h3>Aside: SIMD? GPU?</h3>
<p>If you use many streams, can you use SIMD instructions, or offload work to a GPU? Yes, you can, but the trade-offs get a bit icky here.</p>
<p>Vectorizing the simple decoders outlined above directly is, generally speaking, not great. There&#8217;s not a lot of computation going on per iteration, and operations such as refills end up using gathers, which tend to have a high associated overhead. To hide this overhead, and the associated latencies, you generally still need to be running multiple instances of your SIMD decoder in parallel, so your total number of streams ends up being the number of SIMD lanes times two (or more, if you need more instances). Having a high number of streams may be OK if all your targets have good wide SIMD support, but can be a real pain if you need to decode on at least one that doesn&#8217;t.</p>
<p>The same thing goes for GPUs, but even more so. With single warps/wavefronts of usually 16-64 invocations, we&#8217;re talking <em>many</em> streams just to not be running a kernel at quarter utilization, and we generally need to dispatch multiple warps worth of work to hide memory access latency. Between these two factors, it&#8217;s easy to end up needing well over 100 parallel streams just to not be stalled most of the time. At that scale, the extra overhead for signaling individual stream boundaries is definitely not negligible anymore, and the magic numbers are different between different GPU vendors; striking a useful compromise between the needs of different GPUs while also retaining the ability to decode on a CPU if no suitable GPU is available starts to get quite tricky.</p>
<p>There are techniques to at least make the memory access patterns and interleaving overhead somewhat more palatable (I wrote about this <a href="https://arxiv.org/abs/1402.3392">elsewhere</a>), but this is an area of ongoing research, and so far there&#8217;s no silver bullet I could point at and just say &#8220;do this&#8221;. This is definitely a challenge going forward.</p>
<h3>Tricks with multiple streams</h3>
<p>If you&#8217;re using multiple streams, you need to decide how these multiple streams get assembled into the final output bitstream. If you don&#8217;t have any particular reason to go with a fine-grained interleaving, the easiest and most straightforward option is to concatenate the sub-streams, with a header telling you how long the individual pieces are, here pictured for 3 streams:</p>
<p><img data-attachment-id="7105" data-permalink="https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/streams/" data-orig-file="https://fgiesen.files.wordpress.com/2018/09/streams.png" data-orig-size="638,85" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Three sub-streams, linear layout" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/09/streams.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2018/09/streams.png?w=497" src="https://fgiesen.files.wordpress.com/2018/09/streams.png?w=497" alt="Three sub-streams, linear layout"   class="aligncenter size-full wp-image-7105" srcset="https://fgiesen.files.wordpress.com/2018/09/streams.png?w=497 497w, https://fgiesen.files.wordpress.com/2018/09/streams.png?w=150 150w, https://fgiesen.files.wordpress.com/2018/09/streams.png?w=300 300w, https://fgiesen.files.wordpress.com/2018/09/streams.png 638w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>Also pictured are the initial stream bit pointers before reading anything (pointers in a C-like or assembly-like setting; if you&#8217;re using something higher-level, probably indices into a byte slice). The beginning of stream 0 is implicit&mdash;right after the end of the header&mdash;and the end of the final stream is often supplied by an outer framing layer, but the initial positions of <code>bitptr1</code> and <code>bitptr2</code> need to be signaled in the bytestream somehow, usually by encoding the length of streams 0 and 1 in the header.</p>
<p>One thing I haven&#8217;t mentioned so far are bounds checks. Compressed data is normally untrusted since all the channels you might get that data from tend to be prone to either accidental (error during storage or in transit) or intentional (malicious attacker trying to craft harmful data) corruption, so careful input validation is not optional. What this usually boils down to in practice is that every load from the bitstream needs to be guarded by a range check that guarantees it&#8217;s in bounds. The overhead of this can be reduced in various ways. For example, one popular method is to unroll loops a few times and check at the top that there are enough bytes left for worst-case number of bytes consumed in the number of unrolled iterations, then only dropping to a careful loop that checks every single byte access at the very end of the stream. I&#8217;ve written about <a href="https://fgiesen.wordpress.com/2016/01/02/end-of-buffer-checks-in-decompressors/">another useful technique</a> before.</p>
<p>But why am I mentioning this here? Because it turns out that with multiple streams laid out sequentially, the overhead of bounds checking can be reduced. A direct range check for 3 streams that checks whether there are at least K bytes left would look like this:</p>
<pre>
// This needs to happen before we do any loads:
// If any of the streams are close to exhausted
// (fewer than K bytes left), drop to careful loop
if (bitend0 - bitptr0 &lt; K ||
    bitend1 - bitptr1 &lt; K ||
    bitend2 - bitptr2 &lt; K)
    break;
</pre>
<p>But when the three streams are sequential, we can use a simpler expression. First, we don&#8217;t actually need to worry about reading past the end of stream 0 or stream 1 as long as we still stay within the overall containing byte slice. And second, we can relax the check in the inner loop to use a much weaker test:</p>
<pre>
// Only check the last stream against the end; for
// other streams, simply test whether an the read
// pointer for an earlier stream is overtaking the
// read ponter for a later stream (which is never
// valid)
if (bitptr0 &gt; bitptr1 ||
    bitptr1 &gt; bitptr2 ||
    bitend2 - bitptr2 &lt; K)
    break;
</pre>
<p>The idea is that <code>bitptr1</code> starts out pointing at <code>bitend0</code>, and only keeps increasing from there. Therefore, if we ever have <code>bitptr0</code> &gt; <code>bitptr1</code>, we know for sure that something went wrong and we read past the end of stream 0. That will give us garbage data (which we need to handle anyway), but not read out of bounds, since the checks maintain the invariant that <code>bitptr0</code> &le; <code>bitptr1</code> &le; <code>bitptr2</code> &le; <code>bitend2 - K</code>. A later careful loop should use more precise checking, but this variant of the test is simpler and doesn&#8217;t require most of the <code>bitend</code> values to be reloaded in every iteration of our decoding loop.</p>
<p>Another interesting option is to reverse the order of some of the streams (which flips endianness as a side effect), and then glue pairs of forward and backward streams together, like shown here for streams 1 and 2:</p>
<p><img data-attachment-id="7106" data-permalink="https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/streams2/" data-orig-file="https://fgiesen.files.wordpress.com/2018/09/streams2.png" data-orig-size="653,85" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Three sub-streams with forward/backward pair" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/09/streams2.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2018/09/streams2.png?w=497" src="https://fgiesen.files.wordpress.com/2018/09/streams2.png?w=497" alt="Three sub-streams with forward/backward pair"   class="aligncenter size-full wp-image-7106" srcset="https://fgiesen.files.wordpress.com/2018/09/streams2.png?w=497 497w, https://fgiesen.files.wordpress.com/2018/09/streams2.png?w=150 150w, https://fgiesen.files.wordpress.com/2018/09/streams2.png?w=300 300w, https://fgiesen.files.wordpress.com/2018/09/streams2.png 653w" sizes="(max-width: 497px) 100vw, 497px" /></p>
<p>I admit this sounds odd, but this has a few interesting properties. One of them is that it shrinks the amount of header space somewhat: in the image, the initial stream pointer for stream 2 is the same as the end of the buffer, and if there were 4 streams, the initial read pointers for stream 2 and 3 would start out in the same location (but going opposite directions). In general, we only need to denote the boundaries between stream <em>pairs</em> instead of individual streams. Then we let the decoder run as before, checking that the read cursors for the forward/backward pair don&#8217;t cross. If everything went right, once we&#8217;ve consumed the entire input stream, the final read cursors in a forward/backward pair should end up right next to each other. It&#8217;s a bit strange in that we don&#8217;t know the size of either stream in advance, just their sum, but it works fine.</p>
<p>Another consequence is that there&#8217;s no need to keep track of an explicit end pointer in the inner decoder loop if the final stream is a backwards stream; the pointer-crossing check takes care of it. In our running example, we&#8217;re now down to</p>
<pre>
// Check for pointer crossing; if done right, we get end-of-buffer
// checks for free.
if (bitptr0 &gt; bitptr1 ||
    bitptr1 &gt; bitptr2)
    break;
</pre>
<p>In this version, <code>bitptr0</code> and <code>bitptr1</code> point at the next byte to be read in the forwards stream, whereas <code>bitptr2</code> is offset by -K to ensure we don&#8217;t overrun the buffer; this is just a constant offset however, which folds into the memory access on regular load instructions. It&#8217;s all a bit tricky, but it saves a couple instructions, makes the bitstream slightly smaller and reduces the number of live variables in a hot loop, with the savings usually being larger the cost of a single extra endian swap. With a two-stream layout, generating the second bitstream in reverse also happens to be convenient on the encoder side, because we can reserve memory for the expected (or budgeted) size of the combined bitstream without having to guess how many bytes end up in either half; it&#8217;s just a regular double-ended stack. Once encoding is done, the two parts can be compacted in-place by moving the second half downwards.</p>
<p>None of these properties are a big deal in and of themselves, but they make for a nice package, and a two-stream setup with a forwards/backwards pair is now our default layout for most parts in most parts of the Oodle bitstream (Oodle is a lossless data compression library I work on).</p>
<p>Between the various tricks outlined so far, the size overhead and the extra CPU cost for wrangling multiple streams can be squeezed down quite far. But we still have to deal with the increased number of live variables that multiple streams imply. It turns out that if we&#8217;re willing to tolerate a moderate increase in critical path latency, we can reduce the amount of state variables per bit reader, in some cases while simultaneously (slightly) reducing the number of instructions executed. The advantage here is that we can fit more streams into a given number of working registers than we could otherwise; if we can use enough streams that we&#8217;re primarily limited by execution <em>throughput</em> and not critical path latency, increasing said latency is OK, and reducing the overall number of instructions helps us increase the throughput even more. So how does that work?</p>
<h3>Bit reader variant 5: minimal state, throughput-optimized</h3>
<p>The bit reader variants I&#8217;ve shown so far generally split the bit buffer state across two variables: one containing the actual bits and another keeping track of how many bits are left in the buffer (or, equivalently, keeping track of the current read position within the buffer). But there&#8217;s a simple trick that allows us to reduce this to a single state variable: the bit shifts we use always shift in zeros. If we turn the MSB (for a LSB-first bit buffer) or the LSB (for a MSB-first bit buffer) into a marker bit that&#8217;s always set, we can use that marker to track how many bits we&#8217;ve consumed in total come the next refill. That allows us to get rid of the bit count and the instructions that manipulate it. That means one less variable in need of a register, and depending on which variant we&#8217;re comparing to, also fewer instructions executed per &#8220;consume&#8221;.</p>
<p>I&#8217;ll present this variant in the LSB-first version, and this time there&#8217;s an actual reason to (slightly) prefer LSB-first over MSB-first.</p>
<pre>
const uint8_t *bitptr; // Pointer to current byte
uint64_t bitbuf = 1ull &lt;&lt; 63; // Init to marker in MSB

void refill5_lsb() {
    assert(bitbuf != 0);

    // Count how many bits we consumed using a "leading zero
    // count" instruction. See notes below.
    int bits_consumed = CountLeadingZeros64(bitbuf);

    // Advance the pointer
    bitptr += bits_consumed &gt;&gt; 3;

    // Refill and put the marker in the MSB
    bitbuf = read64LE(bitptr) | (1ull &lt;&lt; 63);

    // Consume the bits in this byte that we've already used.
    bitbuf &gt;&gt;= bits_consumed &amp; 7;
}

uint64_t peekbits5_lsb(int count) {
    assert(count &gt;= 1 &amp;&amp; count &lt;= 56);
    // Just need to mask the low bits.
    return bitbuf &amp; ((1ull &lt;&lt; count) - 1);
}

void consume5_lsb(int count) {
    bitbuf &gt;&gt;= count;
}
</pre>
<p>This &#8220;count leading zeros&#8221; operation might seem strange and weird if you haven&#8217;t seen it before, but it happens to be something that&#8217;s useful in other contexts as well, and most current CPU architectures have fast instructions that do this! Other than the strangeness going on in the refill, where we first have to figure out the number of bits consumed from the old marker bit, then insert a new marker bit and do a final shift to consume the partial bits from the first byte, this is like a hybrid between variants 3 and 4 from last time.</p>
<p>The pseudo-assembly for our running &#8220;refill, two decodes, then another refill&#8221; scenario goes like this: (not writing out the marker constant explicitly here)</p>
<pre>
    // Initial refill
    rBitsConsumed = clz64(rBitBuf);     // CountLZ 0
    rBytesAdv = lsr(rBitsConsumed, 3);  // AdvanceBytes 0
    rBitPtr = rBitPtr + rBytesAdv;      // AdvancePtr 0
    rNext = load64LE(rBitPtr);          // LoadNext 0
    rMarked = rNext | MARKER;           // OrMarker 0
    rLeftover = rBitsConsumed &amp; 7;      // LeftoverBits 0
    rBitBuf = lsr(rMarked, rLeftover);  // ConsumeLeftover 0

    // First decode (peek count==19)
    rBits = rBitBuf &amp; 0x7ffff;          // BitsMasked 0
    rCount = determineCount(rBits);     // DetermineCount 0
    rBitBuf = lsr(rBitBuf, rCount);     // Consume 0

    // Second decode
    rBits = rBitBuf &amp; 0x7ffff;          // BitsMasked 1
    rCount = determineCount(rBits);     // DetermineCount 1
    rBitBuf = lsr(rBitBuf, rCount);     // Consume 1

    // Second refill
    rBitsConsumed = clz64(rBitBuf);     // CountLZ 1
    rBytesAdv = lsr(rBitsConsumed, 3);  // AdvanceBytes 1
    rBitPtr = rBitPtr + rBytesAdv;      // AdvancePtr 1
    rNext = load64LE(rBitPtr);          // LoadNext 1
    rMarked = rNext | MARKER;           // OrMarker 1
    rLeftover = rBitsConsumed &amp; 7;      // LeftoverBits 1
    rBitBuf = lsr(rMarked, rLeftover);  // ConsumeLeftover 1
</pre>
<p>The refill has 7 integer operations, the same as variant 4 (&#8220;looakhead&#8221;) above, and 3 more than variant 3 (&#8220;bit extract&#8221;), while the decode step takes 3 operations (including the <code>determineCount</code> step), one fewer than variants 3 (&#8220;bit extract&#8221;) and 4 (&#8220;lookahead&#8221;). The latter means that we equalize with the regular bit extract form in terms of instruction count when we perform at least 3 decodes per refill, and start to pull ahead if we manage more than 3. For completeness, here&#8217;s the dependency graph:</p>
<p><img data-attachment-id="7107" data-permalink="https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/bitread_var5_lsb-gv/" data-orig-file="https://fgiesen.files.wordpress.com/2018/09/bitread_var5_lsb-gv.png" data-orig-size="463,1172" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Dependency graph for bit reading, variant 5" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/09/bitread_var5_lsb-gv.png?w=119" data-large-file="https://fgiesen.files.wordpress.com/2018/09/bitread_var5_lsb-gv.png?w=405" src="https://fgiesen.files.wordpress.com/2018/09/bitread_var5_lsb-gv.png?w=497" alt="Dependency graph for bit reading, variant 5"   class="aligncenter size-full wp-image-7107" srcset="https://fgiesen.files.wordpress.com/2018/09/bitread_var5_lsb-gv.png 463w, https://fgiesen.files.wordpress.com/2018/09/bitread_var5_lsb-gv.png?w=59 59w, https://fgiesen.files.wordpress.com/2018/09/bitread_var5_lsb-gv.png?w=119 119w" sizes="(max-width: 463px) 100vw, 463px" /></p>
<p>Easily the longest critical path of the variants we&#8217;ve seen so far, and very serial indeed. It doesn&#8217;t help that not only do we not know the load address early, we also have several more steps in the refill compared to the basic variant 3. But having the entire &#8220;hot&#8221; bit buffer state concentrated in a single register (<code>rBitBuf</code>) during the decodes means that we can afford <em>many</em> streams at once, and with enough streams that extra latency can be hidden.</p>
<p>This one definitely needs to be deployed carefully, but it&#8217;s a powerful tool when used in the right place. Several of the fastest (and hottest) decoder loops in Oodle use it.</p>
<p>Note that with this variation, there&#8217;s a reason to stick with the LSB-first version: the equivalent MSB-first version needs a way to count the number of <em>trailing</em> zero bits, which is a much less common instruction, although it can be synthesized from a leading zero count and standard arithmetic/logical operations at <a href="https://fgiesen.wordpress.com/2013/10/18/bit-scanning-equivalencies/">acceptable extra cost</a>. Which brings me to my final topic for this post.</p>
<h3>MSB-first vs. LSB-first: the final showdown</h3>
<p>Throughout this 3-parter series, I&#8217;ve been continually emphasizing that there&#8217;s no <em>major</em> reason to prefer MSB-first or LSB-first for bit IO. Both are broadly equivalent and have efficient algorithms. But having now belabored that point sufficiently, if we can make both of them work, which one should we choose?</p>
<p>There are definitely differences that push you into one direction or another, depending on your intended use case. Here are some you might want to consider, in no particular order:</p>
<ul>
<li>As we saw in <a href="https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/">part 2</a>, the natural MSB-first <code>peekbits</code> and <code>getbits</code> implementations run into trouble (of the undefined-behavior and hardware-actually-behaving-in-surprising-ways kind) when <code>count == 0</code>, whereas with the natural LSB-first implementation, this case is unproblematic. If you need to support counts of 0 (usefol for e.g. variable-length codes), LSB-first tends to be slightly more convenient. Alternatives for MSB-first are a rotate-based implementation (which has no problems with 0 count) or using an extra shift, turning <code>x &gt;&gt; (64 - count)</code> into <code>(x &gt;&gt; 1) &gt;&gt; (63 - count)</code>.</li>
<li>MSB-first coding tends to have a big edge for universal variable-length codes. Unary codes can be decoded quickly via the aforementioned &#8220;count leading zero&#8221; instructions; gamma codes and the closely related Exp-Golomb codes also admit <a href="https://fgiesen.wordpress.com/2011/01/19/a-small-note-on-the-elias-gamma-cod/">direct decoding</a> in a fairly slick way; and the same goes for Golomb-Rice codes and a few others. If you&#8217;re considering universal codes, MSB-first is definitely handier.</li>
<li>At the other extreme, LSB-first coding often ends up slightly cheaper for the table-based decoders commonly used when a code isn&#8217;t fixed as part of the format; Huffman decoders for example.</li>
<li>MSB-first meshes somewhat more naturally with big-endian byte order, and LSB-first with little-endian. If you&#8217;re deeply committed to either side in this particular holy war, this might drive you one way or the other.</li>
</ul>
<p>Charles and me both tend to default to MSB-first but will switch to LSB-first where it&#8217;s a win on multiple target architectures (or on a single important target).</p>
<h3>Conclusion</h3>
<p>That&#8217;s it for both this post and this mini-series; apologies for the long delay, caused by first a surprise deadline that got dropped in my lap right as I was writing the series originally, and then exacerbated by a combination of technical difficulties (alas, still ongoing) and me having gotten &#8220;out of the groove&#8221; in the intervening time.</p>
<p>This post ended up longer than my usual, and skips around topics a bit more than I&#8217;d like, but I really didn&#8217;t want to make this series a four-parter; I still have a few notes here and there, but I don&#8217;t want to drag this topic out much longer, not on this general level anyway. Instead, my plan is to write about some more down-to-earth case studies soon, so I can be less hand-wavy, and maybe even do some actual assembly-level analysis for an actual real-world CPU instead of an abstract idealized machine. We&#8217;ll see.</p>
<p>Until then!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/09/bitread_var3_lsb-gv.png" medium="image">
			<media:title type="html">Dependency graph for bit reading, variant 3</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/09/bitread_var4_lsb-gv.png" medium="image">
			<media:title type="html">Dependency graph for bit reading, variant 4</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/09/streams.png" medium="image">
			<media:title type="html">Three sub-streams, linear layout</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/09/streams2.png" medium="image">
			<media:title type="html">Three sub-streams with forward/backward pair</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/09/bitread_var5_lsb-gv.png" medium="image">
			<media:title type="html">Dependency graph for bit reading, variant 5</media:title>
		</media:content>
	</item>
		<item>
		<title>A whirlwind introduction to dataflow graphs</title>
		<link>https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/</link>
					<comments>https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Mon, 05 Mar 2018 10:39:19 +0000</pubDate>
				<category><![CDATA[Computer Architecture]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7084</guid>

					<description><![CDATA[While in the middle of writing &#8220;Reading bits in far too many ways, part 3&#8221;, I realized that I had written a lot of background material that had absolutely nothing to do with bit I/O and really was worth putting in its own post. This is that post. The problem I&#8217;m concerned with is fairly [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>While in the middle of writing &#8220;Reading bits in far too many ways, part 3&#8221;, I realized that I had written a lot of background material that had absolutely nothing to do with bit I/O and really was worth putting in its own post. This is that post.</p>
<p>The problem I&#8217;m concerned with is fairly easy to state: say we have some piece of C++ code that we&#8217;re trying to understand (and perhaps improve) the performance of. A good first step is to profile it, which will give us some hints <em>which</em> parts are slow, but not necessarily <em>why</em>. On a fundamental level, any kind of profiling (or other measurement) is <em>descriptive</em>, not <em>predictive</em>: it can tell you how an existing system is behaving, but if you&#8217;re designing something that&#8217;s more than a few afternoons worth of work, you probably don&#8217;t have the time or resources to implement 5 or 6 completely different design alternatives, pick whichever one happens to work best, and throw the rest away. You should be able to make informed decisions up front from an algorithm sketch without having to actually write a fleshed-out implementation.</p>
<p>One thing I want to emphasize particularly here is that experiments coupled with before/after measurements are no adequate substitute for a useful performance model. These kinds of measurements can tell you how much you&#8217;ve improved, but not if you are where you should be: if I tell you that by tweaking some config files, I managed to double the number of requests served per second by the web server, that sounds great. It sounds less good if I give you the additional piece of information that with this fix deployed, we&#8217;re now at a whopping 1.5 requests per second; having an absolute scale of reference matters!</p>
<p>This goes especially for microbenchmarks. With microbenchmarks, like a trial lawyer during cross-examination, <em>you should never ask a question you don&#8217;t know the answer to</em> (or at least have a pretty good idea of what it is). Real-world systems are generally too complex and intertwined to understand from surface measurements alone. If you have no idea how a system works at all, you don&#8217;t know what the right questions are, nor how to ask them, and any answers you get will be opaque at best, if not outright garbage. Microbenchmarks are a useful tool to confirm that an existing model is a good approximation to reality, but not very helpful in building these models to begin with.</p>
<h3>Machine models</h3>
<p>So, if we want to go deeper than just squinting at C/C++ code and doing some hand-waving, we need to start looking at a somewhat lower abstraction level and define a machine model that is more sophisticated than &#8220;statements execute one by one&#8221;. If you&#8217;re only interested in a single specific processor, one option is to use whatever documentation and tools you can find for the chip in question and analyze your code in detail for that specific machine. And if you&#8217;re willing to go all-out on microarchitectural tweaking, that&#8217;s indeed the way to go, but it&#8217;s a giant step from looking at C++ code, and complete overkill in most cases.</p>
<p>Instead, what I&#8217;m going to do is use a simplified machine model that allows us to make quantitative predictions about the behavior of straightforward compute-bound loops, which is simple to describe but still gives us a lot of useful groundwork for more complex scenarios. Here&#8217;s what I&#8217;ll use:</p>
<ul>
<li>We have an unlimited set of 64-bit integer general-purpose registers, which I&#8217;ll refer to by names like <code>rSomething</code>. Any &#8220;identifiers&#8221; that aren&#8217;t prefixed with a lowercase r are either symbolic constants or things like labels.</li>
<li>We have the usual 64-bit integer arithmetic and logic operations. All operations can either be performed between two registers or a register and an immediate constant, and the result is placed in another register. All arithmetic uses two&#8217;s complement. For simplicity, all 64-bit values are permitted as immediate constants.</li>
<li>There&#8217;s a flat, byte-granular 64-bit address space, and pointers are just represented as integers.</li>
<li>All memory accesses require explicit load and store operations. Memory accesses are either 8, 16, 32, or 64 bits in size and can use (for my convenience) both little-endian or big-endian byte ordering, when requested. One of these is the default, but both are the same cost. Narrow stores store the least significant bits of the register in question; narrow loads zero-extend to 64 bits. Loads and stores have a few common addressing modes (that I&#8217;ll introduce as I use them). Unaligned loads and stores are supported.</li>
<li>There&#8217;s unconditional branches, which just jump to a given location, and conditional branches, which compare a register to either another register or an immediate constant, and branch to a given destination if the condition is true.</li>
</ul>
<p>Code will be written in a pseudo-C form, at most one instruction per line. Here&#8217;s a brief example showing what kind of thing I have in mind:</p>
<pre>
loop:                            // label
  rFoo = rBar | 1;               // bitwise logical OR
  rFoo = lsl(rFoo, 3);           // logical shift left
  rBar = asr(rBar, rBaz);        // arithmetic shift right
  rMem = load64LE(rBase + rFoo); // little-endian load
  store16BE(rDest + 3, rMem);    // big-endian store
  rCount = rCount - 1;           // basic arithmetic
  if rCount != 0 goto loop;      // branch
</pre>
<p>Shifts use explicit mnemonics because there&#8217;s different types of right shifts and at this level of abstraction, registers are generally treated as untyped bags of bits. I&#8217;ll introduce other operations and addressing modes as we get to them. What we&#8217;ve seen so far is quite close to classic RISC instruction sets, although I&#8217;ll allow a larger set of addressing modes than some of the more minimalist designs, and require support for unaligned access on all loads and stores. It&#8217;s also close in spirit to an IR (Intermediate Representation) you&#8217;d expect to see early in the backend of a modern compiler: somewhat lower-level than LLVM IR, and comparable to early-stage LLVM Machine IR or GCC RTL.</p>
<p>This model requires us to make the distinction between values kept in registers and memory accesses explicit, and flattens down control flow to basic blocks connected by branches. But it&#8217;s still relatively easy to look at a small snippet of C++ and e.g. figure out how many arithmetic instructions it boils down to: just count the number of operations.</p>
<p>As a next step, we could now specify a virtual processor to go with our instruction set, but I don&#8217;t want to really get into that level of detail; instead of specifying the actual processor, I&#8217;ll work the same way actual architectures do: we require that the end result (eventual register and memory contents in our model) of running a program must be as if we had executed the instructions sequentially one by one (as-if rule). Beyond that, an aggressive implementation is free to cut corners as much as it wants provided it doesn&#8217;t get caught. We&#8217;ll assume we&#8217;re in an environment&mdash;the combination of compilers/tools and the processor itself&mdash;that uses pipelining and tries to extract instruction-level parallelism to achieve higher performance, in particular:</p>
<ul>
<li>Instructions can launch independent from each other, and take some number of clock cycles to complete. For an instruction to start executing, all the operands it depends on need to have been computed. As long as the dependencies are respected, all reorderings are valid.</li>
<li>There is <em>some</em> limit W (&#8220;width&#8221;) on how many new instructions we can start per clock cycle. In-flight instructions don&#8217;t interfere with each other; as long as we have enough independent work, we can start W new instructions every cycle. We&#8217;re going to treat W as variable.</li>
<li>Memory operations have a latency of 4 cycles, meaning that the result of a load is available 4 cycles after the load issued, and a load reading the bytes written by a prior store can issue 4 cycles after the store. That&#8217;s a fairly typical latency for a load that hits in the L1 cache, in case you were wondering.</li>
<li>Branches (conditional or not) count as a single instruction, but their latency is variable. Unconditional branches or easily predicted branches such as the loop counter in along-running loop have an effective latency of 0 cycles, meaning the instructions being branched to can issue at the same time as the branch itself. Unpredictable branches have a nonzero cost that depends on how unpredictable they are&mdash;I won&#8217;t even try to be more precise here.</li>
<li>Every other instruction has a latency of 1 clock cycle, meaning the result is available in the next cycle.</li>
</ul>
<p>This model can be understood as approximating either a <a href="https://en.wikipedia.org/wiki/Dataflow_architecture">dataflow architecture</a>, an out-of-order machine with a very large issue window (and infinitely fast front-end), or a statically scheduled in-order machine running code compiled with a Sufficiently Smart Scheduler. (The kind that actually exists; e.g. a compiler implementing <a href="https://en.wikipedia.org/wiki/Software_pipelining">software pipelining</a>).</p>
<p>Furthermore, I&#8217;m assuming that while there is explicit control flow (unlike a pure dataflow machine), there is a branch prediction mechanism in place that allows the machine to guess the control flow path taken arbitrarily far in advance. When these guesses are correct, the branches are effectively free other than still taking an instruction slot, during which time the machine checks whether its prediction was correct. When the guess was incorrect, the machine reverts all computations that were down the incorrectly guessed path, and takes some number of clock cycles to recover. If this idea of branch prediction is new to you, I&#8217;ll refer you to <a href="http://danluu.com/branch-prediction/">Dan Luu&#8217;s excellent article</a> on the subject, which explains both how and why computers would be doing this.</p>
<p>The end result of these model assumptions is that while control flow exists, it&#8217;s on the sidelines: its only observable effect is that it sometimes causes us to throw away a bunch of work and take a brief pause to recover when we guessed wrong. Dataflow, on the other hand&mdash;the dependencies between instructions, and how long it takes for these dependencies to be satisfied&mdash;is front and center.</p>
<h3>Dataflow graphs</h3>
<p>Why this emphasis? Because dataflow and data dependencies is because they can be viewed as the fundamental expression of the structure of a particular computation, whether it&#8217;s done on a small sequential machine, a larger superscalar out-of-order CPU, a GPU, or in hardware (be it a hand-soldered digital circuit, a FPGA, or an ASIC). Dataflow and keeping track of the shape of data dependencies is an organizing principle of both the machines themselves and the compilers that target them.</p>
<p>And these dependencies are naturally expressed in graph form, with individual operations being the nodes and data dependencies denoted by directed edges. In this post, I&#8217;ll have dependent operations point towards the operations they depend on, with the directed edges labeled with their latency. To reduce clutter, I&#8217;ll only write latency numbers when they&#8217;re not 1.</p>
<p>With all that covered, and to see what the point of this all is, let&#8217;s start with a simple, short toy program that just sums the 64-bit integers in some array delineated by two pointers stored in <code>rCurPtr</code> (which starts pointing to the first element) and <code>rEndPtr</code> (which points to one past the last element), idiomatic C++ iterator-style.</p>
<pre>
loop:
  rCurInt = load64(rCurPtr);        // Load
  rSum = rSum + rCurInt;            // Sum
  rCurPtr = rCurPtr + 8;            // Advance
  if rCurPtr != rEndPtr goto loop;  // Done?
</pre>
<p>We load a 64-bit integer from the current pointer, add it to our current running total in register <code>rSum</code>, increment the pointer by 8 bytes (since we grabbed a 64-bit integer), and then loop until we&#8217;re done. Now let&#8217;s say we run this program for a short 6 iterations and draw the corresponding dataflow graph (click to see full-size version):</p>
<p><a href="https://fgiesen.files.wordpress.com/2018/03/array_sum-gv1.png"><img data-attachment-id="7086" data-permalink="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/array_sum-gv/" data-orig-file="https://fgiesen.files.wordpress.com/2018/03/array_sum-gv1.png" data-orig-size="832,481" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Dataflow graph for basic array sum" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/03/array_sum-gv1.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2018/03/array_sum-gv1.png?w=497" src="https://fgiesen.files.wordpress.com/2018/03/array_sum-gv1.png?w=497" alt="Dataflow graph for basic array sum"   class="alignnone size-full wp-image-7086" srcset="https://fgiesen.files.wordpress.com/2018/03/array_sum-gv1.png?w=497 497w, https://fgiesen.files.wordpress.com/2018/03/array_sum-gv1.png?w=150 150w, https://fgiesen.files.wordpress.com/2018/03/array_sum-gv1.png?w=300 300w, https://fgiesen.files.wordpress.com/2018/03/array_sum-gv1.png?w=768 768w, https://fgiesen.files.wordpress.com/2018/03/array_sum-gv1.png 832w" sizes="(max-width: 497px) 100vw, 497px" /></a></p>
<p>Note I group nodes into ranks by which cycle they can execute in, at the earliest, assuming we can issue as many instructions in parallel as we want, purely constrained by the data dependencies. The &#8220;Load&#8221; and &#8220;Advance&#8221; from the first iteration can execute immediately; the &#8220;Done?&#8221; check from the first iteration looks at the updated <code>rCurPtr</code>, which is only known one cycle later; and &#8220;Sum&#8221; from the first iteration needs to wait for the load to finish, which means it can only start a full 4 cycles later.</p>
<p>As we can see, during the first four cycles, all we do is keep issuing more loads and advancing the pointer. It takes until cycle 4 for the results of the first load to become available, so we can actually do some summing. After that, one more load completes every cycle, allowing us to add one more integer to the running sum in turn. If we let this process continue for longer, all the middle iterations would look the way cycles 4 and 5 do: in our state state, we&#8217;re issuing a copy of all four instructions in the loop every cycle, but from different iterations.</p>
<p>There&#8217;s a few conclusions we can draw from this: first, we can see that this four-instruction loop achieves a steady-state throughput of one integer added to the sum in every clock cycle. We take a few cycles to get into the steady state, and then a few more cycles at the end to drain out the pipeline, but if we start in cycle 0 and keep running N iterations, then the final sum will be completed by cycle N+4. Second, even though I said that our model has infinite lookahead and is free to issue as many instructions per cycle as it wants, we &#8220;only&#8221; end up using at most 4 instructions per cycle. The limiter here ends up being the address increment (&#8220;Advance&#8221;); we increment the pointer after every load, per our cost model this increment takes a cycle of latency, and therefore the load in the next iteration of the loop (which wants to use the updated pointer) can start in the next cycle at the earliest.</p>
<p>This is a crucial point: the longest-latency instruction in this loop is definitely the load, at 4 cycles. But that&#8217;s not a limiting factor; we can schedule around the load and do the summing later. The actual problem here is with the pointer advance; every single instruction that comes after it in program order depends on it either directly or indirectly, and therefore, its 1 cycle of latency determines when the next loop iteration can start. We say it&#8217;s on the <em>critical path</em>. In loops specifically, we generally distinguish between intra-iteration dependencies (between instructions within the same iteration, say &#8220;Sum 0&#8221; depending on &#8220;Load 0&#8221;) and inter-iteration or loop-carried dependencies (say &#8220;Sum 1&#8221; depending on &#8220;Sum 0&#8221;, or &#8220;Load 1&#8221; depending on &#8220;Advance 0&#8221;). Intra-iteration dependencies may end up delaying instructions within that iteration quite a lot, but it&#8217;s inter-iteration dependencies that determine how soon we can start working on the next iteration of the loop, which is usually more important because it tends to open up more independent instructions to work on.</p>
<p>The good news is that W=4 is actually a fairly typical number of instructions decoded/retired per cycle in current (as of this writing in early 2018) out-of-order designs, and the instruction mixture here (1 load, 1 branch, 2 arithmetic instructions) is also one that is quite likely to be able to issue in parallel on a realistic 4-wide decode/retire design. While many machines can issue a lot more instructions than that in short bursts, a steady state of 4 instructions per cycle is definitely good. So even though we&#8217;re not making much of the infinite parallel computing power of our theoretical machine, in practical terms, we&#8217;re doing OK, although on real machines we might want to apply some more transforms to the loop; see below.</p>
<p>Because these real-world machines can&#8217;t start an arbitrary number of instructions at the same time, we have another concern: throughput. Say we&#8217;re running the same loop on a processor that has W=2, i.e. only two instructions can start every cycle. Because our loop has 4 instructions, that means that we can&#8217;t possibly start a new loop iteration more often than once every two clock cycles, and the limiter aren&#8217;t the data dependencies, but the number of instructions our imaginary processor can execute in a clock cycle; we&#8217;re throughput-bound. We would also be throughput-bound on a machine with W=3, with a steady state of 3 new instructions issued per clock cycle, where we can start working on a new iteration every 4/3&asymp;1.33 cycles.</p>
<h3>A different example</h3>
<p>For the next example, we&#8217;re going to look atÂ what&#8217;s turned into everyone&#8217;s favorite punching-bag of a data structure, the linked list. Let&#8217;s do the exact same task as before, only this time, the integers are stored in a singly-linked list instead of laid out as an array. We store first a 64-bit integer and then a 64-bit pointer to the next element, with the end of the list denoted by a special value stored in <code>rEndPtr</code> as before. We also assume the list has at least 1 element. The corresponding program looks like this:</p>
<pre>
loop:
  rCurInt = load64(rCurPtr);        // LoadInt
  rSum = rSum + rCurInt;            // Sum
  rCurPtr = load64(rCurPtr + 8);    // LoadNext
  if rCurPtr != rEndPtr goto loop;  // Done?
</pre>
<p>Very similar to before, only this time, instead of incrementing the pointer, we do another load to grab the &#8220;next&#8221; pointer. And here&#8217;s what happens to the dataflow graph if we make this one-line change:</p>
<p><a href="https://fgiesen.files.wordpress.com/2018/03/linked_list_sum-gv.png"><img data-attachment-id="7087" data-permalink="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/linked_list_sum-gv/" data-orig-file="https://fgiesen.files.wordpress.com/2018/03/linked_list_sum-gv.png" data-orig-size="607,629" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Dataflow graph for linked list sum" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/03/linked_list_sum-gv.png?w=290" data-large-file="https://fgiesen.files.wordpress.com/2018/03/linked_list_sum-gv.png?w=497" src="https://fgiesen.files.wordpress.com/2018/03/linked_list_sum-gv.png?w=497" alt="Dataflow graph for linked list sum"   class="alignnone size-full wp-image-7087" srcset="https://fgiesen.files.wordpress.com/2018/03/linked_list_sum-gv.png?w=497 497w, https://fgiesen.files.wordpress.com/2018/03/linked_list_sum-gv.png?w=145 145w, https://fgiesen.files.wordpress.com/2018/03/linked_list_sum-gv.png?w=290 290w, https://fgiesen.files.wordpress.com/2018/03/linked_list_sum-gv.png 607w" sizes="(max-width: 497px) 100vw, 497px" /></a></p>
<p>Switching from a contiguous array to a linked list means that we have to wait for the load to finish before we can start the next iteration. Because loads have a latency of 4 cycles in our model, that means we can&#8217;t start a new iteration any more often than once every 4 cycles. With our 4-instruction loop, we don&#8217;t even need any instruction-level parallelism to reach that target; we might as well just execute one instruction per cycle and still hit the same overall throughput.</p>
<p>Now, this example, with its short 4-instruction loop, is fairly extreme; if our loop had say a total of 12 instructions that worked out nicely, the same figure might well end up averaging 3 instructions per clock cycle, and that&#8217;s not so bad. But the underlying problem here is a nasty one: because our longest-latency instruction is on the critical path between iterations, it ends up determining the overall loop throughput.</p>
<p>In our model, we&#8217;re still primarily focused on compute-bound code, and memory access is very simple: there&#8217;s no memory hierarchy with different cache levels, all memory accesses take the same time. If we instead had a more realistic model, we would also have to deal with the fact that some memory accesses take a whole lot longer than 4 cycles to complete. For example, suppose we have three cache levels and, at the bottom, DRAM. Sticking with the powers-of-4 theme, let&#8217;s say that a L1 cache hit takes 4 cycles (i.e. our current memory access latency), a L2 hit takes 16 cycles, a L3 hit takes 64 cycles, and an actual memory access takes 256 cycles&mdash;for what it&#8217;s worth, all these numbers are roughly in the right ballpark for high-frequency desktop CPUs under medium memory subsystem load as of this writing.</p>
<p>Finding work to keep the machine otherwise occupied for the next 4 cycles (L1 hit) is usually not that big a deal, unless we have a very short loop with unfavorable dependency structure, as in the above example. Fully covering the 16 cycles for a L1 miss but L2 hit is a bit trickier and requires a larger out-of-order window, but current out-of-order CPUs have those, and as long as there&#8217;s enough other independent work and not too many hard-to-predict branches along the way, things will work out okay. With a L3 cache hit, we&#8217;ll generally be hard-pressed to find enough independent work to keep the core usefully busy during the wait for the result, and if we actually miss all the way to DRAM, then in our current model, the machine is all but guaranteed to stall; that is, to have many cycles with no instructions executed at all, just like the gaps in the diagram above.</p>
<p>Because linked lists have this nasty habit of putting memory access latencies on the critical path, they have a reputation of being slow &#8220;because they&#8217;re bad for the cache&#8221;. Now while it&#8217;s definitely true that most CPUs with a cache would much rather have you iterate sequentially over an array, we have to be careful how we think about it. To elaborate, suppose we have yet another sum kernel, this time processing an array of pointers to integers, to compute the sum of the pointed-to values.</p>
<pre>
loop:
  rCurIntPtr = load64(rCurPtr);      // LoadPtr
  rCurInt = load64(rCurIntPtr);      // LoadInt
  rSum = rSum + rCurInt;             // Sum
  rCurPtr = rCurPtr + 8;             // Advance
  if rCurPtr != rEndPtr goto loop;   // Done?
</pre>
<p>And this time, I&#8217;ll prune the dataflow graph to show only the current iteration and its direct dependency relationships with earlier and later iterations, because otherwise these more complicated graphs will get cluttered and unreadable quickly:</p>
<p><a href="https://fgiesen.files.wordpress.com/2018/03/array_indir_sum-gv.png"><img data-attachment-id="7088" data-permalink="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/array_indir_sum-gv/" data-orig-file="https://fgiesen.files.wordpress.com/2018/03/array_indir_sum-gv.png" data-orig-size="499,531" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Dataflow graph for indirect array sum" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/03/array_indir_sum-gv.png?w=282" data-large-file="https://fgiesen.files.wordpress.com/2018/03/array_indir_sum-gv.png?w=497" src="https://fgiesen.files.wordpress.com/2018/03/array_indir_sum-gv.png?w=497" alt="Dataflow graph for indirect array sum"   class="alignnone size-full wp-image-7088" srcset="https://fgiesen.files.wordpress.com/2018/03/array_indir_sum-gv.png?w=497 497w, https://fgiesen.files.wordpress.com/2018/03/array_indir_sum-gv.png?w=141 141w, https://fgiesen.files.wordpress.com/2018/03/array_indir_sum-gv.png?w=282 282w, https://fgiesen.files.wordpress.com/2018/03/array_indir_sum-gv.png 499w" sizes="(max-width: 497px) 100vw, 497px" /></a></p>
<p>A quick look over that graph shows us that copies of the same instruction from different iterations are all spaced 1 cycle apart; this means that in the steady state, we will again execute one iteration of the loop per clock cycle, this time issuing 5 instructions instead of 4 (because there are 5 instructions in the loop). Just like in the linked list case, the pointer indirection here allows us to jump all over memory (potentially incurring cache misses along the way) if we want to, but there&#8217;s a crucial difference: in this setup, we can keep setting up future iterations of the loop and get more loads started while we&#8217;re waiting for the first memory access to complete.</p>
<p>To explain what I mean, let&#8217;s pretend that every single of the &#8220;LoadInt&#8221;s misses the L1 cache, but hits in the L2 cache, so its actual latency is 16 cycles, not 4. But a latency of 16 cycles just means that it takes 16 cycles between issuing the load and getting the result; we can keep issuing other loads for the entire time. So the only thing that ends up happening is that the &#8220;Sum k&#8221; in the graph above happens 12 cycles later. We still start two new loads every clock cycle in the steady state; some of them end up taking longer, but that does not keep us from starting work on a new iteration of the loop in every cycle.</p>
<p>Both the linked list and the indirect-sum examples have the opportunity to skip all over memory if they want to; but in the linked-list case, we need to wait for the result of the previous load until we can get started on the next one, whereas in the indirect-sum case, we get to overlap the wait times from the different iterations nicely. As a result, in the indirect-sum case, the extra latency towards reaching the final sum is essentially determined by the worst single iteration we had, whereas in the linked-list case, <em>every single cache miss</em> makes our final result later (and costs us throughput).</p>
<p>The fundamental issue isn&#8217;t that the linked-list traversal might end up missing the cache a lot; while this isn&#8217;t ideal (and might cost us in other ways), the far more serious issue is that <em>any such cache miss prevents us from making progress elsewhere</em>. Having a lot of cache misses isn&#8217;t necessarily a problem if we get to overlap them; having long stretches of time were we can&#8217;t do anything else, because everything else we could do depends on that one cache-missing load, is.</p>
<p>In fact, when we hit this kind of problem, our best bet is to just switch to doing something else entirely. This is what CPUs with simultaneous multithreading/hardware threads (&#8220;hyperthreads&#8221;) and essentially all GPUs do: build the machine so that it can process instructions from multiple instruction streams (threads), and then if one of the threads isn&#8217;t really making progress right now because it&#8217;s waiting for something, just work on something else for a while. If we have enough threads, then we can hopefully fill those gaps and always have something useful to work on. This trade-off is worthwhile if we have many threads and aren&#8217;t really worried about the extra latency caused by time-slicing, which is why this approach is especially popular in throughput-centric architectures that don&#8217;t worry about slight latency increases.</p>
<h3>Unrolling</h3>
<p>But let&#8217;s get back to our original integer sum code for a second:</p>
<pre>
loop:
  rCurInt = load64(rCurPtr);        // Load
  rSum = rSum + rCurInt;            // Sum
  rCurPtr = rCurPtr + 8;            // Advance
  if rCurPtr != rEndPtr goto loop;  // Done?
</pre>
<p>We have a kernel with four instructions here. Out of these four, two (&#8220;Load&#8221; and &#8220;Sum&#8221;) do the actual work we want done, whereas &#8220;Advance&#8221; and &#8220;Done?&#8221; just implement the loop itself and are essentially overhead. This type of loop is a prime target for <a href="https://en.wikipedia.org/wiki/Loop_unrolling">unrolling</a>, where we collapse two or more iterations of the loop into one to decrease the overhead fraction. Let&#8217;s not worry about the setup or what to do when the number of elements in the array is odd right now, and only focus on the &#8220;meat&#8221; of the loop. Then a 2&times; unrolled version might look like this:</p>
<pre>
loop:
  rCurInt = load64(rCurPtr);        // LoadEven
  rSum = rSum + rCurInt;            // SumEven
  rCurInt = load64(rCurPtr + 8);    // LoadOdd
  rSum = rSum + rCurInt;            // SumOdd
  rCurPtr = rCurPtr + 16;           // Advance
  if rCurPtr != rEndPtr goto loop;  // Done?
</pre>
<p>which has this dataflow graph:</p>
<p><a href="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum1-gv.png"><img data-attachment-id="7089" data-permalink="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/unroll2x_array_sum1-gv/" data-orig-file="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum1-gv.png" data-orig-size="793,451" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Dataflow graph for unrolled array sum, first attempt" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum1-gv.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum1-gv.png?w=497" src="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum1-gv.png?w=497" alt="Dataflow graph for unrolled array sum, first attempt"   class="alignnone size-full wp-image-7089" srcset="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum1-gv.png?w=497 497w, https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum1-gv.png?w=150 150w, https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum1-gv.png?w=300 300w, https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum1-gv.png?w=768 768w, https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum1-gv.png 793w" sizes="(max-width: 497px) 100vw, 497px" /></a></p>
<p>Note that even though I&#8217;m writing to <code>rCurInt</code> twice in an iteration, which constitutes a write-after-write (WAW) or &#8220;output dependency&#8221;, there&#8217;s no actual dataflow between the loads and sums for the first and second version of <code>rCurInt</code>, so the loads can issue in parallel just fine.</p>
<p>This isn&#8217;t bad: we now have two loads every iteration and spend 6N instructions to sum 2N integers, meaning we take 3 instructions per integer summed, whereas our original kernel took 4. That&#8217;s an improvement, and (among other things) means that while our original integer-summing loop needed a machine that sustained 4 instructions per clock cycle to hit full throughput, we can now hit the same throuhgput on a smaller machine that only does 3 instructions per clock. This is definitely progress.</p>
<p>However, there&#8217;s a problem: if we look at the diagram, we can see that we can indeed start a new pair of loads every clock cycle, but there&#8217;s a problem with the summing: we have two dependent adds in our loop, and as we can see from the relationship between &#8220;SumEven k&#8221; and &#8220;SumEven k+1&#8221;, the actual summing part of the computation still takes 2 cycles per iteration. On our idealized dataflow machine with infinite lookahead, that just means that all the loads will get front-loaded, and then the adds computing the final sum proceed at their own pace; the result will eventually be available, but it will still take a bit more than 2N cycles, no faster than we were in the original version of the loop. On a more realistic machine (which can only look ahead by a limited number of instructions), we would eventually stop being able to start new loop iterations until some of the old loop iterations have completed. No matter how we slice it, we&#8217;ve gone from adding one integer to the sum per cycle to adding two integers to the sum every two cycles. We might take fewer instructions to do so, which is a nice consolation prize, but this is not what we wanted!</p>
<p>What&#8217;s happened is that unrolling shifted the critical path. Before, the critical path between iterations went through the pointer advance (or, to be more precise, there were two critical paths, one through the pointer advance and one through the sum, and they were both the same length). Now that we do half the number of advances per item, that isn&#8217;t a problem anymore; but the fact that we&#8217;re summing these integers sequentially is now the limiter.</p>
<p>A working solution is to change the algorithm slightly: instead of keeping a single sum of all integers, we keep two separate sums. One for the integers at even-numbered array positions, and one for the integers at odd-numberd positions. Then we need to sum those two values at the end. This is the algorithm:</p>
<pre>
loop:
  rCurInt = load64(rCurPtr);        // LoadEven
  rSumEven = rSumEven + rCurInt;    // SumEven
  rCurInt = load64(rCurPtr + 8);    // LoadOdd
  rSumOdd = rSumOdd + rCurInt;      // SumOdd
  rCurPtr = rCurPtr + 16;           // Advance
  if rCurPtr != rEndPtr goto loop;  // Done?

  rSum = rSumEven + rSumOdd;        // FinalSum
</pre>
<p>And the dataflow graph for the loop kernel looks as follows:</p>
<p><a href="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum2-gv.png"><img data-attachment-id="7090" data-permalink="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/unroll2x_array_sum2-gv/" data-orig-file="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum2-gv.png" data-orig-size="778,333" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Dataflow graph for unrolled array sum, second attempt" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum2-gv.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum2-gv.png?w=497" src="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum2-gv.png?w=497" alt="Dataflow graph for unrolled array sum, second attempt"   class="alignnone size-full wp-image-7090" srcset="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum2-gv.png?w=497 497w, https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum2-gv.png?w=150 150w, https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum2-gv.png?w=300 300w, https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum2-gv.png?w=768 768w, https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum2-gv.png 778w" sizes="(max-width: 497px) 100vw, 497px" /></a></p>
<p>Where before all the summing was in what&#8217;s called the same dependency chain (the name should be self-explanatory by now, I hope), we have now split the summation into two dependency chains. And this is enough to make a sufficiently-wide machine that can sustain 6 instructions per cycle complete our integer-summing task in just slightly more than half a cycle per integer being summed. Progress!</p>
<p>On a somewhat narrower 4-wide design, we are now throughput-bound, and take around 6/4=1.5 cycles per two integers summed, or 0.75 cycles per integer. That&#8217;s still a good improvement from the 1 cycle per integer we would have gotten on the same machine from the non-unrolled version; this gain is purely from reduction the loop overhead fraction, and further unrolling could reduce it even further. (That said, unless your loop really is as tiny as our example, you don&#8217;t generally want to go overboard with unrolling.)</p>
<h3>Tying it all together</h3>
<p>In the introduction, I talked about the need for a model detailed enough to make quantitative, not just qualitative, predictions; and at least for very simple compute-bound loops, that is exactly what we have now. At this point, you should know enough to look at the dependency structure of simple loops, and have some idea for how much (or how little) latent parallelism there is, and be able to compute a coarse upper bound on their &#8220;speed of light&#8221; on various machines with different peak instructions/cycle rates.</p>
<p>Of course, there are many simplifications here, most of which have been already noted in the text; we&#8217;re mostly ignoring the effects of the memory hierarchy, we&#8217;re not worrying at all about where the decoded instructions come from and how fast they can possibly be delivered, we&#8217;ve been flat-out assuming that our branch prediction oracle is perfect, and we&#8217;ve been pretending that while there may be a limit on the total number of instructions we can issue per cycle, it doesn&#8217;t matter what these instructions are. None of these are true. And even if we&#8217;re still compute-bound, we need to worry at least about that latter constraint: sometimes it can make a noticeable difference to tweak the &#8220;instruction mix&#8221; so it matches better what the hardware can actually do in a given clock cycle.</p>
<p>But all these caveats aside, the basic concepts introduced here are very general, and even just sketching out the dependency graph of a loop like this and seeing it in front of you should give you useful ideas about what potential problems are and how you might address them. If you&#8217;re interested in performance optimization, it is definitely worth your time practicing this so you can look at loops and get a &#8220;feel&#8221; for how they execute, and how the shape of your algorithm (or your data structures, in the linked list case) aids or constrains the compiler and processor.</p>
<p><b>UPDATE</b>: Some additional clarifications in answer to some questions: paraphrasing one, &#8220;if you have to first write C code, translate it to some pseudo-assembly, and then look at the graph, how can this possibly be a better process than just measuring the code in the first place?&#8221; Well, the trick here is that to measure anything, you actually need a working program. You don&#8217;t to draw a dataflow graph. For example, a common scenario is that there are many ways you could structure some task, and they all want their data structured differently. Actually implementing and testing multiple variants like this requires you to write a lot of plumbing to massage data from one format into another (all of which can be buggy). Drawing a graph can be done from a brief description of the inner loop alone, and you can leave out the parts that you don&#8217;t currently care about, or &#8220;dummy them out&#8221; by replacing them with a coarse approximation (&#8220;random work here, maybe 10 cycles latency?&#8221;). You only need to make these things precise when they become close to the critical path (or you&#8217;re throughput-bound).</p>
<p>The other thing I&#8217;ll say is that even though I&#8217;ve been talking about adding cycle estimates for compute-bound loops here, this technique works and is useful at pretty much any scale. It&#8217;s applicable in any system where work is started and then processed asynchronously, with the results arriving some time later. If you&#8217;re analyzing a tight, compute-bound loop, cycle-level granularity is the way to go. But you can zoom out and use the same technique to figure out how your decomposition of an algorithm into tasklets processed by a thread pool works out: do you actually have some meaningful overlap, or is there still one long serial dependency chain that dominates everything, and all you&#8217;re doing by splitting it into tasklets like that is adding overhead? Zooming out even further, it works to analyze RPCs you&#8217;re sending to a different machine, or queries to some database. Say you have a 30ms target response time, and each RPC takes about 2ms to return its results. In a system that takes 50 RPCs to produce a result, can you meet that deadline? The answer depends on how the dataflow between them looks. If they&#8217;re all in series, almost certainly not. If they&#8217;re in 5 &#8220;layers&#8221; that each fan out to 10 different machines then collect the results, you probably can. It certainly applies in project scheduling, and is one of the big reasons the &#8220;man-month&#8221; isn&#8217;t a very useful metric: adding manpower increases your available resources but does nothing to relax your dependencies. In fact, it often adds more of them, to bring new people up to speed. If the extra manpower ends up resulting in more work on the critical path towards finishing your project (for example to train new hires), then adding these extra people to the project made it finish later. And so forth. The point being, this is <em>not</em> just limited to cycle-by-cycle analysis, even though that&#8217;s the context I&#8217;ve been introducing it in. It&#8217;s far more general than that.</p>
<p>And I think that&#8217;s enough material for today. Next up, I&#8217;ll continue my &#8220;Reading bits in far too many ways&#8221; series with the third part, where I&#8217;ll be using these techniques to get some insight into what kind of difference the algorithm variants make. Until then!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/03/array_sum-gv1.png" medium="image">
			<media:title type="html">Dataflow graph for basic array sum</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/03/linked_list_sum-gv.png" medium="image">
			<media:title type="html">Dataflow graph for linked list sum</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/03/array_indir_sum-gv.png" medium="image">
			<media:title type="html">Dataflow graph for indirect array sum</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum1-gv.png" medium="image">
			<media:title type="html">Dataflow graph for unrolled array sum, first attempt</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2018/03/unroll2x_array_sum2-gv.png" medium="image">
			<media:title type="html">Dataflow graph for unrolled array sum, second attempt</media:title>
		</media:content>
	</item>
		<item>
		<title>Reading bits in far too many ways (part 2)</title>
		<link>https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/</link>
					<comments>https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Tue, 20 Feb 2018 12:02:01 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<category><![CDATA[Compression]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7068</guid>

					<description><![CDATA[(Continued from part 1.) Last time, I established the basic problem and went through various ways of doing shifting and masking, and the surprising difficulties inherent therein. The &#8220;bit extract&#8221; style I picked is based on a stateless primitive, which made it convenient to start with because there&#8217;s no loop invariants involved. This time, we&#8217;re [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>(Continued from <a href="https://fgiesen.wordpress.com/2018/02/19/reading-bits-in-far-too-many-ways-part-1/">part 1</a>.)</p>
<p>Last time, I established the basic problem and went through various ways of doing shifting and masking, and the surprising difficulties inherent therein. The &#8220;bit extract&#8221; style I picked is based on a stateless primitive, which made it convenient to start with because there&#8217;s no loop invariants involved. </p>
<p>This time, we&#8217;re going to switch to the stateful style employed by most bit readers you&#8217;re likely to encounter (because it ends up cheaper). We&#8217;re also going to switch from a monolithic <code>getbits</code> function to something a bit more fine-grained. But let&#8217;s start at the beginning.</p>
<h3>Variant 1: reading the input one byte at a time</h3>
<p>Our &#8220;extract&#8221;-style reader assumed the entire bitstream was available in memory ahead of time. This is not always possible or desirable; so let&#8217;s investigate the other extreme, a bit reader that requests additional bytes one at a time, and only when they&#8217;re needed.</p>
<p>In general, our stateful variants will dribble in input a few bytes at a time, and have partially processed bytes lying around. We need to store that data in a variable that I will call the &#8220;bit buffer&#8221;:</p>
<pre>
// Again, this is understood to be per-bit-reader state or local
// variables, not globals.
uint64_t bitbuf = 0;   // value of bits in buffer
int      bitcount = 0; // number of bits in buffer
</pre>
<p>While processing input, we will always be seesawing between putting more bits into that buffer when we&#8217;re running low, and then consuming bits from that buffer while we can.</p>
<p>Without further ado, let&#8217;s do our first stateful <code>getbits</code> implementation, reading one byte at a time, and starting with MSB-first this time:</p>
<pre>
// Invariant: there are "bitcount" bits in "bitbuf", stored from the
// MSB downwards; the remaining bits in "bitbuf" are 0.

uint64_t getbits1_msb(int count) {
    assert(count &gt;= 1 &amp;&amp; count &lt;= 57);

    // Keep reading extra bytes until we have enough bits buffered
    // Big endian; our current bits are at the top of the word,
    // new bits get added at the bottom.
    while (bitcount &lt; count) {
        bitbuf |= (uint64_t)getbyte() &lt;&lt; (56 - bitcount);
        bitcount += 8;
    }

    // We now have enough bits present; the most significant
    // "count" bits in "bitbuf" are our result.
    uint64_t result = bitbuf &gt;&gt; (64 - count);

    // Now remove these bits from the buffer
    bitbuf &lt;&lt;= count;
    bitcount -= count;

    return result;
}
</pre>
<p>As before, we can get rid of the <code>count</code>â¥1 requirement by changing the way we grab the result bits, as explained in the last part. This is the last time I&#8217;ll mention this; just keep in mind that whenever I show any algorithm variant here, the variations from last time automatically apply.</p>
<p>The idea here is quite simple: first, we check whether there&#8217;s enough bits in our buffer to satisfy the request immediately. If not, we dribble in extra bytes one at a time until we have enough. <code>getbyte()</code> here is understood to ideally use some <a href="https://fgiesen.wordpress.com/2011/11/21/buffer-centric-io/">buffered IO mechanism</a> that just boils down to dereferencing and incrementing a pointer on the hot path; it should <em>not</em> be a function call or anything expensive if you can avoid it. Because we insert 8 bits at a time, the maximum number of bits we can read in a single call is 57 bits; that&#8217;s the largest number of bits we can refill the buffer to without risking anything dropping out.</p>
<p>After that, we grab the top <code>count</code> bits from our buffer, then shift them out. The invariant we maintain here is that the first unconsumed bit is kept at the MSB of the buffer.</p>
<p>The other thing I want you to notice is that this process breaks down neatly into three separate smaller operations, which I&#8217;m going to call &#8220;refill&#8221;, &#8220;peek&#8221; and &#8220;consume&#8221;, respectively. The &#8220;refill&#8221; phase ensures that a certain given minimum number of bits is present in the buffer; &#8220;peek&#8221; returns the next few bits in the buffer, without discarding them; and &#8220;consume&#8221; removes bits without looking at them. These all turn out to be individually useful operations; to show how things shake out, here&#8217;s the equivalent LSB-first algorithm, factored into smaller parts:</p>
<pre>
// Invariant: there are "bitcount" bits in "bitbuf", stored from the
// LSB upwards; the remaining bits in "bitbuf" are 0.
void refill1_lsb(int count) {
    assert(count &gt;= 0 &amp;&amp; count &lt;= 57);
    // New bytes get inserted at the top end.
    while (bitcount &lt; count) {
        bitbuf |= (uint64_t)getbyte() &lt;&lt; bitcount;
        bitcount += 8;
    }
}

uint64_t peekbits1_lsb(int count) {
    assert(bit_count &gt;= count);
    return bitbuf &amp; ((1ull &lt;&lt; count) - 1);
}

void consume1_lsb(int count) {
    assert(bit_count &gt;= count);
    bitbuf &gt;&gt;= count;
    bitcount -= count;
}

uint64_t getbits1_lsb(int count) {
    refill1_lsb(count);
    uint64_t result = peekbits1_lsb(count);
    consume1_lsb(count);
    return result;
}
</pre>
<p>Writing <code>getbits</code> as the composition of these three smaller primitives is not always optimal. For example, if you use the rotate method for MSB-first bit buffers, you really want to have only rotate shared by the <code>peekbits</code> and <code>consume</code> phases; an optimal implementation shares that work between the two. However, breaking it down into these individual steps is still a useful thing to do, because once you conceptualize these three phases as distinct things, you can start putting them together differently.</p>
<h3>Lookahead</h3>
<p>The most important such transform is amortizing refills over multiple decode operations. Let&#8217;s start with a simple toy example: say we want to read our three example bit fields from the last part:</p>
<pre>
    a = getbits(4);
    b = getbits(3);
    c = getbits(5);
</pre>
<p>With <code>getbits</code> implemented as above, this will do the refill check (and potentially some actual refilling) up to 3 times. But this is silly; we know in advance that we&#8217;re going to be reading 4+3+5=12 bits in one go, so we might as well grab them all at once:</p>
<pre>
    refill(4+3+5);
    a = getbits_no_refill(4);
    b = getbits_no_refill(3);
    c = getbits_no_refill(5);
</pre>
<p>where <code>getbits_no_refill</code> is yet another getbits variant that does <code>peekbits</code> and <code>consume</code>, but, as the name suggests, no refilling. And once you get rid of the refill loop between the individual <code>getbits</code> invocations, you&#8217;re just left with straight-line integer code, which compilers are good at optimizing further. That said, the all-fixed-length case is a bit of a cheap shot; it gets far more interesting when we&#8217;re not sure exactly how many bits we&#8217;re actually going to consume, like in this example:</p>
<pre>
    temp = getbits(5);
    if (temp &lt; 28)
        result = temp;
    else
        result = 28 + (temp - 28)*16 + getbits(4);
</pre>
<p>This is a simple variable-length code where values from 0 through 27 are sent in 5 bits, and values from 28 through 91 are sent in 9 bits. The point being, in this case, we don&#8217;t know in advance how many bits we&#8217;re eventually going to consume. We do know that it&#8217;s going to be no more than 9 bits, though, so we can still make sure we only refill once:</p>
<pre>
    refill(9);
    temp = getbits_no_refill(5);
    if (temp &lt; 28)
        result = temp;
    else
        result = 28 + (temp - 28)*16 + getbits_no_refill(4);
</pre>
<p>In fact, if you want to, you can go wild and split operations even more, so that both execution paths only <code>consume</code> bits once. For example, assuming a MSB-first bit buffer, we could write this small decoder as follows:</p>
<pre>
    refill(9);
    temp = peekbits(5);
    if (temp &lt; 28) {
        result = temp;
        consume(5);
    } else {
        // The "upper" and "lower" code are back-to-back,
        // and combine exactly the way we want! Synergy!
        result = getbits_no_refill(9) - 28*16 + 28;
    }
</pre>
<p>This kind of micro-tweaking is <em>really</em> not recommended outside very hot loops, but as I mentioned in the previous part, some of these decoder loops are quite hot indeed, and in that case saving a few instructions here and a few instructions there adds up. One particularly important technique for decoding variable-length codes (e.g. Huffman codes) is to peek ahead by some fixed number of bits and then do a table lookup based on the result. The table entry then lists what the decoded symbol should be, and how many bits to consume (i.e. how many of the bits we just peeked at really belonged to the symbol). This is <em>significantly</em> faster than reading the code a bit or two at a time and consulting a Huffman tree at every step (the method sadly taught in many textbooks and other introductory texts.)</p>
<p>There&#8217;s a problem, though. The technique of peeking ahead a bit (no pun intended) and then later deciding how many bits you actually want to consume is quite powerful, but we&#8217;ve just changed the rules: the <code>getbits</code> implementation above is careful to only read extra bytes when it&#8217;s strictly necessary. But our modified variable-length code reader example above always refills so the buffer contains at least 9 bits, even when we&#8217;re only going to consume 5 bits in the end. Depending on where that refill happens, it might even cause us to read past the end of the actual data stream.</p>
<p>In short, we&#8217;ve introduced <em>lookahead</em>. The modified code reader starts grabbing extra input bytes before it&#8217;s sure whether it needs them. This has many advantages, but the trade-off is that it may cause us to read past the logical end of a bit stream; it certainly implies that we have to make sure this case is handled correctly. It certainly should never crash or read out of bounds; but beyond that, it implies certain thing about the way input buffering or the framing layer have to work.</p>
<p>Namely, if we&#8217;re going to do any lookahead, we need to figure out a way to handle this. The primary options are as follows:</p>
<ul>
<li>We can punt and make it someone else&#8217;s problem by just requiring that everyone hand us valid data with some extra padding bytes after the end. This makes our lives easier but is an inconvenience for everyone else.</li>
<li>We can arrange things so the outer framing layer that feeds bytes to our <code>getbits</code> routine knows when the real data stream is over (either due to some escape mechanism or because the size is sent explicitly); then we can either stop doing any lookahead and switch to a more careful decoder when we&#8217;re getting close to the end, or pad the stream with some dummy value after its end (zeroes being the most popular choice).</li>
<li>We can make sure that whatever bytes we might grab during lookahead while decoding a valid stream are still part of our overall byte stream that&#8217;s being processed by our code. For example, if you use a 64-bit bit buffer, we can finagle our way around the problem by requiring that some 8-byte footer (say a checksum or something) be present right after a valid bit stream. So while our bit buffer might overshoot, it&#8217;s still data that&#8217;s ultimately going to be consumed by the decoder, which simplifies the logistics considerably.</li>
<li>Barring that, whatever I/O buffering layer we&#8217;re using needs to allow us to return some extra bytes we didn&#8217;t actually consume into the buffer. Namely, whatever lookahead bytes we have left in our bit buffer after we&#8217;re done decoding need to be returned to the buffer for whoever is going to read it next. This is essentially what the C standard library function <a href="http://pubs.opengroup.org/onlinepubs/009696899/functions/ungetc.html"><code>ungetc</code></a> does, except you&#8217;re not allowed to call <code>ungetc</code> more than once, and we might need to. So going along this route essentially dooms you to taking charge of IO buffer management.</li>
</ul>
<p>I won&#8217;t sugarcoat it, all of these options are a pain in the neck, some more so than others: hand-waving it away by putting something else at the end is easiest, handling it in some outer framing layer isn&#8217;t too bad, and taking over all IO buffering so you can un-read multiple bytes is positively hideous, but you don&#8217;t have great options when you don&#8217;t control your framing. In the past, I&#8217;ve written <a href="https://fgiesen.wordpress.com/2011/11/21/buffer-centric-io/">posts</a> about <a href="https://fgiesen.wordpress.com/2016/01/02/end-of-buffer-checks-in-decompressors/">handy techniques</a> that might help you in this context; and in some implementations you can work around it, for example by setting <code>bitcount</code> to a huge value just after inserting the final byte from the stream. But in general, if you want lookahead, you&#8217;re going to have to put some amount of work into it. That said, the winnings tend to be fairly good, so it&#8217;s not all for nothing.</p>
<h3>Variant 2: when you <em>really</em> want to read 64 bits at once</h3>
<p>The methods I&#8217;ve discussed so far both have some &#8220;slop&#8221; from working in byte granularity. The extract-style bit reader started with a full 64-bit read but then had to shift by up to 7 positions to discard the part of the current byte that&#8217;s already consumed, and the <code>getbits1</code> above inserts one byte at a time into the bit buffer; if there&#8217;s 57 bits already in the buffer, there&#8217;s no space for another byte (because that would make 65 bits, more than the width of our buffer), so that&#8217;s the maximum width the <code>getbits1</code> method supports. Now 57 bits is a useful amount; but if you&#8217;re doing this on a 32-bit platform, the equivalent magic number is 25 bits (32-7), and that&#8217;s definitely on the low side, enough so to be inconvenient sometimes.</p>
<p>Luckily, if you want the full width, there&#8217;s a way to do it (like the rotate-and-mask technique for MSB-first bit buffers, I learned this at RAD). At this point, I think you get the correspondence between the MSB-first and LSB-first methods, so I&#8217;m only going to show one per variant. Let&#8217;s do LSB-first for this one:</p>
<pre>
// Invariant: "bitbuf" contains "bitcount" bits, starting from the
// LSB up; 1 &lt;= bitcount &lt;= 64
uint64_t bitbuf = 0;     // value of bits in buffer
int      bitcount = 0;   // number of bits in buffer
uint64_t lookahead = 0;  // next 64 bits
bool     have_lookahead = false;

// Must do this to prime the pump!
void initialize() {
    bitbuf = get_uint64LE();
    bitcount = 64;
    have_lookahead = false;
}

void ensure_lookahead() {
    // grabs the lookahead word, if we don't have
    // it already.
    if (!have_lookahead) {
        lookahead = get_uint64LE();
        have_lookahead = true;
    }
}

uint64_t peekbits2_lsb(int count) {
    assert(bitcount &gt;= 1);
    assert(count &gt;= 0 &amp;&amp; count &lt;= 64);

    if (count &lt;= bitcount) { // enough bits in buf
        return bitbuf &amp; width_to_mask_table[count];
    } else {
        ensure_lookahead();

        // combine current bitbuf with lookahead
        // (lookahead bits go above end of current buf)
        uint64_t next_bits = bitbuf;
        next_bits |= lookahead &lt;&lt; bitcount;
        return next_bits &amp; width_to_mask_table[count];
    }
}

void consume2_lsb(int count) {
    assert(bitcount &gt;= 1);
    assert(count &gt;= 0 &amp;&amp; count &lt;= 64);

    if (count &lt; bitcount) { // still in current buf
        // just shift the bits out
        bitbuf &gt;&gt;= count;
        bitcount -= count;
    } else { // all of current buf consumed
        ensure_lookahead();
         
        // we advanced fully into the lookahead word
        int lookahead_consumed = count - bitcount;
        bitbuf = lookahead &gt;&gt; lookahead_consumed;
        bitcount = 64 - lookahead_consumed;
        have_lookahead = false;
    }

    assert(bitcount &gt;= 1);
}

uint64_t getbits2_lsb(int count) {
    uint64_t result = peekbits2_lsb(count);
    consume2_lsb(count);
    return result;
}
</pre>
<p>This one is a bit more complicated than the ones we&#8217;ve seen before, and needs an explicit initialization step to make the invariants work out <em>just right</em>. It also involves several extra branches compared to the variants we&#8217;ve seen before, which makes it less than ideal for deeply pipelined machines, which includes desktop PCs, and note that I&#8217;m using the <code>width_to_mask_table</code> again, and not just for show: none of the arithmetic expressions we saw last time to compute the mask for a given width work for the full 0-64 range of allowed <code>width</code>s on any common 64-bit architecture that&#8217;s not POWER, and even that only if we ignore them invoking undefined behavior, which we <em>really</em> shouldn&#8217;t.</p>
<p>The underlying idea is fairly simple: instead of just one bit buffer, we keep track of two values. We have however many bits are left of the last 64-bit value we read, and when that&#8217;s not enough for a <code>peekbits</code>, we grab the next 64-bit value from the input stream (via some externally-implemented <code>get_uint64LE()</code>) to give us the bits we&#8217;re missing. Likewise, <code>consume</code> checks whether there will still be any bits left in the current input buffer after consuming <code>width</code> bits. If not, we switch over to the bits from the lookahead value (shifting out however many of them we consumed) and clear the <code>have_lookahead</code> flag to indicate that what used to be our lookahead value is now just the contents of our bit buffer.</p>
<p>There are some contortions in this code to ensure we don&#8217;t do out-of-range (undefined-behavior-inducing) shifts. For example, note how <code>peekbits</code> tests whether <code>count &lt;= bitcount</code> to detect the bits-present-in-buffer case, whereas <code>consume</code> uses <code>count &lt; bitcount</code>. This is not an accident: in <code>peekbits</code>, the <code>next_bits</code> calculation involves a right-shift by <code>bitcount</code>. Since it only happens in the path where <code>bitcount</code> &lt; <code>count</code> &le; 64, that means that <code>bitcount &lt; 64</code>, and we&#8217;re safe. In <code>consume</code>, the situation is reversed: we shift by <code>lookahead_consumed = count - bitcount</code>. The condition around the block guarantees that <code>lookahead_consumed</code> &ge; 0; in the other direction, because <code>count</code> is at most 64 and <code>bitcount</code> is at least 1, we have <code>lookahead_consumed</code> &le; 64 &#8211; 1 = 63. That said, to paraphrase Knuth: beware of bugs in the above code; I&#8217;ve only proved it correct, not tried it.</p>
<p>This technique has another thing going for it besides supporting bigger bit field widths: note how it always reads full 64-bit uints at a time. Variant 1 above only reads bytes at a time, but requires a refill loop; the various branchless variants we&#8217;ll see later implicitly rely on the target CPU supporting fast unaligned reads. This version alone has the distinction of doing reads of a single size and with consistent alignment, which makes it more attractive on targets that don&#8217;t support fast unaligned reads, such as many old-school RISC CPUs.</p>
<p>Finally, as usual, there&#8217;s several more variations here that I&#8217;m not showing. For example, if you happen to have the data you&#8217;re decoding fully in memory, there&#8217;s no reason to bother with the boolean <code>have_lookahead</code> flag; just keep a pointer to the current lookahead word, and bump that pointer up whenever the current lookahead is consumed.</p>
<h3>Variant 3: bit extraction redux</h3>
<p>The original bit extraction-based bit reader from the previous part was a bit on the expensive side. But as long as we&#8217;re OK with the requirement that the entire input stream be in memory at once, we can wrangle it into the refill/peek/consume pattern to get something useful. It still gives us a bit reader that looks ahead (and hence has the resulting difficulties), but such is life. For this one, let&#8217;s do MSB again:</p>
<pre>
const uint8_t *bitptr; // Pointer to current byte
uint64_t       bitbuf = 0; // last 64 bits we read
int            bitpos = 0; // how many of those bits we've consumed

void refill3_msb() {
    assert(bitpos &lt;= 64);

    // Advance the pointer by however many full bytes we consumed
    bitptr += bitpos &gt;&gt; 3;

    // Refill
    bitbuf = read64BE(bitptr);

    // Number of bits in the current byte we've already consumed
    // (we took care of the full bytes; these are the leftover
    // bits that didn't make a full byte.)
    bitpos &amp;= 7;
}

uint64_t peekbits3_msb(int count) {
    assert(bitpos + count &lt;= 64);
    assert(count &gt;= 1 &amp;&amp; count &lt;= 64 - 7);

    // Shift out the bits we've already consumed
    uint64_t remaining = bitbuf &lt;&lt; bitpos;

    // Return the top "count" bits
    return remaining &gt;&gt; (64 - count);
}

void consume3_msb(int count) {
    bitpos += count;
}
</pre>
<p>This time, I&#8217;ve also left out the <code>getbits</code> built from refill / peek / consume calls, because that&#8217;s yet another pattern that should be pretty clear by now.</p>
<p>It&#8217;s a pretty sweet variant. Once we break the bit extraction logic into separate &#8220;refill&#8221; and &#8220;peek&#8221;/&#8221;consume&#8221; pieces, it becomes clear how all of the individual pieces are fairly small and clean. It&#8217;s also completely branchless! It does expect unaligned 64-bit big-endian reads to exist and be reasonably cheap (not a problem on mainstream x86s or ARMs), and of course a realistic implementation needs to include handling of the end-of-buffer cases; see the discussion in the &#8220;lookahead&#8221; section.</p>
<h3>Variant 4: a different kind of lookahead</h3>
<p>And now that we&#8217;re here, let&#8217;s do another branchless lookahead variant. This exact variant is, to the best of my knowledge, another RAD special &#8211; discovered by my colleague Charles Bloom and me while working on Kraken (<b>UPDATE:</b> as Yann points out in the comments, this basic idea was apparently used in Eric Biggers&#8217; &#8220;Xpack&#8221; long before Kraken was launched; I wasn&#8217;t aware of this and I don&#8217;t think Charles was either, but that means we&#8217;re definitely not the first ones to come up with the idea. Our variant has an interesting wrinkle though &#8211; details <a href="https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/#comment-10923">in my reply</a>). Now all branchless (well, branchless if you ignore end-of-buffer checking in the refill etc.) bit readers look very much alike, but this particular variant has a few interesting properties (some of which I&#8217;ll only discuss later because we&#8217;re lacking a bit of necessary background right now), and that I haven&#8217;t seen anywhere else in this combination; if someone else did it first, feel free to inform me in the comments, and I&#8217;ll add the proper attribution! Here goes; back to LSB-first again, because I&#8217;m committed to hammering home just how similar and interchangeable LSB-first/MSB-first are at this level, holy wars notwithstanding.</p>
<pre>
const uint8_t *bitptr;   // Pointer to next byte to insert into buf
uint64_t bitbuf = 0;     // value of bits in buffer
int      bitcount = 0;   // number of bits in buffer

void refill4_lsb() {
    // Grab the next few bytes and insert them right above
    // the current top.
    bitbuf |= read64LE(bitptr) &lt;&lt; bitcount;

    // Advance the read pointer for next iteration
    bitptr += (63 - bitcount) &gt;&gt; 3;

    // Update the available bit count
    bitcount |= 56; // now bitcount is in [56,63]
}

uint64_t peekbits4_lsb(int count) {
    assert(count &gt;= 0 &amp;&amp; count &lt;= 56);
    assert(count &lt;= bitcount);
    
    return bitbuf &amp; ((1ull &lt;&lt; count) - 1);
}

void consume4_lsb(int count) {
    assert(count &lt;= bitcount);

    bitbuf &gt;&gt;= count;
    bitcount -= count;
}
</pre>
<p>The peek and consume phases are nothing we haven&#8217;t seen before, although this time the maximum permissible bit width seems to have shrunk by one more bit down to 56 bits for some reason.</p>
<p>That reason is in the refill phase, which works slightly differently from the ones we&#8217;ve seen so far. Reading 64 little-endian bits and shifting them up to align with the top of our current bit buffer should be straightforward at this point. But the <code>bitptr</code> / <code>bitcount</code> manipulation needs some explanation.</p>
<p>It&#8217;s actually easier to start with the <code>bitcount</code> part. The variants we&#8217;ve seen so far generally have between 57 and 64 bits in the buffer after refill. This version instead targets having between 56 and 63 bits in the buffer (which is also why the limit on count went down by one). But why? Well, inserting some integer number of bytes means <code>bitcount</code> is going to be incremented by some multiple of 8 during the refill; that means that <code>bitcount &amp; 7</code> (the low 3 bits) won&#8217;t change. And if we refill to a target of [56,63] bits in the buffer, we can compute the updated bit count with a single binary OR operation.</p>
<p>Which brings me to the question of how many bytes we should advance the pointer by. Well, let&#8217;s just look at the values of the original <code>bitcount</code>:</p>
<ul>
<li>If 56 â¤ <code>bitcount</code> â¤ 63, we were already in our target range and don&#8217;t want to advance by another byte.</li>
<li>If 48 â¤ <code>bitcount</code> â¤ 55, we&#8217;re adding exactly 1 byte (and so want to advance <code>bit_ptr</code> by that much).</li>
<li>If 40 â¤ <code>bitcount</code> â¤ 47, we&#8217;re adding exactly 2 bytes.</li>
</ul>
<p>and so forth. This works out to the <code>(63 - bitcount) &gt;&gt; 3</code> bytes we&#8217;re adding to <code>bitptr</code>.</p>
<p>Now, the bits in <code>bitbuf</code> above <code>bitcount</code> can end up getting ORed over multiple times. However, when that happens, we OR over the same value every time, so it doesn&#8217;t change the result. Therefore, once they later travel downwards (from the right-shift in the consume function), they&#8217;re fine; no need to worry about garbage.</p>
<p>Okay. So what&#8217;s interesting, but what&#8217;s so special about this particular variant? When would you choose this over, say, variant 3 above?</p>
<p>One simple reason: in this variant, the address the <code>refill</code> is loading from does not have a dependency on the current value of <code>bitcount</code>. In fact, the next load address is known as soon as the <em>previous</em> refill is complete. This is a subtle distinction that turns out to be a fairly major advantage on an out-of-order CPU. Among integer operations, even when hitting the L1 cache, loads are on the high latency side (typically somewhere between 3 and 5 cycles, whereas most integer operations take a single cycle), and the exact value of <code>bitcount</code> at the end of some loop iteration is often only known late (consider the simple variable-length code example I gave above).</p>
<p>Having the load address not depend on <code>bitcount</code> means the load can potentially issue as soon as the previous refill is complete; then we have plenty of time to complete the load, potentially byte-swap the value if the endianness of our load doesn&#8217;t match the target ISA (say because we&#8217;re using a MSB-first bit buffer on a little-endian CPU), and then the only thing that depends on the previous value of <code>bitcount</code> is the shift, which is a regular ALU operation and generally takes a single cycle.</p>
<p>In short, this somewhat obscure form of refill looks weird, but provides a tangible increase in available instruction-level parallelism. It was good for about a 10% throughput improvement on desktop PCs (vs. the earlier branchless refill it replaced) in the then-current version of the Kraken Huffman decoder when I tested it in early 2016.</p>
<p>Consider this a teaser for the next (and hopefully last) part of this series, in which I <em>won&#8217;t</em> introduce many more variants (maybe one more), and will instead talk a lot more about the performance of bit stream decoders and what kinds of things to watch out for.</p>
<p>Until then!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/feed/</wfw:commentRss>
			<slash:comments>9</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
	</channel>
</rss>
