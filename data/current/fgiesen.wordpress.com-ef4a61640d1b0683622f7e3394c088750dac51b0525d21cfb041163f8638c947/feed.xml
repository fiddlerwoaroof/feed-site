<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>The ryg blog</title>
	<atom:link href="https://fgiesen.wordpress.com/feed/" rel="self" type="application/rss+xml"/>
	<link>https://fgiesen.wordpress.com</link>
	<description>When I grow up I'll be an inventor.</description>
	<lastBuildDate>Tue, 08 Nov 2022 23:09:51 +0000</lastBuildDate>
	<language>en</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
<cloud domain="fgiesen.wordpress.com" port="80" path="/?rsscloud=notify" registerProcedure="" protocol="http-post"/>
<image>
		<url>https://s0.wp.com/i/buttonw-com.png</url>
		<title>The ryg blog</title>
		<link>https://fgiesen.wordpress.com</link>
	</image>
	<atom:link rel="search" type="application/opensearchdescription+xml" href="https://fgiesen.wordpress.com/osd.xml" title="The ryg blog"/>
	<atom:link rel="hub" href="https://fgiesen.wordpress.com/?pushpress=hub"/>
	<item>
		<title>Whatâ€™s that magic computation in stb__RefineBlock?</title>
		<link>https://fgiesen.wordpress.com/2022/11/08/whats-that-magic-computation-in-stb__refineblock/</link>
					<comments>https://fgiesen.wordpress.com/2022/11/08/whats-that-magic-computation-in-stb__refineblock/#respond</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Tue, 08 Nov 2022 21:20:18 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<category><![CDATA[Compression]]></category>
		<category><![CDATA[Maths]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7384</guid>

					<description><![CDATA[Back in 2007 I wrote my DXT1/5 (aka BC1/3) encoder rygdxt, originally for &#8220;fr-041: debris&#8221; (so it was size-constrained). A bit later I put up the source and Sean Barrett adapted it into &#8220;stb_dxt&#8221;, which is probably the form that most know it in today. It&#8217;s a simple BC1 encoder that gives decent quality, the [&#8230;]]]></description>
										<content:encoded><![CDATA[
<p>Back in 2007 I wrote my DXT1/5 (aka BC1/3) encoder rygdxt, originally for &#8220;fr-041: debris&#8221; (so it was size-constrained). A bit later I put up the source and Sean Barrett adapted it into &#8220;stb_dxt&#8221;, which is probably the form that most know it in today.</p>



<p>It&#8217;s a simple BC1 encoder that gives decent quality, the underlying algorithm is reasonably fast (fast enough to say bake textures you produce once per session in a game from a character creator, say, which is one of the cases I&#8217;ve ended up using it in in a professional context), and is easy to integrate.</p>



<p>The basic algorithm uses the same primitives most BC1 encoders use (I&#8217;ll assume in the following you know how BC1 works): compute the average and covariance matrix of the block of pixels, compute the principal component of the covariance to get an initial guess for what direction the vector between the two endpoints should point in. Then we project all pixel values onto that vector to find the min/max support points in that direction as the initial seed endpoints (which determines the initial palette), assign each pixel the palette entry closest to it, and do some iterative refinement of the whole thing.</p>



<p>rygdxt/stb_dxt only uses the 4-color mode. It did not bother with the 3-color + 1-bit transparency mode. It&#8217;s much more niche, increases the search space appreciably and complicates the solver, and is rarely useful. The original code was written for 64k intros and the like and this was an easy small potential win to give up on to save on code size.</p>



<p>Nothing in there was invented by me, but at the time I wrote it anyway, DXT1/BC1 encoders (at least the ones I was looking at) were doing some of these steps in a way that was more complicated than necessary. For example, one thing I vividly recall is that one encoder I was looking at at the time (I believe it was Squish?) was determining the principal component of the covariance matrix by forming the characteristic polynomial (which for a 3&#215;3 matrix is cubic), using Cardano&#8217;s formula to find the roots of the polynomial to determine eigenvalues, and finally using Gaussian Elimination (I think it was) to find vectors spanning the nullspace of the covariance matrix minus the eigenvalue times the identity. That&#8217;s a very &#8220;undergrad Linear Algebra homework&#8221; way of attacking the problem; it works, but it&#8217;s complicated with a fair amount of tricky code and numerical issues to wrestle with.</p>



<p>rygdxt, with its focus on size, goes for a much simpler approach: use power iteration. Which is to say, pick a start vector (the only tricky bit), and then iterate multiplying covariance matrix by that vector and normalizing the result a few times. This gives you a PCA vector directly. In practice, 3-6 iterations usually sufficient to get as good a direction vector as makes sense for BC1 encoding (stb_dxt uses 4), and the whole thing fits in a handful lines of code with absolutely nothing subtle or numerically tricky going on. I don&#8217;t claim to have invented this, but I will say that before rygdxt I saw a bunch of encoders futzing around with more complicated and messier approaches like Squish, or even bothering with computing a general 3&#215;3 (or even NxN) eigendecomposition, and these days power iteration seems to be the go-to, so if nothing else I think rygdxt helped popularize a much simpler technique in that space.</p>



<p>Another small linear algebra trick in there is with the color matching. We have a 4-entry palette with colors that lie (approximately, because everything is quantized to an 8-bit integer grid) on a line segment through RGB space. The brute-force way to find the best match is to compute the 4 squared distances between the target pixel value and each of the 4 palette entries. This is not necessarily a bad way to do it (especially if you use narrow data types and SIMD instructions, because the dataflow is very simple and regular), but it is essentially computing four 4-element dot products per pixel. What rygdxt/stb_dxt uses instead is the fact that if it were a perfect line segment in a continuous space, we could just use an orthogonal projection to find the nearest point on the line, which with appropriate normalization boils down to a single dot product per pixel. In that continuous simplification, the two interpolated colors sit exactly 1/3rd and 2/3rds along the way between the two endpoints. However working on the aforementioned 8-bit integer grid means that the interpolated colors can sometimes be noticeably off from their ideal placement, especially when the two endpoints are close together. What rygdxt therefore does is compute where the actual interpolated 1/3rd-of-the-way and 2/3rds-of-the-way colors land on the line (two more dot products), and then we can do our single dot product with the line direction and use the values we computed earlier to figure out which of these 4 colors is closest in the 1D space along the line, which is just a few comparisons and can be done branchlessly if desired.</p>



<p>The result doesn&#8217;t always match with the distances code using the brute-force solution would get, but it&#8217;s very close in practice, and reducing the computations by a factor of nearly four sped up the BC1 encoding process nicely (old 2008 evaluation by my now-colleague Charles <a href="http://cbloomrants.blogspot.com/2008/12/12-08-08-dxtc-summary.html">here</a>).<a href="http://cbloomrants.blogspot.com/2008/12/12-08-08-dxtc-summary.html"></a></p>



<p>That leaves us with the actual subject of this blog post, the iterative refinement logic! I just answered an email by someone asking for an explanation of what that code does and why, so here goes.</p>


<h3>The refinement function</h3>


<p>The code in question is <a href="https://github.com/nothings/stb/blob/8b5f1f37b5b75829fc72d38e7b5d4bcbf8a26d55/stb_dxt.h#L399">here</a>.</p>



<p>Ultimately, what rygdxt/stb_dxt does to refine the results is linear least-squares minimization. (Another idea that&#8217;s not mine, this one I definitely got from Squish). We&#8217;re holding the DXT indices (interpolation weights) constant and solving for optimal endpoints in a least-squares sense. In each of the RGB color channels, the i&#8217;th target pixel is approximated by a linear interpolation</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%281-w_i%29+x_0+%2B+w_i+x_1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%281-w_i%29+x_0+%2B+w_i+x_1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%281-w_i%29+x_0+%2B+w_i+x_1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle (1-w_i) x_0 + w_i x_1" class="latex" /></p>



<p>where x0, x1 are the endpoints we&#8217;re solving for and w<sub>i</sub> is one of {0, 1/3, 2/3, 3/3} depending on which of the 4 indices is used for that pixel. Writing that out for the whole block in say the red channel turns into an overdetermined system of 16 linear equations</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Bpmatrix%7D+1-w_0+%26+w_0+%5C%5C+1-w_1+%26+w_1+%5C%5C+%5Cvdots+%26+%5Cvdots+%5C%5C+1-w_%7B15%7D+%26+w_%7B15%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+x_%7B0r%7D+%5C%5C+x_%7B1r%7D+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+r_0+%5C%5C+r_1+%5C%5C+%5Cvdots+%5C%5C+r_%7B15%7D+%5Cend%7Bpmatrix%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Bpmatrix%7D+1-w_0+%26+w_0+%5C%5C+1-w_1+%26+w_1+%5C%5C+%5Cvdots+%26+%5Cvdots+%5C%5C+1-w_%7B15%7D+%26+w_%7B15%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+x_%7B0r%7D+%5C%5C+x_%7B1r%7D+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+r_0+%5C%5C+r_1+%5C%5C+%5Cvdots+%5C%5C+r_%7B15%7D+%5Cend%7Bpmatrix%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Bpmatrix%7D+1-w_0+%26+w_0+%5C%5C+1-w_1+%26+w_1+%5C%5C+%5Cvdots+%26+%5Cvdots+%5C%5C+1-w_%7B15%7D+%26+w_%7B15%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+x_%7B0r%7D+%5C%5C+x_%7B1r%7D+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+r_0+%5C%5C+r_1+%5C%5C+%5Cvdots+%5C%5C+r_%7B15%7D+%5Cend%7Bpmatrix%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle &#92;begin{pmatrix} 1-w_0 &amp; w_0 &#92;&#92; 1-w_1 &amp; w_1 &#92;&#92; &#92;vdots &amp; &#92;vdots &#92;&#92; 1-w_{15} &amp; w_{15} &#92;end{pmatrix} &#92;begin{pmatrix} x_{0r} &#92;&#92; x_{1r} &#92;end{pmatrix} = &#92;begin{pmatrix} r_0 &#92;&#92; r_1 &#92;&#92; &#92;vdots &#92;&#92; r_{15} &#92;end{pmatrix}" class="latex" /></p>



<p>to be solved for x0r, x1r (the first/second endpoint&#8217;s R value).</p>



<p>Let&#8217;s call that tall and skinny matrix on the left A; <img src="https://s0.wp.com/latex.php?latex=x+%3D+%28x_%7B0r%7D%2C+x_%7B1r%7D%29%5ET&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x+%3D+%28x_%7B0r%7D%2C+x_%7B1r%7D%29%5ET&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=x+%3D+%28x_%7B0r%7D%2C+x_%7B1r%7D%29%5ET&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="x = (x_{0r}, x_{1r})^T" class="latex" /> is the 2D column vector we&#8217;re solving for, and the RHS vector of the pixel r values we can just call &#8220;r&#8217;.</p>



<p>That means our problem is to minimize <img src="https://s0.wp.com/latex.php?latex=%7C%7CAx+-+r%7C%7C_2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%7CAx+-+r%7C%7C_2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%7CAx+-+r%7C%7C_2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="||Ax - r||_2" class="latex" /> (2-norm), or equivalently <img src="https://s0.wp.com/latex.php?latex=%7C%7CAx+-+r%7C%7C%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%7CAx+-+r%7C%7C%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%7CAx+-+r%7C%7C%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="||Ax - r||^2" class="latex" />.</p>



<p><img src="https://s0.wp.com/latex.php?latex=%7C%7CAx-r%7C%7C%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%7CAx-r%7C%7C%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%7CAx-r%7C%7C%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="||Ax-r||^2" class="latex" /> is quadratic; the way you find its minimum is by computing the derivative and equating it to 0, which leads us to what&#8217;s called the Normal Equations, which for this problem are</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+A%5ET+A+x+-+A%5ET+r+%3D+0+%5C%5C+%5CLeftrightarrow+A%5ET+A+x+%3D+A%5ET+r&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+A%5ET+A+x+-+A%5ET+r+%3D+0+%5C%5C+%5CLeftrightarrow+A%5ET+A+x+%3D+A%5ET+r&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+A%5ET+A+x+-+A%5ET+r+%3D+0+%5C%5C+%5CLeftrightarrow+A%5ET+A+x+%3D+A%5ET+r&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle A^T A x - A^T r = 0 &#92;&#92; &#92;Leftrightarrow A^T A x = A^T r" class="latex" /></p>



<p>A is a 16Ã—2 matrix, so A<sup>T</sup>A is a tiny symmetric matrix (2Ã—2) and contains the dot products of the columns of A with each other.</p>



<p>We have 3 color channels, not just r but g and b as well. That means we have 3 copies of the same linear system with 3 different right-hand sides, or equivalently we can view the whole thing as a matrix equation with a 3-wide right-hand side. Either way, all 3 systems have the same A matrix, the only thing that differs is the right-hand sides.</p>



<p>We accumulate <img src="https://s0.wp.com/latex.php?latex=A%5ET+r&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=A%5ET+r&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A%5ET+r&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="A^T r" class="latex" /> (and g and b as well) in the first pass, and also compute the entries of <img src="https://s0.wp.com/latex.php?latex=A%5ET+A&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=A%5ET+A&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A%5ET+A&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="A^T A" class="latex" />. To solve the system above, because we just have a 2&#215;2 matrix, we can use Cramer&#8217;s rule to solve it directly, no need to mess around with factorizations or Gaussian Elimination or similar.</p>



<p>That&#8217;s the basic idea. The RefineBlock function uses two more tricks:</p>


<ol>
<li>instead of the weights being {0, 1/3, 2/3, 3/3}, we multiply them by 3 and also scale the RHS by 3 (the latter, we never actually do explicitly). Getting the extra scaling is essentially free during&nbsp; the linear system solve, especially since we already need to do some scaling per-channel anyway, because the R/B endpoint values we solve for are in [0,31] (instead of [0,255] for the input pixel values), and the G values are in [0,63]. Scaling everything by 3 means there are no fractions involved in the computation, it&#8217;s all small integers, which will be useful for the second trick. It also means that when we compute the determinant of the 2&#215;2 system for Cramer&#8217;s rule, it&#8217;s an integer computation, so we don&#8217;t have to worry about near-zero values and the like. (Although in this case, it&#8217;s not too hard to prove that A is singular, i.e. has determinant 0, if and only if all the w<sub>i</sub> are the same, which is easy enough to detect up front.)</li>
<li>now our weights w<sub>i</sub> (matrix entries in A) are all in {0,1,2,3}. The three entries in <img src="https://s0.wp.com/latex.php?latex=A%5ET+A&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=A%5ET+A&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A%5ET+A&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="A^T A" class="latex" /> sum, respectively (note we scaled by 3, so <img src="https://s0.wp.com/latex.php?latex=1-w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1-w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=1-w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="1-w_i" class="latex" /> turns into <img src="https://s0.wp.com/latex.php?latex=3-w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=3-w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=3-w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="3-w_i" class="latex" />): <img src="https://s0.wp.com/latex.php?latex=%283-w_i%29%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%283-w_i%29%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%283-w_i%29%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(3-w_i)^2" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%283-w_i%29+w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%283-w_i%29+w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%283-w_i%29+w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(3-w_i) w_i" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=w_i%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=w_i%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=w_i%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="w_i^2" class="latex" />. Note that all three values we&#8217;re summing only depend on w<sub>i</sub>, and w<sub>i</sub> is one of 4 possible values (depending on the index), so we can just precompute all of them. Also note they&#8217;re all quite small: <img src="https://s0.wp.com/latex.php?latex=%283-w_i%29%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%283-w_i%29%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%283-w_i%29%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(3-w_i)^2" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=w_i%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=w_i%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=w_i%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="w_i^2" class="latex" /> are at most 9, and <img src="https://s0.wp.com/latex.php?latex=%283-w_i%29+w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%283-w_i%29+w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%283-w_i%29+w_i&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(3-w_i) w_i" class="latex" /> is either 0 or 2, so they all comfortably fit in 4 bits. We&#8217;re summing 16 of these values (one per pixel in the block), and they&#8217;re all positive. That means the final sums fit into 8 bits no problem. Therefore the actual table we have packs the 3 values into one integer:<br><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%283-w_i%29%5E2+%5Cll+16%29+%2B+%28%283-w_i%29+w_i+%5Cll+8%29+%2B+%28w_i%29%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%283-w_i%29%5E2+%5Cll+16%29+%2B+%28%283-w_i%29+w_i+%5Cll+8%29+%2B+%28w_i%29%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%283-w_i%29%5E2+%5Cll+16%29+%2B+%28%283-w_i%29+w_i+%5Cll+8%29+%2B+%28w_i%29%5E2&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle ((3-w_i)^2 &#92;ll 16) + ((3-w_i) w_i &#92;ll 8) + (w_i)^2" class="latex" />.<br>We sum that one integer per pixel. All of the individual sums are guaranteed to be &lt;256 so we can extract the corresponding bits after the accumulation loop. That means the cost of computing the entries of A^T A becomes quite cheap (a single 4-entry table lookup and 32-bit integer add per pixel), and the bulk of the work in that first loop is just computing the right-hand sides.</li>
</ol>


<p>And that&#8217;s about it. I know that approach was later also used by NVidia Texture Tools, beyond that I have no idea of its reach, but if it&#8217;s handy for someone, cool!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2022/11/08/whats-that-magic-computation-in-stb__refineblock/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>On AlphaTensorâ€™s new matrix multiplication algorithms</title>
		<link>https://fgiesen.wordpress.com/2022/10/06/on-alphatensors-new-matrix-multiplication-algorithms/</link>
					<comments>https://fgiesen.wordpress.com/2022/10/06/on-alphatensors-new-matrix-multiplication-algorithms/#respond</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Fri, 07 Oct 2022 03:33:31 +0000</pubDate>
				<category><![CDATA[Maths]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7367</guid>

					<description><![CDATA[Two acquaintances independently asked about this today, so it seems worth a write-up: recently (as of this writing), DeepMind published a new paper about a new practical fast matrix multiplication algorithm, along with a press release that is a bit misleading and seems to have led to some confusion already. In particular, while the paper [&#8230;]]]></description>
										<content:encoded><![CDATA[
<p>Two acquaintances independently asked about this today, so it seems worth a write-up: recently (as of this writing), DeepMind published a new <a href="https://www.nature.com/articles/s41586-022-05172-4">paper</a> about a new practical fast matrix multiplication algorithm, along with a <a href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor?utm_campaign=AlphaTensor&amp;utm_medium=bitly&amp;utm_source=Twitter+Organic">press release</a> that is a bit misleading and seems to have led to some confusion already.</p>



<p>In particular, while the paper does introduce many new &#8220;practical&#8221; (not scare quotes, I&#8217;m using the word in a specific sense here that I&#8217;ll make more precise in a minute) fast small-matrix multiplication algorithms, that does not mean that you should replace your small-matrix library routines for 3&#215;3 or 4&#215;4 matrix multiplication with them. That&#8217;s not actually what they&#8217;re meant for, and they wouldn&#8217;t be good at it.</p>



<p>If you just want the executive summary, here it is: these are definitely interesting algorithms from an arithmetic complexity theory standpoint &#8211; especially for the case of 4&#215;4 matrices over finite fields, where (to the best of my knowledge) Strassen&#8217;s algorithm from 1969 was still the reigning champion. These algorithms are also <em>practically</em> relevant, meaning that not only do they have better asymptotic lower bounds than Strassen&#8217;s algorithm, they are still algorithms you might actually use in practice, unlike essentially everything else that has been written on the topic in the last 50 years: these algorithms are correct, and will in principle win over Strassen&#8217;s algorithm with large enough matrices, but that cut-off is well beyond the sizes that anyone is actually doing computations with.</p>



<p>And if you know what Strassen&#8217;s algorithm is, you might be in the market for the results from this paper, In fact, I&#8217;ll go one further and say that if you are currently using Strassen&#8217;s algorithm somewhere, you should definitely check the paper out. For the rest of you, I&#8217;ll try to give a very short summary of the topic and explain why actual small matrices are not really the intended use case.</p>



<h2>Strassen&#8217;s algorithm</h2>



<p>Regular matrix multiplication of 2&#215;2 matrices uses the standard &#8220;dot products of rows with columns&#8221; algorithm:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D+a_%7B11%7D+%26+a_%7B12%7D+%5C%5C+a_%7B21%7D+%26+a_%7B22%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+b_%7B11%7D+%26+b_%7B12%7D+%5C%5C+b_%7B21%7D+%26+b_%7B22%7D+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+a_%7B11%7D+b_%7B11%7D+%2B+a_%7B12%7D+b_%7B21%7D+%26+a_%7B11%7D+b_%7B12%7D+%2B+a_%7B12%7D+b_%7B22%7D+%5C%5C+a_%7B21%7D+b_%7B11%7D+%2B+a_%7B22%7D+b_%7B21%7D+%26+a_%7B21%7D+b_%7B12%7D+%2B+a_%7B22%7D+b_%7B22%7D+%5Cend%7Bpmatrix%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D+a_%7B11%7D+%26+a_%7B12%7D+%5C%5C+a_%7B21%7D+%26+a_%7B22%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+b_%7B11%7D+%26+b_%7B12%7D+%5C%5C+b_%7B21%7D+%26+b_%7B22%7D+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+a_%7B11%7D+b_%7B11%7D+%2B+a_%7B12%7D+b_%7B21%7D+%26+a_%7B11%7D+b_%7B12%7D+%2B+a_%7B12%7D+b_%7B22%7D+%5C%5C+a_%7B21%7D+b_%7B11%7D+%2B+a_%7B22%7D+b_%7B21%7D+%26+a_%7B21%7D+b_%7B12%7D+%2B+a_%7B22%7D+b_%7B22%7D+%5Cend%7Bpmatrix%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D+a_%7B11%7D+%26+a_%7B12%7D+%5C%5C+a_%7B21%7D+%26+a_%7B22%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+b_%7B11%7D+%26+b_%7B12%7D+%5C%5C+b_%7B21%7D+%26+b_%7B22%7D+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+a_%7B11%7D+b_%7B11%7D+%2B+a_%7B12%7D+b_%7B21%7D+%26+a_%7B11%7D+b_%7B12%7D+%2B+a_%7B12%7D+b_%7B22%7D+%5C%5C+a_%7B21%7D+b_%7B11%7D+%2B+a_%7B22%7D+b_%7B21%7D+%26+a_%7B21%7D+b_%7B12%7D+%2B+a_%7B22%7D+b_%7B22%7D+%5Cend%7Bpmatrix%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;begin{pmatrix} a_{11} &amp; a_{12} &#92;&#92; a_{21} &amp; a_{22} &#92;end{pmatrix} &#92;begin{pmatrix} b_{11} &amp; b_{12} &#92;&#92; b_{21} &amp; b_{22} &#92;end{pmatrix} = &#92;begin{pmatrix} a_{11} b_{11} + a_{12} b_{21} &amp; a_{11} b_{12} + a_{12} b_{22} &#92;&#92; a_{21} b_{11} + a_{22} b_{21} &amp; a_{21} b_{12} + a_{22} b_{22} &#92;end{pmatrix}" class="latex" /></p>



<p>As written, this does 8 multiplications and 4 additions. In 1969, Volker Strassen discovered an algorithm for this that <a href="https://en.wikipedia.org/wiki/Strassen_algorithm#Algorithm">only uses 7 multiplications </a>but 18 additions (follow the link for more details, I won&#8217;t go into it here); Winograd later presented a variant that only uses 15 additions. This is interesting in principle if multiplications are much more expensive than additions, something which is true in some settings but does not commonly apply to hardware floating-point math these days. Hardware floating-point now commonly implements <em>fused multiply-add</em> (FMA) instructions, and the 2&#215;2 matrix multiplication above can be implemented using 4 regular multiplications, 4 fused multiply-adds, and no separate additions at all. Moreover, Strassen&#8217;s algorithm has some numerical stability issues when used with floating-point (these concerns do not exist when it&#8217;s used for finite field arithmetic, though!) that means it also must be used carefully. What this means is that you would never actually consider using Strassen&#8217;s algorithm on 2&#215;2 matrices, and that is in fact not how it&#8217;s normally presented.</p>



<p>The form of Strassen&#8217;s algorithm that is of practical interest works not with 2&#215;2 matrices, but with 2&#215;2 <a href="https://en.wikipedia.org/wiki/Block_matrix"><em>block matrices</em>,</a> that is, 2&#215;2 matrices whose elements are themselves matrices. Matrix multiplication has a very regular structure that looks exactly the same when multiplying block matrices as it does when multiplying matrices of scalars, you just need to be careful about operand order for multiplications because matrix multiplications, unlike multiplications in the scalar ring or field, are not commutative:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D+A_%7B11%7D+%26+A_%7B12%7D+%5C%5C+A_%7B21%7D+%26+A_%7B22%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+B_%7B11%7D+%26+B_%7B12%7D+%5C%5C+B_%7B21%7D+%26+B_%7B22%7D+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+A_%7B11%7D+B_%7B11%7D+%2B+A_%7B12%7D+B_%7B21%7D+%26+A_%7B11%7D+B_%7B12%7D+%2B+A_%7B12%7D+B_%7B22%7D+%5C%5C+A_%7B21%7D+B_%7B11%7D+%2B+A_%7B22%7D+B_%7B21%7D+%26+A_%7B21%7D+B_%7B12%7D+%2B+A_%7B22%7D+B_%7B22%7D+%5Cend%7Bpmatrix%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D+A_%7B11%7D+%26+A_%7B12%7D+%5C%5C+A_%7B21%7D+%26+A_%7B22%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+B_%7B11%7D+%26+B_%7B12%7D+%5C%5C+B_%7B21%7D+%26+B_%7B22%7D+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+A_%7B11%7D+B_%7B11%7D+%2B+A_%7B12%7D+B_%7B21%7D+%26+A_%7B11%7D+B_%7B12%7D+%2B+A_%7B12%7D+B_%7B22%7D+%5C%5C+A_%7B21%7D+B_%7B11%7D+%2B+A_%7B22%7D+B_%7B21%7D+%26+A_%7B21%7D+B_%7B12%7D+%2B+A_%7B22%7D+B_%7B22%7D+%5Cend%7Bpmatrix%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D+A_%7B11%7D+%26+A_%7B12%7D+%5C%5C+A_%7B21%7D+%26+A_%7B22%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+B_%7B11%7D+%26+B_%7B12%7D+%5C%5C+B_%7B21%7D+%26+B_%7B22%7D+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+A_%7B11%7D+B_%7B11%7D+%2B+A_%7B12%7D+B_%7B21%7D+%26+A_%7B11%7D+B_%7B12%7D+%2B+A_%7B12%7D+B_%7B22%7D+%5C%5C+A_%7B21%7D+B_%7B11%7D+%2B+A_%7B22%7D+B_%7B21%7D+%26+A_%7B21%7D+B_%7B12%7D+%2B+A_%7B22%7D+B_%7B22%7D+%5Cend%7Bpmatrix%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;begin{pmatrix} A_{11} &amp; A_{12} &#92;&#92; A_{21} &amp; A_{22} &#92;end{pmatrix} &#92;begin{pmatrix} B_{11} &amp; B_{12} &#92;&#92; B_{21} &amp; B_{22} &#92;end{pmatrix} = &#92;begin{pmatrix} A_{11} B_{11} + A_{12} B_{21} &amp; A_{11} B_{12} + A_{12} B_{22} &#92;&#92; A_{21} B_{11} + A_{22} B_{21} &amp; A_{21} B_{12} + A_{22} B_{22} &#92;end{pmatrix}" class="latex" /></p>



<p>This looks identical to what we had before (except it&#8217;s now all upper case), but the operations mean something different: before we were multiplying scalars with each other, so we had something like individual real number multiplications (or floating-point multiplications in actual numerical code), now the products are matrix-matrix products (which are O(NÂ³) operations when multiplying two square NxN matrices using the standard algorithm, a mix of multiplications and additions or possibly fused-multiply-adds) and the sums are matrix-matrix sums (which are O(NÂ²) additions). And because what we&#8217;re describing here is a matrix multiplication algorithm to begin with, the smaller sub-matrix multiplications can also use Strassen&#8217;s algorithm if they want to.</p>



<p>In short, not only does each multiplication and addition in this block matrix represent many underlying scalar operations, but the relative cost of multiplications compared to additions keeps growing as N (the size of the blocks in our block matrix) increases. And that&#8217;s where Strassen&#8217;s algorithm is actually used in practice: eventually, once N becomes large enough, the many extra additions and irregular structure become worthwhile. It&#8217;s important to note that arithmetic operation count is not the only factor here: the extremely regular structure of conventional matrix multiplication, and the ease with which it can use FMAs, means that the arithmetic operations in a tuned matrix multiplication kernel are a lot cheaper than you might expect, because these kernels tend to do an extremely good job of keeping the machine busy. With small matrices (and actual 4&#215;4 matrices definitely fit that description), other overheads dominate. Somewhat larger blocks are mostly limited by memory subsystem effects and carefully manage their footprint in the various cache levels and TLBs, which tends to include techniques such as extra memory copying and packing stages that might seem like a waste because they&#8217;re not spamming floating-point math, but are worth the cost because they make everything else go faster. With much larger blocks, eventually Strassen can become attractive, although the actual cut-off varies wildly between architectures. I&#8217;ve seen reports of Strassen multiplication becoming useful for blocks as small as 128&#215;128, but more usually, it&#8217;s used for blocks that are at least 512&#215;512 elements, if not much more. All this assuming that its less-than-ideal numerical properties are not a show-stopper for the given application.</p>



<h2>AlphaTensor</h2>



<p>That brings us back to AlphaTensor. What DeepMind has implemented is, in essence, a variant of the neural-net-guided Monte Carlo Tree Search they&#8217;ve been using with great success to build Chess and Go AIs. This time, they use it to search for small-matrix multiplication algorithms. I&#8217;m not the right person to go into the details here, and it&#8217;s not important for the purposes of this post. We can just treat this procedure as a black-box optimizer that can do a guided search for matrix-multiplication algorithms meeting a user-specified set of criteria.</p>



<p>One obvious criterion would be optimizing for minimum multiplication count, and in fact one of the discoveries reported is a &#8220;Strassen-like&#8221; factorization that uses 47 multiplications to multiply two 4&#215;4 matrices (compared to 7*7=49 multiplications for two nested applications of Strassen&#8217;s 2&#215;2 algorithm, or 64 multiplications for the direct algorithm). And separate from the more theoretical concern of minimum operation count for a &#8220;practical&#8221; algorithm, the same optimizer can also incorporate measured runtimes on actual devices into its operation, and thus be used to perform a guided search for algorithms that are fast on certain devices.</p>



<p>That&#8217;s the process used to yield <a href="https://www.nature.com/articles/s41586-022-05172-4/figures/5">figure 5</a> in the paper, which reports speed-ups of optimized 4&#215;4 matrix multiplication factorizations against the baseline (which is the regular algorithm). Note that what this does is one high-level 4&#215;4 block matrix multiply using the algorithm in question at the top level; all the smaller lower-level matrix multiplies use regular matrix multiplication. Also note that the reported matrix sizes start at 8192&#215;8192, i.e. in this case, the 2048&#215;2048-element blocks are multiplied regularly.</p>



<p>Furthermore, note that the reported speed-ups that the press release refers to as &#8220;10-20%&#8221; are compared to the baseline of using regular matrix multiplication, not against Strassen (in this case, &#8220;Strassen-square&#8221;, the aforementioned 4&#215;4 factorization obtained by applying Strassen&#8217;s algorithm twice to a 4&#215;4 matrix). Strassen results are reported in the figure as well, they&#8217;re the red bars.</p>



<p>In short, the new algorithms are a sizeable improvement, especially on the TPU, but the perspective here is important: this type of algorithm is interesting when individual multiplications are quite expensive, in this case because they are actually multiplications of fairly large matrices themselves (2048&#215;2048 blocks are nothing to sneeze at).</p>



<p>If you&#8217;re actually multiplying 4&#215;4 matrices of scalars, the standard algorithm remains the way to go, and is likely to stay that way for the foreseeable future.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2022/10/06/on-alphatensors-new-matrix-multiplication-algorithms/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>Morton codes addendum</title>
		<link>https://fgiesen.wordpress.com/2022/09/09/morton-codes-addendum/</link>
					<comments>https://fgiesen.wordpress.com/2022/09/09/morton-codes-addendum/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Fri, 09 Sep 2022 21:25:17 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7353</guid>

					<description><![CDATA[I wrote about Morton codes long ago, and I see that post and the accompanying code referenced frequently, but there&#8217;s a few points I want to make about it. First, if you&#8217;re on a x86 CPU with BMI2 support, you have access to PDEP and PEXT, which make Morton encoding/decoding trivial. 2D Morton encoding one [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>I wrote about Morton codes <a href="https://fgiesen.wordpress.com/2009/12/13/decoding-morton-codes/">long ago</a>, and I see that post and the accompanying code referenced frequently, but there&#8217;s a few points I want to make about it.</p>
<p>First, if you&#8217;re on a x86 CPU with BMI2 support, you have access to <code>PDEP</code> and <code>PEXT</code>, which make Morton encoding/decoding trivial. 2D Morton encoding one 16-bit coordinate to a 32-bit word is a single 32-bit <code>PDEP</code> with <code>0x55555555</code>, decoding it is a <code>PEXT</code> with the same mask, and you can also use the same thing to encode 64-bit values with appropriate longer masks extended in the obvious way. The only fly in the ointment is that earlier AMD Zen CPUs technically support <code>PDEP</code> and <code>PEXT</code> but with an incredibly slow implementation, so this is not a good implementation if your target HW includes those CPUs. That said on Zen 3 and later the two instructions are fine, and ARM has added an equivalent to their vector pipes with SVE2, so while a bit iffy right now, I expect this might be the method of choice eventually, for CPUs anyway. I&#8217;ve been seeing this a fair amount in GPU shaders too, though, which brings me to the next subject for today.</p>
<p>The methods presented in the original post assume that the Morton-coded index value is 32 bits. I did not point this out explicitly in the text back in 2009, but you can either add another pass to produce a 64-bit result, or remove passes if you don&#8217;t need full pairs of 16-bit coordinates (or triples of 10-bit coordinates) interleaved into one 32-bit value.</p>
<p>For example, I gave this code for Morton-encoding a pair of unsigned 16-bit values to form a 32-bit index: (Dropping the comments that detail the bit layout after each step because they cause formatting issues)</p>
<pre>// "Insert" a 0 bit after each of the 16 low bits of x
uint32_t Part1By1(uint32_t x)
{
  x &amp;= 0x0000ffff;
  x = (x ^ (x &lt;&lt;  8)) &amp; 0x00ff00ff;
  x = (x ^ (x &lt;&lt;  4)) &amp; 0x0f0f0f0f;
  x = (x ^ (x &lt;&lt;  2)) &amp; 0x33333333;
  x = (x ^ (x &lt;&lt;  1)) &amp; 0x55555555;
  return x;
}

uint32 EncodeMorton2(uint32_t x, uint32_t y)
{
  return (Part1By1(y) &lt;&lt; 1) + Part1By1(x);
}</pre>
<p>If we only care about 8 bits worth of x and y, that first shift-and-xor pass (which tries to move the high 8 bits of <code>x</code> into place) is useless and we can just skip it:</p>
<pre>// "Insert" a 0 bit after each of the 16 low bits of x
uint32_t Part1By1_8b(uint32_t x)
{
  x &amp;= 0xff; // 8 bits only this time!
  x = (x ^ (x &lt;&lt;  4)) &amp; 0x0f0f0f0f; // 0x0f0f would suffice
  x = (x ^ (x &lt;&lt;  2)) &amp; 0x33333333; // 0x3333 would suffice
  x = (x ^ (x &lt;&lt;  1)) &amp; 0x55555555; // 0x5555 would suffice
  return x;
}

// Only uses low 8 bits of x, y
uint32 EncodeMorton2_8b(uint32_t x, uint32_t y)
{
  return (Part1By1_8b(y) &lt;&lt; 1) + Part1By1_8b(x);
}</pre>
<p>Lastly, if you&#8217;re encoding or decoding multiple values, which is extremely common because after all the whole point is to turn a pair (or triple) of values into a single integer, and the values are sufficiently narrow, we can handle x and y at once, by packing them into a suitable value before we do the rest of the computation.</p>
<p>For example, the case above where we only have 8-bit x and y coordinates (not that uncommon, say within a tile) can do a variant of the trick above to handle the full pair at only slightly more work than just handling a single coordinate in the original code would be:</p>
<pre>uint32_t EncodeMorton2_8b_better(uint32_t x, uint32_t y)
{
  // Pack x and y into t, starting at bits 0 and 16 respectively
  uint32_t t = (x &amp; 0xff) | ((y &amp; 0xff) &lt;&lt; 16);

  // Do the full interleave as above (this time the full
  // 32-bit masks are necessary)
  t = (t ^ (t &lt;&lt;  4)) &amp; 0x0f0f0f0f;
  t = (t ^ (t &lt;&lt;  2)) &amp; 0x33333333;
  t = (t ^ (t &lt;&lt;  1)) &amp; 0x55555555;

  // Merge the halves.
  // All the odd bits are clear, so instead of shifting t right by
  // 16 to get y and then shifting left by 1, we can just shift right
  // by 15.
  return (t &gt;&gt; 15) | (t &amp; 0xffff);
}
</pre>
<p>All in all is this is roughly the same cost of a single invocation of the original <code>Part1By1</code>. If you need a full 16-bit x and y but have 64-bit registers available, you can get a 32-bit result using a 64-bit extension of the original code (you just need to expand the masks appropriately).</p>
<p>This technique is independent of how exactly you do the bit-(de-)interleaving, so you can in principle combine this with <code>PDEP</code> and <code>PEXT</code>. There&#8217;s not much point though, because if you have a good implementation then doing two <code>PDEP</code> with different masks and a final OR will be cheaper, and if you might encounter one of the bad implementations you just want to avoid these instructions entirely and use the above SIMD-within-a-register (SWAR) algorithms.</p>
<p>I&#8217;ve only showed 2D encoding here, the same trick works for decoding:</p>
<pre>pair&lt;uint32_t,uint32_t&gt; DecodeMorton2_8b(uint32_t code)
{
  // Separate even and odd bits to top and bottom half, respectively
  uint32_t t = (code &amp; 0x5555) | ((code &amp; 0xaaaa) &lt;&lt; 15);

  // Decode passes
  t = (t ^ (t &gt;&gt; 1)) &amp; 0x33333333;
  t = (t ^ (t &gt;&gt; 2)) &amp; 0x0f0f0f0f;
  t ^= t &gt;&gt; 4; // No final mask, we mask next anyway:

  // Return x and y
  return make_pair(t &amp; 0xff, (t &gt;&gt; 16) &amp; 0xff);
}
</pre>
<p>And of course the same idea works for the 3D variants as well.</p>
<p>In summary, To Whom It May Concern: 1. PDEP/PEXT and friends (when available and reliably good) make this trivial, 2. if you only need a few bits you can skip passes and make it cheaper, 3. the bit movement is completely regular so you can also pack multiple values ahead of time and get two (de)interleaves for the price of one.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2022/09/09/morton-codes-addendum/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>Entropy decoding in Oodle Data: x86-64 3-stream Huffman decoders</title>
		<link>https://fgiesen.wordpress.com/2022/09/05/entropy-decoding-in-oodle-data-x86-64-3-stream-huffman-decoders/</link>
					<comments>https://fgiesen.wordpress.com/2022/09/05/entropy-decoding-in-oodle-data-x86-64-3-stream-huffman-decoders/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Mon, 05 Sep 2022 09:10:56 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<category><![CDATA[Compression]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7283</guid>

					<description><![CDATA[The adventure continues! Last time I went over the core decoder loop we use on AMD Jaguar-based consoles, i.e. Xbox One and PS4. I felt that one was a good example to start with since the target hardware is fixed, behaves quite predictably, and the various features and limitations of the machine conspire to make [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>The adventure continues! <a href="https://fgiesen.wordpress.com/2022/04/04/entropy-decoding-in-oodle-data-huffman-decoding-on-the-jaguar/">Last time</a> I went over the core decoder loop we use on AMD Jaguar-based consoles, i.e. Xbox One and PS4. I felt that one was a good example to start with since the target hardware is fixed, behaves quite predictably, and the various features and limitations of the machine conspire to make the solution essentially canonical: the narrow CPU front-end makes it so that minimizing macro-ops issued is a significant concern, and the <code>BEXTR</code>-based decode is a very natural fit.</p>
<p>This time, our hardware target is &#8220;any 64-bit x86&#8221;, used on less specialized targets like Linux, Windows and Mac 64-bit x86. We don&#8217;t have a single CPU type we&#8217;re optimizing for; it needs to run on everything, and we&#8217;d like to have good performance on anything released within the last 10 years (at the time this code was written in 2016) or so. We do get to have variants for newer ISA extensions when they help but we&#8217;d like a good baseline version for &#8220;any 64-bit x86 CPU&#8221;.</p>
<p>As with the previous two parts, you should read the <a href="https://fgiesen.wordpress.com/2021/08/30/entropy-coding-in-oodle-data-huffman-coding/">introduction</a> to the Huffman decoders first, continue with the <a href="https://fgiesen.wordpress.com/2018/02/19/reading-bits-in-far-too-many-ways-part-1/">&#8220;Reading bits in far too many ways&#8221;</a> series on this blog (<a href="https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/">part 2</a>, <a href="https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/">part 3</a>) which is also a prerequisite, and if you can&#8217;t read and understand x86-64 assembly language this will be hard to follow.</p>
<h3>You know how this goes by now</h3>
<p>I&#8217;m not going into as much detail in this post as I did in the Jaguar one, both because our target this time is somewhat fuzzier and because we&#8217;re exploring variations on a theme here. It&#8217;s interesting to cover the key ideas, not so interesting to re-do variants of the same analysis many times in a row.</p>
<p>Our setting, then, is the same as it was last time: we have a Huffman code stream where the code lengths are constrained to be relatively short so we can do 5 in a row between refills, we use a 4KiB table with 2048 entries covering all possible 11-bit codewords, and decoding proceeds without having to do any significant data-dependent branches.<sup><a href="#fn1">1</a></sup> We know that decoding back-to-back values from the same bitstream is limited by critical path latency, so we use multiple bitstreams.</p>
<p>On the AMD Jaguar, 3 independent bitstreams was sufficient to get us out of the miserable latency-bound hole and into the more desirable throughput-bound regime where we&#8217;re doing a decent job keeping the execution units busy. On the wider out-of-order cores that can sustain 4 or more instructions per cycle, 3 streams is not going to be enough for that, which is why we also have 6-stream variants that will be covered in a later part.<sup><a href="#fn2">2</a></sup></p>
<p>Once again, I&#8217;ll go over this from the inside out: first figure out how to decode individual bytes, then sort out the refill strategy, then deal with the remaining plumbing.</p>
<h3>The basic decode step</h3>
<p>Our game plan hasn&#8217;t changed: peek ahead at the next 11 bits, do our table lookup, consume the right number of bits, emit the resulting byte. The most basic version of this, assuming we keep an explicit &#8220;remaining bits&#8221; count per stream, works out to something like this:</p>
<pre>  ; peek at low 11 bits
  mov rcx, rBits
  and rcx, 2047
  ; table fetch
  movzx ecx, word [rTablePtr + rcx*2]
  ; shift out consumed bits, update bit count
  shr rBits, cl
  sub rBitCount, cl
  ; emit decoded byte
  shr ecx, 8
  mov [rDest + &lt;index&gt;], cl
</pre>
<p>So far, there should be no surprises here.<sup><a href="#fn3">3</a></sup> As given, this works out to 7 instructions per byte decoded, quite a lot more than the 3 we had in the Jaguar version, but this time we don&#8217;t have guaranteed access to BMI instructions &#8211; and even if we did, they would not be as good as they are on Jaguar.</p>
<h3>Two more small tweaks</h3>
<p>Part of x86&#8217;s historical baggage is that bits 8 through 15 inclusive of the a, b, c and d registers can be accessed as their own register, and this can be used on the <em>emit</em> sequence to save an instruction, getting us down to 6:</p>
<pre>  mov [rDest + &lt;index&gt;], ch
</pre>
<p>Due to the way the x86-64 instruction encoding works out, <code>rDest</code> in this code can&#8217;t be just anything. It has to be from a limited set of registers<sup><a href="#fn4">4</a></sup> but other than that, we&#8217;re still not doing anything special here.</p>
<p>Note that we want to be using <code>ECX</code> as the register we load the table entry into because the variable-distance shifts in basic x86-64 all require the shift amount to live in <code>CL</code>.<sup><a href="#fn5">5</a></sup> We don&#8217;t particularly care what register name we use for our scratch temporary; RCX/ECX is as good as any other. Nevertheless, using RCX (and only RCX) as the temporary to load table entries into is <em>clearly</em> the right choice here, but compilers would frequently insert extra moves (sometimes more than one extra on the critical path) here, which is how we ended up hand-writing these loops in assembly to begin with.<sup><a href="#fn6">6</a></sup></p>
<p>The second fun wrinkle is that even though we can easily arrange for our shift count to always be in <code>CL</code> to begin with, on Intel CPUs with BMI2 available, we always want to use the new <code>SHLX</code> instruction instead, even when we&#8217;re shifting by the low bits of <code>RCX</code>, because <code>SHLX</code> takes 1 uop instead of 2 or 3 on newer Intel uArchs.<sup><a href="#fn7">7</a></sup></p>
<p>The final non-obvious change we do for the &#8220;real&#8221; version of the decode is that instead of the peek logic presented above, we actually do this instead:</p>
<pre>  mov ecx, 2047
  and rcx, rBits
</pre>
<p>Why load the immediate first using a separate instruction and then AND with <code>rBits</code>? Well, the important realization here is that moving an immediate value into <code>ecx/rcx</code> has no dependencies and can execute at any time, whereas <code>mov rcx, rBits</code> does depend on <code>rBits</code> and thus ends up becoming a part of the critical path. Now this register-register move is generally &#8220;free&#8221; on microarchitectures that can implement reg-reg moves via renaming at an effective 0-cycle latency, which on the Intel side is Ivy Bridge and later, but there are two important caveats: first, not every machine we&#8217;re targeting (or at least were targeting at the time this code was written, 6 years ago) is Ivy Bridge and later, and second, some later microarchitectures had subtle bugs in their implementation of <code>MOV</code> elimination and ended up disabling it later. Hence, as a general rule, the load-immediate-first version is preferable since it takes 1 cycle off the critical path for every decode when <code>MOV</code> elimination is not performed, for whatever reason.<sup><a href="#fn8">8</a></sup></p>
<h3>Refill</h3>
<p>As mentioned above, for this version we prefer the methods with explicit &#8220;remaining bits&#8221; counts, specifically what I described as variant 4 in my <a href="https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/">write-up on bit reading</a>.</p>
<p>The reason to prefer this approach here is that it keeps the load for the bit buffer refill independent of the final bit count, meaning that refill load&#8217;s latency does not necessarily end up on the critical path. Since the latency along the critical path is the primary concern for this loop, this is definitely what we want.</p>
<p>In the &#8220;production&#8221; version of the loop, we keep the remaining bit count for stream 0 in <code>al</code>, the refill pointer in <code>r8</code>, and the bit buffer for stream 0 in <code>r11</code>. The version we use is this (with the instructions scheduled slightly differently, but that&#8217;s extremely minor):</p>
<pre>  ; refill streams
  mov    rbp, [r8] ; next0 = load(in0)
  movzx  rcx, al   ; bitc0
  shl    rbp, cl   ; next0 &lt;&lt; bitc0; use shlx with BMI2+!
  or     r11, rbp  ; bits0 |= next0 &lt;&lt; bitc0
  shr    ecx, 3    ; 7 - bytes_consumed0
  sub    r8, rcx   ; in0 += bytes_consumed0 - 7
  or     al, 56    ; bitc0 |= 56
  add    r8, 7     ; in0 += 7 
</pre>
<p>That&#8217;s a fair number of instructions, but the dependency structure is quite favorable:</p>
<ul>
<li>The load of the next bits to refill only depends on <code>r8</code>, which is known as soon as the previous iterations&#8217; refill is complete (all the instructions that update <code>r8</code> are in the above code snippet!) and in particular is completely independent of the 5 table loads per stream.</li>
<li>Insertion of the new bits depends on the final bit count being known, but once the 3 instructions (<code>MOVZX</code>, <code>SHL</code>, <code>OR</code>, all pure int ALU) for that part of the dependency chain are done, we can start decoding the next byte.</li>
<li>Updating the bit count and the read pointers takes several more instructions, but neither of these are as critical because the bit count doesn&#8217;t determine any load offsets and the read pointer is only used for the next iterations&#8217; refill.</li>
</ul>
<p>Once again, using <code>SHLX</code> for the shift when available is preferred. In this case, in addition to the 2 uops saved, it also lets us remove the 1 cycle for the <code>MOVZX</code> from the critical path.</p>
<p>Beyond that, there&#8217;s a few more instructions in the loop to increment the destination pointer, maintain/test the &#8220;bytes to decode&#8221; count and check whether the input pointers have crossed, but all of this works the same as in the last part and is not on the critical part so I won&#8217;t spend time on it here.</p>
<h3>Analysis</h3>
<p>With all that said, what&#8217;s the expected cost of this decoder? Unlike the Jaguar example which targets a single known microarchitecture, this version of the loop targets a general PC target and thus should be good for numerous uArch generations from multiple vendors over something like a 10-year window.</p>
<p>We have in fact done just that kind of testing, and there are some minor variants (mainly depending on whether BMI2 and thus <code>SHLX</code> is available, as noted above), but the details here get very repetitive and tedious. I don&#8217;t particularly feel like writing all of this down, nor would it make for interesting reading.</p>
<p>Instead, I&#8217;m going to take a single uArch as our proxy. I&#8217;ll be using Intel&#8217;s Skylake (SKL), which Intel essentially (in progressively tweaked versions) kept re-releasing in between 2015 and 2021, meaning there&#8217;s a nearly 7-year window of Intel consumer CPUs that are all essentially SKL spin-offs. In the notebook space we saw Ice Lake in late 2019 but for desktop, 2021&#8217;s Rocket Lake (the first &#8220;real&#8221; SKL successor for desktop) was neither well-received nor sold well, and was soon replaced by Alder Lake in late 2021.</p>
<p>In practice, this means that at the time of this writing, the mean, median and mode Intel desktop x86 that is at least a year or so old is some SKL variant; during that time AMD made major inroads into the desktop space with their Zen cores (which, fortunately for our analysis, have quite similar performance characteristics). Notebooks are a bit more varied, but if I&#8217;m going to look at only one x86 uArch, SKL seems like a solid choice.</p>
<p>As mentioned multiple times throughout this post already, we&#8217;re mainly concerned about latency here. As such, I won&#8217;t go into many uArch details here and just point out that the relevant cores are a lot wider than the Jaguars we looked at last time (namely, they support 4+ instructions decoded/dispatched/retired per cycle), have 4 integer ALUs, can (under the right conditions) support 2 loads and 1 store per cycle, up to 2 branches per cycle, and in general have just more of essentially everything compared to the small Jaguar cores we looked at last time. (They also typically clock at twice the frequency, or more.)</p>
<p>Ironically, precisely <em>because</em> these cores are so much wider, the analysis gets a lot simpler. Here&#8217;s the gist of it: the all important critical path between iterations for a single stream starts with the final bit count for the last iteration and then goes through the refill and 5 subsequent symbols decodes until the bit count for the current iteration is known.</p>
<p>In the code as presented above, that means we start with the <code>MOVZX</code>, <code>SHL</code> and <code>OR</code> from the refill (all basic ALU ops that are 1-cycle latency on pretty much every x86 core you can buy these days) until we know the new value of <code>rBits</code>, for 3 cycles refill latency (can be 2 if we use SHLX).</p>
<p>Then we have 5 subsequent decodes where we care about the latency until we have the updated (shifted) rBits. In the version with the hoisted immediate that uses <code>and rcx, rBits</code>, the 3 relevant instructions on that dependency chain are this AND, the table load, and then the shift by CL. The AND and shift are also 1-cycle latency. In the final iteration, we also need to know the final bit count; this is computed by another independent instruction (the <code>SUB</code>) that depends on the table load and is likely to issue at the same time as our shift, so it doesn&#8217;t change the latency estimate. Finally, the load is the longest-latency instruction. Now SKL has 4-cycle-latency loads for certain very restricted cases, if the address itself originated on the load unit (i.e. for pointer chasing), but for our table lookup, we have a complicated addressing mode and the index register originates on the ALU not the load unit, so 5-cycle load latency is what we get.</p>
<p>Everything else is not critical, so for now, we&#8217;re going to ignore it. What does that leave us with? For one of our 3 streams, the latency along that critical path ends up being 3 + 5Ã—(1 + 5 + 1) = 38 cycles.</p>
<p>Skylake can sustain 4+ fused-domain<sup><a href="#fn9">9</a></sup> uOps per cycle during those 38 cycles, giving us 38 Ã— 4 = 152 issue slots. One refill takes around 8 fused-domain uOps (with SHLX), each decode step takes 6. That means one stream&#8217;s worth of decodes takes around 8 + 5Ã—6 = 38 uops, so for 3 streams that&#8217;s 114 uops out of our budget, leaving another 38 issue slots remaining.</p>
<p>That leaves the work we haven&#8217;t accounted for so far: one of our streams needs to do a BSWAP (another 3 uops), we have the two pointer-crossing checks (2-4 uops depending on what macro-fusion happens), the destination pointer increment (1 uop), and the final loop condition/test (1-2 uops, again depending on macro-fusion). And that&#8217;s it. At worst, that takes another 10 uops out of our budget.</p>
<p>We&#8217;re left with about 124 issue slots used over 38 cycles, about 81% of what the machine can sustain. That leaves enough air in the schedule that we don&#8217;t need to be too worried about exactly what the schedulers and port load-balancing logic do; we really do expect to be mostly latency-limited still, but at least latency-limited with a decent uArch utilization. Finally, our instruction mix is balanced enough, and the machine has enough resources, that we needn&#8217;t worry too much about any particular type of resource (like say load units or shifters) getting overcrowded.</p>
<p>In short, the best we can expect out of the loop described above is to decode 15 bytes (5 bytes each from 3 streams) every 38 cycles, which works out to 2.53 cycles per byte. In practice, on my Skylake test machine, benchmarks hover around 41.8 cycles per 15 bytes (i.e. around 2.79 cycles/byte), both in the idealized test rig and on real-world data.</p>
<p>That&#8217;s approximately 10% above our rough lower-bound estimate, purely from operation latencies, assuming scheduling is perfect, there are no cache misses, and so forth. At this point, we could dig deeper and try to figure out where we&#8217;re losing these 3.8 cycles relative to our critical path estimate, and if there&#8217;s something we can do about it, but the writing&#8217;s already on the wall: with one stream, we would be utterly latency-limited with a mostly-empty machine. The 3 streams we&#8217;re using now get us to a more respectable level of uArch utilization, but while they were enough to get us into our target throughput-bound regime on Jaguar, they&#8217;re not enough, not even in a lower-bound estimate where there are never any delays due to cache misses or suboptimal uop scheduling.</p>
<h3>Discussion</h3>
<p>Really, this should not be a surprise. On a machine twice as wide, with many more execution resources, instructions per cycle, and 2 cycle longer load latency, <em>of course</em> we need more streams to hide those latencies.</p>
<p>However, we&#8217;re starting to bump into other problems. Our bit-IO method uses 3 registers worth of state per stream: the bits, the bit count, and the refill pointer. That&#8217;s 9 registers used. We also definitely need a stack pointer, a destination buffer pointer, at least one scratch register (<code>rcx</code>), and also a base pointer for our Huffman table and preferably also an &#8220;end-of-buffer&#8221; pointer (or, equivalently, a counter) in a register for the final loop test. That&#8217;s 14 registers spoken for, and x86-64 only has 16 GPRs.</p>
<p>Now, some of those registers don&#8217;t <em>need</em> to be live for most of the loop. The &#8220;end-of-buffer&#8221; pointer never changes and is only used once at the end so it&#8217;s not a big problem to leave in memory, and our refill pointers are only used during refill so we can spill some of them without wreaking havoc if we don&#8217;t mind the extra instructions to spill and reload them, but we&#8217;re getting on thin ice here.</p>
<p>Using more streams would be interesting, but we need to be careful not to introduce a lot of extra instructions. Potentially troublesome spills/refills introducing latency in the wrong places would suck as well. In short, we&#8217;d like a few more streams (just one extra is cutting it too close), but that in turn means 3 GPRs per stream is not going to work. We need something more compact, which leads us to a quite different design for the 6-stream decoders we added in Oodle 2.3.0. But that will have to wait for another post. Until then!</p>
<h3>Footnotes</h3>
<p><span id="fn1">[1] The &#8220;how many bytes are left&#8221; loop condition is just a counted loop and <em>extremely</em> predictable. The &#8220;did the pointers cross&#8221; check is technically data-dependent, but it is also extremely predictable (in the fast decoder loops anyway), because the decoder loops only run until our already-advanced pointers first cross, at which point we exit the fast loops and switch to the more careful variants.</span></p>
<p><span id="fn2">[2]</span> The trickiest part to negotiate when doing this experimentally is the transition from latency- to throughput-bound. As long as you&#8217;re latency-bound, your best move is to use strategies that minimize latency, even when doing so increases dynamic instruction count or utilization of limited execution resources. Once you have enough streams to cover the critical path latency with some slack left over, you need to worry about those bottlenecks instead, even when doing so  adds latency. Options that were clearly inferior in the latency-bound regime will end up preferable in the throughput-bound regime.</p>
<p><span id="fn3">[3]</span> When writing this code in C/C++, you would probably keep the remaining bit count for the bit buffer in a 32- or 64-bit integer, not a byte, but done naively that would result in an extra instruction to extract and zero-extend the &#8220;length&#8221; field from the low 8 bits of the table entry. Using a byte register avoids the extra instruction, or alternatively, you could subtract the full 16-bit table entry from the bit count and mask the final bit count down to 8 bits (effectively reducing modulo 256) before using it. The two approaches are equivalent; for this presentation I went with the byte register variant because it&#8217;s more straightforward.</p>
<p><span id="fn4">[4]</span> x86-64 was designed to share as much of the instruction encoding (and hence the decoder logic) between the 32-bit and 64-bit versions. 32-bit x86 has 8 general-purpose registers and 3-bit register numbers; x64-64 increases this to 16 GPRs (which take 4 bits), and the high bits of the up to 3 register numbers in an instruction are grouped together in what&#8217;s called a REX prefix byte. Only some instructions have a REX prefix: if all 3 register number high bits are zero and the instruction is not 64-bit either, it&#8217;s not required. However, the register numbering for 8-bit registers worked differently than it did for the other registers: for &#8220;classic&#8221; x86 code, the 8 8-bit registers are AL, CL, DL, BL, AH, CH, DH, and BH (in that order), where those last 4 are refer to [15:8] in the corresponding 32-bit register. With the REX prefix present, every 64-bit register has its low 8 bits addressable as a corresponding &#8220;L&#8221; register, but the &#8220;H&#8221; register names are inaccessible. Thus, if you want to store <code>CH</code> to <code>[rDest + immed]</code>, <code>rDest</code> needs to be one of the low 8 registers.</p>
<p><span id="fn5">[5]</span> Other (non-x86) architectures mostly don&#8217;t care about which architectural register the shift amount lives in, but almost everyone looks only at the low 5-8 bits (depending on architecture and context) of the shift count. This makes it so that arranging our (len,sym) pairs so that len is in the low byte (which for LE byte order means first byte) is almost universally the best choice.</p>
<p><span id="fn6">[6]</span> Hand-writing in ASM also made it practical to use the &#8220;store from CH&#8221; trick, which is not a huge deal in this particular version since there are other problems that we&#8217;ll get to in a minute, but this particular quirk of x86 is one that compilers generally won&#8217;t even try to use, even though it&#8217;s advantageous in cases like this one.</p>
<p><span id="fn7">[7]</span> This is one of my &#8220;favorite&#8221; x86 warts. The reason <code>SHL</code> by CL is awkward, and often more uOps than you would expect, is because the architectural definition of this (and other variable-shift instructions) is quite strange for backwards-compatibility reasons. Namely, all the shift-by-CL instructions set different flags (and set them in different ways) depending on whether the effective shift count ends up being 0, 1, or something else. This goes back to a problem with the original 8086. The 8086 didn&#8217;t have a barrel shifter or any other way to do fast distant shifts; &#8220;<code>SHL AX, CL</code>&#8221; existed but effectively just repeated <code>SHL AX, 1</code> <code>CL</code> times. This definition was presumably convenient given the <em>extremely</em> limited microcode ROM space the 8086 worked with but has the two unfortunate side effects that a) you could make shifts take a very long time on the 8086 by setting <code>CL</code> to 255 and b) when CL is 0, no shifting takes place at all, and in particular the flags stay unchanged from before. Moreover, there&#8217;s another wrinkle in that <code>SHL AX, 1</code> sets the overflow flag &#8220;correctly&#8221; (probably mostly by accident, because for left-shifts specifically the natural implementation of it on a 8086-like ALU data path is <code>ADD AX, AX</code>, which does, and right-shifts can&#8217;t overflow), but when doing an iterated sequence of such shifts, the final overflow flag will indicate whether the last shift-by-1 overflowed, which is useless when the count is not 1, because for a &#8220;shift by 3&#8221; operation what you really want to know is whether there was an overflow at any point during the process, not whether the last instruction in a lengthy expansion overflowed (which is neither here nor there). The tightened definition that appears in the original 80286 manuals specifies that only the last 5 bits of the shift count matter (no doubt anticipating the coming expansion to 32-bit with the 80386) and further says that 1-bit-shifts set the overflow flag correctly but multi-bit shifts leave the overflow flags in an undefined state (not mentioned in these early manuals is that an effective shift count of 0 leaves the flags unchanged, but all x86 CPUs I&#8217;m aware of nevertheless behave this way). Long story short, the actual data portion of x86 <code>SHL reg, CL</code> is exactly what you would expect, but which flags get updated and how is a tangled mess that, on most Intel CPUs, takes another 1 or 2 uops to resolve. Interestingly, no AMD uArchs I&#8217;m aware of, present or historical, have a problem with it.</p>
<p><span id="fn8">[8]</span> Another option available on machines with BMI1 or later is to use <code>ANDN</code> to perform our AND operation. The idea is to put the ones&#8217; complement of our desired mask (so <code>~2047</code>) into a scratch register and then use <code>andn rcx, rInvertedMask, rBits</code> to initialize <code>rcx</code>. In short, we don&#8217;t actually care about the &#8220;not&#8221; part of the <code>ANDN</code>, but it conveniently comes as a non-destructive 3-operand instruction that we can use to avoid the move. This does the job in one instruction, which is great, but requires us to sacrifice another register, which is not so great. In this particular code it&#8217;s not really worthwhile since we&#8217;re entirely latency-limited on sufficiently wide cores, so having a <code>MOV</code>/<code>AND</code> pair is not a problem as long as we don&#8217;t extend the critical path unnecessarily.</p>
<p><span id="fn9">[9]</span> I won&#8217;t go into what exactly fused-domain uOps are; it&#8217;s there for precision, but if you don&#8217;t know what that means, just ignore it, it&#8217;s way beyond the scope of what we need for this post.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2022/09/05/entropy-decoding-in-oodle-data-x86-64-3-stream-huffman-decoders/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>Entropy decoding in Oodle Data: Huffman decoding on the Jaguar</title>
		<link>https://fgiesen.wordpress.com/2022/04/04/entropy-decoding-in-oodle-data-huffman-decoding-on-the-jaguar/</link>
					<comments>https://fgiesen.wordpress.com/2022/04/04/entropy-decoding-in-oodle-data-huffman-decoding-on-the-jaguar/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Mon, 04 Apr 2022 09:23:18 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<category><![CDATA[Compression]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7237</guid>

					<description><![CDATA[In the last part we went over the general ideas of Huffman coding as implemented in the newer Oodle Data coders, this time we&#8217;ll be looking at one particular implementation that is both interesting and &#8220;historically relevant&#8221;: Oodle was designed with games in mind, an important class of hardware to consider for game middleware is [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>In the last part we went over the general ideas of Huffman coding as implemented in the newer Oodle Data coders, this time we&#8217;ll be looking at one particular implementation that is both interesting and &#8220;historically relevant&#8221;: Oodle was designed with games in mind, an important class of hardware to consider for game middleware is game consoles, and versions of the AMD Jaguar CPU were in both the PS4 and Xbox One (mostly unmodified except for a bump in clock rate in the &#8220;mid-lifecycle upgrade&#8221; models of both). We wanted Kraken to perform well on those machines, so we spent some time optimizing Oodle for it. Before I go into the details, let&#8217;s do a bit of background on the machine itself, but be advised that this will be in-depth and that you may need to re-read the <a href="https://fgiesen.wordpress.com/2021/08/30/entropy-coding-in-oodle-data-huffman-coding/">previous part</a> first. Furthermore, this post contains plenty of x86 assembly; if you&#8217;re uncomfortable or unfamiliar with that, you probably won&#8217;t get much out of it, sorry.</p>
<h3>Meet the AMD Jaguar</h3>
<p>The Jaguar, or less prosaically &#8220;Family 16h&#8221;, is a small, low-power, out-of-order 64-bit x86 CPU core designed for small systems and embedded applications such as, well, game consoles. In the game console variants, the Jaguar CPUs appear on the main SoC along with the GPU (and most other components). It&#8217;s designed for multi-core operation and cores usually appear in clusters of four that share a common L2 cache, usually 512kb of L2 per core. In the Xbox/PS4 versions, there are two such clusters and thus two L2 cache slices. This post is only concerned with tasks that run on a single thread so I won&#8217;t be spending time on this part of the architecture.</p>
<p>Each core has 32KiB of L1 instruction and 32KiB of L1 data cache. The frontend decode/dispatch/retire logic is 2 instructions wide, and the relevant unit for most of it is what is variously (depending on the source) called &#8220;macro-ops&#8221; or &#8220;cops&#8221; (complex ops), depending on the source. I&#8217;ll stick with macro-ops. Macro-ops are typically a data-processing instruction along with a memory reference, so something like the x86 instruction <code>add rax, [rsi]</code> would be a single macro-op.<sup><a href="#fn1">1</a></sup> Macro-ops get broken into either one or two micro-ops (I&#8217;ll write uops in the following) for execution, but instruction decoding, tracking and retirement all works on macro-ops. The backend has six execution units, each of which can accept one micro-op per cycle: two integer (which I&#8217;ll refer to as I0 and I1), one load (L), one store (S), and two SIMD/floating point (which I&#8217;ll refer to as F0 and F1). The pipelines are very symmetric: almost all integer instructions can execute in either I0 and I1 (the biggest exceptions being multiplies and divides, which are I1 only), and most FP/SIMD instructions can execute in either F0 or F1 (FP addition and SIMD integer multiplication are only supported in F0, and FP multiplies and store/convert are F1 only). Consequently, most pairings of two independent instructions can execute in the same cycle, if they&#8217;re not both contending for the same resource. Of these backend limitations, in my experience the one you&#8217;re most likely to hit is the one load per cycle limit.</p>
<p>That said, whenever I&#8217;ve looked, the two instructions per cycle decode/dispatch limit is usually the more relevant one. On the Jaguars, using the load-operate and even read-modify-write instructions where possible is a good idea (because it gives you two uops per macro-op), and generally preferable to splitting loads out.</p>
<p>Speaking of loads, the L1 data cache is 8-way associative, write-back, and internally splits 64-byte cache lines into 16-byte sectors. Unaligned loads and stores that stay within a single sector are free, crossing a sector boundary occupies the load/store pipes for an extra cycle (potentially more if it also crosses a page boundary etc.). The load-to-use latency for the L1 data cache is 3 cycles to the integer pipes, 5 cycles to the FP/SIMD pipes, both of which are quite low numbers compared to most of its contemporaries.<sup><a href="#fn2">2</a></sup> The theme of low latencies continues for other parts of the backend: FP32 multiplies and SIMD integer multiplies complete in 2 clock cycles, FP32 and FP64 adds in 3, and most SIMD ALU operations take a single cycle. L2 misses take relatively long though, at a minimum load-to-use latency of 25 cycles.</p>
<p>Lots of console developers found these cores underwhelming, mostly due to the narrow design and fairly low clock rates (around 1.6 and 1.7GHz in the original PS4 and Xbox One, respectively). On the other hand, these cores are quite small, power-efficient, and the PS4/Xb1 console generation came with 8 of them, at a time when more than 4 cores was a rarity in the consumer space. Personally, I quite like them: they&#8217;re not the fastest but what they are is extremely even-tempered and predictable. They have a relatively low ceiling on the instructions per cycle and peak performance they can achieve, but getting there is generally a fairly straightforward process, and there&#8217;s not much in the way of gotchas or nasty surprises. They&#8217;re a pleasant core to optimize for<sup><a href="#fn3">3</a></sup>, and AMD helped by providing <a href="http://support.amd.com/TechDocs/52128_16h_Software_Opt_Guide.zip">good documentation</a> for it.</p>
<h3>The Plan</h3>
<p>Because of the aforementioned decode/dispatch/retire limits and low instruction latencies, optimizing code with reasonably nice memory access patterns for the Jaguars is, more often than not, an exercise in minimizing number of instructions executed for a given task. (As I said, they&#8217;re fairly straightforward to optimize for!) Therefore, if we want a fast Huffman decoder on these machines, it&#8217;s a good idea to see if we can do it with as few instructions as possible.</p>
<p>While reviewing the above-quoted docs, one thing I noticed was that <code>BEXTR</code>, an instruction from BMI1, turns into one uop, is supported on both integer pipes, and has 1-cycle latency. <code>BEXTR</code> is an odd duck: it extracts a given number of bits from a given starting point in the first source operand, and as such is essentially a counterpart to PowerPCs <code>rlwinm</code> or ARMs <code>UBFM</code>, but while these latter two instructions have the bitfield position and width given as an immediate operand, <code>BEXTR</code> takes a register operand for the bitfield specification.<sup><a href="#fn4">4</a></sup> Code that wants to do repeated bitfield extraction with the same operands can burn a register on a constant (itself a fairly steep cost on the relatively register-starved x86) and then use <code>BEXTR</code>, which replaces a move, shift, and an bitwise AND instruction.<sup><a href="#fn5">5</a></sup> The second source register operand to <code>BEXTR</code> contains, itself, bit-packed values: the lower 8 bits give the index of the LSB of the bitfield to extract, the next 8 bits give the width in bits.</p>
<p>This is usable for the bitstream decoding part of our Huffman decoder. Using a &#8220;bit extraction&#8221; style decoder (variant 3 in <a href="https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/">this post</a>) means we repeatedly do operations of the form <code>(bit_buf &gt;&gt; bit_pos) &amp; ((1 &lt;&lt; 11) - 1))</code> to peek at our next 11 bits, and that is just <code>BEXTR(bit_buf, bit_pos + (11 &lt;&lt; 8))</code>. It doesn&#8217;t cause any problems to have a constant bias that shows up only in the high bytes added to our bit position, so we can just declare our bit positions to have that offset added at all times while in registers, and that lets us do our bit buffer peek in a single 1-uop instruction on Jaguar cores. Because of another x86 quirk, namely that byte-sized instructions exist and preserve the remaining bits of the register, we can do updates of bit_pos using byte-sized additions or subtractions that leave the high bits alone, if we want to.<sup><a href="#fn6">6</a></sup></p>
<p>Finally, we don&#8217;t want to do a store for every byte we decode, because that&#8217;s an extra instruction and we&#8217;re easily limited by instructions (or rather, macro-ops) executed. Fortunately we can use the SSE4.1 instruction <code>PINSRB</code> (packed insert byte), which inserts a byte value from an integer register or memory into a given lane of a vector register. Vector registers hold 128 bits (16 bytes), which means we can amortize the number of stores and do one every 16 or so bytes instead of for every codeword. Finally, because Jaguar cores treat memory references inside an instruction as separate uops but not separate macro-ops, and macro-ops are one of our main limiters, we want to use memory references liberally if doing so lets us reduce the number of macro-ops we need.</p>
<p>Putting this all together, note that the pseudocode for the per-symbol processing in a LSB-first Huffman decoder, as outlined in the previous part, looks something like this:</p>
<pre>  // peek
  uint32_t bits = (bit_buf &gt;&gt; bit_pos) &amp; 2047;
  // consume bits
  bit_pos += table[bits].len;  
  // decode symbol
  emit(table[bits].sym);
</pre>
<p>and using the various techniques outlined above, we can turn this into a mere 3 x86 instructions:</p>
<pre>  ; peek. rBitPos[15:8] = 11
  bextr   rBits, rBitBuf, rBitPos
  ; advance bit offset (update low byte only)
  add     rBitPosb, [rTableBase + rBits*2]
  ; put table[bits].sym at position N into xmm0
  vpinsrb xmm0, xmm0, [rTableBase + rBits*2 + 1], N
</pre>
<p>On the Jaguar, this decomposes into 3 macro-ops and 5 uops: 2 loads, 2 integer ops, 1 SIMD. The bit extract to grab rBits from rBitBuf takes a single cycle; the bit position update takes 3 cycles to load the value from the table and an extra cycle to complete the addition. We don&#8217;t actually care about the top bytes being preserved here, since we don&#8217;t expect overflows, but we do care about our load-operand being byte-sized. Either way, that&#8217;s 5 cycles critical path latency from one decoded symbol to the next. Finally, the vector byte inserts to collect the output bytes are not on the critical path. They need to be fast enough to keep up with our decoding bytes once the table loads finish (and they are, single they can complete at a rate of 1 per cycle) but that&#8217;s about it. With the Jaguar frontend supporting at most 2 macro-ops per cycle, this code takes at least 1.5 cycles per symbol decoded in the frontend, and 2 cycles per symbol decoded in the load pipeline. Meaning that as given, this code is limited more by the load pipeline than the frontend. However, this is not the only work that needs to happen in this loop, and the Jaguar is out-of-order, so we can build up a backlog of load pipeline work; if we later need to do more integer work in the loop that does not take many loads (spoiler: we will), the load pipeline will get to catch up.</p>
<p>Finally, as mentioned above, our critical path between back-to-back loads from the same stream is 5 cycles on the Jaguar. If we use 3 streams and interleave their processing during decode, then the frontend will get around to the first instruction for the second byte of stream 0 about 4.5 cycles in (although the load pipeline will take about 6 cycles to work through its backlog before then). In other words, the timing here can roughly work out, but it&#8217;s not perfectly matched; we will build up a bit of backlog in the load pipeline and the reorder buffer before this is done, but as long as we choose our instructions carefully and don&#8217;t go too lopsided, we can make this work while keeping the core nice and busy the whole time through.</p>
<p>I was pretty excited when I first realized this 3-instruction sequence was a viable candidate for the core of our Huffman decoder on Jaguar, but to get a real decoder we also need to deal with bit buffer refills, pointer advancing, and end-of-buffer checks.</p>
<h3>The actual decoder</h3>
<p>As mentioned above (and in the previous part), we use 3 separate bitstreams for parallelism. Of these, two bitstreams are regular &#8220;forward&#8221; bitstreams in increasing address order, and one is written backwards. The numbering of these is a bit odd: in the physical Oodle format, the layout is <code>strm0-&gt; | strm2-&gt; | &lt;-strm1</code>, i.e. stream 0 is forward and comes first (as you would expect), stream 1 is backward and comes last, and stream 2 is also forward and somewhat awkwardly sandwiched in the middle, for &#8220;historical reasons&#8221;. Namely, Kraken uses forward-backward stream pairs in many places.<sup><a href="#fn7">7</a></sup> The Huffman decoder used to be the same way; when we noticed (while working on this Jaguar decoder, in fact) that three streams would be advantageous, we had to put the third stream <em>somewhere</em>. Putting stream 2 in the middle turned out to be slightly easier.<sup><a href="#fn8">8</a></sup> The advantage of the odd-looking backward stream is that it saves us a bit of signaling in the container format (not a trivial concern for a compression format) and also gives us a nice way to do end-of-buffer checks. Namely, the three are contiguous, and all three read pointers (called <code>in0</code>, <code>in1</code> and <code>in2</code> in the following) are in that single contiguous region. Furthermore, in0 keeps increasing, in1 keeps decreasing, and at any point in a well-formed stream, we have <code>buffer_begin</code> â‰¤ <code>in0</code> â‰¤ <code>in2</code> â‰¤ <code>in1</code> â‰¤ <code>buffer_end</code>. During decoding, we do the two interior checks of the read pointers against each other; the end-of-buffer checks on either end are implied by transitivity, and we don&#8217;t need to actually do them, or keep those extra pointers around. The only pointers we need to check are the ones we already keep around anyway. Neat!</p>
<p>Now, loads aren&#8217;t zero-sized; we use the (common in C) convention that &#8220;end&#8221; pointers point one past the last element of arrays. So we don&#8217;t want to start loading from <code>in1</code>, and with 64-bit (8-byte) loads, the largest address we can ever safely load from is at <code>buffer_end - 8</code>, assuming the buffer is at least 8 bytes to begin with (which we check beforehand). Decrementing <code>in1</code> by 8 before the loop takes care of both issues: now in1 points to the last address we can do a valid 64-bit load from, and as a side effect <code>in0</code> â‰¤ <code>in2</code> â‰¤ <code>in1</code> ends up guaranteeing that in0 and in2 are also good to safely load 8 bytes from without overrunning the buffer. Finally, the optimized decoder loop described here decodes 5 bytes each from 3 streams and writes the results using a 16-byte SIMD store, so it can only safely run until 16 bytes before the intended end of the buffer. All the remaining special cases (less than 8 bytes left in some of the streams, very short input streams, or close to the end of the output buffer) are left to a dedicated safe loop that generally handles the last few bytes, needs to do more careful checking, and is certainly not using hand-tweaked assembly. There would be no point for speed since it only ever handles very few bytes, and besides that&#8217;s the exact loop where you very much want a higher-level language for better debugging facilities and good integrations with sanitizers, fuzzers etc.</p>
<p>With a plan for all those details, all we need to take care of now is the refill logic and sort out the remaining plumbing. Looking at the decoder sketch above, we see that we need at least 2 registers worth of state per bitstream: one register to contain <code>bit_buf</code> (rBitBuf in the pseudo-ASM), and one for <code>bit_pos</code>. Once we consider refilling, we also need the corresponding read pointer (the <code>inN</code> I was just taking about). For 3 streams, 3 registers of state per stream works out to 9 registers, a bit more than half of our general-purpose register name pool, which is workable.</p>
<p>As for refill, that is luckily straightforward in a &#8220;bit extract&#8221; style scheme. At the top of every iteration, we want to load the next 8 bytes from the current input pointer:</p>
<pre>  mov  rBitBuf0, [in0]
</pre>
<p>For the reverse byte order <code>in1</code> stream, we use a big-endian load (<code>MOVBE</code>) instead, which is the same cost as the regular load on the Jaguars.<sup><a href="#fn9">9</a></sup></p>
<p>Then we decode 5 values from each of the 3 streams. With our 11 bits code length limit, that means we end up consuming at most 55 bits from each stream. Most relevant bit reading techniques support at most either 56 or 57 bits in a row without a refill when using 64-bit registers, so this fits well.<sup><a href="#fn10">10</a></sup> Decoding 3Ã—5 = 15 symbols also works out very nicely with our 128-bit vector registers, so we do a single unaligned vector store every 15 bytes.<sup><a href="#fn11">11</a></sup></p>
<p>Finally, after each stream has decoded 5 symbols, we need to check how many bytes to advance the read pointer by, and what the new start position within the byte is. The number of bytes we need to advance the read pointer by is <code>(bit_pos &gt;&gt; 3) &amp; 7</code>, which on the Jaguar, we can compute using a single BEXTR if we can afford a register just to store the constant 0x303, which we can.<sup><a href="#fn12">12</a></sup> We then either add or subtract this from the corresponding <code>in</code> pointer. Finally, we need to clear the bits corresponding to the byte position (that we just took care of) in <code>bit_pos</code>, which is an AND by ~0x38. This keeps the high bits, containing the 11 bitfield length that we need, intact. The actual code below does this masking at the start of the next iteration instead of at the end of the current iteration, but conceptually this belongs with the pointer advance.</p>
<p>And that&#8217;s pretty much it. Here&#8217;s the full decoder loop, written in NASM. We originally tried to write this in C++ with intrinsics, but that got nasty, so we eventually switched to a real assembler. The original version has the comments laid out differently but I need to fit this into an annoyingly narrow blog CMS theme, so this will look a bit clunky:</p>
<pre>        ; main decode loop
        ; rax = scratch
        ; rbx = bitextr0
        ; rcx = bitextr1
        ; rdx = bitextr2
        ; rbp = bextr const
        ; rsi = table ptr
        ; rdi = -bytes_left_to_decode
        ; r8  = in0
        ; r9  = in1
        ; r10 = in2
        ; r11 = bits0 (only live in inner loop)
        ; r12 = bits1 (only live in inner loop)
        ; r13 = bits2 (only live in inner loop)
        ; r14 = decodeend
        ; r15 = (unused)

        sub             r9, 8 ; in1 -= 8
        mov             ebx, 0xb00 ; 11 field width
        mov             ecx, 0xb00
        mov             edx, 0xb00
        mov             ebp, 0x303 ; for byte step

        align           16
.bulk_inner:
        ; non-crossing invariant: in0 &lt;= in2 &amp;&amp; in2 &lt;= in1
        cmp             r8, r10
        ja              .bulk_done
        cmp             r10, r9
        ja              .bulk_done

        ; refill stream 0
        ; read next bits0, keep bit offset within byte
        mov             r11, [r8]
        and             ebx, ~0x38

        ; refill stream 1
        movbe           r12, [r9]
        and             rcx, ~0x38

        ; refill stream 2
        mov             r13, [r10]
        and             rdx, ~0x38

        %assign i 0
%rep N_DECS_PER_REFILL
        ; stream 0
        ; peek
        bextr           rax, r11, rbx
        ; consume
        add             bl, [rsi+rax*2]
        ; grab sym
        vpinsrb         xmm0, xmm0, [rsi+rax*2+1], i+0

        ; stream 1
        bextr           rax, r12, rcx
        add             cl, [rsi+rax*2]
        vpinsrb         xmm0, xmm0, [rsi+rax*2+1], i+1

        ; stream 2
        bextr           rax, r13, rdx
        add             dl, [rsi+rax*2]
        vpinsrb         xmm0, xmm0, [rsi+rax*2+1], i+2

        %assign i i+3
%endrep
        %undef i

        ; final advances
        ; num_bytes_step0
        bextr           rax, rbx, rbp
        ; in0 += num_bytes_step0
        add             r8, rax
        bextr           rax, rcx, rbp
        sub             r9, rax
        bextr           rax, rdx, rbp
        add             r10, rax

        vmovdqu         [rdi+r14], xmm0
        add             rdi, 15
        ; loop while bytes_to_decode &gt; 0
        js              .bulk_inner 
</pre>
<p>That&#8217;s the core 3-stream Huffman decoder loop. Time to quit it with the hand-waving and do an actual analysis (if only back of the envelope) to make sure we&#8217;re on the right track here.</p>
<h3>Analysis</h3>
<p>We already looked at the core decode step earlier and noted that it has 3 macro-ops (I&#8217;ll write 3M in the following), and for the backend: 2 integer 0/1 ops (just 2I for short), 2 load unit cycles for aligned loads (2L for short), and 1 FP/SIMD op (1F for short). We do this 5 times per stream. Also per stream is the refill/advance logic, which we now know the instructions for: 1 load for the refill, and 3 integer ALU ops for the byte advance and bitpos update. The load in the refill is almost always unaligned, though. It&#8217;s a 64-bit load, and as noted in the introduction, unaligned loads are free if they stay within an aligned 16-byte sector, and cost at least 1 cycle extra when they don&#8217;t. Out of the possible load offsets mod 16, 9 (0 through 8 inclusive) stay within a 16-byte sector, the other 7 do not. That&#8217;s 7/16=0.4375 odds of at least one cycle extra, and some of those cases (such as crossing cache lines and pages) get more expensive than just adding a cycle. For sanity in the following, let&#8217;s just say that we bake this all down to somewhat simpler numbers and expect around 1.44 cycles average case (but probably closer to 1.5 in realistic conditions) for those unaligned refill loads, 2 cycles for a much more pessimistic estimate. In other words, we want to bill the unaligned refill loads at costing more than single aligned load, since the expected number the load pipelines are occupied with them is larger.</p>
<p>Taking that into account, the four instructions involved in refill and advance for a single stream boil down to 4M, 1.44-2L, and 3I.</p>
<p>Then, we have some cross-stream shared instructions: the two compare/jump pairs for our pointer-crossing check at the beginning account for 4M 4I, the final store accounts for 1M 1.44-2S (since it&#8217;s also unaligned), and the final <code>ADD</code>/<code>JS</code> pair contribute another 2M 2I to the tally. That&#8217;s all instructions in the loop accounted for.</p>
<p>For an overall throughput estimate, we get:</p>
<ul>
<li>15 Ã— 3M (decodes) + 3 Ã— 4M (stream refill/advance) + 7M (shared rest) = 64M total, so 64 macro-ops, enough to occupy the front-end for at least 32 cycles.</li>
<li>15 Ã— 2L (decodes) + 3 Ã— 1.44-2L (stream refills) = 34.3-36L total, so the load unit is busy for 34.3-36 cycles.</li>
<li>15 Ã— 2I (decodes) + 3 Ã— 3I (stream refills) + 6I (shared) = 45I total, evenly distributes over both integer ALU pipes for 22.5 cycles worth of pressure.</li>
<li>15 Ã— 1F (decodes) = 15F total, distributed over both FP/SIMD pipes for 7.5 cycles worth of pressure, so they&#8217;re loafing.</li>
<li>1.44-2S for 1.44-2 cycles worth of pressure on the store pipe which I assume is sitting on the sidelines munching popcorn.</li>
</ul>
<p>Purely in terms of pressure on the execution resources, we&#8217;re mainly limited by the load pipes which are busy for around 34.5-36 cycles every iteration, closely followed by the frontend which is occupied for at least 32 cycles if everything goes perfectly. 34.3-36 cycles to decode 15 bytes works out to 2.287-2.4 cycles per byte decoded. This is assuming we can ever get throughput-bound to begin with, and is budgeting absolutely no time for L1 cache misses and such.</p>
<p>How does the critical path look? By my reckoning, the most likely candidate takes a freshly updated in pointer from the end of a previous iteration, does an unaligned load to refill which takes 4 cycles for the data to show up, then does 5 back-to-back decodes from that stream which we know have a critical path latency of 5 cycles each, and then finally needs to do a <code>BEXTR</code> on the resulting bitpos followed by an integer add/subtract to produce the next load address. That&#8217;s 4 + 5 Ã— 5 + 2 = 31 cycles of critical path latency through the stream decodes, worse if anything bad happens, like extra delays due to page crossings on a load or similar. 31 cycles is close enough to our other 2 limiters for it to be considered a 3-way near-tie. A hitch in the front-end or load pipes or any extra delay along the critical path is likely to end up delaying any given iteration if it occurs. Note I&#8217;m purely looking an ideal throughput estimates here, there is no modeling or simulation of machine details going on, all we&#8217;re doing is tallying up some figures based on known machine characteristics.</p>
<p>In short, from this rough estimate, we would expect somewhere around 2.3-2.4 cycles per byte for this decoder under very idealized circumstances where there&#8217;s not a single cache miss or hitch anywhere along the way, and &#8220;a bit worse&#8221; (to be decided what that means) when less idealized.</p>
<h3>The rubber hits the road</h3>
<p>So what happens when we actually run it?</p>
<p>One of the nice things about coding for game consoles is that the hardware is known and tends to have very predictable, repeatable performance. With that said, here&#8217;s stats for the exact loop quoted above running on a PS4 on a synthetic test set (decoding a random stream with a very boring &#8220;every symbol is 8 bits&#8221; Huffman table, which of course you&#8217;d never do, but makes for a test run that&#8217;s very easy to validate the results of) 1000 times, and reporting 1st, 50th (median) and 95th percentile cycles per byte:</p>
<pre>    huff3_jaguar_asm: med 2.28/b, 1st% 2.25/b, 95th% 2.38/b</pre>
<p>I swear I did not fudge this in any way, that&#8217;s the actual figures I got on a real test run just now. So that much, ahem, very encouraging, to say the least. But what happens when you time it in the middle of an actual Kraken decode of ~250MB of real non-synthetic test data?</p>
<pre>SimpleProf              :seconds  calls     count :     clk/call    clk/count
get_array_huff          : 0.3879  17689 178617138 :      34946.1         3.46
huff_x64jaguar_loop     : 0.3035  31350 178617138 :      15427.6         2.71</pre>
<p>Here <code>get_array_huff</code> includes everything the Huffman decoder has to do, including reading the headers, the Huffman table descriptions, validating the code lengths, setting up the tables, the fast decode loops, and the slower near-end-of-data tail decoders, and <code>huff_x64jaguar_loop</code> is just the core optimized decode loop that handles most of the bulk data. &#8220;Calls&#8221; is the number of calls to either function and &#8220;count&#8221; is the number of bytes decoded. In this case, 250MB of data &#8220;only&#8221; decode about 178MB through the Huffman decoders; less than 250MB because we also do LZ-style dictionary compression (not covered in this series). So in this particular real-world use case (which very much does not have all the buffers already nicely in the L1/L2 caches as it&#8217;s running), we&#8217;re about 17% slower than our ideal average-case throughput estimate for this loop, which is still respectable. Also visible from these two lines is that the core decode loop where around 75-80% of the overall Huffman decoding time is spent with the rest being in setup or tail handling. That is fairly typical for our decoder implementations on various platforms. And you can infer from the figures given that our average array of bytes that use a single Huffman table is about 10k long. For this part, looking at averages is misleading: the distribution is quite wide. Many arrays are 60k+, but many others are well below 3k. The former spend more time in the core decoder (always nice since that part is flat), the latter spend a lot more time proportionately in header parsing and table initialization, which is why we can&#8217;t neglect it.</p>
<p>This last part is also why the Jaguar decoder (or, for that matter, all other decoders in Oodle) doesn&#8217;t bother with trying to set up tables to decode multiple symbols at once. This sounds enticing but it makes table setup more complicated and slower, and also adds many complications to the decoders because everything emits a variable number of bytes now. For example, using <code>PINSRB</code> to group output bytes would not work if each decode step produce either 1 or 2 bytes; we would need to do individual stores for every symbol, and also increment the destination pointer after every decode. This would add at least 2 instructions per byte decoded (a store and a destination pointer add), probably more. When your single-byte-at-a-time decode kernel runs 3 instructions per byte to begin with and instruction count is a major limiting factor, adding 2 extra more instructions to maybe decode 2 bytes at a time isn&#8217;t all that tempting. We can decode another byte in an extra three instructions with the single-byte-at-a-time decoder <em>guaranteed</em>, and we don&#8217;t need to do any expensive extra work during table setup to do so. We also don&#8217;t build up a debt of 2 instructions every time we don&#8217;t manage to decode 2 symbols at once that we later have to make up just to break even. Multi-symbol decoding is an old standby when decoding from a single stream, because you can hide a lot of extra work in the shadow of that nasty long critical path, but decoding from multiple streams simultaneously gives you more productive ways to spend those CPU cycles and maybe even get to the holy grail of being primarily throughput bound.</p>
<p>And that&#8217;s all I got for this post! I&#8217;m not sure which of the other decoder variants I&#8217;ll tackle next. Apologies for the long delay, but writing these up takes more effort than my usual blog post, and I need to be in the right headspace to even try doing it.</p>
<h3>Footnotes</h3>
<p><span id="fn1">[1]</span> If you&#8217;re familiar with Intel microarchitectures but not AMD, macro-ops are roughly comparable to what Intel calls &#8220;fused-domain micro-ops&#8221;, except they&#8217;re &#8220;even CISCier&#8221;, in the sense that even read-modify-write instructions like <code>add [rdx], rax</code> count as a single macro-op, where Intel would split them into a add-from-memory and a store internally. I&#8217;ll also add that historically speaking describing AMD macro-ops as similar to Intel fused-domain uops is backwards; AMD has been using &#8220;fat&#8221; macro-ops as part of their x86 instruction decomposition for a long time, since at least the K7 (original Athlon) architectures. Intel added fused micro-ops (which are more restricted) years later to their microarchitectures when they realized that having the frontend deal with these &#8220;chunkier&#8221; units was beneficial.</p>
<p><span id="fn2">[2]</span> Typical L1D load-to-use times for contemporary designs were 4 or 5 cycles. For that matter, they still are at time of writing, 9 years after Jaguar-based HW hit the shelves. That said, the Jaguars target much lower clock rates than those other designsâ€”the fastest Jaguar descendants ran a bit above 2GHz, other OoO x86 cores from the same timeframe have similar L1D sizes and were typically designed to hit 4GHz or above, so presumably the Jaguar cores can fit a lot more logic into a pipeline stage.</p>
<p><span id="fn3">[3]</span> To editorialize even more, the Jaguar&#8217;s nearly complete lack of sharp edges and huge performance cliffs was a welcome change after the PS3/Xbox 360 generation, where the main CPU cores seemed at times like they had nothing but.</p>
<p><span id="fn4">[4]</span> Presumably due to a problem with the way immediate operand encoding in x86 works. Specifying a bitfield position and width needs at least 6 + 6 = 12 bits to be generally useful on a 64-bit machine. But x86 immediate operands for instructions with 32- or 64-bit operands only come in two sizes: 8 bits and 32 bits. The former is not enough, the latter is very wasteful. 16-bit immediates only exist for 16-bit register instructions, and this part of the encoding would be quite expensive to add exceptions to. Interestingly AMD added a short-lived immediate-operand version of <code>BEXTR</code> that indeed spends a full 32 bits, but this version of the instruction decodes to 2 macro-ops on Jaguar and was removed from Zen 3. It never shipped on any Intel CPU.</p>
<p><span id="fn5">[5]</span> All &#8220;big core&#8221; Intel CPUs that support <code>BEXTR</code> (that I&#8217;m aware of, anyway) decode it into 2 uops. The same CPUs can usually eliminate register-register moves during register renaming and would also take 2 uops for a shift-and-mask combination, so <code>BEXTR</code> has never been particularly interesting on them, since it offers at best some minor advantages in the frontend. It&#8217;s more interesting on the Intel Atom-derived cores such as the Alder Lake E-cores (which like Jaguar have a 1 uop, 1 cycle version) and AMD Zen cores, though.</p>
<p><span id="fn6">[6]</span> Yet another AMD/Intel difference: Intel has been renaming the 8-bit parts of registers such as AL and AH of RAX or R9B of R9 separately for a long time, meaning AL and AH can reside in separate locations in the physical register file. When referencing the merged halves as a single register (such as AX, EAX or RAX) later, Intel CPUs used to either stall (the &#8220;partial register stall&#8221; of long ago) or, later, started inserting merge operations that combined the results into the instruction stream. AMD has never done this, and instead seems to do partial updates on every operation. That means that on Intel CPUs, code that alternately updates AL and AH can execute as two independent dependency streams, whereas on AMD CPUs all these updates run in series. However, AMD never needs to insert any merge ops either, and has no special penalty for referring to the full register after a partial update, which is handy in our use case: on the Jaguar we&#8217;re always concerned with macro-ops through the front-end, so injecting merge operations would suck for us. Good that we don&#8217;t get any!</p>
<p><span id="fn7">[7]</span> Two streams so that we can alternate decoding from them, because (as seen in this post already and also mentioned in previous parts) sequential decoding from a single bitstream tends to result in very long dependency chains and is a bottleneck. Having pairs of forward and backwards streams with the two meeting in the middle allows us to store a single size in the bitstream to signal the start position of the reverse stream and the combined size as a single value. They also can, in some contexts, act as padding for each other. During decoding, ensuring the pointers don&#8217;t cross corresponds to a end-of-buffer check; we don&#8217;t know the exact size of either stream up front, but we know that the read pointers may never cross, and once decoding is done they should point to the same location.</p>
<p><span id="fn8">[8]</span> Arguably, we should have at least renumbered the streams and swapped labels of streams 1 and 2; but as is often the case, this was quickly prototyped, found to be working, then for a while we were concerned with other things such as buffer overflow safety and such, and by the time we realized it was pretty odd for stream 2 to appear in the bitstream before stream 1, it had long shipped to customers and was very much not worth a format-breaking change to rectify.</p>
<p><span id="fn9">[9]</span> Very convenient how the bitstream layout chosen works out so nicely for the most constrained of the important target platforms for Oodle.</p>
<p><span id="fn10">[10]</span> Yet another very-much-not-a-coincidence.</p>
<p><span id="fn11">[11]</span> This one actually is a coincidence, but I&#8217;ll take it.</p>
<p><span id="fn12">[12]</span> Yes, the decoder loop keeps 0x303 pinned in a register the entire time it&#8217;s running. Long-time friends will realize why this delights me; sometimes the universe just smiles at you like that. This one&#8217;s for you, Felix.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2022/04/04/entropy-decoding-in-oodle-data-huffman-decoding-on-the-jaguar/feed/</wfw:commentRss>
			<slash:comments>8</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>GPU BCn decoding</title>
		<link>https://fgiesen.wordpress.com/2021/10/04/gpu-bcn-decoding/</link>
					<comments>https://fgiesen.wordpress.com/2021/10/04/gpu-bcn-decoding/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Mon, 04 Oct 2021 07:00:22 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<category><![CDATA[Graphics Pipeline]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7185</guid>

					<description><![CDATA[BC1-7 (variously also known by the older names S3TC, DXTC, RGTC and BPTC) are the standard compressed texture formats on PC. The newer BC6H and BC7 formats have precise specs that require bit-exact decoding, which means that encoders know exactly what results they&#8217;re going to get. The older BC1-5 are a lot more loosely specified, [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>BC1-7 (variously also known by the older names S3TC, DXTC, RGTC and BPTC) are the standard compressed texture formats on PC. The newer BC6H and BC7 formats have precise specs that require bit-exact decoding, which means that encoders know exactly what results they&#8217;re going to get. The older BC1-5 are a lot more loosely specified, which has been a source of annoyance and problems for BCn encoder authors for some time.</p>
<p>While working on Oodle Texture, I did some experimentation trying to figure out what various GPU hardware decoders actually did. I did get good results for BC1-3 but BC4 and BC5 turned out to be a bit more troublesome. More recently, my colleague Sean Barrett spend some more time on this and we managed to get bit-exact equivalent expressions for Intel and NVidia hardware, but not AMD, where we were off by the last bit on some 32-bit float results in a small number of cases. This is completely irrelevant in practice but it bothered me, so last weekend I spent some more time trying to figure out those results too. I now have satisfying models for all three vendor decoders which match bit-exactly in my tests. That level of precision is unlikely to ever matter to anyone, but along the way there are several common mistakes in BC1-5 implementations and misunderstandings about the format to clear up, so it seems worth it to do a write-up.</p>
<h3>Overview</h3>
<p>BC1-7 encode fixed-size blocks of 4&#215;4 pixels to a fixed-size output: 64 bits (8 bytes) per block for BC1 and BC4, 128 bits (16 bytes) for all the other formats. The basic idea is to approximate the colors of pixels in a block by a small number of colors spaced evenly along one or more line segments in RGBA space. Per block, we send the endpoints of the line segment (quantized to a relatively low number of bits), plus an index per pixel that selects which of the colors along the line segment to use. These indices are usually between 2 and 4 bits each. Additionally, almost all of these formats have multiple modes that distribute the values slightly differently. Let&#8217;s briefly go over each format in turn:</p>
<p>BC1 sends color endpoints in RGB565 encoding (5 bits for red and blue, 6 bits for green). Somewhat unfortunately, this choice of encoding contains no exact grayscale values other than black and white, which means that slow grayscale gradients in this format tend to oscillate between areas tinged green and areas tinged purple. There are two modes, both of which use a 4-color palette and 2 index bits per pixel to select the palette entry (2Ã—16 bits for the endpoints plus 16Ã—2 bits for 16 2-bit indices, one per pixel, makes 64 bits total). The more commonly used mode uses the two specified endpoint colors, plus another two interpolated colors located 1/3rd and 2/3rds of the way along the line segment between them respectively, for a palette of 4 colors (all 4 colors have 1.0 in the alpha channel). The second mode has only one interpolated color halfway between the two endpoints; the third color has R=G=B=A=0, &#8220;transparent black&#8221; (in some cases, this is considered a separate &#8220;BC1A&#8221; format, and stock &#8220;BC1&#8221; decodes as R=G=B=0 and A=1; this distinction is not available everywhere though). This was intended for use with 1-bit alpha textures. If whoever reads the texture doesn&#8217;t care about the alpha channel, this second mode can also be used to get a &#8220;free black&#8221; even when the line between endpoints goes nowhere near actual black.</p>
<p>BC2 is for textures with an alpha channel that needs more than 1 bit. It consists of what is basically a BC1 color block (except here, all blocks are always in the first mode with 4 interpolated colors; the secondary mode is not available) and 4 bits of alpha per pixel, stored in memory before the BC1-esque color bits. BC2 has existed since the original S3TC, but I&#8217;ve never actually seen it used in practice. My guess is that the main reason it&#8217;s still around is that it costs very little hardware if you already support BC1 and 3, so despite little practical use, it&#8217;s cheap enough to support that nobody has lobbied to get it deprecated.</p>
<p>BC3 is the other format for RGBA textures. Once again, it contains a BC1 color block that is locked into the 4-color interpolation mode, and a different 64-bit encoding for the alpha bits in front. This one specifies two endpoints in the A channel as 8-bit values, followed by 48 bits (3 bits per pixel) to select the palette index. Once again, there are two modes: the more common one has the 8 colors spaced evenly along the line segment, the secondary mode only spaces 6 points along the line and adds two special indices that always decode to 0.0 and 1.0 no matter what the endpoint values are. BC3 is one of the two primary formats used for RGBA texture data.</p>
<p>BC4, unlike the previous formats, is intended for single-channel textures. It is often described as &#8220;just a BC3 alpha block&#8221;, which is close but not quite true as we&#8217;ll see later. There are unsigned and signed variants; the unsigned variants use the regular 8-bit UNorm encoding for values between 0 and 1, the signed variants use 8-bit SNorm for values between -1 and 1. In the signed variant, the special indices in the secondary mode correspond to -1 and +1, respectively.</p>
<p>BC5 also has unsigned and signed variants. A BC5 block is really just two independent BC4 blocks, one for the red and one for the green channel.</p>
<p>BC6H is meant for HDR color data. It only encodes RGB channels, the alpha channel always decodes to 1. There are unsigned and signed variants. The format decodes to 16-bit half-float format and color interpolation works by essentially treating the 16-bit half-floats as a 16-bit sign-magnitude integer value and interpolating those linearly, which works decently for unsigned data, not so well for signed data for blocks containing sign changes. BC6H has 14 modes, sort of, but most of these work essentially the same except for allocating endpoint bits slightly differently, depending on how close the values are to each other. There are two &#8220;major&#8221; modes in the format, one which specifies 2 sets of line segments (plus a code denoting which pixels use which line segment, from a small set of supported patterns) and has 3 bits of index data per pixel, and one of which specifies just one line segment with 4 bits of index data per pixel. BC6H decoding is complicated but specified bit-exactly so I won&#8217;t be talking about it in the rest of this post.</p>
<p>BC7 is a more complicated version of BC3 that adds several more options. It has 8 modes, 4 of which are RGB-only (with A decoding to 1.0), the idea being that even textures with alpha often have large opaque regions, and it helps everything to have more options for coding these color-only regions well. The different modes in BC7 contain various combination of either 1, 2, or 3 different line segments (and corresponding sets of endpoints), once again with the assignment of pixels to line segments chosen from a small, fixed set; either one index per pixel (when RGB and A behave similarly) or two (when they do not); and various index pixel depth, plus several other features that I won&#8217;t get into here. BC7 decoding is also complicated but specified bit-exactly and won&#8217;t appear in the rest of this post.</p>
<h3>Common (software) mistakes</h3>
<p>This is the part that&#8217;s probably the most applicable to the majority of readers: before we even get to specification details, many encoders and decoders make two small mistakes in how they implement the basics.</p>
<p>The first and most common one is about the different modes in the BC1 format. The way a BC1 encoder selects which mode it wants a block to use is by sending the coded endpoints in a different order. Note that both color endpoints get packed to RGR565 format. When the first endpoint, interpreted as an unsigned 16-bit integer, is larger than the second one, this selects the more common four-color mode. If the first endpoint is less than or equal to the second, that selects three-color + transparent black mode. Note that when both colors are equal, you have no choice; you always end up in three-color mode (where the 3rd palette entry is special). This is not much of a problem because both endpoints being equal is a degenerate case anyway; all three other palette entries are equal in that situation, so just avoid the third. In all other cases, when the colors you picked result in the wrong order, the encoder can just swap them around and adjust the indices to match. Linear interpolation is symmetric so this does not change the set of available colors. This much, essentially all implementations get right. The tricky part involves BC2 and BC3: BC2 and BC3 blocks <em>almost</em> contain regular BC1 blocks, except they don&#8217;t have any mode-switching. The color values in BC2/3 always use the four-color mode. (This is pointed out in all specifications of the format, but easy to miss.) Several encoders only ever use the four-color mode to begin with, and swapping endpoints into the order you would need for BC1 never hurts; just be aware that three-color mode is not an option for BC2 or 3, and encoding assuming you have it available will give the wrong results.</p>
<p>The second mistake involves BC4 and 5. BC4 encoders/decoders often take &#8220;it&#8217;s just like a BC3 alpha block&#8221; too far and actually encode or decode unsigned BC4 (or BC5) as BC3 8-bit data. This is not far off, but it&#8217;s an approximation. BC4 and BC5 actually decode somewhat differently than BC3 alpha does, with measurably different results and higher internal precision. For most textures this doesn&#8217;t matter too much, but the extra precision is definitely useful for content like normal maps. BC5 two-channel normal maps can get up to nearly 11 bits of precision in regions with small value range, as opposed to the at most 8 bits you would get out of regular R8G8B8A8 or BC3 alpha encoding. It&#8217;s highly contextual whether this matters or not, and especially for rough surfaces there&#8217;s usually not much difference, but it can be useful especially when rendering things with reflective, smooth surfaces like machine parts, cars and so forth.</p>
<h3>What do the specs say?</h3>
<p>The premise of this post is that GPU decoders don&#8217;t exactly agree with each other, but of course they at least approximately agree or the formats would be useless in practice. Can we say something more about what &#8220;approximately agree&#8221; means? The two primary sources for this sort of thing in the PC space are the Khronos Data Format Specification and the D3D11 Functional Spec. The Khronos spec is in force for Vulkan and supplants the older OpenGL extensions saying the same thing; as of this writing, the current version is 1.3 and the description of the BC1 formats starts <a href="https://www.khronos.org/registry/DataFormat/specs/1.3/dataformat.1.3.html#S3TC">here</a>. It short and to the point, but unfortunately fairly light on details and completely missing any compliance requirements; it specifies what results an ideal decoder should produce, but not how close practical implementations need to be to be considered good enough.</p>
<p>Our secondary primary source is the D3D11 Functional Spec (a descendant of the D3D10 spec and the D3D12 spec is given as differences from D3D11, so this is still in force). The relevant sections start <a href="https://microsoft.github.io/DirectX-Specs/d3d/archive/D3D11_3_FunctionalSpec.htm#19.5%20Block%20Compression%20Formats">here</a>. In theory, GL/Vulkan devices could do something completely different, but in practice the same hardware is designed to work for both (for obvious reasons), so whichever spec has the more detailed requirements ends up determining the requirements that the hardware is actually trying to satisfy. And the D3D11 version of the spec is a lot more detailed; in particular it has the short section 19.5.2 &#8220;Error Tolerance&#8221; which I&#8217;ll quote verbatim:</p>
<blockquote><p>Valid implementations of BC formats other than BC6H and BC7 may optionally promote or do round-to-nearest division, so long as they meet the following equation for all channels of all texels:</p>
<pre>| generated - reference | &lt; absolute_error + 0.03
    *MAX( | endpoint_0 - endpoint_1 |,
          | endpoint_0_promoted - endpoint_1_promoted | )</pre>
<p>absolute_error is defined in the description of each format.</p>
<p>endpoint_0, endpoint_1, and their promoted counterparts have been converted to float from either UNORM or SNORM as specified in the Integer Conversion rules. Values that the reference decodes to 0.0, 1.0 or -1.0 must always be exact.</p>
<p>For BC6H and BC7, decompression hardware is required to be bit accurate; the hardware must give results that are identical to the decoder described in this specification.</p></blockquote>
<p>The part about promotion is clarified in the next 2 subsections and boils down to implementations being allowed to do fixed-point computations at an intermediate wider bit width, as long as values are extended to that intermediate bit width as specified. For the rest, I&#8217;ll note that all formats under consideration permit endpoint_0=endpoint_1, in which case the error term reduces to <code>| generated - reference | &lt; absolute_error</code>, so the absolute error term (I&#8217;ll get to that in a minute) primarily affects how precise decoding of the endpoint values themselves is. This value is 1/255 for BC1-3, 1/32767 for the signed BC4/BC5 formats, and 1/65535 for unsigned BC4/5 (indicating that BC4/5 have higher internal resolution). The second, relative error term requires that implementations have to be within Â±3% of the reference, which is really not a tight bound: even for usual 8-bit texels in a range of [0,255], Â±3% works out to Â±7 in the integer space (actually 7.65, but when only considering integers, we can round down here) in the worst case when we choose endpoint_0 so that it decodes 0 to and endpoint_1 to decode to 1, or vice versa. This relative error term matters most when the endpoints are far from each other in a given channel, and the rationale for it is that the BC1 quantization is pretty harsh to begin with (you only get 4 values to cover that range). An extra 3% is less than one fifth the expected quantization error from getting to use at most 4 colors, so in that sense, it&#8217;s not a big deal. The problem for encoders is that there are often many candidate encodings for a block that have similar error; more possible encodings gives us more chances to get overall errors low if we can manage to find just the right combination of endpoints and indices, including spacing endpoints much further apart than necessary given the range of values in the block because it makes one of the interpolated values land exactly where we need it. Having this relative error term in the mix means we have to play it safe here.</p>
<p>More to the point, having this level of allowed differences between hardware encoders also means that the same texture wil decode differently on different GPUs, even if we&#8217;re not trying to do anything tricky. Sometimes blocks legitimately have a large value range because that&#8217;s what those blocks&#8217; contents are, and having large differences possible between decoder implementations is a bummer, because we don&#8217;t even get an accurate picture of what the error from encoding any particular texture in BC1-5 is; it depends on what GPU it&#8217;s decoded on!</p>
<p>The reason I originally looked into this for Oodle Texture was because I wanted tighter error bounds than just the spec requirements. While in theory there is a large space of possible decoder implementations permitted by the spec, in practice there is a very small number of GPU vendors active in the PC space, and once they&#8217;ve settled on a particular decoder implementation, they really have no reason to ever change it unless the spec changes again. So I did some testing, and as far as I can tell, indeed neither NVidia, AMD nor Intel seem to have touched their BCn decoder blocks in the last 10 years or so (and why would they?). That means that both in terms of the current marketplace and installed base, there&#8217;s mostly just 3 decoders we have to worry about; that excludes really old hardware as well as vendors with a tiny market share, but if nothing else it will give us an idea of what we should really expect.</p>
<h3>Probing what decoders do</h3>
<p>Luckily, the BCn formats have many properties that make it fairly tractable to completely capture decoder behavior. For the BC1/BC2-3 color portion, the R, G, B channels are decoded independently from each other, and the pixels don&#8217;t influence each other. R and B endpoints are specified with 5 bits of precision and G with 6. That means we can look at all possible combinations of R/B endpoint values with a 128Ã—128 pixel texture (32Ã—32 blocks). In each block, we make sure to use each of the 4 indices at least once, and then write a compute shader that does a <code>Load</code> / <code>texelFetch</code> operation for every pixel (so that filtering doesn&#8217;t confuse the results) and writes the result to a 32-bit float/channel texture. For G we need a texture with 256Ã—256 pixels for 64Ã—64 blocks. That&#8217;s a set of 3 small images that completely captures BC1 decoder behavior. For BC3 alpha and BC4 unsigned/signed we have 8-bit endpoints in a single channel, so we need an entire 1024Ã—1024 pixels (256Ã—256 blocks). For BC2 and BC5 we can do some quick tests just to confirm that they behave as expected (BC2: alpha is a 4-bit encoding that decodes as expected, BC5: same as two BC4 blocks, one for R and one for G).</p>
<p>Some more quick tests indicated that:</p>
<ul>
<li>As far as I can tell, on all three vendors tested (AMD, NV, Intel), BC1 internally decodes to 8-bit fixed point, which then, depending on whether the format is UNorm or UNorm_sRGB, gets converted to float either using the same conversion the GPU uses for regular 8-bit UNorm textures, or a 256-entry LUT (or maybe LUT + math combination?) used for sRGB-&gt;float conversions (hard to be sure, but tested by noting that all values obtained for 8-bit BC1 RGB channels appear in the list of 256 values output by regular UNorm or UNorm_sRGB conversions; if this is not what&#8217;s happening the observed behavior appears to be equivalent).</li>
<li>On all three vendors, BC3 alpha appears to decode to 8-bit fixed point, once again matching the 8-bit UNorm-&gt;float conversions, which in turn for all 3 vendors exactly match the results of rounding &#8220;x/255&#8221; to the nearest floating-point value. Setting e.g. endpoint_0=0 and endpoint_1=1 (8-bit integer values) did <em>not</em> result in any interpolated values inside the open interval (0,1/255).</li>
<li>On all three vendors, BC4 and BC5 decode to more than 8 bits of precision. For Intel and NVidia, all interpolated values for unsigned/signed BC4/5 formats occur on the list of decoded 16-bit UNorm and 16-bit SNorm values, respectively, and they are consistent with first decoding to 16-bit int then converting to float via the UNorm/SNorm conversions. On all three vendors, 16-bit UNorm decode as correctly rounded <code>x/65535</code> and 16-bit SNorm as correctly rounded <code>max(x,-32767)/32767</code>, respectively. On AMD, the resulting floats do <em>not</em> all appear on the list of UNorm16/SNorm16 values; instead the decoder appears to use an internal 14-bit format, details below. Setting e.g. endpoint_0=0, endpoint_1=1 (integer byte values) for BC4 unsigned <em>does</em> result in interpolated values inside (0,1/255).</li>
</ul>
<p>Furthermore, Ignacio CastaÃ±o wrote a <a href="http://www.ludicon.com/castano/blog/2009/03/gpu-dxt-decompression/">2009 blog post</a> on how NVidia GPUs decompress BC1 (with mention of the relevant patent), and it was straightforward to verify that more recent NV GPUs (tested on GeForce 1080 and 2070 series devices) still appear to use the same logic. For Intel, testing was done on a late-2013 laptop with Haswell integrated GPU, and we contacted Tom Forsyth at Intel to confirm that the BCn decoder logic was still essentially unchanged and answer some questions regarding the exact BC4/5 decoder behavior. For AMD, testing was done with an old Radeon HD 7000 series GPU we had kicking around the office as well as game console devkits for the previous and current generation (GCN2 and RDNA2 variants, respectively), the latter on account of them already being on the network and set up for remote testing. (So no pre-GCN AMD GPU in the mix, not sure if that makes a difference).</p>
<h3>What all decoders seem to agree on</h3>
<p>For all vendors, the endpoint_0 and endpoint_1 values in all formats (which always appear as palette entries 0 and 1, respectively) appear to be reproduced exactly the same. BC1 as written in the D3D11 functional spec first expands the endpoint values from 5 or 6 bits to 8 bits by replicating the top bits; all three vendors appear to do this or something equivalent, and then convert the result from 8-bit UNorm to float exactly. For BC4/5, likewise palette entries 0 and 1 always seem to exactly make the result of the respective correctly rounded UNorm8 or SNorm8 to float conversion, respectively. Furthermore the D3D11 functional spec requires (quoted above) that the values specified to decode to exactly 0, 1 or -1 indeed decode to these values in all implementations, and this appears to hold.</p>
<p>Taking these all together, this means that for BC1/BC2-3 color, the parts that decoders actually differ in are how exactly the colors 1/3rd and 2/3rds of the way (in four-color mode) or 1/2 of the way (in 3-color mode) are computed, and for BC3 alpha and BC4/5, how the 6 (respectively 4, depending on the mode) actually interpolated values are determined. I&#8217;ll cover these vendor by vendor, in order of increasing complexity (of description though not necessarily hardware).</p>
<h3>Intel GPUs</h3>
<p>Intel&#8217;s decoders are the most straightforward to describe. BC1-3 color is computed in fixed point, from the endpoint values extended to 8 bits using bit replication, using the formula <code>((256-w)*a + w*b + 128) &gt;&gt; 8</code> or something equivalent, i.e. 8-bit fixed-point linear interpolation with round-to-nearest. For the 1/2-of-the-way color in 3-color mode, the weight is w=128 (of course); in 4-color mode, the interpolation weights are w=85 (for the 1/3rd point) and w=171 (for 2/3rds), respectively. These weights are just <code>round(k*256 / 3)</code> for k=1, 2.</p>
<p>BC3 alpha appears to use the weights <code>round(k*256/7)</code> for k=1,&#8230;,6 in 6-interpolated color mode, and <code>round(k*256/5)</code> for k=1,&#8230;,4 in 4-interpolated-color mode.</p>
<p>BC4, signed and unsigned both, takes the integer UNorm/SNorm endpoint values (for SNorm, -128 is an alternative encoding for -127, they mean the same thing), and use the 16-bit interpolation formula <code>t = ((65536-w)*a + w*b + 128) &gt;&gt; 8</code> to get the interpolation result, where <code>w=round(k*65536/7)</code>, k=1,&#8230;,6 in 6-interpolated-color mode, <code>w=round(k*65536/5)</code>, k=1,&#8230;,4 in 4-interpolated-color mode.</p>
<p>For BC4/5 unsigned, the GPU then computes <code>t + (t &gt;&gt; 8)</code>; the resulting integer is treated as a 16-bit UNorm value and finally converted to float. For signed, the GPU computes <code>abs(t) + (abs(t) &gt;&gt; 7) + (abs(t) &gt;&gt; 14)</code> with the sign of the original t (i.e. if t was negative, this result is then negated). This number is treated as 16-bit SNorm value and converted to float.</p>
<h3>AMD GPUs</h3>
<p>AMDs GPUs still behave in a way that is straightforward to describe, with a wrinkle involving BC4/5. But let&#8217;s start with BC1-3: BC1-3 is computed in fixed point, from the endpoint values extended to 8 bits using bit replication, using the formula <code>((64-w)*a + w*b + 32) &gt;&gt; 6</code> or something equivalent, i.e. 6-bit fixed-point linear interpolation with round-to-nearest. This is the exact same computation that is prescribed for interpolation in BC7 decoders, so it likely uses the same hardware. In BC1 3-color mode halfway points, we have w=32 (again, no surprise), and for 4-color mode the weights are 21 and 43, again matching BC7 and equivalent to <code>w=round(k*64/3)</code> for k=1, 2.</p>
<p>BC3 alpha uses the weights <code>round(k*64/7)</code> for k=1,&#8230;,6 in 6-color mode, <code>round(k*64/5)</code> for k=1,&#8230;4 in 4-color mode.</p>
<p>BC4/5, signed and unsigned both, tales the integer UNorm/SNorm endpoint values (for SNorm, after the -128 -&gt; -127 correction) and uses the same interpolation formula, just without the shift or rounding term: <code>t = (64-w)*a + w*b</code>, again suggesting that it&#8217;s probably all using the same hardware. The resulting product fits in a 14-bit integer. Note that by construction, t is either in [-127*64,127*64] for signed values or [0,255*64] for unsigned ones.<br />
For BC4/5 SNorm, this result is then converted to float by effectively doing a correctly rounded division by 127*64 = 8128. This turns out to be somewhat more complicated than the straight division by 127 used for 8-bit SNorm values, mainly due to the extra 6 bits in the input. For BC4/5 UNorm, the result is converted to float by doing an <em>almost</em> correctly rounded division by 255*64 = 16320. Once again, this would ordinarily be a bit more complicated than the division by 255 used for UNorm8-&gt;float, again due to the extra 6 bits of input; however, here, for whatever reason, be it just a bug or a small hardware optimization that is not clear to me, a very small subset of values (13 total of the 16077 unique possible results for all combinations of inputs) is rounded incorrectly.</p>
<p>This result differs in final mantissa bit of the 32-bit float values only and it truly does not matter for any use case I can imagine since actual BC4/5 encoding-induced errors are many orders of magnitude larger than this, but in case anyone cares about reproducing the results exactly, my hacky conversion function that manually detects the incorrectly-rounded cases is <a href="https://gist.github.com/rygorous/ea042174cc289c3153876d2cace970d2">here</a>. (The version for signed decoding is unnecessary because that case is correctly rounded. I thought implementing it might give me an idea of what&#8217;s going on with the unsigned version, but no dice.)</p>
<p>OK, so both Intel and AMD are reasonably easy to explain, and probably just reuse hardware that is already there; it&#8217;s worth pointing out that BC6H also uses the same interpolation formula as BC7 and works on signed 17-bit integers internally, so there&#8217;s probably at least 3 color channels worth of 6Ã—17-bit multipliers lying around somewhere in the decoder path. I was initially very surprised about the 16-bit weights used by the Intel version, but if they just swap the operands between BC4/5 and BC6 and build something like a 8Ã—17-bit multiplier instead, that might be totally fine. (Or maybe they even have their own wider multiplier for BC4/5. Don&#8217;t know, I&#8217;m just guessing at likely implementation options from the observed behavior here.)</p>
<h3>NVidia GPUs</h3>
<p>The formulas used by NVidia GPUs are nothing like the other two vendors, and do not look like they can share any hardware with BC6H/BC7 decoding at all. They do, however, very much look like they&#8217;re designed to share as much as possible between BC1-5, and by someone who is dead set to spend as little logic on it as they can reasonably get away with. My best guess is that this design probably predates BC6H/BC7; BC6 forces you into having at least a few rectangular multipliers in the BCn decoder block and once you have them, there&#8217;s not much reason to avoid using them.</p>
<p>Anyway, the decoders we&#8217;ve seen before use simple linear interpolation formulas with round-to-nearest, or at least give results that are equivalent to them. The NVidia decoders use different formulas for the 5-bit-endpoint red/blue channels in BC1 than they use for the 6-bit-endpoint green channel, and then green/alpha channel logic seems to be the same, with BC4 and BC5 using just the green/alpha channel decoders (and ignoring red/blue). This in turn means that red/blue only ever work from 5-bit inputs, whereas green needs to support both 6-bit and 8-bit endpoints (the latter for use in BC5), and alpha always has 8-bit endpoints (for BC3).</p>
<p>Let&#8217;s do the simpler red/blue channels first. Ignacio&#8217;s 2009 article gives the formulas for palette entries 0/1 as well, but those are just equivalent ways of writing the 5-&gt;8 bit expand using bit replication. The interesting palette entries are 2 and 3, and those are (again, not following Ignacio&#8217;s presentation here, but this gives bit-equivalent results):</p>
<pre>  // red/blue, four-color mode; a, b are 5-bit
  col2 = ((2*a + b) * 22) &gt;&gt; 3;
  col3 = ((2*b + a) * 22) &gt;&gt; 3;
</pre>
<p>The rationale here is that expanding from 5 bits to 8 is effectively multiplying by 255/31, and if we compute (2*a + b) we need to divide the result by 3; the final fraction of 255/(31*3) = 2.741935 is quite close to 11/4, so that&#8217;s what it&#8217;s approximated by. So this calculation folds bit-expansion and linear interpolation into one and can work with slightly smaller intermediate values as a result (the multiplications by known constants are just shifts and adds).</p>
<p>For red and blue, the halfway point in three-color mode is computed similarly:</p>
<pre>  // red/blue, three-color mode; a, b are 5-bit
  col2 = ((a + b) * 33) &gt;&gt; 3;
</pre>
<p>This is a decent approximation, but somewhat annoyingly, it just behaves differently from the more straightforward AMD/Intel ones; folding the scaling and lerp into one makes the weight constants slightly smaller but also makes the overall rounding behavior different, which is awkward. Oh well.</p>
<p>Anyway, green and alpha is when the fun really starts. Both green and alpha do use a more explicit linear interpolation formulation with round-to-nearest, but the values being multiplied by and the overall way things are put together are still decidedly odd. But let&#8217;s start at the beginning. First, BC1 green, four-color mode (again, presentation different from Ignacio&#8217;s, but equivalent)</p>
<pre>  // green, four-color mode, a, b are 6-bit
  ae = (a &lt;&lt; 2) | (a &gt;&gt; 4); // bit expand
  be = (b &lt;&lt; 2) | (b &gt;&gt; 4);
  diff = be - ae;
  scaled_diff = 80*diff + (diff &gt;&gt; 2);
  col2 = ae + ((128 + scaled_diff) &gt;&gt; 8);
  col3 = be + ((128 - scaled_diff) &gt;&gt; 8);
</pre>
<p>and in three-color mode:</p>
<pre>  // green, three-color mode, ae, be as above
  diff = be - ae;
  scaled_diff = 128*diff + (diff &gt;&gt; 2);
  col2 = ae + ((128 * scaled_diff) &gt;&gt; 8);
</pre>
<p>Here, the effective factor is something like 80.25/256, which is pretty far off (the correct fraction for a real 1/3rd here would be around 85.33/256).</p>
<p>On to BC4/5. Here, our interpolation factor are multiples of 1/7th and 1/5th, respectively. NVidia conceptually splits this into two parts: we first multiply by the small integer numerator, and then by a fixed-point approximation of the reciprocal of 1/5 and 1/7, and all of that is sandwiched into our interpolation formula above.</p>
<p>The set of numbers we might want to multiply by here are thus 0,1,2,3,4,5,6, and 7. For one final trick, note that instead of saying that we want &#8220;1/5th of the way from a to b&#8221;, we might equivalently say (by symmetry) &#8220;4/5ths of the way from b to a&#8221;, and NV decoder uses this to avoid doing any multiplies in the &#8220;multiply by numerator&#8221; portion: of the values 0 to 5 we need for the multiples of one fifth, 0 is 0 (which is very cheap to multiply by indeed) and 1, 2, and 4 are powers of 2 (shifts). Of the remaining numbers, 5=5-0 (so instead of going 5/5ths from a to b, we can go 0/5ths from b to a) and 3=5-2 (turn 3/5ths from a to b into 2/5ths from b to a). For the multiples of 1/7th, it works similarly: 0,1,2,4 are already easy, and for the remaining ones, 7-3=4 (which is easy), 7-5=2 (easy), 7-6=1 (easy), and 7-7=0 (easy). So the HW here can switch &#8220;a&#8221; and &#8220;b&#8221; around so that the factor is either 0 or a power of 2, choosing which end of the line segment to interpolate from.</p>
<p>Either way, the interpolation procedure for BC4 in 6-interpolated-color mode works out to:</p>
<pre> // BC4; a, b are 8-bit, 6-interp mode
  ae = expand_to_16(a);
  be = expand_to_16(b);
  diff = b - a; // b, a not be, ae!
  col0 = ae;
  col1 = be;
  col2 = ae + 36*(diff); // 1/7
  col3 = ae + 36*(diff&lt;&lt;1); // 2/7
  col4 = be - 36*(diff&lt;&lt;2); // 3/7
  col5 = ae + 36*(diff&lt;&lt;2); // 4/7
  col6 = be - 36*(diff&lt;&lt;1); // 5/7
  col7 = be - 36*(diff); // 6/7
</pre>
<p>where <code>expand_to_16(x) = (x&lt;&lt;8) | x</code> for unsigned formats and the way less nice <code>expand_to_16(x) = copysign((abs(x) * ((1&lt;&lt;14) + (1&lt;&lt;7) + 1) &gt;&gt; 6, x)</code> (aside from the sign manipulation this is just shifts and ORs, I just wrote it more compactly).</p>
<p>Here 36=32+4 is a convenient number with just 2 set bits which makes it easy to multiply by with just shifts and adds. The actual factor we would want to approximate for the reciprocal here is 65535/(255 * 7) is about 36.714, so this is not terrible. In 4-interpolated-color mode for BC4, we get:</p>
<pre> // BC4; a, b are 8-bit, 4-interp mode
  ae = expand_to_16(a);
  be = expand_to_16(b);
  diff = b - a; // b, a not be, ae!
  col0 = ae;
  col1 = be;
  col2 = ae + 48*(diff); // 1/5
  col3 = ae + 48*(diff&lt;&lt;1); // 2/5
  col4 = be - 48*(diff&lt;&lt;1); // 3/5
  col5 = be - 48*(diff); // 4/5
</pre>
<p>48 = 32 + 16 is again just 2 set bits, the factor to approximate is 65535/(255*5) = 51.4, so that one&#8217;s somewhat dubious, but oh well, it is what it is.</p>
<p>BC5 is just two of those. Finally, for BC3 alpha, we get variation of the BC4 unsigned formulas, but this time we want a 8-bit result not a 16-bit result, so the mystery shift along with the rounding bias is back. Let <code>scale_and_round(k,d) = ((k*d + (d &gt;&gt; 3) + 128) &gt;&gt; 8)</code>, then</p>
<pre> // BC3 alpha; a, b are 8-bit, 6-interp mode
  diff = b - a;
  col0 = a;
  col1 = b;
  col2 = a + scale_and_round(36, diff); 
  col3 = a + scale_and_round(36, diff*2);
  col4 = b + scale_and_round(36, -diff*4);
  col5 = a + scale_and_round(36, diff*4);
  col6 = b + scale_and_round(36, -diff*2);
  col7 = b + scale_and_round(36, -diff);
</pre>
<p>and finally:</p>
<pre> // BC3 alpha; a, b are 8-bit, 4-interp mode
  diff = b - a;
  col0 = a;
  col1 = b;
  col2 = a + scale_and_round(48, diff); 
  col3 = a + scale_and_round(48, diff*2);
  col4 = b + scale_and_round(48, -diff*2);
  col5 = b + scale_and_round(48, -diff);
</pre>
<p>And that, at long last, is it.</p>
<h3>Conclusions</h3>
<p>If you got here, you have no-one to blame but yourself. If you&#8217;re one of the handful of people actively working on BCn encoders (hi!), this is probably actionable information to you. If you read this far out of curiosity or because you want to know exactly what you&#8217;re getting on the various GPUs, hopefully now you know. And in the not unlikely scenario that you are someone from the future with a question about weird BCn decoding behavior that I referred to this post, well, hopefully this makes it a bit clearer what&#8217;s going on. It&#8217;s also hopefully usable if you want a software model of the given three GPU vendors&#8217; decoders without having to juggle multiple machines or video cards around all the time.</p>
<p>If you&#8217;re an encoder author and not sure what to do about this, one of the main takeaways from this for me was that even though the D3D spec formulation of BC1 decoding explicitly works on 8-bit integer values and divides by 2 or 3 with truncation, and the Khronos spec treats everything as real numbers with unspecified precision, both Intel and AMD actually use the (somewhat nicer) division with round to nearest, and NVidia uses it as well in the green channel, while in the red/blue channels &#8220;it&#8217;s complicated&#8221; (because of the scaling factor folded into the linear interpolation). My older rygdxt (then turned into stb_dxt) used round-to-nearest, Oodle Texture used to default to the D3D spec behavior of truncating, but between AMD and Intel explicitly rounding and NVidia being somewhere in the middle, the next Oodle Texture release (2.9.5) will probably switch to round to nearest.</p>
<p>Also, instead of using the same weights for red, green, and blue, we are currently planning to use 85/256ths as our new approximation to 1/3 (we used to divide by 3 exactly) in the red and blue channels, and 83/256ths in the green channel, in the spirit of making everyone equally unhappy, to land somewhere between Intel&#8217;s choice of 85/256ths, AMDs of 21/64ths = 84/256ths, and NVidias sort-of 80.25/256th&#8217;s. It was chosen as a convenient dyadic fraction with not too many significant digits that had OK error estimates over the relevant range for all three vendors; somewhat higher for NV than for AMD or Intel, but they chose this crappy approximation themselves, so that&#8217;s on them. (For the halfway point, we use 1/2 with round to nearest, same as AMD and Intel.)</p>
<p>The main reason for this change is that Oodle Texture used to solely encode for the a D3D reference decoder that does not actually ship anywhere; we worked to stay within the relevant tolerances (the 3% relative error etc.) to avoid truly nasty surprises, but at the end of the day nobody ships against the D3D reference rasterizer, they ship on actual GPUs. The new approximate decoder model is not particularly complicated (the main weirdness is the different weights for G and R/B) and targeting it made the errors as computed with the accurate hardware models go down across the board, even though the error as measured against the D3D reference increased (it would have to, seeing as that was the sole metric we used to target before). As of this writing we haven&#8217;t done anything about BC3 alpha or BC4/5 yet, but we might change our reference/target decoder for that as well in the foreseeable future.</p>
<p>All three decoders are well within the error bounds required by the D3D spec (and Khronos doesn&#8217;t have any particular requirements to begin with). In an ideal world, we&#8217;d just pick one of these options (doesn&#8217;t matter which) and standardize on it; we have standardized the behavior of pretty much all other numbers formats that GPUs work with and that&#8217;s been good for everyone. It wouldn&#8217;t do much good for any new software for a while, as long as the multiple HW variants are around, but I don&#8217;t see texture memory bandwidth concerns or the BCn formats going away any time soon, the more complex BC6-7 and ASTC already nail down their respective decoder behavior, and it sure would be nice to put the current mess behind us <em>eventually</em>, even if it&#8217;s another 10 years from now.</p>
<p>Finally, the formulas and explanations given here are collated from data collected over several runs and experiments conducted over a period of one and a half years or so; I tried to unify the presentation somewhat for this blog post but I probably introduced mistakes along the way. If you find any problems, it&#8217;s probably a bug. Ping me and I&#8217;ll try to fix it.</p>
<p>Many thanks to Sean Barrett who did a lot of the legwork to figure out the last few details of Intels and AMDs BC4/5 decoders, Tom Forsyth who crucially told us that yes indeed the Intel decoders for BC4/5 used full 16-bit weights (something I dismissed as implausible for a good long while), and Ignacio CastaÃ±o for his 2009 write-up on the NVidia BCn decoders.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2021/10/04/gpu-bcn-decoding/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>Entropy coding in Oodle Data: Huffman coding</title>
		<link>https://fgiesen.wordpress.com/2021/08/30/entropy-coding-in-oodle-data-huffman-coding/</link>
					<comments>https://fgiesen.wordpress.com/2021/08/30/entropy-coding-in-oodle-data-huffman-coding/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Mon, 30 Aug 2021 08:49:26 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<category><![CDATA[Compression]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7171</guid>

					<description><![CDATA[Last time I covered the big picture, so we know the ground rules for the modular entropy coding layer in Oodle Data: bytestream consisting of several independent streams, pluggable algorithms, bytes in and bytes out, and entropy decoding is done as a separate pass, not inlined into the code that consumes the decoded data. There [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>Last time I covered the big picture, so we know the ground rules for the modular entropy coding layer in Oodle Data: bytestream consisting of several independent streams, pluggable algorithms, bytes in and bytes out, and entropy decoding is done as a separate pass, not inlined into the code that consumes the decoded data.</p>
<p>There are good and bad news here: the good news is that the encoders and decoders need to do just one thing, they have homogeneous data to work with, and they ultimately boil down to very simple operations in extremely predictable loops so they can pull out all the stops when it comes to optimization. The bad news is that since they are running on their own, if they&#8217;re inefficient, there&#8217;s no other useful work being accomplished when the decoders are stalled. Careful design is a necessity if we want to ensure that things go smoothly come decode time.</p>
<p>In this post, I&#8217;ll start by looking at Huffman coding, or rather, Huffman decoding; encoding, we won&#8217;t have to worry about much, since it&#8217;s an inherently easier problem to begin with and the design decisions we&#8217;ll make to enable fast decoding will also make encoding faster. Required reading for everything that follows will be my earlier posts <a href="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/">&#8220;A whirlwind introduction to dataflow graphs&#8221;</a> as well as my series <a href="https://fgiesen.wordpress.com/2018/02/19/reading-bits-in-far-too-many-ways-part-1/">&#8220;Reading bits in far too many ways&#8221;</a> (<a href="https://fgiesen.wordpress.com/2018/02/20/reading-bits-in-far-too-many-ways-part-2/">part 2</a>, <a href="https://fgiesen.wordpress.com/2018/09/27/reading-bits-in-far-too-many-ways-part-3/">part 3</a>). I will assume the contents of these posts as known, so if something is unclear and has to do with bit IO, it&#8217;s in there.</p>
<h3>Huffman (de)coding basics</h3>
<p>Huffman coding is a form of variable-length coding that in its usual presentation is given a finite symbol alphabet <em>Î£</em> along with a positive count <em>f<sub>s</sub></em> (the <em>frequency</em>) of how often each symbol <em>s</em> occurs in the source. Huffman&#8217;s classic algorithm then assigns variable-length binary codes (meaning strings of 0s and 1s) of length <em>c<sub>s</sub></em> to each symbol that guarantee unique decodability and minimize the number of bits in the coded version of the source; we want to send the symbols from the source in fewer bits, and the way to do so is to assign shorter codes to more frequent symbols. To be precise, it solves the optimization problem</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin+%5Csum_%7Bs+%5Cin+%5CSigma%7D+f_s+c_s&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin+%5Csum_%7Bs+%5Cin+%5CSigma%7D+f_s+c_s&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin+%5Csum_%7Bs+%5Cin+%5CSigma%7D+f_s+c_s&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle &#92;min &#92;sum_{s &#92;in &#92;Sigma} f_s c_s" class="latex" /></p>
<p>(i.e. minimize the number of bits sent for the payload, because each <em>c<sub>s</sub></em>-bit code word occurs <em>f<sub>s</sub></em> times in the source) for integer <em>c<sub>s</sub></em> subject to the constraints</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+c_s+%5Cin+%5Cmathbb%7BZ%7D%2C+1+%5Cle+c_s+%5Cforall+s+%5Cin+%5CSigma%2C+%5Csum_%7Bs+%5Cin+%5CSigma%7D+2%5E%7B-c_s%7D+%3D+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+c_s+%5Cin+%5Cmathbb%7BZ%7D%2C+1+%5Cle+c_s+%5Cforall+s+%5Cin+%5CSigma%2C+%5Csum_%7Bs+%5Cin+%5CSigma%7D+2%5E%7B-c_s%7D+%3D+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+c_s+%5Cin+%5Cmathbb%7BZ%7D%2C+1+%5Cle+c_s+%5Cforall+s+%5Cin+%5CSigma%2C+%5Csum_%7Bs+%5Cin+%5CSigma%7D+2%5E%7B-c_s%7D+%3D+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle c_s &#92;in &#92;mathbb{Z}, 1 &#92;le c_s &#92;forall s &#92;in &#92;Sigma, &#92;sum_{s &#92;in &#92;Sigma} 2^{-c_s} = 1" class="latex" /></p>
<p>The first of these constraints just requires that code lengths be positive integers and is fairly self-explanatory, the second requires equality in the Kraft-McMillan inequality which guarantees that the resulting code is both complete and uniquely decodable; in fact, let&#8217;s just call the quantity <img src="https://s0.wp.com/latex.php?latex=K+%3D+%28%5Csum_%7Bs+%5Cin+%5CSigma%7D+2%5E%7B-c_s%7D%29+-+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=K+%3D+%28%5Csum_%7Bs+%5Cin+%5CSigma%7D+2%5E%7B-c_s%7D%29+-+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K+%3D+%28%5Csum_%7Bs+%5Cin+%5CSigma%7D+2%5E%7B-c_s%7D%29+-+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="K = (&#92;sum_{s &#92;in &#92;Sigma} 2^{-c_s}) - 1" class="latex" /> the &#8220;Kraft defect&#8221;. When <em>K</em>&lt;0, the code has redundancy (i.e. there free code space left for assignment), for <em>K</em>=0 it is complete, and <em>K</em>&gt;0 is over-complete (not uniquely decodable or using more code space than is available).</p>
<p>Huffman&#8217;s algorithm to perform this construction is a computer science classic, intuitive, a literal textbook example for greedy algorithms and matroids, and it even gives us not just the sequence of code lengths, but an actual code assignment that achieves those code lengths. Nevertheless, actual Huffman codes are of limited use in applications, primarily due to one big problem: the above problem places no upper bound on the code lengths, and indeed a <em>n</em>-symbol alphabet can reach a maximum code length of up to <em>n</em>-1 given the right (adversarial) frequency distribution that produces a fully left- or right-leaning tree.<sup><a href="#fn1">1</a></sup> This is bad for both encoders and decoders, which generally <em>have</em> to adhere at the very least to limits derived from the choice of target platform (mostly register width) and bit I/O refill strategy (see my long series on the topic mentioned earlier), although it is often desirable to enforce much tighter limits (more on that below). One way to accomplish this is by taking the results of the standard Huffman construction and using heuristics and local rewrite rules on either the tree or the code lengths to make the longest codes shorter at the expense of making some other (more frequent) codes longer than they would otherwise have to be. Another easy heuristic option is to just use the regular Huffman construction and if codes end up over-long, divide all symbol frequencies by 2 while rounding up (so that non-zero symbol frequencies never drop to zero) and keep trying again until success. This works because it keeps the frequencies of more popular symbols approximately correct and in the right relation to each other, while it flattens out the distribution of the less popular symbols, which all end up eventually having symbol frequencies of 1, resulting in a uniform symbol distribution at the bottom end. A non-heuristic, actually optimal option is to directly solve a constrained version of the above optimized problem that adds a maximum codeword length constraint. The standard solution to the resulting combinatorial optimization problem is called the <a href="https://en.wikipedia.org/wiki/Package-merge_algorithm">package-merge algorithm</a>.</p>
<p>A related observation is that there is really no particular reason to prefer the code assignment produced by Huffman&#8217;s algorithm. Even for cases where no length-limiting is required to fall within usual limits, there are usually many possible code assignments that all result in the exact same bit count for the payload portion; and given an assignment of code length to symbols, it is easy to come up with a codeword assignment that matches those code lengths. In short, the per-symbol code lengths make a difference to how efficient a given code is, but we can freely choose which codes we assign subject to the length constraint, and can do so in a way that is convenient for us.</p>
<p>This has the effect of deciding on a single <em>canonical</em> Huffman tree for each valid sequence of code lengths (valid here meaning <em>K</em>=0), even in cases where Huffman&#8217;s algorithm generates distinct trees and code assignments that happen to have the same code lengths. The actual coded bits are just part of the puzzle; an actual codec needs to transmit not just the coded bits, but also enough information for the decoder to work out what codes were assigned to which symbols, and specifying a canonical Huffman code requires less information than its non-canonical counterpart would.<sup><a href="#fn2">2</a></sup> There are several options as to how exactly we assign codes from code lengths, the most common of which is to assign code words sequentially to symbols in lexicographic order for the (<em>c<sub>s</sub></em>, <em>s</em>) pairsâ€”that is, code lengths paired up with symbol values, ordered by ascending code length and then symbol value.<sup><a href="#fn3">3</a></sup></p>
<p>Using canonical, length-limited codes is a common choice. For example, <a href="https://datatracker.ietf.org/doc/html/rfc1951">Deflate</a> (the algorithm used in zlib/ZIP) uses canonical codes limited to a maximum length of 15 bits, and  JPEG limits the maximum code length to 16 bits.</p>
<p>In principle, such a data stream can be decoded by reading a bit at a time and walking the Huffman tree accordingly, starting from the root. Whenever a leaf is reached, the corresponding symbol is emitted and the tree walk resets back to the root. However, processing data one bit at a time in this fashion is very slow, and implementations generally avoid it. Instead, decoders try to decode many bits at once. At the extreme, when all code lengths are below some upper bound <em>N</em>, a decoder can just &#8220;peek ahead&#8221; <em>N</em> bits into the bitstream, and prepare a table that contains the results of the tree walks for all 2<sup><em>N</em></sup> possible bit patterns; table entries store which symbol was eventually reached and how many bits were actually consumed along the way. The decoder emits the symbol, advances its read cursor by the given number of bits, and reads the next symbol.</p>
<p>This method can be much faster and is easy to implement; however, when <em>N</em> is large, the resulting tables will be too, and there are steep penalties whenever table sizes start exceeding the sizes of successive cache levels, since the table lookups themselves are essentially random.<sup><a href="#fn4">4</a></sup> Furthermore, most sources being compressed aren&#8217;t static, so Huffman tables aren&#8217;t used forever. A typical Huffman table will<br />
last somewhere between 5 and 100 kilobytes of data. For larger <em>N</em>, it&#8217;s easy for table initialization to take as long, or longer, than actual bitstream decoding with that table, certainly for shorter-lived Huffman tables. The compromise most commonly used in practical implementations is to use a multi-level table scheme, where individual tables cover something like 9 to 12 bits worth of input. All the shorter symbols are decoded in a single table access, but the entries for longer symbols don&#8217;t contain a symbol value; instead, they reference a secondary table that resolves the remaining bits (there can be many such secondary tables). The point of Huffman coding is to assign shorter codes to more common symbols, so having to take a secondary table walk is relative uncommon. And of course there are many possible variants of this technique: table walks can do more than 2 levels, and in the other direction, if we have many shorter codes (say at most 4 bits long), then a single access to a 512-entry, 9-bit table can resolve two symbols (or even more) at once. This can accelerate decoding of very frequent symbols, at the expense of a more complicated decoder inner loop and slower table build step.<sup><a href="#fn5">5</a></sup></p>
<h3>Huffman coding in Oodle</h3>
<p>Our older codecs, namely Oodle LZH and LZHLW, used a 15-bit code length limit and multi-level tables, both common choices. The problem with this design is that every symbol decode needs to check whether a second-level table lookup is necessary; a related problem is that efficient bit IO implementations maintain a small buffer that is periodically refilled to contain somewhere between 23 and 57 bits of data (details depend on the choice of bit IO implementation and whether we&#8217;re on a 32-bit or 64-bit platform, see the bit-IO series I referenced earlier). When we know that codes are guaranteed to be short enough, we can decode multiple symbols per refill. This is significant because refilling the bit buffer is about as expensive as decoding a symbol itself; being able to always decode two symbols per refill, as opposed to one, has the potential to reduce refill overhead from around 50% of decode time to about 33% on 32-bit targets!</p>
<p>When Charles first prototyped what later turned into Kraken, he was using our standard bit IO implementation, which uses byte-wise refill so post-refill, the 32-bit version guarantees at least 25 usable bits, while for in 64-bit the number is 57 bits, all typical limits.</p>
<p>Therefore, if codes are limited to at most 12 bits, the 32-bit decoder can sustain a guaranteed two symbols per refill; for 64-bit, we can manage 4 decodes per refill (using 48 out of 57 bits). As a side effect, the tables with a 12-bit limit are small enough that even without a multi-level table scheme, everything fits in the L1D cache. And if codes are limited to 11 bits, the 32-bit decoders still manage 2 symbols per refill, but the 64-bit decoders now go up to 5 decodes per refill, which is a nice win.  Therefore, the newer Oodle codecs use Huffman codes length-limited to only 11 bits, which allows branch-free decoding of 2 symbols per refill on 32-bit platforms, and 5 symbols per refill on 64-bit targets.<sup><a href="#fn6">6</a></sup></p>
<p>The other question is, of course, how much limiting the code lengths aggressively in this way costs in terms of compression ratio, to which the answer is: not much, although I don&#8217;t have good numbers for the 11-bit limit we eventually ended up with. For a 12-bit limit, the numbers in our tests were below a 0.1% hit vs. the original 15-bit limit (albeit with a note that this was when using proper package-merge, and that the hit was appreciably worse with some of the hackier heuristics). We ended up getting a fairly significant speed-up from this choice, so that slight hit is well worth it.</p>
<p>With these simplifications applied, our core Huffman decoder uses pre-baked table entries like this:</p>
<pre>struct HuffTableEntry {
    uint8_t len; // length of code in bits
    uint8_t sym; // symbol value
};</pre>
<p>and the decoder loop looks something like this (32-bit version):</p>
<pre>while (!done) {
    bitbuf.refill();
    // can decode up to 25 bits without refills!

    // Decode first symbol
    {
        intptr_t index = bitbuf.peek(11);
        HuffTableEntry e = table[index];
        bitbuf.consume(e.len);
        output.append(e.sym);
    }

    // Decode second symbol
    {
        intptr_t index = bitbuf.peek(11);
        HuffTableEntry e = table[index];
        bitbuf.consume(e.len);
        output.append(e.sym);
    }
}</pre>
<p>Note that with a typical bit buffer implementation, the <code>peek</code> operation uses a single bit-wise AND or shift instruction; the table access is a 16-bit load (and load address calculation, which depending on the target CPU might or might not be an addressing mode). The bit buffer <code>consume</code> operation usually takes an integer shift and an add, and finally the &#8220;append&#8221; step boils down to an integer store, possibly an integer shift to get the symbol value in the right place prior to the store (the details of this very much depend on the target ISA, I&#8217;ll get into such details in later posts in this series), and some pointer updating that are easily amortized if the loop is unrolled more than just twice.</p>
<p>That means that in terms of the work we&#8217;re doing, we&#8217;re already starting out with something around 5-7 machine instructions per symbol, and if we want good performance, it matters a lot what exactly those instructions are and how they relate. But before we get there, there&#8217;s a few other details about the resulting dataflow to understand first.</p>
<p>The first and most important thing to note here is what the critical path for the entire decoder operation is: everything is in serialized through the bit buffer access. The <code>peek</code> operation depends on the bit buffer state after the previous <code>consume</code>, the table access depends on the result of the <code>peek</code> operation, and the <code>consume</code> depends on the result of the table access (namely the <code>len</code> field) which completes our cycle. This is the critical dependency chain for the entire decoder, and the actual symbol decoding or output isn&#8217;t even part of it. Nothing in this loop depends on or cares about <code>e.sym</code> at all; its load and eventual store to the output buffer should be computed eventually, but there&#8217;s no rush. The code length determination, on the other hand, is our primary bottleneck. If the critical computation for <code>peek</code> and <code>consume</code> works out to a single 1-cycle latency integer instruction each (which it will turn out it does on most of the targets we&#8217;ll see later in this series), then everything hinges on how fast we can do that table access.</p>
<p>On current out-of-order CPUs (no matter the ISA), the answer usually turns out to be 4 or 5 clock cycles, making the critical path from one symbol decoded to the next either 6 or 7 clock cycles. This is very bad news, because as just pointed out, we&#8217;re executing around 5-7 instructions per symbol decoded, total. This means that, purely from the dependency structure, we&#8217;re limited to around 1 instruction per cycle average on CPUs that can easily sustain more than 3 instructions per cycle given the right code. Viewed another way, in those 7 cycles per symbol decoded, we could easily run 21 independent instructions (or even more) if we had them, but all we do is something like 6 or 7.<sup><a href="#fn7">7</a></sup></p>
<p>The obvious solution here is to not have a single bit buffer (or indeed bitstream), but several at once. Once again we were not the first or only ones to realize this, and e.g. Yann Collet&#8217;s Huff0 uses the same idea. If the symbols we want to decode are evenly distributed between multiple bitstreams, we can have multiple one-symbol-at-a-time dependency chains in flight at once, and hopefully get to a state where we&#8217;re mostly (or even completely) limited by instruction throughput (i.e. total instruction count) and not latency.</p>
<p>Our high-level plan of attack will be exactly that. Huff0 uses 4 parallel bitstreams. For reasons that will become clearer in subsequent parts of this series, Huffman coding in the newer Oodle codecs has two different modes, a 3-bitstream mode and a 6-bitstream mode. Using more bitstreams enables more parallelism but the flipside is that it also requires more registers to store the per-bitstream state and is more sensitive to other micro-architectural issues; the actual counts were chosen to be a good compromise between the desires of several different targets.<sup><a href="#fn8">8</a></sup></p>
<p>For the rest of this post, I&#8217;ll go over other considerations; in the next part of this series, I plan to start looking at some actual decoder inner loops.</p>
<h3>Aside: tANS vs. Huffman</h3>
<p>The ANS family of algorithms is the new kid on the block among entropy coders. Originally we meant to use tANS in the original Kraken release back in 2016, but the shipping version of Kraken doesn&#8217;t contain it; it was later added as a new entropy coding variant in 2018 along with Leviathan, but is rarely used.</p>
<p>However, one myth that was repeatedly claimed around the appearance of tANS was that it is &#8220;faster than Huffman&#8221;. While it is true that some tANS decoders are faster than some Huffman decoders, this is mostly an artifact of other implementation choices and the resulting dependency structure of the decoder, so I want to spend a few paragraphs explaining when and why this effect appears, and why despite it, it&#8217;s generally not a good idea to replace Huffman decoders with tANS decoders.</p>
<p>I&#8217;m not going to explain any of the theory or construction of tANS here; all I&#8217;ll note is that the individual per-symbol decode step uses tables similar to a Huffman decode table, and paraphrasing <a href="http://cbloomrants.blogspot.com/2015/10/huffman-performance.html">old code from Charles</a> here, the key decode step ends up being:</p>
<pre>HuffTableEntry e = table[state];
state = next_state[state] | bitbuffer.get(e.len); 
output.append(e.sym);</pre>
<p>In essence, this is a &#8220;lookahead Huffman&#8221; decoder, where each stream keeps <em>N</em> bits of lookahead at all times. Instead of looking at the next few bits in the bit buffer to figure out the next code length, we already have enough bits to figure out what the next code is stored in our <code>state</code>, and after decoding a symbol we shift out a number of bits corresponding to the length of the code and replenish the bottom of our state from the bitstream.<sup><a href="#fn9">9</a></sup></p>
<p>The bits come from a different place, and we now have the extra <code>state</code> de-facto shift register in the mix, so why would this run faster, at least sometimes? Again we need to look at the data dependencies.</p>
<p>In the regular Huffman decode, we had a single critical dependency chain with the peek/table lookup/consume loop, as well as the bit-buffer refill every few symbols. In this version of the loop, we get not one but two separate dependency chains:</p>
<ol>
<li>The <code>state</code> update has a table lookup (two in fact, but they&#8217;re independent and can overlap), the bit buffer <code>get</code> operation to get the next set of low bits for the next state (usually something like an integer add and a shift), and an OR to combine them. This is not any better than the regular Huffman decoder (in fact it&#8217;s somewhat worse in multiple ways), but note that the high-latency memory loads happen <em>before</em> any interactions with the bit buffer.</li>
<li>The bit buffer update has the periodic refill, and then the <code>get</code> operations after the <code>state</code> table lookups (which are a combined <code>peek</code> and <code>consume</code>).</li>
</ol>
<p>The crucial difference here is that in the regular Huffman decoder, most of the &#8220;refill&#8221; operation (which needs to know how many bits were consumed, load extra bytes and shift them into place, then do some accounting) is on the same dependency chain as the symbol decoding, and decoding the next symbol after a refill can make no meaningful progress until that refill is complete.  In the TANS-style decoder, the refill can overlap with the state-dependent table lookups; and then once those table lookups complete, the updates to the bit buffer state involve only ALU operations. In throughput terms the amount of work per symbol in the TANS-style decoder is not better (in fact it&#8217;s appreciably worse), but the critical-path latency is improved, and when not using enough streams to keep the machine busy, that latency is all that matters.</p>
<p>A related technique that muddles this further is that ANS-based coders can use interleaving, where (in essence) we just keep multiple <code>state</code> values around, all of which use the same underlying bitstream. This gives us twice the parallelism: two TANS decoders with their own <code>state</code> are independent, and even making them share the same underlying bitstream keeps much of the advantage, because the high-latency portion (the table lookup) is fully overlapped, and individual reads from the bit buffer (once all the lengths are known) can usually be staggered just 1 cycle apart from each other.</p>
<p>A 2Ã— interleaved tANS decoder is substantially faster than a single-stream Huffman decoder, and has lower incremental cost (a single extra integer state is easier to add and track than the registers required for a second bitstream state). However it also does more work per symbol than a 2-stream Huffman decoder does, so asymptotically it is slowerâ€”if you ever get close to the asymptote, that is.</p>
<p>A major disadvantage of tANS-style decoding is that interleaving multiple states into the same bitstream means that there is essentially only a single viable decoder implementation; bits need to be read in the same way and at the same time by all decoders to produce the same results, fast implementations need larger and more expensive to set up tables than a straight Huffman decoder, plus other overheads in the bitstream. In contrast, as we&#8217;ll see later, the 6-stream Huffman<br />
coding variant is decoded on some platforms as two 3-stream blocks, which would not easily map to a tANS-style implementation. In the end, the lower asymptotic performance and decreased flexibility is what made us stick with a regular Huffman decoder in Oodle Data, because as it turns out we can get decent wins from decoding the bitstreams somewhat differently on different machines.</p>
<h3>Huffman table initialization</h3>
<p>As mentioned several times throughout this post, it&#8217;s easy for Huffman table initialization to become a major time sink, especially when tables are large, used for just a short time, or the table builder is unoptimized. Luckily, optimizing the table builder isn&#8217;t too hard, and it also gives a nice and intuitive (to me anyway) view of how canonical Huffman codes work in practice.</p>
<p>For now, suppose that decoding uses a MSB-first bit packing convention, i.e. a bit-at-a-time Huffman decoder would read the MSB of a codeword first and keep reading additional less significant bits until the symbol is determined. Although our 11-bit code length limit is quite short, that&#8217;s still a large number of bits to work through in an example, so let&#8217;s instead consider at a toy scenario with a code length limit of 4 bits and four symbols a, b, c, d with code lengths 1, 3, 2, and 3 bits, respectively.</p>
<p>The code length limit of 4 bits means we have a total space of 16 possible 4-bit patterns to cover; I&#8217;ll refer to each such bit pattern as a &#8220;slot&#8221;. Our 1-bit symbol &#8220;a&#8221; is the first and shortest, which means it gets assigned first, and as such gets a 1-bit code of &#8220;0&#8221;, with the next 3 bits irrelevant. In short, all slots with a leading 0 bit correspond to the symbol &#8216;a&#8217;; there&#8217;s 2<sup>4-1</sup> = 8 of them. The next-shortest symbol is &#8216;c&#8217;, which get the next available 2-bit code. All codes starting with 0 are taken; the next available slot starts with a 2-bit pattern of &#8220;10&#8221;, so that&#8217;s what we use. There are 2<sup>4-2</sup> = 4 slots corresponding to a bit pattern starting &#8220;10&#8221;, so that&#8217;s what we assign to c. The two remaining symbols &#8216;b&#8217; and &#8216;d&#8217; get 3-bit codes each. Anything starting with &#8220;0&#8221; or &#8220;10&#8221; is taken, so they need to start with &#8220;11&#8221;, which means our choice is forced: &#8216;b&#8217; being earlier in the alphabet (since we break ties in code length by which symbol is lower-valued) gets the codes starting &#8220;110&#8221;, &#8216;d&#8217; gets &#8220;111&#8221;, and both get 2<sup>4-3</sup>=2 table slots each. If we write up the results, we get:</p>
<table>
<tbody>
<tr>
<th>Code</th>
<th>Sym</th>
<th>Len</th>
</tr>
<tr>
<td><b>0</b>000</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>001</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>010</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>011</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>100</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>101</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>110</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>0</b>111</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td><b>10</b>00</td>
<td>c</td>
<td>2</td>
</tr>
<tr>
<td><b>10</b>01</td>
<td>c</td>
<td>2</td>
</tr>
<tr>
<td><b>10</b>10</td>
<td>c</td>
<td>2</td>
</tr>
<tr>
<td><b>10</b>11</td>
<td>c</td>
<td>2</td>
</tr>
<tr>
<td><b>110</b>0</td>
<td>b</td>
<td>3</td>
</tr>
<tr>
<td><b>110</b>1</td>
<td>b</td>
<td>3</td>
</tr>
<tr>
<td><b>111</b>0</td>
<td>d</td>
<td>3</td>
</tr>
<tr>
<td><b>111</b>1</td>
<td>d</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Note that we simply assigned all of the code space in linear order to symbols sorted by their code length and breaking ties by symbol value, where each symbol gets 2<sup><em>N</em>&#8211;<em>k</em></sup> slots, where <em>N</em> is the overall code length limit and <em>k</em> is the code length for the symbol in question. At any step, the next code we assign simply maps to the first few bits of the next available table entry. It&#8217;s trivial to discover malformed codes in this form: we need to assign exactly 2<sup><em>N</em></sup> slots total. If any symbol has a code length larger than <em>N</em> or we end up assigning more or less than that number of slots total, the code is, respectively, over- or under-complete; the current running total of slots assigned is a scaled version of the number in the Kraft inequality.</p>
<p>Moreover, it should also be obvious how the canonical table initialization for a MSB-first code works out in practice: the table is simply written top to bottom in linear order, with each entry repeated 2<sup><em>N</em>&#8211;<em>k</em></sup> times; in essence, a slightly more complicated <code>memset</code>. Eventually, each slot is just duplicated a small number of times, like 4, 2 or just 1, so as code lengths get closer to the limit it&#8217;s better to use specialized loops and not treat it as a big memset anymore, but the basic idea remains the same.</p>
<p>This approach requires a list of symbols sorted by code length; for Oodle, the code that decodes Huffman table descriptions already produces the symbol list in this form. Other than that, it is straightforward.</p>
<p>The only wrinkle is that, for reasons to do with the details of the decoder implementation (which will be covered in future posts), the fast Huffman decoders in Oodle do not actually work MSB-first; they work LSB-first. In effect, we produce the table as above, but then reverse all the bit strings. While it is possible to directly build a canonical Huffman decode table in a LSB-first fashion, all the nice (and SIMD-friendly) sequential memory writes we had before now turn into scatters, which is a lot messier. It turns out to be easier and faster to first build a MSB-first table as described above and then apply a separate bit-reverse permutation to the elements to obtain the corresponding LSB-first decoding table. This is extra work, but bit-reverse permutations have a very regular structure and also admit a fast SIMD implementation that ends up being essentially a variant of a matrix transpose operation. In our experiments, the SIMD-friendly (and memset-able) MSB-first table initialization followed by a MSBâ†’LSB bit-reverse was much faster than a direct LSB-first canonical table initialization, and LSB-first decoding saved enough work in the core decode loops to be worth the extra step during table building for the vast majority of our Huffman tables, so that&#8217;s what we settled on.</p>
<p>And that&#8217;s it for the high-level overview of Huffman decoding in Oodle Data. For the next few upcoming posts, I plan to get into detail about a bunch of the decoder loops we use and the CPUs they are meant for, which should keep us busy for a while!</p>
<h3>Footnotes</h3>
<p><span id="fn1">[1]</span> An easy way to construct a pathological frequency distribution is to pick the symbol frequencies as Fibonacci numbers. They grow exponentially with <em>n</em>, but not so quickly as to make the symbol frequencies required to exceed common practical code length limits impossible to occur for typical block sizes. The existence of such cases for at least certain input blocks means that implementations need to handle them.</p>
<p><span id="fn2">[2]</span> There are strictly more Huffman trees than there are canonical Huffman trees. For example, if symbol &#8216;a&#8217; has a 1-bit code and symbols &#8216;b&#8217; through &#8216;e&#8217; all get 3-bit codes, then our corresponding canonical tree for that length distribution might be <code>(a ((b c) (d e)))</code>, but it is equally possible (with the right symbol frequencies) to get say the tree <code>(a ((b d) (c e)))</code> or one of many other possible trees that all result in the same code lengths. The encoded size depends only on the chosen code lengths, and spending extra bits on distinguishing between these equivalent trees is wasteful unless the tree shape carries other useful information, which it can in some applications, but not in usual Huffman coding. Along the same lines, for any given Huffman tree there are in general many possible sets of symbol frequencies that produce it, so sending an encoding of the original symbol frequencies rather than the tree shape they result in is even more wasteful.</p>
<p><span id="fn3">[3]</span> When assigning codewords this way and using a MSB-first bit packing convention, the blocks of codes assigned to a given code length form intervals. When codes are limited to <em>N</em> bits, that means the length of a variable-length code word in the bitstream can be determined by figuring out which interval a code falls into, which takes at most <em>N</em>-1 comparisons against constants of width â‰¤<em>N</em>-1 bits. This allows for different, and often more efficient, implementation options for the latency-critical length determination step.</p>
<p><span id="fn4">[4]</span> This is inherent: the entire point of entropy coding is to make bits in the output bitstream approach an actual information content around 1 bit. In the process of Huffman coding, we are purposefully making the output bitstream &#8220;more random&#8221;, so we can&#8217;t rely on locality of reference to give us better cache hit rates here.</p>
<p><span id="fn5">[5]</span> Decoding multiple symbols at once may sound like an easy win, but it does add extra work and complications to decoding symbols that are long enough not to participate in pair-at-a-time decoding, and once again the table build time needs to be carefully watched; generally speaking multi-symbol decoding tends to help on extremely redundant input but slows down decoding of data that achieves maybe 7 bits per byte from Huffman coding, and the latter tends to be more common than the former. The actual evaluation ends up far from straightforward and ends up highly dependent on other implementation details. Multi-symbol decoding tends to look good when plugged into otherwise inefficient decoders, but as these inefficiencies are eliminated the trade-offs get more complicated.</p>
<p><span id="fn6">[6]</span> No doubt we were neither the first nor last to arrive at that particular design point; with an 8-bit alphabet, the set of viable options for at least two decodes per refill if 32b targets matter essentially boils down to 9b to 12b, with 11b being the first where 5 decodes/refill on 64b targets are possible.</p>
<p><span id="fn7">[7]</span> This gap is one of the things that makes multi-symbol decoding look great when plugged into a simple toy decoder. Decoding multiple symbols at once requires extra bookkeeping and instructions, and also introduces extra dependencies between the stores since the number of symbols decoded in each step is no longer known at compile time and folded into an addressing mode, but when we&#8217;re only using about a third of the capacity of the machine, inserting extra instructions into the symbol decode is very cheap.</p>
<p><span id="fn8">[8]</span> Oodle has the same bitstream format for everyone; while we can (and do) make choices to avoid pathologies on all of the targets we care about, any choice we make that ends up in the bitstream needs to work reasonably well for all targets.</p>
<p><span id="fn9">[9]</span> <code>next_state</code> here essentially just tabulates a shift and mask operation; in a &#8220;real&#8221; tANS decoder this can be more complicated, but when we&#8217;re looking at a case corresponding to a Huffman table and symbol distribution, that&#8217;s all it does.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2021/08/30/entropy-coding-in-oodle-data-huffman-coding/feed/</wfw:commentRss>
			<slash:comments>5</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>Entropy coding in Oodle Data: the big picture</title>
		<link>https://fgiesen.wordpress.com/2021/07/09/entropy-coding-in-oodle-data-the-big-picture/</link>
					<comments>https://fgiesen.wordpress.com/2021/07/09/entropy-coding-in-oodle-data-the-big-picture/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Sat, 10 Jul 2021 02:41:52 +0000</pubDate>
				<category><![CDATA[Coding]]></category>
		<category><![CDATA[Compression]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7161</guid>

					<description><![CDATA[April 26, 2016 was the release date of Oodle 2.1.5 which introduced Kraken, so it celebrated its 5-year anniversary recently. A few months later we launched Selkie and Mermaid, which wetre already deep in development at the time of the Kraken release but not quite finished yet, and in early 2018 we added Leviathan, adding [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>April 26, 2016 was the release date of Oodle 2.1.5 which introduced Kraken, so it celebrated its 5-year anniversary recently. A few months later we launched Selkie and Mermaid, which wetre already deep in development at the time of the Kraken release but not quite finished yet, and in early 2018 we added Leviathan, adding a higher-compression option to our current suite of codecs, the &#8220;sea beastiary&#8221;.</p>
<p>In those 5 years, these codecs have seen wide adoption in their intended market (which is video games).</p>
<p>There&#8217;s a few interesting things to talk about, and the 5-year anniversary seems as good a reason as any to get started; this is the first in what will be a series of yet to be determined length, in which I&#8217;ll do a deep-dive on one interesting aspect of these codecs, namely the way they handle entropy coding.</p>
<p>Before I can get into any details, first some general notes on how things fit together. Kraken, Mermaid, Selkie, and Leviathan are lossless data compression algorithms, all variations on the basic LZ77 + entropy coding formula that has been the de facto standard in general-purpose compressors since the late 80s, because such codecs can achieve a good balance of compression ratio and compression/decompression speed for practical applications. Other codecs belonging to this family include Deflate (ZIP/gzip), LZX (Amiga LZX/CAB), LZMA (7zip/xz), Zstd, Brotli, LZHAM, and many others, including most of the older Oodle Data codecs (Oodle LZH/LZHLW, LZA, LZNA, and BitKnit).</p>
<p>The &#8220;LZ77&#8221; portion here refers to the LZ77 algorithm, which, broadly speaking, compresses data by replacing repeated byte sequences in a stream with references to prior occurrences; as long as the back-reference is smaller than the bytes themselves, this will result in compression. Nobody actually uses the original LZ77 algorithm per se (which has a very inefficient encoding by today&#8217;s standards), but the general idea remains the same. Some codecs, especially ones designed for faster decoding or encoding, use just this byte/string matching approach without any entropy decoding; this includes many well-known faster codecs such as LZ4, Snappy, LZO, and again many others, including the remaining older Oodle Data<br />
codecs (Oodle LZB/LZBLW and LZNib).</p>
<p>I won&#8217;t talk about the LZ77 portion here (that&#8217;s its whole own thing), and instead focus on the &#8220;entropy coding&#8221; portion of things. In essence, what all these codecs do is identify repeated strings, resulting in some mixture of back-references (&#8220;matches&#8221;) and bytes that couldn&#8217;t be profitably matched to anything earlier in the data stream (&#8220;literals&#8221;). All the codecs mentioned differ in how exactly these things are encoded, but all of them eventually boil it down to one<br />
or more data streams that each have symbols from a finite-size and not too large alphabet (typical sizes include a few dozen to a few hundred distinct symbols in the alphabet), plus maybe a few extra streams that contain raw bits for data that is essentially random.</p>
<p>Entropy coding, then, processes these streams of symbols from a finite alphabet and tries to turn them into an equivalent form that is smaller. There are numerous ways to do this; one well-known method, and indeed a mainstay in data compression to this day (with some caveats), is Huffman coding, which counts how often individual symbols occur and assigns short bit codes to more likely symbols and longer symbols to less likely symbols, in a way that remains unambiguous and uniquely decodable.<sup><a href="#fn1">1</a></sup></p>
<p>The second major class of techniques is arithmetic coding, which compresses slightly better than Huffman for typical data and can give significant improvements on either small alphabets or highly skewed distributions, and is also more suitable for applications when there is not an a priori known fixed symbol distribution, but rather an on-line model that is being updated as new data is seen (&#8220;adaptive coding&#8221;).</p>
<p>Finally, the new kid on the block is the ANS (&#8220;Asymmetric Numeral System&#8221;) family of methods, which is conceptually related to arithmetic coding but works differently, and has different trade-offs. Kraken and Mermaid use ANS sometimes and Leviathan frequently; I&#8217;m sure I&#8217;ll cover its usage in those codecs at some point, and for now there are plenty of my older writings on the topic from around 2014-2016.</p>
<p>Anyway, it is common for LZ77-derived algorithms with a Huffman or arithmetic coding backend to use odd alphabet sizes; when you&#8217;re assigning variable-length codes to everything anyway, it doesn&#8217;t really matter whether your alphabet has just 256 distinct byte values in it, or if the limit is 290 or 400 or something else non-power-of-2. It&#8217;s likewise common to interleave all kinds of data into a single contiguous bitstream. The sea beastiary family does things a bit differently than most codecs.</p>
<p>Namely, the input (uncompressed) data is processed in 128KiB chunks, which all have self-contained bytestreams with an explicitly signaled size.<sup><a href="#fn2">2</a></sup> These chunk bytestreams are themselves made up out of multiple independent bitstreams for different types of data, and most of these streams (except for a few &#8220;misc&#8221;/leftover data streams) use a single shared entropy coding layer to handle what we now call &#8220;primary streams&#8221;.<sup><a href="#fn3">3</a></sup></p>
<p>This layer works on streams that always use a byte alphabet, i.e. unsigned values between 0 and 255. That means that everything that goes through this layer needs to be presented in a byte-packed format. This kind of restriction is common in fast byte-aligned LZ77 coders without a final entropy coding stage (like LZ4 or Snappy), but atypical outside of that space.</p>
<p>Working in bytes needs some care to be taken in other parts of the codec design, but comes with a massive advantage, because it allows us to have an uncompressed mode that just stores streams as uncompressed bytestreams and doesn&#8217;t need any decoding at all. That means that streams that are nearly random, or just short, don&#8217;t have to waste CPU time on setting up a more complicated decoder and then decoding bytes individually through a more complicated algorithm when just grabbing uncompressed bytes straight from the bytestream is good enough.</p>
<p>In short, for most codecs, the choice of entropy coding scheme is a design-time decision. Algorithms like Deflate which uses LZ+Huffman <em>always</em> perform Huffman coding, even when doing so isn&#8217;t actually worthwhile, because they use extended alphabets with more than 256 symbols in them that mean pass-through coding would still need to use larger-than-8-bit units, both introducing extra waste and making it more expensive in terms of CPU time. Sticking with a byte alphabet means that the Oodle sea beastiary codecs can either use a conventional entropy coding stage or choose to send data uncompressed and act like a faster byte-aligned LZ codec. In fact, that is exactly how Selkie works: under the hood, Selkie is the same codec as Mermaid, but with all streams in uncompressed mode at all times (and some other features disabled).</p>
<p>The final big departure from most codecs is that the sea beastiary codecs run all entropy decoding as separate passes that consume an input bytestream with a common header and output an array of bytes. Uncompressed streams can skip the decoding step and consume data straight from the input bytestream. These passes just handle whatever entropy coding scheme is used and nothing else. This means that instead of the choice of entropy coders being fundamentally built into the algorithm, it&#8217;s a collection of small loops that can easily be interchanged or added to, and indeed that&#8217;s what we&#8217;ve been doing; the original version of Kraken shipped in Oodle 2.1.5 with a choice between just uncompressed streams and one particular version of Huffman coding. We&#8217;ve added several more options since then, including an alternative Huffman coding variant, an optional RLE pre-pass, TANS, and the ability to mix-and-match all of these within a single stream, all without it being a big redesign, and we&#8217;re quite happy with the results.</p>
<p>There&#8217;s many interesting things in here that I plan to cover in small installments, starting with the way we do Huffman (de)coding, which is worth several posts on its own. I&#8217;ll try to keep updating this at a reasonable cadence, but no promises since my blogging time is pretty limited these days!</p>
<h3>Footnotes</h3>
<p><span id="fn1">[1]</span> Huffman coding technically refers to such variable-length<br />
coding (meaning not all symbols need to be coded with the same number of bits) only when the code in question is actually generated via Huffman&#8217;s algorithm (which minimizes the length of the coded stream given a certain number of assumptions), but in practice most implementations don&#8217;t actually use Huffman&#8217;s original algorithm which can assign awkwardly long code lengths to rare symbols. Instead the maximum code length is usually capped, which is a different problem and yields codes that are not actually Huffman codes (and are very slightly less efficient), but just general variable-length codes. Having said that, approximately everyone calls it Huffman coding even when the codes are length-limited anyway, and I won&#8217;t sweat it either in the following.</p>
<p><span id="fn2">[2]</span> The chunks being independent is how we implement what is called &#8220;thread-phased&#8221; decoding. As is explained further down in the article, Oodle decodes the bitstreams to temporary memory buffers in a first phase, and the chunks can be processed completely independently for this part of the process, making it a prime candidate to move off to a separate thread (or threads). After phase 1 completes, the actual LZ decoding takes place in &#8220;phase 2&#8221;; this portion can make references to any bytes earlier in the stream, so it must be run in order and can&#8217;t as easily be threaded. Customers that want wider parallelism (or easy seeking) usually chop data into fixed-size blocks, typically between 64KiB and 512KiB, that are encoded independently with no back-references permitted across block boundaries. This decreases compression ratio but makes it trivial to have many workers decoding different blocks at the same time.</p>
<p><span id="fn3">[3]</span> In the original code, these streams of bytes are, very non-distinctly, just called &#8220;arrays&#8221;. When specifying the bitstream more formally, a lot of things suddenly needed proper names to avoid confusion, and &#8220;arrays&#8221; was just too indistinct and overloaded, so &#8220;primary streams&#8221; was what we settled on. Sometimes additional data streams exist within a chunk for information not contained in the primary streams, and these are therefore called &#8220;secondary streams&#8221;.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2021/07/09/entropy-coding-in-oodle-data-the-big-picture/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
		<item>
		<title>Frequency responses of half-pel filters</title>
		<link>https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/</link>
					<comments>https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/#respond</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Sat, 20 Jul 2019 21:58:12 +0000</pubDate>
				<category><![CDATA[Maths]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7125</guid>

					<description><![CDATA[In the previous post, I looked at repeated application of FIR filters on the same signal and laid out why their frequency response is the thing to look at if we&#8217;re wondering about their long-term stability under repeated application, because the Discrete-Time Fourier Transform (DTFT) of the filter coefficients corresponds to the eigenvalues of the [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>In the <a href="https://fgiesen.wordpress.com/2019/04/08/what-happens-when-iterating-filters/">previous post</a>, I looked at repeated application of FIR filters on the same signal and laid out why their frequency response is the thing to look at if we&#8217;re wondering about their long-term stability under repeated application, because the Discrete-Time Fourier Transform (DTFT) of the filter coefficients corresponds to the eigenvalues of the linear map that convolves a signal with the filter once.</p>
<p>Now, let&#8217;s go back to the <a href="https://caseymuratori.com/blog_0035">original problem</a> that Casey asked about: half-pixel (or half-pel) interpolation filters for motion compensation. We know from Casey&#8217;s post which of the filters he tried &#8220;blow up&#8221; under repeated application, and which ones don&#8217;t, and we now know some of the theory. The question is, does it track? Let&#8217;s find out!</p>
<h3>A small filter zoo: 6 taps</h3>
<p>We&#8217;ll be looking at the frequency responses of various filters here. As a reminder, what we&#8217;re looking at are the values of the function</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%28%5Comega%29+%3D+%5Csum_%7Bk%3D0%7D%5E%7Bm-1%7D+f_k+%5Cexp%28-i+%5Comega+k%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%28%5Comega%29+%3D+%5Csum_%7Bk%3D0%7D%5E%7Bm-1%7D+f_k+%5Cexp%28-i+%5Comega+k%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%28%5Comega%29+%3D+%5Csum_%7Bk%3D0%7D%5E%7Bm-1%7D+f_k+%5Cexp%28-i+%5Comega+k%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle &#92;hat{f}(&#92;omega) = &#92;sum_{k=0}^{m-1} f_k &#92;exp(-i &#92;omega k)" class="latex" /></p>
<p>where the <img src="https://s0.wp.com/latex.php?latex=f_k&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=f_k&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f_k&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="f_k" class="latex" /> are our filter coefficients, which are real numbers. It&#8217;s easy to prove from this form that the function <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;hat{f}" class="latex" /> is continuous, 2Ï€-periodic, and has <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28-%5Comega%29+%3D+%5Coverline%7B%5Chat%7Bf%7D%28%5Comega%29%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28-%5Comega%29+%3D+%5Coverline%7B%5Chat%7Bf%7D%28%5Comega%29%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28-%5Comega%29+%3D+%5Coverline%7B%5Chat%7Bf%7D%28%5Comega%29%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;hat{f}(-&#92;omega) = &#92;overline{&#92;hat{f}(&#92;omega)}" class="latex" /> (where the bar denotes complex conjugation, as usual). Therefore the &#8220;interesting&#8221; part of the frequency response (for our purposes anyway) is contained in the values of <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;hat{f}(&#92;omega)" class="latex" /> for Ï‰ in [0,Ï€]. Furthermore, we only care about the &#8220;magnitude response&#8221;, the absolute values of the (complex) frequency response, which is conventionally plotted in decibels (i.e. effectively on a logarithmic scale); the angle denotes phase response, which is not relevant to our question (and also happens to be relatively boring in this case, because all filters under consideration are symmetric and thus linear-phase FIR filters). Finally I&#8217;ll crop the plots I&#8217;m showing to only show frequencies from 0 to 0.8Ï€ (up to 80% <a href="https://en.wikipedia.org/wiki/Nyquist_frequency">Nyquist frequency</a>) because all filters under consideration sharply decay past that point (if not much earlier), so the portion from 0.8Ï€ to Ï€ contributes very little information and makes the y axis scaling awkward.</p>
<p>With all that out of the way, let&#8217;s start with our first filter: the 2-tap linear interpolation filter [0.5, 0.5] (it&#8217;s &#8220;bilinear&#8221; if you do it in both the X and Y directions, but all filters under consideration are separable and we&#8217;re only looking at 1D versions of them, so it&#8217;s just straight-up linear interpolation).</p>
<p><img loading="lazy" data-attachment-id="7126" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_linear/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of linear filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png" alt="Magnitude response of linear interpolation filter" width="614" height="460" class="aligncenter size-full wp-image-7126" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png 614w, https://fgiesen.files.wordpress.com/2019/04/mag_linear.png?w=150&amp;h=112 150w, https://fgiesen.files.wordpress.com/2019/04/mag_linear.png?w=300&amp;h=225 300w" sizes="(max-width: 614px) 100vw, 614px" /></p>
<p>The dotted line at 0dB is the line we&#8217;re not allowed to cross if we want repeated application to be stable. It&#8217;s also the line we want to be <em>exactly on</em> for an ideal interpolation filter. As we can see from the diagram, linear interpolation is good at the first part, not so good at the second part: it&#8217;s unit gain at a frequency of 0 but immediately rolls off, which is what causes it to over-blur. Not great.</p>
<p>The next filter that Casey looks at jumps from 2 taps all the way to 6: it&#8217;s the half-pixel interpolation filter from H.264/AVC. Here&#8217;s its magnitude response:</p>
<p><img loading="lazy" data-attachment-id="7127" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_h-264/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of H.264 filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png" alt="Magnitude response of H.264 filter" width="614" height="460" class="aligncenter size-full wp-image-7127" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png 614w, https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png?w=150&amp;h=112 150w, https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png?w=300&amp;h=225 300w" sizes="(max-width: 614px) 100vw, 614px" /></p>
<p>This one is quite different. We can immediately see that it has above-unit gain for a significant fraction of its spectrum, peaking around 0.5Ï€. That tells us it should blow up in Casey&#8217;s repeated-filtering test, and indeed it does. On the plus side, it has a much wider passband, and is in general much closer to an ideal interpolation filter than basic linear interpolation is.</p>
<p>The image for the 6-tap Lanczos filter is not that different:</p>
<p><img loading="lazy" data-attachment-id="7129" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_lanczos6/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of 6-tap Lancozs filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png" alt="Magnitude response of 6-tap Lancozs filter" width="614" height="460" class="aligncenter size-full wp-image-7129" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png 614w, https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png?w=150&amp;h=112 150w, https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png?w=300&amp;h=225 300w" sizes="(max-width: 614px) 100vw, 614px" /></p>
<p>In fact, this one&#8217;s qualitatively so similar that it seems fair to assume that the H.264 filter was designed as an approximation to Lanczos6 with reduced-precision coefficients. (It would certainly be plausible, but I haven&#8217;t checked.)</p>
<p>Next up, let&#8217;s look at Casey&#8217;s filter with quantized coefficients (numerator of 32):</p>
<p><img loading="lazy" data-attachment-id="7128" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_muratori6_32/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of quantized Muratori6" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png" alt="Magnitude response of quantized Muratori6" width="614" height="460" class="aligncenter size-full wp-image-7128" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png 614w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png?w=150&amp;h=112 150w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png?w=300&amp;h=225 300w" sizes="(max-width: 614px) 100vw, 614px" /></p>
<p>Casey&#8217;s filter stays at or below unit gain; it has noticeably below-unit gain for much of its passband which is not ideal, but at least its passband is significantly wider than basic linear interpolation, and looks decent out to about 0.5Ï€ before it really starts to cut off.</p>
<p>And while we&#8217;re at it, here&#8217;s Casey&#8217;s other 6-tap filter from the second part of his series:</p>
<p><img loading="lazy" data-attachment-id="7130" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_muratori6/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of Muratori6 filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png" alt="Magnitude response of Muratori6 filter" width="614" height="460" class="aligncenter size-full wp-image-7130" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png 614w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png?w=150&amp;h=112 150w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png?w=300&amp;h=225 300w" sizes="(max-width: 614px) 100vw, 614px" /></p>
<p>This one <em>very</em> slightly pokes above unit gain around 0.5Ï€, but evidently little enough not to blow up when the output signal is quantized to 8 bits after every step, as Casey&#8217;s test does. Other than that, we have some passband ripple, but are really quite close to unit gain until at least half-Nyquist, after which our performance starts to deteriorate, as it does for all 6-tap filters.</p>
<p>Our final 6-tap filter in this round is a 6-tap Lagrange interpolator. This one&#8217;s not in Casey&#8217;s series; its coefficients are [ 0.011719, -0.097656, 0.585938, 0.585938, -0.097656, 0.011719 ]. Lagrange interpolators are a classic family of interpolating filters (closely related to Lagrange polynomials) and have extremely flat passbands:</p>
<p><img loading="lazy" data-attachment-id="7131" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_lagrange6/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of 6-tap Lagrange interpolator" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png" alt="Magnitude response of 6-tap Lagrange interpolator" width="614" height="460" class="aligncenter size-full wp-image-7131" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png 614w, https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png?w=150&amp;h=112 150w, https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png?w=300&amp;h=225 300w" sizes="(max-width: 614px) 100vw, 614px" /></p>
<p>On the good side, yes, the passband really is flat. The trade-off is that the filter response visibly starts to dip around 0.4Ï€: the price we pay for that flat passband is losing more of the high frequencies than we would with other filters. On the other hand, this type of filter is not going to explode with repeated application. (Really, this is the second Lagrange-type filter I&#8217;ve shown, since the initial linear interpolation filter coincides with a 2-tap Lagrange interpolator.)</p>
<h3>8-tap filters</h3>
<p>I&#8217;m putting these in their own category: the two extra taps make a significant difference in the attainable filter quality, so they&#8217;re not on even footing with the 6-tap contenders. Let&#8217;s start with the filter from H.265/HEVC:</p>
<p><img loading="lazy" data-attachment-id="7132" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_hevc/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of HEVC filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png" alt="Magnitude response of HEVC filter" width="614" height="460" class="aligncenter size-full wp-image-7132" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png 614w, https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png?w=150&amp;h=112 150w, https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png?w=300&amp;h=225 300w" sizes="(max-width: 614px) 100vw, 614px" /></p>
<p>Some ripple in the passband and an above-unit peak around 0.6Ï€. Looking at the frequency response, this filter should blow up in Casey&#8217;s tests, and indeed it does. However it also manages to pass through frequencies all the way out to nearly 0.7Ï€, not something we&#8217;ve seen so far.</p>
<p>Here&#8217;s 8-tap Lanczos:</p>
<p><img loading="lazy" data-attachment-id="7133" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_lanczos8/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of Lanczos 8-tap" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png" alt="Magnitude response of Lanczos 8-tap" width="614" height="460" class="aligncenter size-full wp-image-7133" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png 614w, https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png?w=150&amp;h=112 150w, https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png?w=300&amp;h=225 300w" sizes="(max-width: 614px) 100vw, 614px" /></p>
<p>Again qualitatively similar to what we see from the HEVC filter, although I overall like the &#8220;real&#8221; HEVC filter a bit better than this one. Another filter that we would expect to blow up (and Casey&#8217;s testing confirms it does).</p>
<p>At the other extreme, 8-tap Lagrange:</p>
<p><img loading="lazy" data-attachment-id="7134" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_lagrange8/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of 8-tap Lagrange filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png" alt="Magnitude response of 8-tap Lagrange filter" width="614" height="460" class="aligncenter size-full wp-image-7134" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png 614w, https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png?w=150&amp;h=112 150w, https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png?w=300&amp;h=225 300w" sizes="(max-width: 614px) 100vw, 614px" /></p>
<p>Passband straight as a ruler, but not much use past 0.5Ï€. The good news is that, once again, it&#8217;s perfectly stable.</p>
<p>Here&#8217;s Casey&#8217;s contender:</p>
<p><img loading="lazy" data-attachment-id="7135" data-permalink="https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/mag_muratori8/" data-orig-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png" data-orig-size="614,460" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Magnitude response of Muratori8 filter" data-image-description="" data-image-caption="" data-medium-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png?w=300" data-large-file="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png?w=497" src="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png" alt="Magnitude response of Muratori8 filter" width="614" height="460" class="aligncenter size-full wp-image-7135" srcset="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png 614w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png?w=150&amp;h=112 150w, https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png?w=300&amp;h=225 300w" sizes="(max-width: 614px) 100vw, 614px" /></p>
<p>Again some passband ripple and it&#8217;s poking slightly above unity around 0.55Ï€, but it manages good results out to about 0.6Ï€ before it starts to cut.</p>
<h3>Conclusions</h3>
<p>That&#8217;s a couple of different filter types, and at this point we&#8217;ve seen enough to start drawing some conclusions, namely:</p>
<p>First, adding more taps gives us frequency responses closer to our ideal, which is not exactly surprising. The trade-offs are that filters with more taps are more expensive to evaluate, and that codecs also care about things other than frequency response. Among other things, we generally want reduced-precision fixed-point approximations to the filter coefficients both for efficiency (particularly in hardware decoders) and to ensure different codec implementations agree (which, for various reasons, gets significantly nastier once floating-point is in the mix).</p>
<p>Second, among the filters I showed, there&#8217;s a fairly clear spectrum: at one end, we have the Lagrange interpolators with strictly unit-or-below gain. They are completely stable under repeated application but also tend to lose high frequencies fairly quickly. In the other direction, we have filters like the Lanczos-derived ones or those used in H.264 or HEVC that have wider pass bands but also clearly above-unit gain for at least some frequencies, meaning that frequency content in those regions will grow and ultimately explode under repeated application. Finally, Casey&#8217;s filters are somewhere in between; they have wider pass bands than pure Lagrange interpolators but keep their maximum gain close enough to 1 to avoid blow-ups when applied to data that is quantized to 8 bit fixed point.</p>
<p>This last part is something I originally intended to do a more in-depth analysis of, which is the reason this post is so delayed. But I just never felt inspired to actually write this part and didn&#8217;t make any headway the 3 times I tried to just sit down and write it <em>without</em> inspiration either, so, well, sorry. No proper analysis here. I figured it&#8217;s better to at least publish the rest of the article without it.</p>
<p>The gist of it is this: if your samples are quantized to say 8-bit fixed point after every step, it seems plausible that you should be able to get away with slightly above unit gain at some frequencies. Essentially, all the inputs (being quantized) are integers, which means they need to change by at least 0.5 steps in either direction to actually round to a different value. If the gain at a given frequency isn&#8217;t high enough to accomplish this change, nothing will happen, and even a filter with above-unit gain for some frequencies that should in theory blow up eventually, won&#8217;t. Experimentally this seems to be true and I meant to do a proper proof but as said, you&#8217;ll have to live without it for the time being. (I might do another post in the future if I do come up with a nice proof.)</p>
<p>Finally: how much does it matter? Casey&#8217;s original posts framed not diverging over repeated application as an essential requirement for a motion interpolation filter in a video codec, but it&#8217;s really not that simple.</p>
<p>There&#8217;s no question that stability under iteration is desirable, the same way that a perfectly flat response over all frequencies is desirable: ideally we&#8217;d like interpolation filters to behave like a perfect all-pass filter. But we care about computational cost and memory bandwidth too, so we don&#8217;t get to use infinitely wide filters, which means we can&#8217;t get a perfect frequency response. This is somewhat hidden by the cropping I did, but all filters shown decay very sharply above around 80% Nyquist. The stability issue is similar: if we care about stability above all else, there is a known answer, which is to use Lagrange interpolators, which give us perfect stability but have subpar response at the high frequencies.</p>
<p>There is another thing to consider here: in a video codec, motion compensation is part of the coding loop. The encoder knows exactly what the decoder will do with a given frame and set of motion vectors, and will use that result to code further updates against. In short, the interpolation filter is not left to its own devices in a feedback loop. The encoder is continuously monitoring what the current state of the frame in a compliant decoder will be. If there is a build-up of high-frequency energy over time, all that really happens is that at some point, the error will become high enough for the encoder to decide to do something about it, which means sending not just a pure motion vector, but instead a motion vector with a residual (a correction applied to the image data after motion compensation). Said residual is also coded in frequency space (essentially), most commonly using a DCT variant. In short, the end result of using a filter that has resonances in the high frequencies is that a few times per second, the encoder will have to send residuals to cancel out these resonances. This costs extra bits, and correcting errors in the high frequencies tends to be more expensive than for the lower frequencies (image and video codecs can generally code lower frequencies more cheaply than high frequencies).</p>
<p>But suppose we didn&#8217;t do that, and instead used say a perfectly stable Lagrange interpolator. It doesn&#8217;t have any resonances that would cause the image to blow up, but it does act like a bit of a low-pass filter. In short, instead of steadily gaining energy in the high frequencies, we end up steadily losing energy in the high frequencies. And this too ends up with the encoder having to send high-frequency heavy residuals periodically, this time to add in the missing high frequencies (instead of removing excess ones).</p>
<p>Neither of these is obviously preferable to, or cheaper than, the other. Sending high-frequency heavy residuals is relatively expensive, but we end up having to do it periodically regardless of which type we choose.</p>
<p>That&#8217;s not to say that it doesn&#8217;t matter at all; it&#8217;s just to point out that the actual decision of which type of filter to use in any real codec is not made in a vacuum and depends on other factors. For example, H.264/AVC and HEVC at low bit rates rely aggressively on their deblocking filters, which are essentially low-pass filters applied over block edges. In that context, a somewhat sharpening motion interpolation filter can help mitigate some of the damage, whereas a Lagrange interpolator would make things even more one-sided.</p>
<p>In short, there is no such thing as a single &#8220;best&#8221; interpolation filter. The decision is made in context and depends on what the rest of the codec does.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2019/07/20/frequency-responses-of-half-pel-filters/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_linear.png" medium="image">
			<media:title type="html">Magnitude response of linear interpolation filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_h.264.png" medium="image">
			<media:title type="html">Magnitude response of H.264 filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos6.png" medium="image">
			<media:title type="html">Magnitude response of 6-tap Lancozs filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6_32.png" medium="image">
			<media:title type="html">Magnitude response of quantized Muratori6</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_muratori6.png" medium="image">
			<media:title type="html">Magnitude response of Muratori6 filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange6.png" medium="image">
			<media:title type="html">Magnitude response of 6-tap Lagrange interpolator</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_hevc.png" medium="image">
			<media:title type="html">Magnitude response of HEVC filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_lanczos8.png" medium="image">
			<media:title type="html">Magnitude response of Lanczos 8-tap</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_lagrange8.png" medium="image">
			<media:title type="html">Magnitude response of 8-tap Lagrange filter</media:title>
		</media:content>

		<media:content url="https://fgiesen.files.wordpress.com/2019/04/mag_muratori8.png" medium="image">
			<media:title type="html">Magnitude response of Muratori8 filter</media:title>
		</media:content>
	</item>
		<item>
		<title>What happens when iterating filters?</title>
		<link>https://fgiesen.wordpress.com/2019/04/08/what-happens-when-iterating-filters/</link>
					<comments>https://fgiesen.wordpress.com/2019/04/08/what-happens-when-iterating-filters/#comments</comments>
		
		<dc:creator><![CDATA[fgiesen]]></dc:creator>
		<pubDate>Mon, 08 Apr 2019 07:56:51 +0000</pubDate>
				<category><![CDATA[Maths]]></category>
		<guid isPermaLink="false">http://fgiesen.wordpress.com/?p=7122</guid>

					<description><![CDATA[Casey Muratori posted on his blog about half-pixel interpolation filters this week, where he ends up focusing on a particular criterion: whether the filter in question is stable under repeated application or not. There are many things about filters that are more an art than a science, especially where perceptual factors are concerned, but this [&#8230;]]]></description>
										<content:encoded><![CDATA[<p>Casey Muratori posted on his blog about <a href="https://caseymuratori.com/blog_0035">half-pixel interpolation filters</a> this week, where he ends up focusing on a particular criterion: whether the filter in question is stable under repeated application or not.</p>
<p>There are many things about filters that are more an art than a science, especially where perceptual factors are concerned, but this particular question is both free of tricky perceptual evaluations and firmly in the realm of things we have excellent theory for, albeit one that will require me to start with a linear algebra infodump. So let&#8217;s get into it!</p>
<h3>Analyzing iterated linear maps</h3>
<p>Take any vector space V over some field <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathbb{F}" class="latex" /> and any linear map <img src="https://s0.wp.com/latex.php?latex=T+%3A+V+%5Crightarrow+V&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=T+%3A+V+%5Crightarrow+V&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=T+%3A+V+%5Crightarrow+V&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="T : V &#92;rightarrow V" class="latex" /> from that space to itself. An <em>eigenvector</em> v of T is a nonzero element of V such that <img src="https://s0.wp.com/latex.php?latex=T%28v%29+%3D+Tv+%3D+%5Clambda+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=T%28v%29+%3D+Tv+%3D+%5Clambda+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=T%28v%29+%3D+Tv+%3D+%5Clambda+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="T(v) = Tv = &#92;lambda v" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cin+%5Cmathbb%7BF%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cin+%5Cmathbb%7BF%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda+%5Cin+%5Cmathbb%7BF%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;lambda &#92;in &#92;mathbb{F}" class="latex" /> &#8211; that is, the result of applying the map T to v is a scaled version of v itself. The scale factor Î» is the corresponding <em>eigenvalue</em>.</p>
<p>Now when we&#8217;re iterating the map T multiple times, eigenvectors of T behave in a very simple way under the iterated map: we know that applying T to v gives back a scaled version of v, and then linearity of T allows us to conclude that:<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T%5E2%28v%29+%3D+T%28T%28v%29%29+%3D+T%28Tv%29+%3D+T%28%5Clambda+v%29+%3D+%5Clambda+T%28v%29+%3D+%5Clambda%5E2+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T%5E2%28v%29+%3D+T%28T%28v%29%29+%3D+T%28Tv%29+%3D+T%28%5Clambda+v%29+%3D+%5Clambda+T%28v%29+%3D+%5Clambda%5E2+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T%5E2%28v%29+%3D+T%28T%28v%29%29+%3D+T%28Tv%29+%3D+T%28%5Clambda+v%29+%3D+%5Clambda+T%28v%29+%3D+%5Clambda%5E2+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle T^2(v) = T(T(v)) = T(Tv) = T(&#92;lambda v) = &#92;lambda T(v) = &#92;lambda^2 v" class="latex" /><br />
and more generally <img src="https://s0.wp.com/latex.php?latex=T%5Ek%28v%29+%3D+%5Clambda%5Ek+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=T%5Ek%28v%29+%3D+%5Clambda%5Ek+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=T%5Ek%28v%29+%3D+%5Clambda%5Ek+v&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="T^k(v) = &#92;lambda^k v" class="latex" /> for any <img src="https://s0.wp.com/latex.php?latex=k+%5Cin+%5Cmathbb%7BN%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=k+%5Cin+%5Cmathbb%7BN%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k+%5Cin+%5Cmathbb%7BN%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="k &#92;in &#92;mathbb{N}" class="latex" />.</p>
<p>The best possible case is that we find lots of eigenvectors &#8211; enough to fully characterize the map purely by what it does on its eigenvectors. For example, if V is a finite-dimensional vector space with <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdim%7D%28V%29%3Dn&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdim%7D%28V%29%3Dn&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdim%7D%28V%29%3Dn&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;mathrm{dim}(V)=n" class="latex" />, then if we can find n linearly independent eigenvectors, we&#8217;re golden: we can select a basis entirely made of eigenvectors, and then written in that basis, T will have a very simple form: we will have <img src="https://s0.wp.com/latex.php?latex=T+%3D+Q+%5CLambda+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=T+%3D+Q+%5CLambda+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=T+%3D+Q+%5CLambda+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="T = Q &#92;Lambda Q^{-1}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5Cmathrm%7Bdiag%7D%28%5Clambda_1%2C+%5Cdots%2C+%5Clambda_n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5Cmathrm%7Bdiag%7D%28%5Clambda_1%2C+%5Cdots%2C+%5Clambda_n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%5Cmathrm%7Bdiag%7D%28%5Clambda_1%2C+%5Cdots%2C+%5Clambda_n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;Lambda = &#92;mathrm{diag}(&#92;lambda_1, &#92;dots, &#92;lambda_n)" class="latex" /> for some Q (whose columns contain n linearly independent eigenvectors of T).</p>
<p>That is, in the right basis (made of eigenvectors), T is just a diagonal matrix, which is to say, a (non-uniform) scale. This makes analysis of repeated applications of T easy, since:<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T%5E2+%3D+Q+%5CLambda+Q%5E%7B-1%7D+Q+%5CLambda+Q%5E%7B-1%7D+%3D+Q+%5CLambda%5E2+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T%5E2+%3D+Q+%5CLambda+Q%5E%7B-1%7D+Q+%5CLambda+Q%5E%7B-1%7D+%3D+Q+%5CLambda%5E2+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T%5E2+%3D+Q+%5CLambda+Q%5E%7B-1%7D+Q+%5CLambda+Q%5E%7B-1%7D+%3D+Q+%5CLambda%5E2+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle T^2 = Q &#92;Lambda Q^{-1} Q &#92;Lambda Q^{-1} = Q &#92;Lambda^2 Q^{-1}" class="latex" /><br />
and in general<br />
<img src="https://s0.wp.com/latex.php?latex=T%5Ek+%3D+Q+%5CLambda%5Ek+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=T%5Ek+%3D+Q+%5CLambda%5Ek+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=T%5Ek+%3D+Q+%5CLambda%5Ek+Q%5E%7B-1%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="T^k = Q &#92;Lambda^k Q^{-1}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5CLambda%5Ek+%3D+%5Cmathrm%7Bdiag%7D%28%5Clambda_1%5Ek%2C+%5Cdots%2C+%5Clambda_n%5Ek%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CLambda%5Ek+%3D+%5Cmathrm%7Bdiag%7D%28%5Clambda_1%5Ek%2C+%5Cdots%2C+%5Clambda_n%5Ek%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CLambda%5Ek+%3D+%5Cmathrm%7Bdiag%7D%28%5Clambda_1%5Ek%2C+%5Cdots%2C+%5Clambda_n%5Ek%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;Lambda^k = &#92;mathrm{diag}(&#92;lambda_1^k, &#92;dots, &#92;lambda_n^k)" class="latex" />: viewed in the basis made of eigenvectors, repeated application of T is just repeated scaling, and behaviour over lots of iterations ultimately just hinges on what the eigenvalues are.</p>
<p>Not every matrix can be written that way; ones that can are called <em>diagonalizable</em>. But there is a very important class of transforms (and now we allow infinite-dimensional spaces again) that is guaranteed to be diagonalizable: so called self-adjoint transforms. In the finite-dimensional real case, these correspond to symmetric matrices (matrices A such that <img src="https://s0.wp.com/latex.php?latex=A+%3D+A%5ET&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=A+%3D+A%5ET&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A+%3D+A%5ET&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="A = A^T" class="latex" />). Such transforms are guaranteed to be diagonalizable, and even better, their eigenvectors are guaranteed to be pairwise orthogonal to each other, meaning the transform Q is an orthogonal matrix (a rotation or reflection), which among other things makes the whole process numerically quite well-behaved.</p>
<p>As a small aside, if you&#8217;ve ever wondered why iterative solvers for linear systems usually require symmetric (or, in the complex case, Hermitian) matrices: this is why. If a matrix is symmetric, it it diagonalizable, which allows us to build an iterative process to solve linear equations that we can analyze easily and <em>know</em> will converge (if we do it right). It&#8217;s not that we can&#8217;t possibly do anything iterative on non-symmetric linear systems; it just becomes a lot trickier to make any guarantees, especially if we allow arbitrary matrices. (Which could be quite pathological.)</p>
<p>Anyway, that&#8217;s a bit of background on eigendecompositions of linear maps. But what does any of this have to do with filtering?</p>
<h3>Enter convolution</h3>
<p>Convolution itself is a <em>bilinear map</em>, meaning it&#8217;s linear in both arguments. That means that if we fix either of the arguments, we get a linear map. Suppose we have a FIR filter f given by its coefficients <img src="https://s0.wp.com/latex.php?latex=%28f_0%2C+f_1%2C+%5Cdots%2C+f_%7Bm-1%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%28f_0%2C+f_1%2C+%5Cdots%2C+f_%7Bm-1%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28f_0%2C+f_1%2C+%5Cdots%2C+f_%7Bm-1%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(f_0, f_1, &#92;dots, f_{m-1})" class="latex" />. Then we can define an associated linear map <img src="https://s0.wp.com/latex.php?latex=T_f&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=T_f&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=T_f&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="T_f" class="latex" /> on a suitable space, say something like <img src="https://s0.wp.com/latex.php?latex=T_f+%3A+%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29+%5Crightarrow+%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=T_f+%3A+%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29+%5Crightarrow+%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=T_f+%3A+%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29+%5Crightarrow+%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="T_f : &#92;ell^&#92;infty(&#92;mathbb{C}) &#92;rightarrow &#92;ell^&#92;infty(&#92;mathbb{C})" class="latex" /> (writing <img src="https://s0.wp.com/latex.php?latex=%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cell%5E%5Cinfty%28%5Cmathbb%7BC%7D%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;ell^&#92;infty(&#92;mathbb{C})" class="latex" /> for the set of bounded sequences of complex numbers) by setting<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T_f%28x%29+%3D+T_f+x+%3A%3D+f+%2A+x&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T_f%28x%29+%3D+T_f+x+%3A%3D+f+%2A+x&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T_f%28x%29+%3D+T_f+x+%3A%3D+f+%2A+x&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle T_f(x) = T_f x := f * x" class="latex" />.</p>
<p>If this is all a bit dense on notation for you, all I&#8217;m doing here is holding one of the two arguments to the convolution operator constant, and trying to at least specify what set our map is working on (in this case, bounded sequences of real numbers).</p>
<p>And now we&#8217;re just about ready for the punchline: we now have a linear map from a set to itself, although in this case we&#8217;re dealing with infinite sequences, not finite ones. Luckily the notions of eigenvectors (eigensequences in this case) and eigenvalues generalize just fine. What&#8217;s even better is that for <em>all</em> discrete convolutions, we get a full complement of eigensequences, and we know exactly what they are. Namely, define the family of sequences <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="e_&#92;omega" class="latex" /> by:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+e_%5Comega%5Bn%5D+%3D+%5Cexp%28i+%5Comega+n%29+%3D+%5Ccos%28%5Comega+n%29+%2B+i+%5Csin%28%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+e_%5Comega%5Bn%5D+%3D+%5Cexp%28i+%5Comega+n%29+%3D+%5Ccos%28%5Comega+n%29+%2B+i+%5Csin%28%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+e_%5Comega%5Bn%5D+%3D+%5Cexp%28i+%5Comega+n%29+%3D+%5Ccos%28%5Comega+n%29+%2B+i+%5Csin%28%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle e_&#92;omega[n] = &#92;exp(i &#92;omega n) = &#92;cos(&#92;omega n) + i &#92;sin(&#92;omega n)" class="latex" /></p>
<p>That&#8217;s a cosine wave with frequency &omega; in the real part and the corresponding sine wave in the imaginary part, if you are so inclined, although I much prefer to stick with the complex exponentials, especially when doing algebra (it makes things easier). Anyway, if we apply our FIR filter f to that signal, we get (this is just expanding out the definition of discrete convolution for our filter and input signal, using the convention that unqualified summation is over all values of k where the sum is well-defined)</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28T_f+e_%5Comega%29%5Bn%5D+%3D+%5Csum_k+f_k+%5Cexp%28i+%5Comega+%28n-k%29%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28T_f+e_%5Comega%29%5Bn%5D+%3D+%5Csum_k+f_k+%5Cexp%28i+%5Comega+%28n-k%29%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28T_f+e_%5Comega%29%5Bn%5D+%3D+%5Csum_k+f_k+%5Cexp%28i+%5Comega+%28n-k%29%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle (T_f e_&#92;omega)[n] = &#92;sum_k f_k &#92;exp(i &#92;omega (n-k))" class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+%5Cexp%28i+%5Comega+n%29+%5Cunderbrace%7B%5Csum_k+f_k+%5Cexp%28-i+%5Comega+k%29%7D_%7B%3D%3A%5Chat%7Bf%7D%28%5Comega%29%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+%5Cexp%28i+%5Comega+n%29+%5Cunderbrace%7B%5Csum_k+f_k+%5Cexp%28-i+%5Comega+k%29%7D_%7B%3D%3A%5Chat%7Bf%7D%28%5Comega%29%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+%5Cexp%28i+%5Comega+n%29+%5Cunderbrace%7B%5Csum_k+f_k+%5Cexp%28-i+%5Comega+k%29%7D_%7B%3D%3A%5Chat%7Bf%7D%28%5Comega%29%7D&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle = &#92;exp(i &#92;omega n) &#92;underbrace{&#92;sum_k f_k &#92;exp(-i &#92;omega k)}_{=:&#92;hat{f}(&#92;omega)}" class="latex" /><br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+%5Chat%7Bf%7D%28%5Comega%29+%5Cexp%28i+%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+%5Chat%7Bf%7D%28%5Comega%29+%5Cexp%28i+%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+%5Chat%7Bf%7D%28%5Comega%29+%5Cexp%28i+%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle = &#92;hat{f}(&#92;omega) &#92;exp(i &#92;omega n)" class="latex" /></p>
<p>There&#8217;s very little that happens here. The first line is just expanding the definition; then in the second line we use the properties of the exponential function (and the linearity of sums) to pull out the constant factor of <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28i+%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cexp%28i+%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cexp%28i+%5Comega+n%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;exp(i &#92;omega n)" class="latex" />. And it turns out the entire rest of the formula doesn&#8217;t depend on n at all, so it turns into a constant factor for the whole sequence. It does depend on f and &omega;, so we label it <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;hat{f}(&#92;omega)" class="latex" />. The final line states exactly what we wanted, namely that the result of applying <img src="https://s0.wp.com/latex.php?latex=T_f&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=T_f&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=T_f&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="T_f" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="e_&#92;omega" class="latex" /> is just a scaled copy of <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="e_&#92;omega" class="latex" /> itself&mdash;we have an eigensequence (with eigenvalue <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28%5Comega%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;hat{f}(&#92;omega)" class="latex" />).</p>
<p>Also note that the formula for the eigenvalue isn&#8217;t particularly scary either in our case, since we&#8217;re dealing with a FIR filter f, meaning it&#8217;s a regular finite sum:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%28%5Comega%29+%3D+%5Csum_%7Bk%3D0%7D%5E%7Bm-1%7D+f_k+%5Cexp%28-i+%5Comega+k%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%28%5Comega%29+%3D+%5Csum_%7Bk%3D0%7D%5E%7Bm-1%7D+f_k+%5Cexp%28-i+%5Comega+k%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%28%5Comega%29+%3D+%5Csum_%7Bk%3D0%7D%5E%7Bm-1%7D+f_k+%5Cexp%28-i+%5Comega+k%29&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="&#92;displaystyle &#92;hat{f}(&#92;omega) = &#92;sum_{k=0}^{m-1} f_k &#92;exp(-i &#92;omega k)" class="latex" /></p>
<p>Oh, and there&#8217;s one more minor detail I&#8217;ve neglected to mention so far: that&#8217;s just the <a href="https://en.wikipedia.org/wiki/Discrete-time_Fourier_transform">discrete-time Fourier transform</a> (DTFT, not to be confused with the DFT, although they&#8217;re related) of f. Yup, we started out with a digital FIR filter, asked what happens when we iterate it a bunch, did a brief detour into linear algebra, and ended up in Fourier theory.</p>
<p>Long story short, if you want to know whether a linear digital filter is stable under repeated application, you want to look at its eigenvalues, which in turn are just given by its frequency response. In particular, for any given frequency &omega;, we have exactly three options:</p>
<ul>
<li><img src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3D+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3D+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3D+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|&#92;hat{f}(&#92;omega)| = 1" class="latex" />. In this case, the amplitude at that frequency is preserved exactly under repeated application.</li>
<li><img src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3C+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3C+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3C+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|&#92;hat{f}(&#92;omega)| &lt; 1" class="latex" />. If the filter dampens a given frequency, no matter how little, then the amplitude of the signal at that frequency will eventually be driven to zero. This is stable but causes the signal to degrade. Typical interpolation filters tend to do this for the higher frequencies, which is why signals tend to lose such frequencies (in visual terms, get blurrier) over time.</li>
<li><img src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3E+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3E+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7C%5Chat%7Bf%7D%28%5Comega%29%7C+%3E+1&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="|&#92;hat{f}(&#92;omega)| &gt; 1" class="latex" />. If a filter amplifies any frequency by more than 1, even by just a tiny bit, then any signal containing a nonzero amount of that frequency will eventually blow up.</li>
</ul>
<p>The proof for all three cases is simply observing that k-fold application of the filter f to the signal <img src="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="e_&#92;omega" class="latex" /> will result in the signal <img src="https://s0.wp.com/latex.php?latex=%28%5Chat%7Bf%7D%28%5Comega%29%29%5Ek+e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%28%5Chat%7Bf%7D%28%5Comega%29%29%5Ek+e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28%5Chat%7Bf%7D%28%5Comega%29%29%5Ek+e_%5Comega&#038;bg=f9f7f5&#038;fg=444444&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x" alt="(&#92;hat{f}(&#92;omega))^k e_&#92;omega" class="latex" />. To generalize this to a wider class of signals (not just complex exponentials) we would need to represent said signals as sum of complex exponentials, which is exactly what Fourier series are all about; so it can be done, but I won&#8217;t bother with the details here, since they&#8217;re out of the scope of this post.</p>
<p>Therefore, all you need to know about the stability of a given filter under repeated application is contained in its Fourier transform. I&#8217;ll try to do another post soon that shows the Fourier transforms of the filters Casey mentioned (or their magnitude response anyway, which is what we care about) and touches on other aspects such as the effect of rounding and quantization, but we&#8217;re at a good stopping point right now, so I&#8217;ll end this post here.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://fgiesen.wordpress.com/2019/04/08/what-happens-when-iterating-filters/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/32870837851c0e5eb620649cb8d3d608?s=96&amp;d=identicon&amp;r=G" medium="image">
			<media:title type="html">fgiesen</media:title>
		</media:content>
	</item>
	</channel>
</rss>
