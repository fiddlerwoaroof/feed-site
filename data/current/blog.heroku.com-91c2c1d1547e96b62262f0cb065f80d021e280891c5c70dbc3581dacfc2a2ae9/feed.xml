<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>Heroku</title>
    <link>http://blog.heroku.com</link>
    <description>The Heroku Blog</description>
    <ttl>60</ttl>
    <item>
      <title>The Life-Changing Magic of Tidying Ruby Object Allocations</title>
      <link>https://blog.heroku.com/tidying-ruby-object-allocations</link>
      <pubDate>Wed, 16 Sep 2020 14:58:00 GMT</pubDate>
      <guid>https://blog.heroku.com/tidying-ruby-object-allocations</guid>
      <description>&lt;p&gt;Your app is slow. It does not spark joy. This post will use memory allocation profiling tools to discover performance hotspots, even when they're coming from inside a library. We will use this technique with a real-world application to identify a piece of optimizable code in Active Record that ultimately leads to a patch with a substantial impact on page speed.&lt;/p&gt;

&lt;p&gt;In addition to the talk, I've gone back and written a full technical recap of each section to revisit it any time you want without going through the video.&lt;/p&gt;

&lt;p&gt;I make heavy use of theatrics here, including a Japanese voiceover artist, animoji, and some edited clips of Marie Kondo's Netflix TV show. This recording was done at EuRuKo on a boat. If you've got the time, here's the talk:&lt;/p&gt;

&lt;div class=&quot;embedded-video-wrapper&quot;&gt;
&lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/Aczy01drwkg?start=287&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;#intro-to-tidying-object-allocations&quot;&gt;Intro to Tidying Object Allocations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#tidying-example-1-active-record-respond_to-logic&quot;&gt;Tidying Example 1: Active Record respond_to? logic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#performance-and-statistical-significance&quot;&gt;Performance and Statistical Significance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#tidying-example-2-converting-strings-to-time-takes-time&quot;&gt;Tidying example 2: Converting strings to time takes time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;tidying-example-3-lightning-fast-cache-keys&quot;&gt;Tidying Example 3: Lightning fast cache keys&lt;/a&gt; &lt;/li&gt;
&lt;/ol&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;intro-to-tidying-object-allocations&quot; href=&quot;#intro-to-tidying-object-allocations&quot;&gt;Intro to Tidying Object Allocations&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;The core premise of this talk is that we all want faster applications. Here I'm making the pitch that you can get significant speedups by focusing on your object allocations. To do that, I'll eventually show you a few real-world cases of PRs I made to Rails along with a &quot;how-to&quot; that shows how I used profiling and benchmarking to find and fix the hotspots.&lt;/p&gt;

&lt;p&gt;At a high level, the &quot;tidying&quot; technique looks like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Take all your object allocations and put them in a pile where you can see them&lt;/li&gt;
&lt;li&gt;Consider each one: Does it spark joy?&lt;/li&gt;
&lt;li&gt;Keep only the objects that spark joy&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An object sparks joy if it is useful, keeps your code clean, and does not cause performance problems. If an object is absolutely necessary, and removing it causes your code to crash, it sparks joy.&lt;/p&gt;

&lt;p&gt;To put object allocations in front of us we'll use:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/SamSaffron/memory_profiler&quot;&gt;memory_profiler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/schneems/derailed_benchmarks&quot;&gt;derailed_benchmarks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To get a sense of the cost of object allocation, we can benchmark two different ways to perform the same logic. One of these allocates an array while the other does not.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;require 'benchmark/ips'

def compare_max(a, b)
  return a if a &amp;gt; b
  b
end

def allocate_max(a, b)
  array = [a, b] # &amp;lt;===== Array allocation here
  array.max
end

Benchmark.ips do |x|
  x.report(&quot;allocate_max&quot;) {
    allocate_max(1, 2)
  }
  x.report(&quot;compare_max &quot;) {
    compare_max(1, 2)
  }
  x.compare!
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This gives us the results:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;Warming up --------------------------------------
        allocate_max   258.788k i/100ms
        compare_max    307.196k i/100ms
Calculating -------------------------------------
        allocate_max      6.665M (¬±14.6%) i/s -     32.090M in   5.033786s
        compare_max      13.597M (¬± 6.0%) i/s -     67.890M in   5.011819s

Comparison:
        compare_max : 13596747.2 i/s
        allocate_max:  6664605.5 i/s - 2.04x  slower
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, allocating an array is 2x slower than making a direct comparison. It's a truism in most languages that allocating memory or creating objects is slow. In the &lt;code&gt;C&lt;/code&gt; programming language, it's a truism that &quot;malloc is slow.&quot;&lt;/p&gt;

&lt;p&gt;Since we know that allocating in Ruby is slow, we can make our programs faster by removing allocations. As a simplifying assumption, I've found that a decrease in bytes allocated roughly corresponds to performance improvement. For example, if I can reduce the number of bytes allocated by 1% in a request, then on average, the request will have been sped up by about 1%. This assumption helps us benchmark faster as it's much easier to measure memory allocated than it is to repeatedly run hundreds or thousands of timing benchmarks.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;tidying-example-1-active-record-code-respond_to-code-logic&quot; href=&quot;#tidying-example-1-active-record-code-respond_to-code-logic&quot;&gt;Tidying Example 1: Active Record &lt;code&gt;respond_to?&lt;/code&gt; logic&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Using the target application &lt;a href=&quot;https://www.codetriage.com&quot;&gt;CodeTriage.com&lt;/a&gt; and derailed benchmarks, we get a &quot;pile&quot; of memory allocations:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;$ bundle exec derailed exec perf:objects

allocated memory by gem
-----------------------------------
    227058  activesupport/lib
    134366  codetriage/app
    # ...


allocated memory by file
-----------------------------------
    126489  ‚Ä¶/code/rails/activesupport/lib/active_support/core_ext/string/output_safety.rb
     49448  ‚Ä¶/code/codetriage/app/views/layouts/_app.html.slim
     49328  ‚Ä¶/code/codetriage/app/views/layouts/application.html.slim
     36097  ‚Ä¶/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb
     25096  ‚Ä¶/code/codetriage/app/views/pages/_repos_with_pagination.html.slim
     24432  ‚Ä¶/code/rails/activesupport/lib/active_support/core_ext/object/to_query.rb
     23526  ‚Ä¶/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/patches/db/pg.rb
     21912  ‚Ä¶/code/rails/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb
     18000  ‚Ä¶/code/rails/activemodel/lib/active_model/attribute_set/builder.rb
     15888  ‚Ä¶/code/rails/activerecord/lib/active_record/result.rb
     14610  ‚Ä¶/code/rails/activesupport/lib/active_support/cache.rb
     11109  ‚Ä¶/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/mini_profiler/storage/file_store.rb
      9824  ‚Ä¶/code/rails/actionpack/lib/abstract_controller/caching/fragments.rb
      9360  ‚Ä¶/.rubies/ruby-2.5.3/lib/ruby/2.5.0/logger.rb
      8440  ‚Ä¶/code/rails/activerecord/lib/active_record/attribute_methods.rb
      8304  ‚Ä¶/code/rails/activemodel/lib/active_model/attribute.rb
      8160  ‚Ä¶/code/rails/actionview/lib/action_view/renderer/partial_renderer.rb
      8000  ‚Ä¶/code/rails/activerecord/lib/active_record/integration.rb
      7880  ‚Ä¶/code/rails/actionview/lib/action_view/log_subscriber.rb
      7478  ‚Ä¶/code/rails/actionview/lib/action_view/helpers/tag_helper.rb
      7096  ‚Ä¶/code/rails/actionview/lib/action_view/renderer/partial_renderer/collection_caching.rb
      # ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&quot;https://gist.github.com/schneems/5ed597c85a0a49659413456652a1befc&quot;&gt;full output is massive&lt;/a&gt;, so I've truncated it here.&lt;/p&gt;

&lt;p&gt;Once you've got your memory in a pile. I like to look at the &quot;allocated memory&quot; by file. I start at the top and look at each in turn. In this case, we'll look at this file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;      8440  ‚Ä¶/code/rails/activerecord/lib/active_record/attribute_methods.rb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you have a file you want to look at, you can focus on it in derailed like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;$ ALLOW_FILES=active_record/attribute_methods.rb \
  bundle exec derailed exec perf:objects

allocated memory by file
-----------------------------------
      8440  ‚Ä¶/code/rails/activerecord/lib/active_record/attribute_methods.rb

allocated memory by location
-----------------------------------
      8000  ‚Ä¶/code/rails/activerecord/lib/active_record/attribute_methods.rb:270
       320  ‚Ä¶/code/rails/activerecord/lib/active_record/attribute_methods.rb:221
        80  ‚Ä¶/code/rails/activerecord/lib/active_record/attribute_methods.rb:189
        40  ‚Ä¶/code/rails/activerecord/lib/active_record/attribute_methods.rb:187
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can see exactly where the memory is being allocated in this file. Starting at the top of the locations, I'll work my way down to understand how memory is allocated and used. Looking first at this line:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;      8000  ‚Ä¶/code/rails/activerecord/lib/active_record/attribute_methods.rb:270
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can open this in an editor and navigate to that location:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;$ bundle open activerecord
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In that file, here's the line allocating the most memory:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;def respond_to?(name, include_private = false)
  return false unless super

  case name
  when :to_partial_path
    name = &quot;to_partial_path&quot;
  when :to_model
    name = &quot;to_model&quot;
  else
    name = name.to_s # &amp;lt;=== Line 270 here
  end

  # If the result is true then check for the select case.
  # For queries selecting a subset of columns, return false for unselected columns.
  # We check defined?(@attributes) not to issue warnings if called on objects that
  # have been allocated but not yet initialized.
  if defined?(@attributes) &amp;amp;&amp;amp; self.class.column_names.include?(name)
    return has_attribute?(name)
  end

  true
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we can see on line 270 that it's allocating a string. But why? To answer that question, we need more context. We need to understand how this code is used. When we call &lt;code&gt;respond_to&lt;/code&gt; on an object, we want to know if a method by that name exists. Because Active Record is backed by a database, it needs to see if a column exists with that name.&lt;/p&gt;

&lt;p&gt;Typically when you call &lt;code&gt;respond_to&lt;/code&gt; you pass in a symbol, for example, &lt;code&gt;user.respond_to?(:email)&lt;/code&gt;. But in Active Record, columns are stored as strings. On line 270, we're ensuring that the &lt;code&gt;name&lt;/code&gt; value is always a string.&lt;/p&gt;

&lt;p&gt;This is the code where name is used:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;  if defined?(@attributes) &amp;amp;&amp;amp; self.class.column_names.include?(name)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here &lt;code&gt;column_names&lt;/code&gt; returns an array of column names, and the &lt;code&gt;include?&lt;/code&gt; method will iterate over each until it finds the column with that name, or its nothing (&lt;code&gt;nil&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;To determine if we can get rid of this allocation, we have to figure out if there's a way to replace it without allocating memory. We need to refactor this code while maintaining correctness. I decided to add a method that converted the array of column names into a hash with symbol keys and string values:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;# lib/activerecord/model_schema.rb
def symbol_column_to_string(name_symbol) # :nodoc:
  @symbol_column_to_string_name_hash ||= column_names.index_by(&amp;amp;:to_sym)
  @symbol_column_to_string_name_hash[name_symbol]
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is how you would use it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;User.symbol_column_to_string(:email) #=&amp;gt; &quot;email&quot;
User.symbol_column_to_string(:foo)   #=&amp;gt; nil
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since the value that is being returned every time by this method is from the same hash, we can re-use the same string and not have to allocate. The refactored &lt;code&gt;respond_to&lt;/code&gt; code ends up looking like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;def respond_to?(name, include_private = false)
  return false unless super

  # If the result is true then check for the select case.
  # For queries selecting a subset of columns, return false for unselected columns.
  # We check defined?(@attributes) not to issue warnings if called on objects that
  # have been allocated but not yet initialized.
  if defined?(@attributes)
    if name = self.class.symbol_column_to_string(name.to_sym)
      return has_attribute?(name)
    end
  end

  true
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running our benchmarks, this patch yielded a reduction in memory of 1%. Using code that eventually became &lt;code&gt;derailed exec perf:library&lt;/code&gt;, I verified that the patch made end-to-end request/response page speed on CodeTriage 1% faster.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;performance-and-statistical-significance&quot; href=&quot;#performance-and-statistical-significance&quot;&gt;Performance and Statistical Significance&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;When talking about benchmarks, it's important to talk about statistics and their impact. I talk a bit about this in &lt;a href=&quot;https://schneems.com/2020/03/17/lies-damned-lies-and-averages-perc50-perc95-ex%0Aplained-for-programmers/&quot;&gt;Lies, Damned Lies, and Averages: Perc50, Perc95 explained for Programmers&lt;/a&gt;. Essentially any time you measure a value, there's a chance that it could result from randomness. If you run a benchmark 3 times, it will give you 3 different results. If it shows that it was faster twice and slower once, how can you be certain that the results are because of the change and not random chance?&lt;/p&gt;

&lt;p&gt;That's precisely the question that &quot;statistical significance&quot; tries to answer. While we can never know, we can make an informed decision. How? Well, if you took a measurement of the same code many times, you would know any variation was the result of randomness. This would give you a distribution of randomness. Then you could use this distribution to understand how likely it is that your change was caused by randomness.&lt;/p&gt;

&lt;p&gt;In the talk, I go into detail on the origins of &quot;Student's T-Test.&quot; In derailed, I've switched to using Kolmogorov-Smirnov instead. When I ran benchmarks on CodeTriage, I wanted to be sure that my results were valid, so I ran them multiple times and ran Kolmogorov Smirnov on them. This gives me a confidence interval. If my results are in that interval, then I can say with 95% certainty that my results are not the result of random chance i.e., that they're valid and are statistically significant.&lt;/p&gt;

&lt;p&gt;If it's not significant, it could mean that the change is too small to detect, that you need more samples, or that there is no difference.&lt;/p&gt;

&lt;p&gt;In addition to running a significance check on your change, it's useful to see the distribution. Derailed benchmarks does this for you by default now. Here is a result from &lt;code&gt;derailed exec perf:library&lt;/code&gt; used to compare the performance difference of two different commits in a library dependency:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;                  Histogram - [winner] &quot;I am the new commit.&quot;
                           ‚îå                                        ‚îê
            [11.2 , 11.28) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 12
            [11.28, 11.36) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 22
            [11.35, 11.43) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 30
            [11.43, 11.51) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 17
   Time (s) [11.5 , 11.58) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 13
            [11.58, 11.66) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 6
            [11.65, 11.73) ‚î§ 0
            [11.73, 11.81) ‚î§ 0
            [11.8 , 11.88) ‚î§ 0
                           ‚îî                                        ‚îò
                                      # of runs in range



                  Histogram - [loser] &quot;Old commit&quot;
                           ‚îå                                        ‚îê
            [11.2 , 11.28) ‚î§‚ñá‚ñá‚ñá‚ñá 3
            [11.28, 11.36) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 19
            [11.35, 11.43) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 17
            [11.43, 11.51) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 25
   Time (s) [11.5 , 11.58) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 15
            [11.58, 11.66) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 13
            [11.65, 11.73) ‚î§‚ñá‚ñá‚ñá‚ñá 3
            [11.73, 11.81) ‚î§‚ñá‚ñá‚ñá‚ñá 3
            [11.8 , 11.88) ‚î§‚ñá‚ñá‚ñá 2
                           ‚îî                                        ‚îò
                                      # of runs in range
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The TLDR of this whole section is that in addition to showing my change as being faster, I was also able to show that the improvement was statistically significant.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;tidying-example-2-converting-strings-to-time-takes-time&quot; href=&quot;#tidying-example-2-converting-strings-to-time-takes-time&quot;&gt;Tidying example 2: Converting strings to time takes time&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;One percent faster is good, but it could be better. Let's do it again. First, get a pile of objects:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;$ bundle exec derailed exec perf:objects

# ...

allocated memory by file
-----------------------------------
    126489  ‚Ä¶/code/rails/activesupport/lib/active_support/core_ext/string/output_safety.rb
     49448  ‚Ä¶/code/codetriage/app/views/layouts/_app.html.slim
     49328  ‚Ä¶/code/codetriage/app/views/layouts/application.html.slim
     36097  ‚Ä¶/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb
     25096  ‚Ä¶/code/codetriage/app/views/pages/_repos_with_pagination.html.slim
     24432  ‚Ä¶/code/rails/activesupport/lib/active_support/core_ext/object/to_query.rb
     23526  ‚Ä¶/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/patches/db/pg.rb
     21912  ‚Ä¶/code/rails/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb
     18000  ‚Ä¶/code/rails/activemodel/lib/active_model/attribute_set/builder.rb
     15888  ‚Ä¶/code/rails/activerecord/lib/active_record/result.rb
     14610  ‚Ä¶/code/rails/activesupport/lib/active_support/cache.rb
     11148  ‚Ä¶/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/mini_profiler/storage/file_store.rb
      9824  ‚Ä¶/code/rails/actionpack/lib/abstract_controller/caching/fragments.rb
      9360  ‚Ä¶/.rubies/ruby-2.5.3/lib/ruby/2.5.0/logger.rb
      8304  ‚Ä¶/code/rails/activemodel/lib/active_model/attribute.rb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Zoom in on a file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;     36097  ‚Ä¶/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Isolate the file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;$ ALLOW_FILE=active_model/type/helpers/time_value.rb \
  bundle exec derailed exec perf:objects

Total allocated: 39617 bytes (600 objects)
Total retained:  0 bytes (0 objects)

allocated memory by gem
-----------------------------------
     39617  activemodel/lib

allocated memory by file
-----------------------------------
     39617  ‚Ä¶/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb

allocated memory by location
-----------------------------------
     17317  ‚Ä¶/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:72
     12000  ‚Ä¶/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:74
      6000  ‚Ä¶/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:73
      4300  ‚Ä¶/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We're going to do the same thing by starting to look at the top location:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;     17317  ‚Ä¶/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:72
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here's the code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;def fast_string_to_time(string)
 if string =~ ISO_DATETIME # &amp;lt;=== line 72 Here
   microsec = ($7.to_r * 1_000_000).to_i
   new_time $1.to_i, $2.to_i, $3.to_i, $4.to_i, $5.to_i, $6.to_i, microsec
 end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On line 72, we are matching the input string with a regular expression constant. This allocates a lot of memory because each grouped match of the regular expression allocates a new string. To understand if we can make this faster, we have to understand how it's used.&lt;/p&gt;

&lt;p&gt;This method takes in a string, then uses a regex to split it into parts, and then sends those parts to the &lt;code&gt;new_time&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;There's not much going on that can be sped up there, but what's happening on this line:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;   microsec = ($7.to_r * 1_000_000).to_i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here's the regex:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;ISO_DATETIME = /\A(\d{4})-(\d\d)-(\d\d) (\d\d):(\d\d):(\d\d)(\.\d+)?\z/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When I ran the code and output $7 from the regex match, I found that it would contain a string that starts with a dot and then has numbers, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;puts $7 # =&amp;gt; &quot;.1234567&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code wants microseconds as an integer, so it turns it into a &quot;rational&quot; and then multiplies it by a million and turns it into an integer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;($7.to_r * 1_000_000).to_i # =&amp;gt; 1234567
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You might notice that it looks like we're basically dropping the period and then turning it into an integer. So why not do that directly?&lt;/p&gt;

&lt;p&gt;Here's what it looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;def fast_string_to_time(string)
  if string =~ ISO_DATETIME
    microsec_part = $7
    if microsec_part &amp;amp;&amp;amp; microsec_part.start_with?(&quot;.&quot;) &amp;amp;&amp;amp; microsec_part.length == 7
      microsec_part[0] = &quot;&quot;         # &amp;lt;=== HERE
      microsec = microsec_part.to_i # &amp;lt;=== HERE
    else
      microsec = (microsec_part.to_r * 1_000_000).to_i
    end
    new_time $1.to_i, $2.to_i, $3.to_i, $4.to_i, $5.to_i, $6.to_i, microsec
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We've got to guard this case by checking for the conditions of our optimization. Now the question is: is this faster?&lt;/p&gt;

&lt;p&gt;Here's a microbenchmark:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;original_string = &quot;.443959&quot;

require 'benchmark/ips'

Benchmark.ips do |x|
  x.report(&quot;multiply&quot;) {
    string = original_string.dup
    (string.to_r * 1_000_000).to_i
  }
  x.report(&quot;new     &quot;) {
    string = original_string.dup
    if string &amp;amp;&amp;amp; string.start_with?(&quot;.&quot;.freeze) &amp;amp;&amp;amp; string.length == 7
      string[0] = ''.freeze
      string.to_i
    end
  }
  x.compare!
end

# Warming up --------------------------------------
#             multiply   125.783k i/100ms
#             new        146.543k i/100ms
# Calculating -------------------------------------
#             multiply      1.751M (¬± 3.3%) i/s -      8.805M in   5.033779s
#             new           2.225M (¬± 2.1%) i/s -     11.137M in   5.007110s

# Comparison:
#             new     :  2225289.7 i/s
#             multiply:  1751254.2 i/s - 1.27x  slower
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The original code is 1.27x slower. YAY!&lt;/p&gt;
&lt;h3 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;tidying-example-3-lightning-fast-cache-keys&quot; href=&quot;#tidying-example-3-lightning-fast-cache-keys&quot;&gt;Tidying Example 3: Lightning fast cache keys&lt;/a&gt;
&lt;/h3&gt;

&lt;p&gt;The last speedup is kind of underwhelming, so you might wonder why I added it. If you remember our first example of optimizing &lt;code&gt;respond_to&lt;/code&gt;, it helped to understand the broader context of how it's used. Since this is such an expensive object allocation location, is there an opportunity to call it less or not call it at all?&lt;/p&gt;

&lt;p&gt;To find out, I added a &lt;code&gt;puts caller&lt;/code&gt; in the code and re-ran it. Here's part of a backtrace:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;====================================================================================================
‚Ä¶/code/rails/activemodel/lib/active_model/type/date_time.rb:25:in `cast_value'
‚Ä¶/code/rails/activerecord/lib/active_record/connection_adapters/postgresql/oid/date_time.rb:16:in `cast_value'
‚Ä¶/code/rails/activemodel/lib/active_model/type/value.rb:38:in `cast'
‚Ä¶/code/rails/activemodel/lib/active_model/type/helpers/accepts_multiparameter_time.rb:12:in `block in initialize'
‚Ä¶/code/rails/activemodel/lib/active_model/type/value.rb:24:in `deserialize'
‚Ä¶/.rubies/ruby-2.5.3/lib/ruby/2.5.0/delegate.rb:349:in `block in delegating_block'
‚Ä¶/code/rails/activerecord/lib/active_record/attribute_methods/time_zone_conversion.rb:8:in `deserialize'
‚Ä¶/code/rails/activemodel/lib/active_model/attribute.rb:164:in `type_cast'
‚Ä¶/code/rails/activemodel/lib/active_model/attribute.rb:42:in `value'
‚Ä¶/code/rails/activemodel/lib/active_model/attribute_set.rb:48:in `fetch_value'
‚Ä¶/code/rails/activerecord/lib/active_record/attribute_methods/read.rb:77:in `_read_attribute'
‚Ä¶/code/rails/activerecord/lib/active_record/attribute_methods/read.rb:40:in `__temp__57074616475646f51647'
‚Ä¶/code/rails/activesupport/lib/active_support/core_ext/object/try.rb:16:in `public_send'
‚Ä¶/code/rails/activesupport/lib/active_support/core_ext/object/try.rb:16:in `try'
‚Ä¶/code/rails/activerecord/lib/active_record/integration.rb:99:in `cache_version'
‚Ä¶/code/rails/activerecord/lib/active_record/integration.rb:68:in `cache_key'
‚Ä¶/code/rails/activesupport/lib/active_support/cache.rb:639:in `expanded_key'
‚Ä¶/code/rails/activesupport/lib/active_support/cache.rb:644:in `block in expanded_key'
‚Ä¶/code/rails/activesupport/lib/active_support/cache.rb:644:in `collect'
‚Ä¶/code/rails/activesupport/lib/active_support/cache.rb:644:in `expanded_key'
‚Ä¶/code/rails/activesupport/lib/active_support/cache.rb:608:in `normalize_key'
‚Ä¶/code/rails/activesupport/lib/active_support/cache.rb:565:in `block in read_multi_entries'
‚Ä¶/code/rails/activesupport/lib/active_support/cache.rb:564:in `each'
‚Ä¶/code/rails/activesupport/lib/active_support/cache.rb:564:in `read_multi_entries'
‚Ä¶/code/rails/activesupport/lib/active_support/cache.rb:387:in `block in read_multi'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I followed it backwards until I hit these two places:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;‚Ä¶/code/rails/activerecord/lib/active_record/integration.rb:99:in `cache_version'
‚Ä¶/code/rails/activerecord/lib/active_record/integration.rb:68:in `cache_key'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like this expensive code is being called while generating a cache key.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;def cache_key(*timestamp_names)
  if new_record?
    &quot;#{model_name.cache_key}/new&quot;
  else
    if cache_version &amp;amp;&amp;amp; timestamp_names.none? # &amp;lt;== line 68 here
      &quot;#{model_name.cache_key}/#{id}&quot;
    else
      timestamp = if timestamp_names.any?
        ActiveSupport::Deprecation.warn(&amp;lt;&amp;lt;-MSG.squish)
          Specifying a timestamp name for #cache_key has been deprecated in favor of
          the explicit #cache_version method that can be overwritten.
        MSG

        max_updated_column_timestamp(timestamp_names)
      else
        max_updated_column_timestamp
      end

      if timestamp
        timestamp = timestamp.utc.to_s(cache_timestamp_format)
        &quot;#{model_name.cache_key}/#{id}-#{timestamp}&quot;
      else
        &quot;#{model_name.cache_key}/#{id}&quot;
      end
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On line 68 in the &lt;code&gt;cache_key&lt;/code&gt; code it calls &lt;code&gt;cache_version&lt;/code&gt;. Here's the code for &lt;code&gt;cache_version&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;def cache_version # &amp;lt;== line 99 here
  if cache_versioning &amp;amp;&amp;amp; timestamp = try(:updated_at)
    timestamp.utc.to_s(:usec)
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is our culprit:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;timestamp = try(:updated_at)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What is happening is that some database adapters, such as the one for Postgres, returned their values from the database driver as strings. Then active record will lazily cast them into Ruby objects when they are needed. In this case, our time value method is being called to convert the updated timestamp into a time object so we can use it to generate a cache version string.&lt;/p&gt;

&lt;p&gt;Here's the value before it's converted:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;User.first.updated_at_before_type_cast # =&amp;gt; &quot;2019-04-24 21:21:09.232249&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here's the value after it's converted:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;User.first.updated_at.to_s(:usec)      # =&amp;gt; &quot;20190424212109232249&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Basically, all the code is doing is trimming out the non-integer characters. Like before, we need a guard that our optimization can be applied:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;# Detects if the value before type cast
# can be used to generate a cache_version.
#
# The fast cache version only works with a
# string value directly from the database.
#
# We also must check if the timestamp format has been changed
# or if the timezone is not set to UTC then
# we cannot apply our transformations correctly.
def can_use_fast_cache_version?(timestamp)
  timestamp.is_a?(String) &amp;amp;&amp;amp;
    cache_timestamp_format == :usec &amp;amp;&amp;amp;
    default_timezone == :utc &amp;amp;&amp;amp;
    !updated_at_came_from_user?
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then once we're in that state, we can modify the string directly:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;# Converts a raw database string to `:usec`
# format.
#
# Example:
#
#   timestamp = &quot;2018-10-15 20:02:15.266505&quot;
#   raw_timestamp_to_cache_version(timestamp)
#   # =&amp;gt; &quot;20181015200215266505&quot;
#
# PostgreSQL truncates trailing zeros,
# https://github.com/postgres/postgres/commit/3e1beda2cde3495f41290e1ece5d544525810214
# to account for this we pad the output with zeros
def raw_timestamp_to_cache_version(timestamp)
  key = timestamp.delete(&quot;- :.&quot;)
  if key.length &amp;lt; 20
    key.ljust(20, &quot;0&quot;)
  else
    key
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There's some extra logic due to the Postgres truncation behavior linked above. The resulting code to &lt;code&gt;cache_version&lt;/code&gt; becomes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-ruby&quot;&gt;def cache_version
  return unless cache_versioning

  if has_attribute?(&quot;updated_at&quot;)
    timestamp = updated_at_before_type_cast
    if can_use_fast_cache_version?(timestamp)
      raw_timestamp_to_cache_version(timestamp)
    elsif timestamp = updated_at
      timestamp.utc.to_s(cache_timestamp_format)
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That's the opportunity. What's the impact?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;lang-term&quot;&gt;Before: Total allocated: 743842 bytes (6626 objects)
After:  Total allocated: 702955 bytes (6063 objects)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The bytes reduced is 5% fewer allocations. Which is pretty good. How does it translate to speed?&lt;/p&gt;

&lt;p&gt;It turns out that time conversion is very CPU intensive and changing this code makes the target application up to 1.12x faster. This means that if your app previously required 100 servers to run, it can now run with about 88 servers.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;wrap-up&quot; href=&quot;#wrap-up&quot;&gt;Wrap up&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Adding together these optimizations and others brings the overall performance improvement to 1.23x or a net reduction of 19 servers. Basically, it's like buying 4 servers and getting 1 for free.&lt;/p&gt;

&lt;p&gt;These examples were picked from my changes to the Rails codebase, but you can use them to optimize your applications. The general framework looks like this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Get a list of all your memory&lt;/li&gt;
&lt;li&gt;Zoom in on a hotspot&lt;/li&gt;
&lt;li&gt;Figure out what is causing that memory to be allocated inside of your code&lt;/li&gt;
&lt;li&gt;Ask if you can refactor your code to not depend on those allocations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want to learn more about memory, here are my recommendations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;a href=&quot;https://www.schneems.com/2019/11/07/why-does-my-apps-memory-usage-grow-asymptotically-over-time/&quot;&gt;Why does my App's Memory Use Grow Over Time?&lt;/a&gt;  - Start here, an excellent high-level overview of what causes a system's memory to grow that will help you develop an understanding of how Ruby allocates and uses memory at the application level.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;https://www.railsspeed.com&quot;&gt;Complete Guide to Rails Performance (Book)&lt;/a&gt; - This book is by Nate Berkopec and is excellent. I recommend it to someone at least once a week.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;https://www.sitepoint.com/ruby-uses-memory/&quot;&gt;How Ruby uses memory&lt;/a&gt; - This is a lower level look at precisely what &quot;retained&quot; and &quot;allocated&quot; memory means. It uses small scripts to demonstrate Ruby memory behavior. It also explains why the &quot;total max&quot; memory of our system rarely goes down.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;https://www.schneems.com/2015/05/11/how-ruby-uses-memory.html&quot;&gt;How Ruby uses memory (Video)&lt;/a&gt; - If you're new to the concepts of object allocation, this might be an excellent place to start (you can skip the first story in the video, the rest are about memory). Memory stuff starts at 13 minutes&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;https://www.schneems.com/2017/04/12/jumping-off-the-memory-cliff/&quot;&gt;Jumping off the Ruby Memory Cliff&lt;/a&gt; - Sometimes you might see a 'cliff' in your memory metrics or a saw-tooth pattern. This article explores why that behavior exists and what it means.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;https://devcenter.heroku.com/articles/ruby-memory-use&quot;&gt;Ruby Memory Use (Heroku Devcenter article I maintain)&lt;/a&gt; - This article focuses on alleviating the symptoms of high memory use.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;https://blog.codeship.com/debugging-a-memory-leak-on-heroku/&quot;&gt;Debugging a memory leak on Heroku&lt;/a&gt; - TLDR; It's probably not a leak. Still worth reading to see how you can come to the same conclusions yourself. Content is valid for other environments than Heroku. Lots of examples of using the tool &lt;code&gt;derailed_benchmarks&lt;/code&gt; (that I wrote).&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=CS11WIalmPM&amp;amp;feature=emb_title&quot;&gt;The Life-Changing Magic of Tidying Active Record Allocations (Video)&lt;/a&gt; - This talk shows how I used tools to track down and eliminate memory allocations in real life. All of the examples are from patches I submitted to Rails, but the process works the same for finding allocations caused by your application logic.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Get ahold of Richard and stay up-to-date with Ruby, Rails, and other programming related content through a &lt;a href=&quot;https://www.schneems.com/mailinglist&quot;&gt;subscription to his mailing list&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>A Fast Car Needs Good Brakes: How We Added Client Rate Throttling to the Platform API Gem</title>
      <link>https://blog.heroku.com/rate-throttle-api-client</link>
      <pubDate>Tue, 07 Jul 2020 20:30:00 GMT</pubDate>
      <guid>https://blog.heroku.com/rate-throttle-api-client</guid>
      <description>&lt;p&gt;When API requests are made one-after-the-other they'll quickly hit rate limits and when that happens:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;blockquote class=&quot;twitter-tweet tw-align-center&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;If you provide an API client that doesn't include rate limiting, you don't really have an API client. You've got an exception generator with a remote timer.&lt;/p&gt;‚Äî Richard Schneeman ü§† Stay Inside (@schneems) &lt;a href=&quot;https://twitter.com/schneems/status/1138899094137651200?ref_src=twsrc%5Etfw&quot;&gt;June 12, 2019&lt;/a&gt;
&lt;/blockquote&gt; &lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;That tweet spawned a discussion that generated a quest to add rate throttling logic to the &lt;a href=&quot;https://rubygems.org/gems/platform-api&quot;&gt;&lt;code&gt;platform-api&lt;/code&gt;&lt;/a&gt; gem that Heroku maintains for talking to its API in Ruby.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If the term &quot;rate throttling&quot; is new to you, read &lt;a href=&quot;https://schneems.com/2020/06/25/rate-limiting-rate-throttling-and-how-they-work-together/&quot;&gt;Rate limiting, rate throttling, and how they work together&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The Heroku API uses &lt;a href=&quot;https://brandur.org/rate-limiting&quot;&gt;Genetic Cell Rate Algorithm (GCRA) as described by Brandur in this post&lt;/a&gt; on the server-side. Heroku's &lt;a href=&quot;https://devcenter.heroku.com/articles/platform-api-reference#rate-limits&quot;&gt;API docs&lt;/a&gt; state:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The API limits the number of requests each user can make per hour to protect against abuse and buggy code. Each account has a pool of request tokens that can hold at most 4500 tokens. Each API call removes one token from the pool. Tokens are added to the account pool at a rate of roughly 75 per minute (or 4500 per hour), up to a maximum of 4500. If no tokens remain, further calls will return 429 Too Many Requests until more tokens become available.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I needed to write an algorithm that never errored as a result of a 429 response. A &quot;simple&quot; solution would be to add a retry to all requests when they see a 429, but that would effectively DDoS the API. I made it a goal for the rate throttling client also to minimize its retry rate. That is, if the client makes 100 requests, and 10 of them are a 429 response that its retry rate is 10%. Since the code needed to be contained entirely in the client library, it needed to be able to function without distributed coordination between multiple clients on multiple machines except for whatever information the Heroku API returned.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;making-client-throttling-maintainable&quot; href=&quot;#making-client-throttling-maintainable&quot;&gt;Making client throttling maintainable&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Before we can get into what logic goes into a quality rate throttling algorithm, I want to talk about the process that I used as I think the journey is just as fascinating as the destination.&lt;/p&gt;

&lt;p&gt;I initially started wanting to write tests for my rate throttling strategy. I quickly realized that while testing the behavior &quot;retries a request after a 429 response,&quot; it is easy to check. I also found that checking for quality &quot;this rate throttle strategy is better than others&quot; could not be checked quite as easily. The solution that I came up with was to write a simulator in addition to tests. I would simulate the server's behavior, and then boot up several processes and threads and hit the simulated server with requests to observe the system's behavior.&lt;/p&gt;

&lt;p&gt;I initially just output values to the CLI as the simulation ran, but found it challenging to make sense of them all, so I added charting. I found my simulation took too long to run and so I added a mechanism to speed up the simulated time. I used those two outputs to write what I thought was a pretty good rate throttling algorithm. The next task was wiring it up to the &lt;code&gt;platform-api&lt;/code&gt; gem.&lt;/p&gt;

&lt;p&gt;To help out I paired with &lt;a href=&quot;https://twitter.com/lolaodelola&quot;&gt;a Heroku Engineer, Lola&lt;/a&gt;, we ended up making several PRs to a bunch of related projects, and that's its own story to tell. Finally, the day came where we were ready to get rate throttling into the &lt;code&gt;platform-api&lt;/code&gt; gem; all we needed was a review.&lt;/p&gt;

&lt;p&gt;Unfortunately, the algorithm I developed from &quot;watching some charts for a few hours&quot; didn't make a whole lot of sense, and it was painfully apparent that it wasn't maintainable. While I had developed a good gut feel for what a &quot;good&quot; algorithm did and how it behaved, I had no way of solidifying that knowledge into something that others could run with. Imagine someone in the future wants to make a change to the algorithm, and I'm no longer here. The tests I had could prevent them from breaking some expectations, but there was nothing to help them make a better algorithm.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;the-making-of-an-algorithm&quot; href=&quot;#the-making-of-an-algorithm&quot;&gt;The making of an algorithm&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;At this point, I could explain the approach I had taken to build an algorithm, but I had no way to quantify the &quot;goodness&quot; of my algorithm. That's when I decided to throw it all away and start from first principles. Instead of asking &quot;what would make my algorithm better,&quot; I asked, &quot;how would I know a change to my algorithm is better&quot; and then worked to develop some ways to quantify what &quot;better&quot; meant. Here are the goals I ended up coming up with:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Minimize average retry rate: The fewer failed API requests, the better&lt;/li&gt;
&lt;li&gt;Minimize maximum sleep time: Rate throttling involves waiting, and no one wants to wait for too long&lt;/li&gt;
&lt;li&gt;Minimize variance of request count between clients: No one likes working with a greedy co-worker, API clients are no different. No client in the distributed system should be an extended outlier&lt;/li&gt;
&lt;li&gt;Minimize time to clear a large request capacity: As the system changes, clients should respond quickly to changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I figured that if I could generate metrics on my rate-throttle algorithm and compare it to simpler algorithms, then I could show why individual decisions were made.&lt;/p&gt;

&lt;p&gt;I moved my hacky scripts for my simulation into a separate repo and, rather than relying on watching charts and logs, moved to have my simulation &lt;a href=&quot;https://github.com/zombocom/rate_throttle_client/blob/master/lib/rate_throttle_client/demo.rb&quot;&gt;produce numbers that could be used to quantify and compare algorithms&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With that work under my belt, I threw away everything I knew about rate-throttling and decided to use science and measurement to guide my way.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;writing-a-better-rate-throttling-algorithm-with-science-exponential-backoff&quot; href=&quot;#writing-a-better-rate-throttling-algorithm-with-science-exponential-backoff&quot;&gt;Writing a better rate-throttling algorithm with science: exponential backoff&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Earlier I mentioned that a &quot;simple&quot; algorithm would be to retry requests. A step up in complexity and functionality would be to retry requests after an exponential backoff. I coded it up and got some numbers for a simulated 30-minute run (which takes 3 minutes of real-time):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Avg retry rate:      60.08 %
Max sleep time:      854.89 seconds
Stdev Request Count: 387.82

Time to clear workload (4500 requests, starting_sleep: 1s):
74.23 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we've got baseline numbers, how could we work to minimize any of these values? In my initial exponential backoff model, I multiplied sleep by a factor of 2.0, what would happen if I increased it to 3.0 or decreased it to 1.2?&lt;/p&gt;

&lt;p&gt;To find out, I plugged in those values and re-ran my simulations. I found that there was a correlation between retry rate and max sleep value with the backoff factor, but they were inverse. I could lower the retry rate by increasing the factor (to 3.0), but this increased my maximum sleep time. I could reduce the maximum sleep time by decreasing the factor (to 1.2), but it increased my retry rate.&lt;/p&gt;

&lt;p&gt;That experiment told me that if I wanted to optimize both retry rate and sleep time, I could not do it via only changing the exponential factor since an improvement in one meant a degradation in the other value.&lt;/p&gt;

&lt;p&gt;At this point, we could theoretically do anything, but our metrics judge our success. We could put a cap on the maximum sleep time, for example, we could write code that says &quot;don't sleep longer than 300 seconds&quot;, but it too would hurt the retry rate. The biggest concern for me in this example is the maximum sleep time, 854 seconds is over 14 minutes which is WAAAYY too long for a single client to be sleeping.&lt;/p&gt;

&lt;p&gt;I ended up picking the 1.2 factor to decrease that value at the cost of a worse retry-rate:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Avg retry rate:      80.41 %
Max sleep time:      46.72 seconds
Stdev Request Count: 147.84

Time to clear workload (4500 requests, starting_sleep: 1s):
74.33 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Forty-six seconds is better than 14 minutes of sleep by a long shot. How could we get the retry rate down?&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;incremental-improvement-exponential-sleep-with-a-gradual-decrease&quot; href=&quot;#incremental-improvement-exponential-sleep-with-a-gradual-decrease&quot;&gt;Incremental improvement: exponential sleep with a gradual decrease&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;In the exponential backoff model, it backs-off once it sees a 429, but as soon as it hits a success response, it doesn't sleep at all. One way to reduce the retry-rate would be to assume that once a request had been rate-throttled, that future requests would need to wait as well. Essentially we would make the sleep value &quot;sticky&quot; and sleep before all requests. If we only remembered the sleep value, our rate throttle strategy wouldn't be responsive to any changes in the system, and it would have a poor &quot;time to clear workload.&quot; Instead of only remembering the sleep value, we can gradually reduce it after every successful request. This logic is very similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/TCP_congestion_control#Slow_start&quot;&gt;TCP slow start&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;How does it play out in the numbers?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Avg retry rate:      40.56 %
Max sleep time:      139.91 seconds
Stdev Request Count: 867.73

Time to clear workload (4500 requests, starting_sleep: 1s):
115.54 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Retry rate did go down by about half. Sleep time went up, but it's still well under the 14-minute mark we saw earlier. But there's a problem with a metric I've not talked about before, the &quot;stdev request count.&quot; It's easier to understand if you look at a chart to see what's going on:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1594232091-ExponentialBackoff.png&quot; alt=&quot;Exponential sleep with gradual decrease chart&quot;&gt;&lt;/p&gt;

&lt;p&gt;Here you can see one client is sleeping a lot (the red client) while other clients are not sleeping at all and chewing through all the available requests at the bottom. Not all the clients are behaving equitably. This behavior makes it harder to tune the system.&lt;/p&gt;

&lt;p&gt;One reason for this inequity is that all clients are decreasing by the same constant value for every successful request. For example, let's say we have a client A that is sleeping for 44 seconds, and client B that is sleeping for 11 seconds and both decrease their sleep value by 1 second after every request. If both clients ran for 45 seconds, it would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Client A) Sleep 44 (Decrease value: 1)
Client B) Sleep 11 (Decrease value: 1)
Client B) Sleep 10 (Decrease value: 1)
Client B) Sleep  9 (Decrease value: 1)
Client B) Sleep  8 (Decrease value: 1)
Client B) Sleep  7 (Decrease value: 1)
Client A) Sleep 43 (Decrease value: 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So while client A has decreased by 1 second total, client B has reduced by 4 seconds total, since it is firing 4x as fast (i.e., it's sleep time is 4x lower). So while the decrease rate is equal, it is not equitable. Ideally, we would want all clients to decrease at the same rate.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;all-clients-created-equal-exponential-increase-proportional-decrease&quot; href=&quot;#all-clients-created-equal-exponential-increase-proportional-decrease&quot;&gt;All clients created equal: exponential increase proportional decrease&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Since clients cannot communicate with each other in our distributed system, one way to guaranteed proportional decreases is to use the sleep value in the decrease amount:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;decrease_value = (sleep_time) / some_value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where &lt;code&gt;some_value&lt;/code&gt; is a magic number. In this scenario the same clients A and B running for 45 seconds would look like this with a value of 100:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Client A) Sleep 44
Client B) Sleep 11
Client B) Sleep 10.89 (Decrease value: 11.00/100 = 0.1100)
Client B) Sleep 10.78 (Decrease value: 10.89/100 = 0.1089)
Client B) Sleep 10.67 (Decrease value: 10.78/100 = 0.1078)
Client B) Sleep 10.56 (Decrease value: 10.67/100 = 0.1067)
Client A) Sleep 43.56 (Decrease value: 44.00/100 = 0.4400)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now client A has had a decrease of 0.44, and client B has had a reduction of 0.4334 (11 seconds - 10.56 seconds), which is a lot more equitable than before. Since &lt;code&gt;some_value&lt;/code&gt; is tunable, I wanted to use a larger number so that the retry rate would be lower than 40%. I chose 4500 since that's the maximum number of requests in the GCRA bucket for Heroku's API.&lt;/p&gt;

&lt;p&gt;Here's what the results looked like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Avg retry rate:      3.66 %
Max sleep time:      17.31 seconds
Stdev Request Count: 101.94

Time to clear workload (4500 requests, starting_sleep: 1s):
551.10 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The retry rate went WAAAY down, which makes sense since we're decreasing slower than before (the constant decrease value previously was 0.8). Stdev went way down as well. It's about 8x lower. Surprisingly the max sleep time went down as well. I believe this to be a factor of a decrease in the number of required exponential backoff events. Here's what this algorithm looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1594232161-ExponentialIncreaseProportionalDecrease.png&quot; alt=&quot;Exponential increase proportional decrease chart&quot;&gt;&lt;/p&gt;

&lt;p&gt;The only problem here is that the &quot;time to clear workload&quot; is 5x higher than before. What exactly is being measured here? In this scenario, we're simulating a cyclical workflow where clients are running under high load, then go through a light load, and then back to a high load. The simulation starts all clients with a sleep value, but the server's rate-limit is reset to 4500. The time is how long it takes the client to clear all 4500 requests.&lt;/p&gt;

&lt;p&gt;What this metric of 551 seconds is telling me is that this strategy is not very responsive to a change in the system. To illustrate this problem, I ran the same algorithm starting each client at 8 seconds of sleep instead of 1 second to see how long it would take to trigger a rate limit:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1594143623-CleanShot%202020-07-07%20at%2010.39.35%402x.png&quot; alt=&quot;Exponential increase proportional decrease chart 7-hour torture test&quot;&gt;&lt;/p&gt;

&lt;p&gt;The graph shows that it takes about 7 hours to clear all these requests, which is not good. What we need is a way to clear requests faster when there are more requests.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;the-only-remaining-option-exponential-increase-proportional-remaining-decrease&quot; href=&quot;#the-only-remaining-option-exponential-increase-proportional-remaining-decrease&quot;&gt;The only remaining option: exponential increase proportional remaining decrease&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;When you make a request to the Heroku API, it tells you how many requests you have left remaining in your bucket in a header. Our problem with the &quot;proportional decrease&quot; is mostly that when there are lots of requests remaining in the bucket, it takes a long time to clear them (if the prior sleep rate was high, such as in a varying workload). To account for this, we can decrease the sleep value quicker when the remaining bucket is full and slower when the remaining bucket is almost empty. To express that in an expression, it might look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;decrease_value = (sleep_time * request_count_remaining) / some_value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In my case, I chose &lt;code&gt;some_value&lt;/code&gt; to be the maximum number of requests possible in a bucket, which is 4500. You can imagine a scenario where workers were very busy for a period and being rate limited. Then no jobs came in for over an hour - perhaps the workday was over, and the number of requests remaining in the bucket re-filled to 4500. On the next request, this algorithm would reduce the sleep value by itself since 4500/4500 is one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;decrease_value = sleep_time * 4500 / 4500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That means it doesn't matter how immense the sleep value is, it will adjust fairly quickly to a change in workload. Good in theory, how does it perform in the simulation?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Avg retry rate:      3.07 %
Max sleep time:      17.32 seconds
Stdev Request Count: 78.44

Time to clear workload (4500 requests, starting_sleep: 1s):
84.23 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This rate throttle strategy performs very well on all metrics. It is the best (or very close) to several metrics. Here's a chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1594232264-ExponentialIncreaseProportionalRemainingDecrease.png&quot; alt=&quot;Exponential increase proportional remaining decrease chart&quot;&gt;&lt;/p&gt;

&lt;p&gt;This strategy is the &quot;winner&quot; of my experiments and the algorithm that I  chose to go into the &lt;code&gt;platform-api&lt;/code&gt; gem.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;my-original-solution&quot; href=&quot;#my-original-solution&quot;&gt;My original solution&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;While I originally built this whole elaborate scheme to prove how my solution was optimal, I did something by accident. By following a scientific and measurement-based approach, I accidentally found a simpler solution that performed better than my original answer. Which I'm happier about, it shows that the extra effort was worth it. To &quot;prove&quot; what I found by observation and tinkering could be not only quantified by numbers but improved upon is fantastic.&lt;/p&gt;

&lt;p&gt;While my original solution had some scripts and charts, this new solution has tests covering the behavior of the simulation and charting code. My initial solution was very brittle. I didn't feel very comfortable coming back and making changes to it; this new solution and the accompanying support code is a joy to work with. My favorite part though is that now if anyone asks me, &quot;what about trying &lt;x&gt;&quot; or &quot;have you considered &lt;y&gt;&quot; is that I can point them at &lt;a href=&quot;https://github.com/zombocom/rate_throttle_client&quot;&gt;my rate client throttling library&lt;/a&gt;, they have all the tools to implement their idea, test it, and report back with a swift feedback loop.&lt;/y&gt;&lt;/x&gt;&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;code-gem-39-platform-api-39-39-gt-3-0-39-code&quot; href=&quot;#code-gem-39-platform-api-39-39-gt-3-0-39-code&quot;&gt;&lt;code&gt;gem 'platform-api', '~&amp;gt; 3.0'&lt;/code&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;While I mostly wanted to talk about the process of writing rate-throttling code, this whole thing started from a desire to get client rate-throttling into the &lt;code&gt;platform-api&lt;/code&gt; gem. Once I did the work to prove my solution was reasonable, we worked on a rollout strategy. We released a version of the gem in a minor bump with rate-throttling available, but with a &quot;null&quot; strategy that would preserve existing behavior. This release strategy allowed us to issue a warning to anyone depending on the original behavior. Then we released a major version with the rate-throttling strategy enabled by default. We did this first with &quot;pre&quot; release versions and then actual versions to be extra safe.&lt;/p&gt;

&lt;p&gt;So far, the feedback has been overwhelming that no one has noticed. We didn't cause any significant breaks or introduce any severe disfunction to any applications. If you've not already, I invite you to upgrade to 3.0.0+ of the &lt;code&gt;platform-api&lt;/code&gt; gem and give it a spin. I would love to hear your feedback.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Get ahold of Richard and stay up-to-date with Ruby, Rails, and other programming related content through a &lt;a href=&quot;https://www.schneems.com/mailinglist&quot;&gt;subscription to his mailing list&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Ruby 2.7.0 Holiday Release</title>
      <link>https://blog.heroku.com/ruby-2-7-0-holiday-release</link>
      <pubDate>Wed, 25 Dec 2019 13:29:00 GMT</pubDate>
      <guid>https://blog.heroku.com/ruby-2-7-0-holiday-release</guid>
      <description>&lt;p&gt;When Heroku launched in 2007 there was only a single Ruby version that could be used on the platform. In 2012 &lt;a href=&quot;https://blog.heroku.com/multiple_ruby_version_support_on_heroku&quot;&gt;Heroku began to support multiple Ruby versions&lt;/a&gt;. Since then, we've had a holiday tradition of releasing the new versions of Ruby on the same day they come out, which always happens on Christmas day (December 25th).&lt;/p&gt;

&lt;p&gt;If you're new to the community, you might be curious about where releasing a new minor version on Christmas comes from. To help answer that question, we interviewed &lt;a href=&quot;https://blog.heroku.com/ruby-2-3-0-on-heroku-with-matz&quot;&gt;Matz's, who works as the Chief Ruby Architect at Heroku&lt;/a&gt; in 2015. In his own words:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Ruby was originally my pet project, my side project. So releases usually happened during my holiday time. Now, it‚Äôs a tradition. It‚Äôs ruby-core‚Äôs gift to the Ruby community.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This year, it's my second opportunity to curl up with a cup of cocoa and help spread Ruby cheer to Heroku customers. As of publishing this article, you can now Ruby 2.7.0 is generally available on the platform. To use it put this in your Gemfile:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;ruby&quot;&gt;ruby '2.7.0'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Commit to git, push your code, and enjoy a happy holiday season from all of us at Heroku.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;what-39-s-new-in-2-7-0&quot; href=&quot;#what-39-s-new-in-2-7-0&quot;&gt;What's new in 2.7.0?&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;You can hear directly for yourself from Matz in his 2019 RubyConf keynote:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/2g9R7PUCEXo&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;There's a ton of new features, and other sites have done a great job summarizing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.ruby-lang.org/en/news/2019/12/25/ruby-2-7-0-released/&quot;&gt;RubyLang's 2.7.0 release announcement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.saeloun.com/category/ruby-2-7/&quot;&gt;Saeloun's What's new in Ruby 2.7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://prathamesh.tech/2019/12/25/all-you-need-to-know-about-ruby-2-7/&quot;&gt;Prathamesh's What's new in Ruby 2.7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.rubyguides.com/2019/12/ruby-2-7-new-features/&quot;&gt;Ruby Guide's What's new in Ruby 2.7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The largest new feature looks to be pattern matching:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=wo4eZ2iVIyo&quot;&gt;RubyConf 2019 video on Pattern Matching&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And the biggest change to existing projects is likely to be that some use of keyword arguments has been deprecated in anticipation of the coming Ruby 3.0 release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.ruby-lang.org/en/news/2019/12/12/separation-of-positional-and-keyword-arguments-in-ruby-3-0/&quot;&gt;Deprecation of some keword argument usage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>The Curious Case of the Table-Locking UPDATE Query</title>
      <link>https://blog.heroku.com/curious-case-table-locking-update-query</link>
      <pubDate>Wed, 18 Dec 2019 18:07:00 GMT</pubDate>
      <guid>https://blog.heroku.com/curious-case-table-locking-update-query</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Update: On closer inspection, the lock type was not on the table, but on a tuple. For more information on this locking mechanism see the &lt;a href=&quot;https://github.com/postgres/postgres/blob/master/src/backend/access/heap/README.tuplock&quot;&gt;internal Postgresql tuple locking documentation&lt;/a&gt;. Postgres does not have lock promotion as suggested in the debugging section of this post.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I maintain an internal-facing service at Heroku that does metadata processing. It's not real-time, so there's plenty of slack for when things go wrong. Recently I discovered that the system was getting bogged down to the point where no jobs were being executed at all. After hours of debugging, I found the problem was an &lt;code&gt;UPDATE&lt;/code&gt; on a single row on a single table was causing the entire table to lock, which caused a lock queue and ground the whole process to a halt. This post is a story about how the problem was debugged and fixed and why such a seemingly simple query caused so much harm.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;no-jobs-processing&quot; href=&quot;#no-jobs-processing&quot;&gt;No jobs processing&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;I started debugging when the backlog on our system began to grow, and the number of jobs being processed fell to nearly zero. The system has been running in production for years, and while there have been occasional performance issues, nothing stood out as a huge problem. I checked our datastores, and they were well under their limits, I checked our error tracker and didn't see any smoking guns. My best guess was the database where the results were being stored was having problems.&lt;/p&gt;

&lt;p&gt;The first thing I did was run &lt;code&gt;heroku pg:diagnose&lt;/code&gt;, which shows &quot;red&quot; (critical) and &quot;yellow&quot; (important but less critical) issues. It showed that I had queries that had been running for DAYS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;68698   5 days 18:01:26.446979  UPDATE &quot;table&quot; SET &amp;lt;values&amp;gt; WHERE (&quot;uuid&quot; = '&amp;lt;uuid&amp;gt;')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which seemed odd. The query in question was a simple update, and it's not even on the most massive table in the DB. When I checked &lt;code&gt;heroku pg:outliers&lt;/code&gt; from the &lt;a href=&quot;https://github.com/heroku/heroku-pg-extras&quot;&gt;pg extras CLI plugin&lt;/a&gt; I was surprised to see this update taking up 80%+ of the time even though it is smaller than the largest table in the database by a factor of 200. So what gives?&lt;/p&gt;

&lt;p&gt;Running the update statement manually didn't reproduce the issue, so I was fresh out of ideas. If it had, then I could have run with &lt;code&gt;EXPLAIN ANALYZE&lt;/code&gt; to see why it was so slow. Luckily I work with some pretty fantastic database engineers, and I pinged them for possible ideas. They mentioned that there might be a locking issue with the database. The idea was strange to me since it had been running relatively unchanged for an extremely long time and only now started to see problems, but I decided to look into it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;SELECT
  S.pid,
  age(clock_timestamp(), query_start),
  query,
  L.mode,
  L.locktype,
  L.granted
FROM pg_stat_activity S
inner join pg_locks L on S.pid = L.pid
order by L.granted, L.pid DESC;
-----------------------------------
pid      | 127624
age      | 2 days 01:45:00.416267
query    | UPDATE &quot;table&quot; SET &amp;lt;values&amp;gt; WHERE (&quot;uuid&quot; = '&amp;lt;uuid&amp;gt;')
mode     | AccessExclusiveLock
locktype | tuple
granted  | f
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I saw a ton of queries that were hung for quite some time, and most of them pointed to my seemingly teeny &lt;code&gt;UPDATE&lt;/code&gt; statement.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;all-about-locks&quot; href=&quot;#all-about-locks&quot;&gt;All about locks&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Up until this point, I basically knew nothing about how PostgreSQL uses locking other than in an explicit advisory lock, which can be used via a gem like &lt;a href=&quot;https://github.com/heroku/pg_lock&quot;&gt;pg_lock&lt;/a&gt; (That I maintain). Luckily Postgres has excellent docs around locks, but it's a bit much if you're new to the field: &lt;a href=&quot;https://www.postgresql.org/docs/11/explicit-locking.html#LOCKING-TABLES&quot;&gt;Postgresql Lock documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Looking up the name of the lock from before &lt;code&gt;Access Exclusive Lock&lt;/code&gt; I saw that it locks the whole table:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ACCESS EXCLUSIVE
Conflicts with locks of all modes (ACCESS SHARE, ROW SHARE, ROW EXCLUSIVE, SHARE UPDATE EXCLUSIVE, SHARE, SHARE ROW EXCLUSIVE, EXCLUSIVE, and ACCESS EXCLUSIVE). This mode guarantees that the holder is the only transaction accessing the table in any way.
Acquired by the DROP TABLE, TRUNCATE, REINDEX, CLUSTER, VACUUM FULL, and REFRESH MATERIALIZED VIEW (without CONCURRENTLY) commands. Many forms of ALTER TABLE also acquire a lock at this level (see ALTER TABLE). This is also the default lock mode for LOCK TABLE statements that do not specify a mode explicitly.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;From the docs, this lock is not typically triggered by an &lt;code&gt;UPDATE&lt;/code&gt;, so what gives? Grepping through the docs showed me that an &lt;code&gt;UPDATE&lt;/code&gt; should trigger a &lt;code&gt;ROW SHARE&lt;/code&gt; lock:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ROW EXCLUSIVE
Conflicts with the SHARE, SHARE ROW EXCLUSIVE, EXCLUSIVE, and ACCESS EXCLUSIVE lock modes.

The commands UPDATE, DELETE, and INSERT acquire this lock mode on the target table (in addition to ACCESS SHARE locks on any other referenced tables). In general, this lock mode will be acquired by any command that modifies data in a table.
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;A database engineer directly told me what kind of lock an &lt;code&gt;UPDATE&lt;/code&gt; should use, but you could find it in the docs if you don't have access to some excellent database professionals.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Mostly what happens when you try to &lt;code&gt;UPDATE&lt;/code&gt; is that Postgres will acquire a lock on the row that you want to change. If you have two update statements running at the same time on the same row, then the second must wait for the first to process. So why on earth, if an &lt;code&gt;UPDATE&lt;/code&gt; is supposed only to take out a row lock, was my query taking out a lock against the whole table?&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;unmasking-a-locking-mystery&quot; href=&quot;#unmasking-a-locking-mystery&quot;&gt;Unmasking a locking mystery&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;I would love to tell you that I have a really great debugging tool to tell you about here, but I mostly duck-duck-go-ed (searched) a ton and eventually found &lt;a href=&quot;https://grokbase.com/t/postgresql/pgsql-general/124s02j3jy/updates-sharelocks-rowexclusivelocks-and-deadlocks&quot;&gt;this forum post&lt;/a&gt;. In the post someone is complaining about a similar behavior, they're using an update but are seeing more aggressive lock being used sometimes.&lt;/p&gt;

&lt;p&gt;Based on the responses to the forum it sounded like if there is more than a few &lt;code&gt;UPDATE&lt;/code&gt; queries that are trying to modify the same row at the same time what happens is that one of the queries will try to acquire the lock, see it is taken then it will instead acquire a larger lock on the table. Postgres queues locks, so if this happens for multiple rows with similar contention, then multiple queries would be taking out locks on the whole table, which somewhat could explain the behavior I was seeing. It seemed plausible, but why was there such a problem?&lt;/p&gt;

&lt;p&gt;I combed over my codebase and couldn't find anything. Then as I was laying down to go to bed that evening, I had a moment of inspiration where I remembered that we were updating the database in parallel for the same UUID using threads:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;@things.map do |thing|
  Concurrent::Promise.execute(executor: :fast) do
    store_results!(thing)
  end
end.each(&amp;amp;:value!)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In every loop, we were creating a promise that would concurrently update values (using a thread pool). Due to a design decision from years ago, each loop causes an &lt;code&gt;UPDATE&lt;/code&gt; to the same row in the database for each job being run. This programming pattern was never a problem before because, as I mentioned earlier, there's another table with more than 200x the number of records, so we've never had any issues with this scheme until recently.&lt;/p&gt;

&lt;p&gt;With this new theory, I removed the concurrency, which meant that each &lt;code&gt;UPDATE&lt;/code&gt; call would be sequential instead of in parallel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;@things.map do |thing|
  store_results!(thing)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While the code is less efficiently in the use of IO on the Ruby program, it means that the chance that the same row will try to be updated at the same time is drastically decreased.&lt;/p&gt;

&lt;p&gt;I manually killed the long-running locked queries using &lt;code&gt;SELECT pg_cancel_backend(&amp;lt;pid&amp;gt;);&lt;/code&gt; and I deployed this change (in the morning after a code review).&lt;/p&gt;

&lt;p&gt;Once the old stuck queries were aborted, and the new code was in place, then the system promptly got back up and running, churning through plenty of backlog.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;locks-and-stuff&quot; href=&quot;#locks-and-stuff&quot;&gt;Locks and stuff&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;While this somewhat obscure debugging story might not be directly relevant to your database, here are some things you can take away from this article. Your database has locks (think mutexes but with varying scope), and those locks can mess up your day if they're doing something different than you're expecting. You can see the locks that your database is currently using by running the &lt;code&gt;heroku pg:locks&lt;/code&gt; command (may need to install the &lt;code&gt;pg:extras&lt;/code&gt; plugin). You can also see which queries are taking out which locks using the SQL query I posted earlier.&lt;/p&gt;

&lt;p&gt;The next thing I want to cover is documentation. If it weren't for several very experienced Postgres experts and a seemingly random forum post about how multiple &lt;code&gt;UPDATE&lt;/code&gt; statements can trigger a more aggressive lock type, then I never would have figured this out. If you're familiar with the Postgres documentation, is this behavior written down anywhere? If so, then could we make it easier to find or understand somehow? If it's not written down, can you help me document it? I don't mind writing documentation, but I'm not totally sure what the expected behavior is. For instance, why does a lock queue for a row that goes above a specific threshold trigger a table lock? And what exactly is that threshold? I'm sure this behavior makes total sense from an implementation point of view, but as an end-user, I would like it to be spelled out and officially documented.&lt;/p&gt;

&lt;p&gt;I hope you either learned a thing or two or at least got a kick out of my misery. This issue was a pain to debug, but in hindsight, a quirky bug to blog about. Thanks for reading!&lt;/p&gt;

&lt;p&gt;And to learn about another potential database issue, check out this other blog post by Heroku Engineer Ben Fritsch, &lt;a href=&quot;https://blog.heroku.com/know-your-database-types&quot;&gt;Know Your Database Types&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Special thanks to &lt;a href=&quot;https://github.com/mble&quot;&gt;Matthew Blewitt&lt;/a&gt; and &lt;a href=&quot;https://github.com/andscoop&quot;&gt;Andy Cooper&lt;/a&gt; for helping me debug this!&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Puma 4: Hammering Out H13s‚ÄîA Debugging Story</title>
      <link>https://blog.heroku.com/puma-4-hammering-out-h13s-a-debugging-story</link>
      <pubDate>Fri, 12 Jul 2019 12:53:00 GMT</pubDate>
      <guid>https://blog.heroku.com/puma-4-hammering-out-h13s-a-debugging-story</guid>
      <description>&lt;p&gt;For quite some time we've received reports from our larger customers about a mysterious &lt;a href=&quot;https://devcenter.heroku.com/articles/error-codes#h13-connection-closed-without-response&quot;&gt;H13 - Connection closed error&lt;/a&gt; showing up for Ruby applications. Curiously it only ever happened around the time they were deploying or scaling their dynos. Even more peculiar, it only happened to relatively high scale applications. We couldn't reproduce the behavior on an example app. This is a story about distributed coordination, the TCP API, and how we debugged and fixed a bug in Puma that only shows up at scale.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1562883126-Screenshot%202019-06-23%2015.04.50.png&quot; alt=&quot;Screenshot showing H13 errors&quot;&gt;&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;connection-closed&quot; href=&quot;#connection-closed&quot;&gt;Connection closed&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;First of all, what even is an H13 error? From our error page documentation:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This error is thrown when a process in your web dyno accepts a connection, but then closes the socket without writing anything to it.
One example where this might happen is when a Unicorn web server is configured with a timeout shorter than 30s and a request has not been processed by a worker before the timeout happens. In this case, Unicorn closes the connection before any data is written, resulting in an H13.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Fun fact: Our error codes start with the letter of the component where they came from. Our Routing code is all written in Erlang and is named &quot;Hermes&quot; so any error codes from Heroku that start with an &quot;H&quot; indicate an error from the router.&lt;/p&gt;

&lt;p&gt;The documentation gives an example of an H13 error code with the unicorn webserver, but it can happen any time a connection is closed via a server, but there has been no response written. Here‚Äôs an example showing how to &lt;a href=&quot;https://github.com/hunterloftis/heroku-node-errcodes/blob/master/h13&quot;&gt;reproduce a H13 explicitly with a node app&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What does it mean for an application to get an H13? Essentially every one of these errors correlates to a customer who got an error page. Serving a handful of errors every time the app restarts or deploys or auto-scales is an awful user experience, so it's worth it to find and fix.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;debugging&quot; href=&quot;#debugging&quot;&gt;Debugging&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;I have maintained the Ruby buildpack for several years, and part of that job is to handle support escalations for Ruby tickets. In addition to the normal deployment issues, I've been developing an interest in performance, scalability, and web servers (I recently started helping to maintain the Puma webserver). Because of these interests, when a tricky issue comes in from one of our larger customers, especially if it only happens at scale, I take particular interest. &lt;/p&gt;

&lt;p&gt;To understand the problem, you need to know a little about the nature of sending distributed messages. Webservers are inherently distributed systems, and to make things more complicated, we often use distributed systems to manage our distributed systems.&lt;/p&gt;

&lt;p&gt;In the case of this error, it didn't seem to come from a customer's application code i.e. they didn't seem to have anything misconfigured. It also only seemed to happen when a dyno was being shut down.&lt;/p&gt;

&lt;p&gt;To shut down a dyno two things have to happen, we need to send a &lt;code&gt;SIGTERM&lt;/code&gt; to the processes on the dyno which &lt;a href=&quot;https://devcenter.heroku.com/articles/what-happens-to-ruby-apps-when-they-are-restarted&quot;&gt;tells the webserver to safely shutdown&lt;/a&gt;. We also need to tell our router to stop sending requests to that dyno since it will be shut down soon.&lt;/p&gt;

&lt;p&gt;These two operations happen on two different systems. The dyno runs on one server, the router which serves our requests is a separate system. It's itself a distributed system. It turns out that while both systems get the message at about the same time, the router might still let a few requests trickle into the dyno that is being shut down after it receives the &lt;code&gt;SIGTERM&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;That explains the problem then, right? The reason this only happens on apps with a large amount of traffic is they get so many requests there is more chance that there will be a race condition between when the router stops sending requests and the dyno receives the &lt;code&gt;SIGTERM&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;That sounds like a bug with the router then right? Before we get too deep into the difficulties of distributed coordination, I noticed that other apps with just as much load weren't getting H13 errors. What did that tell me? It told me that the distributed behavior of our system wasn't to blame. If other webservers can handle this just fine, then we need to update our webserver, Puma in this case.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;reproduction&quot; href=&quot;#reproduction&quot;&gt;Reproduction&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;When you're dealing with a distributed system bug that's reliant on a race condition, reproducing the issue can be a tricky affair. While pairing on the issue with another Heroku engineer, &lt;a href=&quot;https://twitter.com/chapambrose?lang=en&quot;&gt;Chap Ambrose&lt;/a&gt;, we hit an idea. First, we would reproduce the H13 behavior in any app to figure out what &lt;a href=&quot;https://curl.haxx.se/libcurl/c/libcurl-errors.html&quot;&gt;curl exit code&lt;/a&gt; we would get, and then we could try to reproduce the exact failure conditions with a more complicated example.&lt;/p&gt;

&lt;p&gt;A simple reproduction rack app looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;app = Proc.new do |env|
  current_pid = Process.pid
  signal      = &quot;SIGKILL&quot;
  Process.kill(signal, current_pid)
  ['200', {'Content-Type' =&amp;gt; 'text/html'}, ['A barebones rack app.']]
end

run app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you run this &lt;code&gt;config.ru&lt;/code&gt; with Puma and hit it with a request, you'll get a connection that is closed without a request getting written. That was pretty easy.&lt;/p&gt;

&lt;p&gt;The curl code when a connection is closed like this is &lt;code&gt;52&lt;/code&gt; so now we can detect when it happens. &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ curl localhost:9292
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
curl: (52) Empty reply from server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A more complicated reproduction happens when SIGTERM is called but requests keep coming in. To facilitate that we ended up with a reproduction that looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;app = Proc.new do |env|
  puma_pid = File.read('puma.pid').to_i
  Process.kill(&quot;SIGTERM&quot;, puma_pid)
  Process.kill(&quot;SIGTERM&quot;, Process.pid)

  ['200', {'Content-Type' =&amp;gt; 'text/html'}, ['A barebones rack app.']]
end

run app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This &lt;code&gt;config.ru&lt;/code&gt; rack app sends a &lt;code&gt;SIGTERM&lt;/code&gt; to itself and it's parent process on the first request. So other future requests will be coming in when the server is shutting down.&lt;/p&gt;

&lt;p&gt;Then we can write a script that boots this server and hits it with a bunch of requests in parallel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;threads = []

threads &amp;lt;&amp;lt; Thread.new do
  puts `puma &amp;gt; puma.log` unless ENV[&quot;NO_PUMA_BOOT&quot;]
end

sleep(3)
require 'fileutils'
FileUtils.mkdir_p(&quot;tmp/requests&quot;)

20.times do |i|
  threads &amp;lt;&amp;lt; Thread.new do
    request = `curl localhost:9292/?request_thread=#{i} &amp;amp;&amp;gt; tmp/requests/requests#{i}.log`
    puts $?
  end
end

threads.map {|t| t.join }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When we run this reproduction, we see that it gives us the exact behavior we're looking to reproduce. Even better, when this code is deployed on Heroku we can see an H13 error is triggered:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;2019-05-10T18:41:06.859330+00:00 heroku[router]: at=error code=H13 desc=&quot;Connection closed without response&quot; method=GET path=&quot;/?request_thread=6&quot; host=ruby-h13.herokuapp.com request_id=05696319-a6ff-4fad-b219-6dd043536314 fwd=&quot;&amp;lt;ip&amp;gt;&quot; dyno=web.1 connect=0ms service=5ms status=503 bytes=0 protocol=https
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can get all this code and some more details on the &lt;a href=&quot;https://github.com/schneems/puma_connection_closed_reproduction&quot;&gt;reproduciton script repo&lt;/a&gt;. And here's the &lt;a href=&quot;https://github.com/puma/puma/issues/1802&quot;&gt;Puma Issue I was using to track the behavior&lt;/a&gt;&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;closing-the-connection-closed-bug&quot; href=&quot;#closing-the-connection-closed-bug&quot;&gt;Closing the connection closed bug&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;With a reproduction script in hand, it was possible for us to add debugging statements to Puma internals to see how it behaved while experiencing this issue.&lt;/p&gt;

&lt;p&gt;With a little investigation, it turned out that Puma never explicitly closed the socket of the connection. Instead, it relied on the process stopping to close it. &lt;/p&gt;

&lt;p&gt;What exactly does that mean? Every time you type a URL into a browser, the request gets routed to a server. On Heroku, the request goes to our router. The router then attempts to connect to a dyno (server) and pass it the request. The underlying mechanism that allows this is the webserver (Puma) on the dyno opening up a TCP socket on a $PORT. The request is accepted onto the socket, and it will sit there until the webserver (Puma) is ready to read it in and respond to it.&lt;/p&gt;

&lt;p&gt;What behavior do we want to happen to avoid this H13 error? In the error case, the router tries to connect to the dyno, it's successful, and since the request is accepted by the dyno, the router expects the dyno to handle writing the request. If instead, the socket is closed when it tries to pass on the request it will know that Puma cannot respond. It will then retry passing the connection to another dyno. There are times when a webserver might reject a connection, for example, if the socket is full (default is only to allow 1024 connections on the socket backlog), or if the entire server has crashed.&lt;/p&gt;

&lt;p&gt;In our case, closing the socket is what we want. It correctly communicates to the router to do the right thing (try passing the connection to another dyno or hold onto it in the case all dynos are restarting). &lt;/p&gt;

&lt;p&gt;So then, the solution to the problem was to close the socket before attempting to shut down explicitly. Here's the &lt;a href=&quot;https://github.com/puma/puma/pull/1808&quot;&gt;PR&lt;/a&gt;. The main magic is just one line:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;@launcher.close_binder_listeners
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you're a worrier (I know I am) you might be afraid that closing the socket prevents any in-flight requests from being completed successfully. Lucky for us closing a socket prevents incoming requests but still allows us to respond to existing requests. If you don't believe me, think about how you could test it with one of my above example repos.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;testing-distributed-behavior&quot; href=&quot;#testing-distributed-behavior&quot;&gt;Testing distributed behavior&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;I don't know if this behavior in Puma broke, or maybe it never worked. To try to make sure that it continues to work in the future, I wanted to write a test for it. I reached out to &lt;a href=&quot;https://twitter.com/touchingvirus?lang=en&quot;&gt;dannyfallon&lt;/a&gt; who has helped out on some other Puma issues, and we remote paired on the tests using &lt;a href=&quot;https://tuple.app/&quot;&gt;Tuple&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The tests ended up being &lt;a href=&quot;https://github.com/puma/puma/pull/1808/files#diff-ad8d9f1e0cf07519c2372ca5f60ca4d2&quot;&gt;not terribly different than our example reproduction above&lt;/a&gt;, but it was pretty tricky to get it to have consistent behavior.&lt;/p&gt;

&lt;p&gt;With an issue that doesn't regularly show up unless it's on an app at scale, it's essential to test, as &lt;a href=&quot;https://twitter.com/mipsytipsy&quot;&gt;Charity Majors&lt;/a&gt; would say &quot;in production&quot;. We had several Heroku customers who were seeing this error try out my patch. They reported some other issues, which we were able to resolve, after fixing those issues, it looked like the errors were fixed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1562883272-59190728-7bf56a80-8b4b-11e9-8e01-84238fecf24c.png&quot; alt=&quot;Screenshot showing no more H13 errors&quot;&gt;&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;rolling-out-the-fix&quot; href=&quot;#rolling-out-the-fix&quot;&gt;Rolling out the fix&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Puma 4, which came with this fix, &lt;a href=&quot;https://github.com/puma/puma/releases/tag/v4.0.0&quot;&gt;was recently released&lt;/a&gt;. We reached out to a customer who was using Puma and seeing a large number of H13s, and this release stopped them in their tracks.&lt;/p&gt;

&lt;p&gt;Learn more about Puma 4 below.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot; data-align=&quot;center&quot; data-conversation=&quot;none&quot; data-dnt=&quot;true&quot; data-id=&quot;1143577608791220224 &quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;By the coders who brought you Llamas in Pajamas. A new cinematic Ruby server experience. Directed by &lt;a href=&quot;https://twitter.com/evanphx?ref_src=twsrc%5Etfw&quot;&gt;@evanphx&lt;/a&gt;, cinematography by &lt;a href=&quot;https://twitter.com/nateberkopec?ref_src=twsrc%5Etfw&quot;&gt;@nateberkopec&lt;/a&gt;, produced by &lt;a href=&quot;https://twitter.com/schneems?ref_src=twsrc%5Etfw&quot;&gt;@schneems&lt;/a&gt;.&lt;br&gt;&lt;br&gt;Introducing - Puma: 4 Fast 4 Furious&lt;a href=&quot;https://t.co/06PG0lzubk&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://t.co/06PG0lzubk&quot;&gt;https://t.co/06PG0lzubk&lt;/a&gt; &lt;a href=&quot;https://t.co/O1dLfwnctJ&quot;&gt;pic.twitter.com/O1dLfwnctJ&lt;/a&gt;&lt;/p&gt;‚Äî Richard Schneeman ü§† (@schneems) &lt;a href=&quot;https://twitter.com/schneems/status/1143577608791220224?ref_src=twsrc%5Etfw&quot;&gt;June 25, 2019&lt;/a&gt;
&lt;/blockquote&gt; &lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Debugging in Ruby‚ÄîBusting a Year-old Bug in Sprockets</title>
      <link>https://blog.heroku.com/debugging-year-old-sprockets-bug</link>
      <pubDate>Tue, 26 Feb 2019 18:30:00 GMT</pubDate>
      <guid>https://blog.heroku.com/debugging-year-old-sprockets-bug</guid>
      <description>&lt;p&gt;Debugging is an important skill to develop as you work your way up to more complex projects. Seasoned engineers have a sixth sense for squashing bugs and have built up an impressive collection of tools that help them diagnose and fix bugs.&lt;/p&gt;

&lt;p&gt;I'm a member of Heroku‚Äôs Ruby team and creator of &lt;a href=&quot;https://www.codetriage.com/&quot;&gt;CodeTriage&lt;/a&gt; and today we‚Äôll look at the tools that I used on a journey to fix a gnarly bug in &lt;a href=&quot;https://github.com/rails/sprockets&quot;&gt;Sprockets&lt;/a&gt;. Sprockets is an asset packaging system written in Ruby that lies at the heart of  Rails‚Äô asset processing pipeline.&lt;/p&gt;

&lt;p&gt;At the end of the post, you will know how Sprockets works and how to debug in Ruby.&lt;/p&gt;
&lt;h1 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;unexpected-behavior-in-sprockets&quot; href=&quot;#unexpected-behavior-in-sprockets&quot;&gt;Unexpected Behavior in Sprockets&lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;Sprockets gives developers a convenient way to compile, minify, and serve JavaScript and CSS files. Its extensible preprocessor pipeline has support for languages like CoffeeScript, SaaS, and SCSS. It is included in Rails via the &lt;a href=&quot;https://github.com/rails/sprockets-rails&quot;&gt;sprockets-rails&lt;/a&gt; gem but can also be used in a standalone fashion, for example, to &lt;a href=&quot;http://recipes.sinatrarb.com/p/asset_management/sprockets&quot;&gt;package Sinatra assets&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Earlier this month, we recorded a &lt;a href=&quot;https://www.youtube.com/watch?v=ZEoF_OWpXZY&amp;amp;feature=youtu.be&quot;&gt;live-debugging session&lt;/a&gt; where we experienced particularly curious issue in Sprockets. We noticed that the bug broke the critical asset precompilation rake task, but only if the name of the project folder was changed between successive task executions. While project folder renames might seem relatively uncommon, they happen frequently on Heroku because each build happens in a unique directory name. &lt;/p&gt;

&lt;p&gt;While this bug itself is interesting, what‚Äôs even more interesting is learning from our debugging process. You can learn about the tools and steps we use to narrow down the root cause, and ultimately fix the bug. &lt;/p&gt;

&lt;p&gt;If you‚Äôd like to watch the full debugging session, check out the video or just follow along by reading the text below. We‚Äôll walk through a debug workflow and find the root cause of this bug.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;500px&quot; src=&quot;https://www.youtube-nocookie.com/embed/ZEoF_OWpXZY&quot; style=&quot;padding-bottom: 30px;&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h1 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;a-guide-to-debugging-in-ruby&quot; href=&quot;#a-guide-to-debugging-in-ruby&quot;&gt;A Guide to Debugging in Ruby&lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;Head-scratching, non-obvious bugs are worth investigating because they may lead to other unnoticed or unreported bugs.&lt;/p&gt;

&lt;p&gt;Thankfully, Ruby comes with some powerful debugging tools that are easy to use for beginners. For a nice overview, check out this &lt;a href=&quot;https://www.rubyguides.com/2015/07/ruby-debugging/&quot;&gt;Ruby debugging guide&lt;/a&gt; that covers basics like the difference between &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;puts&lt;/code&gt; and also discusses a few of the interactive debuggers that are available in the Ruby ecosystem. For the rest of this post, however, you won‚Äôt need to know anything more advanced than &lt;code&gt;puts&lt;/code&gt;.&lt;/p&gt;
&lt;h1 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;reproducing-the-bug&quot; href=&quot;#reproducing-the-bug&quot;&gt;Reproducing the Bug&lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;The best way to learn debugging is just to dive in and try it. Let‚Äôs set up Sprockets in a local environment.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;clone-codetriage&quot; href=&quot;#clone-codetriage&quot;&gt;Clone CodeTriage&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;We need a Rails app to reproduce this bug so we‚Äôll use an open source example. I am the creator of CodeTriage so it‚Äôs natural to use that application to demonstrate the problem, although you can reproduce it with any Rails app that uses Sprockets. CodeTriage has helped developers triage issues for &lt;a href=&quot;https://www.codetriage.com/what&quot;&gt;thousands of open-source projects&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, clone the CodeTriage repository, install dependencies, then switch to a branch that contains the code we need to reproduce the bug. A working Ruby environment is assumed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ git clone git@github.com:codetriage/codetriage
$ cd codetriage

$ gem install bundler
$ bundle install

$ cp config/database.example.yml config/database.yml
$ git checkout 52d57d13
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;compile-the-assets-with-rake&quot; href=&quot;#compile-the-assets-with-rake&quot;&gt;Compile the Assets with Rake&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Next, execute the following steps to make the bug show up in our local environment.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ rm -rf tmp/cache
$ rm -rf public/assets
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, run the rake task for precompiling assets, which should succeed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ RAILS_ENV=production RAILS_SERVE_STATIC_FILES=1 RAILS_LOG_TO_STDOUT=1 bin/rake assets:precompile
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;rename-the-project-folder&quot; href=&quot;#rename-the-project-folder&quot;&gt;Rename the Project Folder&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Now, change the name of the project directory by copying its files into a new directory called &lt;code&gt;codetriage-after&lt;/code&gt; and deleting the old &lt;code&gt;codetriage&lt;/code&gt; directory.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ cd ..
$ cp -r codetriage codetriage-after
$ rm -rf codetriage
$ cd codetriage-after
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One more time, run the &lt;code&gt;assets:precompile&lt;/code&gt; rake task:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ RAILS_ENV=production RAILS_SERVE_STATIC_FILES=1 RAILS_LOG_TO_STDOUT=1 bin/rake assets:precompile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The task should fail this time and produce the following error message:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1551205073-Screen%20Shot%202019-02-26%20at%2010.17.16%20AM.png&quot; alt=&quot;Screen Shot 2019-02-26 at 10&quot;&gt;&lt;/p&gt;

&lt;p&gt;Sprockets is complaining that it can‚Äôt find the file &lt;code&gt;/private/tmp/repro/codetriage/app/assets/javascripts/application.js.erb&lt;/code&gt;. &lt;/p&gt;

&lt;p&gt;This actually makes sense because in the last step we changed &lt;code&gt;codetriage&lt;/code&gt; to &lt;code&gt;codetriage-after&lt;/code&gt; as our project folder name, yet it is looking in &lt;code&gt;codetriage&lt;/code&gt;. &lt;/p&gt;

&lt;p&gt;(Note that the &lt;code&gt;/private/tmp/repro&lt;/code&gt; part of the path may be different for you based on where you cloned the &lt;code&gt;codetriage&lt;/code&gt; repository.)&lt;/p&gt;
&lt;h1 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;finding-the-root-cause-of-the-bug&quot; href=&quot;#finding-the-root-cause-of-the-bug&quot;&gt;Finding the Root Cause of the Bug&lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;Now that we have reproduced the bug in the video, the next step is to jump into the code of the Sprockets dependency at one of the lines in the stack trace in a method called &lt;code&gt;fetch_asset_from_dependency_cache&lt;/code&gt;. Your application depends on reading code in the libraries, which is required when debugging, especially once you have ruled out any issues with the code you‚Äôve written.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;read-gem-code-with-bundle-open&quot; href=&quot;#read-gem-code-with-bundle-open&quot;&gt;Read gem code with bundle open&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Ruby‚Äôs de-facto gem manager &lt;a href=&quot;https://bundler.io/&quot;&gt;Bundler&lt;/a&gt; contains a helpful command called &lt;code&gt;bundle open&lt;/code&gt; that opens the source code of a gem in your favorite editor. Run it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ bundle open sprockets
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As long as you have a &lt;code&gt;$EDITOR&lt;/code&gt; or &lt;code&gt;$BUNDLER_EDITOR&lt;/code&gt; environment variable set, your preferred code editor will open to the project directory of the specified gem.&lt;/p&gt;

&lt;p&gt;Now you can browse the gem source code and even modify it, adding print statements to see the value of variables or trying out various fixes to see if they work.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;how-sprockets-caches-files&quot; href=&quot;#how-sprockets-caches-files&quot;&gt;How Sprockets Caches Files&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;The error message above implied that the wrong value is being stored in the Sprockets cache, so the next step is to look at the cache to confirm. The cache is stored on disk across many files, so first we need to find the specific file that contains the record we want to inspect. The key to that record is a digest of the Sprockets cache ID. That‚Äôs the value we‚Äôll try to find in the files.&lt;/p&gt;

&lt;p&gt;Once you have the Sprockets code open, navigate to &lt;code&gt;lib/sprockets/loader.rb&lt;/code&gt;, where you‚Äôll find the method &lt;code&gt;fetch_asset_from_dependency_cache&lt;/code&gt; toward the end. The documentation for this method provides insight into how Sprockets uses the idea of pipelines, histories, and dependencies to aid in caching. To get more of the backstory, I recommend watching the video starting from about the six-minute mark. &lt;/p&gt;

&lt;p&gt;We examined the on-disk contents of the Sprockets cache, looking for the ID cache key of a specific object in the Sprockets cache.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ grep -R 5d0abb0a8654a1f03d6b27 tmp/cache
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a helpful debugging command to file away for later. &lt;code&gt;grep -R&lt;/code&gt; searches through the &lt;code&gt;tmp/cache&lt;/code&gt; directory looking for any files that contain the string ‚Äú5d0abb0a8654a1f03d6b27‚Äù, which is a Sprockets cache key. The -R flag is what makes it traverse directories recursively.&lt;/p&gt;

&lt;p&gt;In our case, the grep command does produce a cache file and we can use &lt;code&gt;cat&lt;/code&gt; to view the contents. Inside of that cache file, we find something unexpected: an absolute path to an asset. Sprockets should only cache relative paths, not absolute paths. Since we changed the absolute path to our project directory to create this bug, it‚Äôs quite likely that this is the culprit.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;loading-up-irb&quot; href=&quot;#loading-up-irb&quot;&gt;Loading Up IRB&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;To investigate further and confirm this suspicion, we fire up IRB, the interactive Ruby debugger. If you‚Äôre new to Ruby or the IRB, we recommend &lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-use-irb-to-explore-ruby&quot;&gt;How To Use IRB to Explore Ruby&lt;/a&gt; as a good way to see how to use it. It‚Äôs simple but powerful and is a must-have in your Ruby debugging toolkit.&lt;/p&gt;

&lt;p&gt;We then use IRB to inspect the file cache from Sprockets point of view.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ irb
reirb(main):001:0&amp;gt; require ‚Äòsprockets‚Äô
reirb(main):001:0&amp;gt; Sprockets::Environment.new.cache
reirb(main):001:0&amp;gt; Sprockets::Environment.new.cache.get(‚Äú5d0abb0a8654a1f03d6b27‚Äù)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately, this does not work because the cache key is not the same as the cache ID. So, we move on to confirming this hypothesis in another way. We still include this example here to let you know that IRB is something you can use for any Ruby code and specifically with the handy Environment class in Sprockets.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;fixing-to_load-and-to_link&quot; href=&quot;#fixing-to_load-and-to_link&quot;&gt;Fixing to_load and to_link&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;To fix the bug, let‚Äôs modify the &lt;code&gt;to_load&lt;/code&gt; and &lt;code&gt;to_link&lt;/code&gt; methods in &lt;code&gt;loader.rb&lt;/code&gt; to force relative paths for objects going into the cache and coming out, using the &lt;code&gt;compress_from_root&lt;/code&gt; and &lt;code&gt;expand_from_root&lt;/code&gt; utility methods from Sprockets &lt;code&gt;base.rb&lt;/code&gt;. This ensures that absolute paths won‚Äôt make their way into the cache again, and consequently, that renaming the project directory won‚Äôt cause any issues in subsequent asset compilations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;if cached_asset[:metadata][:to_load] &amp;amp;&amp;amp; !cached_asset[:metadata][:to_load].empty?
  cached_asset[:metadata][:to_load] = cached_asset[:metadata][:to_load].dup
  cached_asset[:metadata][:to_load].map! { |uri| compress_from_root(uri) }
end

 if cached_asset[:metadata][:to_link] &amp;amp;&amp;amp; !cached_asset[:metadata][:to_link].empty?
  cached_asset[:metadata][:to_link] = cached_asset[:metadata][:to_link].dup
  cached_asset[:metadata][:to_link].map! { |uri| compress_from_root(uri) }
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our &lt;a href=&quot;https://github.com/rails/sprockets/pull/547/files&quot;&gt;pull request to fix the bug&lt;/a&gt; contains a test to prove that the fix works. Writing tests for your bug fixes is a best practice that you should always strive to follow. It‚Äôs the best way to prevent old bugs from crawling back into your codebase.&lt;/p&gt;
&lt;h1 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;wrap-up&quot; href=&quot;#wrap-up&quot;&gt;Wrap-up&lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;Inevitably, your code will do something that couldn‚Äôt possibly happen. That‚Äôs when you need to get out your debugging tools. We hope that you have picked up a few new ones from this post.&lt;/p&gt;

&lt;p&gt;Your code will do something that couldn‚Äôt possibly happen in production. If your app runs on Heroku, make sure to familiarize yourself with the variety of &lt;a href=&quot;https://devcenter.heroku.com/articles/logging&quot;&gt;logging solutions available&lt;/a&gt; as add-ons. These add-ons will make running and debugging problems on Heroku easier and they only take seconds to set up.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Cache Invalidation Complexity: Rails 5.2 and Dalli Cache Store</title>
      <link>https://blog.heroku.com/cache-invalidation-rails-5-2-dalli-store</link>
      <pubDate>Tue, 16 Oct 2018 16:43:00 GMT</pubDate>
      <guid>https://blog.heroku.com/cache-invalidation-rails-5-2-dalli-store</guid>
      <description>&lt;p&gt;Rails applications that use ActiveRecord objects in their cache may experience an issue where the entries cannot be invalidated if all of these conditions are true:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;They are using Rails 5.2+&lt;/li&gt;
&lt;li&gt;They have configured &lt;code&gt;config.active_record.cache_versioning = true&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;They are using a cache that is not maintained by Rails, such as &lt;code&gt;dalli_store&lt;/code&gt; (2.7.8 or prior)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this post, we discuss the background to a change in the way that cache keys work with Rails, why this change introduced an API incompatibility with 3rd party cache stores, and finally how you can find out if your app is at risk and how to fix it.&lt;/p&gt;

&lt;p&gt;Even if you're not at Rails 5.2 yet, you'll likely get there one day. It's important to read and potentially mitigate this issue before you run into it in production.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;background-what-are-recyclable-cache-keys&quot; href=&quot;#background-what-are-recyclable-cache-keys&quot;&gt;Background: What are Recyclable Cache keys?&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;One of the &lt;a href=&quot;https://weblog.rubyonrails.org/2018/1/31/Rails-5-2-RC1-Active-Storage-Redis-Cache-Store-HTTP2-Early-Hints-Credentials/%60&quot;&gt;hallmark features of Rails 5.2 was &quot;recyclable&quot; cache keys&lt;/a&gt;. What does that mean and why do you want them? If you're caching a view partial that has an Active Record object when the object changes then you want the cache to invalidate and be replaced with the new information.&lt;/p&gt;

&lt;p&gt;The old way that Rails accomplished cache invalidation is to put version information directly into the cache key. For an Active Record object this means a formatted string of &lt;code&gt;:updated_at&lt;/code&gt;. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# config.active_record.cache_versioning = false
user = User.first
user.name = &quot;richard&quot;
user.save
user.cache_key
# =&amp;gt; &quot;users/1-&amp;lt;version-1&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This scheme is quite robust. When the object changes so does the key:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# config.active_record.cache_versioning = false
user = User.first
user.name = &quot;schneems&quot;
user.save
user.cache_key
# =&amp;gt; &quot;users/1-&amp;lt;version-2&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, this causes unnecessary cache invalidations. For example, let's say that you have three objects and three slots in your cache. Letters A, B, and C differentiate the objects, while a number indicates their versions, these are all version one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[A(1), B(1), C(1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When object A changes, it doesn't evict the cache for object A, instead, it evicts the last cache entry which is C. Now the cache looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[A(2), A(1), B(1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next time that C is requested it won't be found, and it will be re-calculated and get added to the front of the cache. This addition pushes out the copy of B:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[C(1), A(2), A(1)] 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the next time that B is requested it won't be found, and it will be re-calculated and get added to the front of the cache:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[B(1), C(1), A(2)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While we only made one change to object A, it resulted in clearing and resetting the values for both B(1) and C(1) even though they never changed. This method of cache invalidation adds unnecessary time spent recalculating already valid cache entries. Cache versioning's goal is to fix this unneeded cache invalidation.&lt;/p&gt;
&lt;h3 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;cache-invalidation-with-cache-versioning-recyclable-cache-keys&quot; href=&quot;#cache-invalidation-with-cache-versioning-recyclable-cache-keys&quot;&gt;Cache invalidation with cache versioning (recyclable cache keys)&lt;/a&gt;
&lt;/h3&gt;

&lt;p&gt;With the new method of cache versioning, the keys stay consistent, but the &lt;code&gt;cache_version&lt;/code&gt; is stored inside the cache entry and manually checked when pulling an entry from the cache.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# config.active_record.cache_versioning = true
user = User.first
user.name = &quot;richard&quot;
user.save
user.cache_key
# =&amp;gt; &quot;users/1&quot;

user.cache_version
# =&amp;gt; &quot;&amp;lt;version-1&amp;gt;&quot;

user.name = &quot;schneems&quot;
user.save
user.cache_key
# =&amp;gt; &quot;users/1&quot;

user.cache_version
# =&amp;gt; &quot;&amp;lt;version-2&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here's an example of how the cache works with cache versioning:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[A(1), B(1), C(1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When object A changes, to version two, it will pull the A(1) object from the cache, see that it has a different version, and replace the entry in the same slot using a consistent cache key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[A(2), B(1), C(1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now future calls to retrieve the A object will show that the version is correct, and the cached value can be used.&lt;/p&gt;

&lt;p&gt;With this new scheme, changing one object does not have a cascade effect on other cached values. In this way, we're able to keep valid items in our cache longer and do less work.&lt;/p&gt;

&lt;p&gt;How well does it work? DHH at Basecamp had this to say:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We went from only being able to keep 18 hours of caching to, I believe, 3 weeks. It was the single biggest performance boost that Basecamp 3 has ever seen.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;By enabling recyclable cache key versioning (&lt;code&gt;config.active_record.cache_versioning = true&lt;/code&gt;), instead of having to recalculate every cache entry every 18 hours effectively, the churn spread out over 3 weeks, which is very impressive.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;what-39-s-the-issue&quot; href=&quot;#what-39-s-the-issue&quot;&gt;What's the issue?&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Now that you know what recyclable cache keys are and how Rails implements them you should know that the client that talks to the cache provider needs to be aware of this new scheme. Rails &lt;a href=&quot;https://guides.rubyonrails.org/caching_with_rails.html#activesupport-cache-store&quot;&gt;ships with a few cache stores&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;:memory_store&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:file_store&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:mem_cache_store&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:redis_cache_store&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you're using one of these stores then you get a cache client that supports this feature flag. However, you can also provide a custom cache store and other gems ship with a store. Most notably:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;code&gt;:dalli_store&lt;/code&gt; (not maintained by Rails)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you're using a custom cache store then it's up to that library to implement this new scheme.&lt;/p&gt;

&lt;p&gt;If you're using &lt;code&gt;:dalli_store&lt;/code&gt; right now and have &lt;code&gt;config.active_record.cache_versioning = true&lt;/code&gt; then you are quietly running in production without the ability to invalidate caches. For example, you can see &lt;a href=&quot;https://www.codetriage.com&quot;&gt;CodeTriage, an app that helps people contribute to Open Source&lt;/a&gt; not change the view when the underlying database entry is modified:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1539677220-dalli_store_rails_codetriage_demo.gif&quot; alt=&quot;dalli_store_rails_codetriage_demo&quot;&gt;&lt;/p&gt;

&lt;p&gt;Why is this happening? Remember how we showed that the cache key is the same no matter if the model changes? The Dalli gem (as of version 2.7.8) only understands the &lt;code&gt;cache_key&lt;/code&gt;, but does not understand how to insert and use cache versions. When using the &lt;code&gt;:dalli_store&lt;/code&gt; and you've enabled recyclable cache keys then the &lt;code&gt;cache_key&lt;/code&gt; doesn't change and it will always grab the same value from the cache.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;how-to-detect-if-you-39-re-affected&quot; href=&quot;#how-to-detect-if-you-39-re-affected&quot;&gt;How to detect if you're affected&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;First confirm what cache store you're using, make sure to run this in a production env otherwise you might be using a different cache store for different environments:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;puts Rails.application.config.cache_store.inspect
# =&amp;gt; :dalli_store
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If it's not on the above &lt;a href=&quot;https://guides.rubyonrails.org/caching_with_rails.html#cache-stores&quot;&gt;list of officially supported Rails cache backends&lt;/a&gt; then you might be affected. Next, inspect your cache versioning:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;puts Rails.application.config.active_record.cache_versioning
# =&amp;gt; true # truthy values along with `:dalli_store` cause this issue
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;how-to-mitigate&quot; href=&quot;#how-to-mitigate&quot;&gt;How to mitigate&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;There are several options, each with their own trade-off.&lt;/p&gt;
&lt;h3 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;switch-from-dalli-to-code-mem_cache_store-code&quot; href=&quot;#switch-from-dalli-to-code-mem_cache_store-code&quot;&gt;Switch from dalli to &lt;code&gt;:mem_cache_store&lt;/code&gt;&lt;/a&gt;
&lt;/h3&gt;

&lt;p&gt;You can switch away from the &lt;code&gt;:dalli_store&lt;/code&gt; and instead use the official &lt;code&gt;:mem_cache_store&lt;/code&gt; that ships with Rails:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;config.cache_store = :mem_cache_store
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: This cache store still uses the &lt;code&gt;dalli&lt;/code&gt; gem for communicating with your memcache server.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you were previously passing in arguments it looks like you can just change the store name, for instance if you're using the memcachier service it might look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;config.cache_store = [:mem_cache_store, (ENV[&quot;MEMCACHIER_SERVERS&quot;] || &quot;&quot;).split(&quot;,&quot;),
                      { :username =&amp;gt; ENV[&quot;MEMCACHIER_USERNAME&quot;],
                        :password =&amp;gt; ENV[&quot;MEMCACHIER_PASSWORD&quot;],
                        :failover =&amp;gt; true,
                        :socket_timeout =&amp;gt; 1.5,
                        :socket_failure_delay =&amp;gt; 0.2 }]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This was &lt;a href=&quot;https://github.com/codetriage/codetriage/pull/786&quot;&gt;tested on CodeTriage&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt; With this store you get cache key recycling, you also get cache compression which helps significantly with time transferring bytes over a network to your memcache service. To achieve these features this cache store does more work than the raw &lt;code&gt;:dalli_store&lt;/code&gt;, in preliminary benchmarks on &lt;a href=&quot;https://www.codetriage.com&quot;&gt;CodeTriage&lt;/a&gt; while connecting to an external memcache server the performance is roughly equivalent (within 1% of original performance). With the decreased space from compression and the extra time that cache keys can &quot;live&quot; before being evicted with key recycling, this makes this store a net positive.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt; The cache keys for &lt;code&gt;:mem_cache_store&lt;/code&gt; are identical to the ones generated via &lt;code&gt;:dalli_store&lt;/code&gt;, however it does not have the version information stored in the cache entry yet. When &lt;code&gt;:mem_cache_store&lt;/code&gt; sees this it falls back to the old behavior of not validating the &quot;freshness&quot; of the entry. This means in order to get the updated behavior where changing an Active Record object actually updates the database you'll need to invalidate old entries. The &quot;easiest&quot; way to do this is to is to flush the whole cache. The problem with this is that will significantly slow your service as your entire application is then functioning with a cold cache.&lt;/p&gt;
&lt;h3 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;disable-recyclable-cache-keys-cache-versioning&quot; href=&quot;#disable-recyclable-cache-keys-cache-versioning&quot;&gt;Disable recyclable cache keys (cache versioning)&lt;/a&gt;
&lt;/h3&gt;

&lt;p&gt;If you don't want to replace your cache store, disabling the cache versioning feature will also fix the issue of changing Active Record objects not invalidating the cache. You can disable this feature like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;config.active_record.cache_versioning = false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you're wondering about the config naming as I was it's &lt;code&gt;cache_versioning&lt;/code&gt; because the version of the object lives in the cache rather than in the key. It's effectively the same thing as enabling or disabling recyclable caching.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt; You don't have to switch your cache store. Doesn't require a cache flush (but will instead manually invalidate keys automatically due to changing cache key format). You can use this information to slowly roll out the cache key changes if you're able to do blue/green deploys and roll out to a percentage of your fleet. You'll still get some instances operating under a cold cache but by the time 100% of instances are running with the new version then the cache should be fairly &quot;warm&quot;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt; You won't have to flush your old cache, BUT the cache key format will change which effectively does the same thing. When you change this config then your whole app will not be able to use any cache keys from before and will effectively be working with a cold cache while you're re-building old keys. You do not get recyclable keys. You do not get cache compression. Disabling the cache versioning will also mean that dalli must do more work to build cache keys which actually makes caching go slightly slower.&lt;/p&gt;

&lt;p&gt;Overall I would recommend switching to &lt;code&gt;:mem_cache_store&lt;/code&gt; and then flushing the cache.&lt;/p&gt;
&lt;h3 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;upgrade-dalli-to-2-7-9&quot; href=&quot;#upgrade-dalli-to-2-7-9&quot;&gt;Upgrade Dalli to 2.7.9+&lt;/a&gt;
&lt;/h3&gt;

&lt;p&gt;Dalli version &lt;a href=&quot;https://github.com/petergoldstein/dalli/pull/698&quot;&gt;2.7.9 has introduced a bugfix for the above problem&lt;/a&gt;. Upgrading to version 2.7.9 of this gem will mitigate the issue, however it will not use &quot;recyclable cache keys&quot; as described above, and instead manually inserts the version into the cache key rather than relying on only the &lt;code&gt;cache_key&lt;/code&gt; method. To use memcache with &quot;recyclable cache keys&quot;, you'll need to use the &lt;code&gt;:mem_cache_store&lt;/code&gt;.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;next-steps&quot; href=&quot;#next-steps&quot;&gt;Next steps&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;At Heroku we've taken efforts to update all of our documentation to suggest using &lt;code&gt;:mem_cache_store&lt;/code&gt; instead of directly using dalli. That being said there are still a ton of &lt;a href=&quot;https://duckduckgo.com/?q=%22%3Adalli_store%22&amp;amp;t=h_&amp;amp;ia=software&quot;&gt;historical references to using the older store&lt;/a&gt; if you see one in the wild please make a comment and point at this post.&lt;/p&gt;

&lt;p&gt;Since the issue is deeper than the &lt;code&gt;:dalli_store&lt;/code&gt;, it potentially affects any custom cache we need a way to systematically let people know when they're at risk for running in a bad configuration.&lt;/p&gt;

&lt;p&gt;My proposal is to add a predicate method to all maintained and supported Rails cache stores for example &lt;code&gt;ActiveStorage::Cache::MemCacheStore.supports_in_cache_versioning?&lt;/code&gt; (method name TBD). If the app specifies &lt;code&gt;config.active_record.cache_versioning = true&lt;/code&gt; without using a cache that responds affirmatively to &lt;code&gt;supports_in_cache_versioning?&lt;/code&gt; then we can raise a helpful error that explains the issue.&lt;/p&gt;

&lt;p&gt;There's also work being done on &lt;a href=&quot;https://github.com/petergoldstein/dalli&quot;&gt;dalli&lt;/a&gt; both for adding a limited form of support and for adding documentation.&lt;/p&gt;

&lt;p&gt;As it is said there are two truly hard problems in computer science: cache invalidation, naming, and off by one errors. While this incompatibility is unfortunate it's hard to make a breaking API change that fully anticipates how all external consumers will work. I've spent a lot of time in the Rails contributor chat talking with &lt;a href=&quot;https://github.com/dhh&quot;&gt;DHH&lt;/a&gt; and &lt;a href=&quot;https://github.com/rafaelfranca&quot;&gt;Rafael&lt;/a&gt; and there's a &lt;a href=&quot;https://github.com/rails/rails/pull/33835&quot;&gt;really good thread of some of the perils of changes that touch cache keys in one of my performance PRs&lt;/a&gt;. We realize the sensitive nature of changes anywhere near caching. In addition to bringing more scrutiny and awareness to these types of changes, we're working towards making more concrete policies.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Rails Asset Pipeline Directory Traversal Vulnerability (CVE-2018-3760)</title>
      <link>https://blog.heroku.com/rails-asset-pipeline-vulnerability</link>
      <pubDate>Tue, 19 Jun 2018 16:01:00 GMT</pubDate>
      <guid>https://blog.heroku.com/rails-asset-pipeline-vulnerability</guid>
      <description>&lt;p&gt;All previously released versions of &lt;a href=&quot;https://github.com/rails/sprockets&quot;&gt;Sprockets&lt;/a&gt;, the software that powers the &lt;a href=&quot;https://github.com/rails/rails&quot;&gt;Rails&lt;/a&gt; asset pipeline, contain a &lt;a href=&quot;https://en.wikipedia.org/wiki/Directory_traversal_attack&quot;&gt;directory traversal vulnerability&lt;/a&gt;. This vulnerability has been assigned &lt;a href=&quot;https://groups.google.com/forum/#!topic/rubyonrails-security/ft_J--l55fM&quot;&gt;CVE-2018-3760&lt;/a&gt;.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;how-do-i-know-if-i-39-m-affected&quot; href=&quot;#how-do-i-know-if-i-39-m-affected&quot;&gt;How do I know if I'm affected?&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Rails applications are vulnerable if they have this setting enabled in their application:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;ruby&quot;&gt;# config/environments/production.rb
config.assets.compile = true # setting to true makes your app vulnerable
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: The default value of this setting that ships with Rails in &lt;code&gt;production.rb&lt;/code&gt; is &lt;code&gt;false&lt;/code&gt;. By default, Rails apps running in production mode are not vulnerable to this exploit.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;how-do-i-fix-it&quot; href=&quot;#how-do-i-fix-it&quot;&gt;How do I fix it?&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;To remediate this vulnerability, applications can either change the setting above to &lt;code&gt;false&lt;/code&gt; or upgrade to the latest version of Sprockets. Heroku highly recommends upgrading to the latest patch release for your version of Sprockets by running &lt;code&gt;bundle update sprockets&lt;/code&gt;. Make sure that the update puts your application on one of these Sprockets versions (or newer):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2.12.5&lt;/li&gt;
&lt;li&gt;3.7.2&lt;/li&gt;
&lt;li&gt;4.0.0.beta8&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are &lt;a href=&quot;https://devcenter.heroku.com/changelog-items/1437#upgrade-sprockets&quot;&gt;instructions on how to upgrade your Sprockets version&lt;/a&gt;.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;what-was-at-risk&quot; href=&quot;#what-was-at-risk&quot;&gt;What was at risk?&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;If your application was targeted using this exploit and you had the &lt;code&gt;assets.compile&lt;/code&gt; setting enabled on your production app, it's possible that secrets contained in your repo or your environment variables have been compromised. As a precaution, you may wish to rotate your database credentials, along with any other credentials stored in your application code or environment.&lt;/p&gt;
&lt;h3 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;instructions-for-rotating-heroku-data-services-are-below&quot; href=&quot;#instructions-for-rotating-heroku-data-services-are-below&quot;&gt;Instructions for rotating Heroku data services are below:&lt;/a&gt;
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://devcenter.heroku.com/articles/heroku-postgresql#pg-credentials&quot;&gt;Rotate Heroku Postgres credentials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://devcenter.heroku.com/articles/heroku-redis#redis-credentials&quot;&gt;Rotate Heroku Redis credentials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://devcenter.heroku.com/articles/kafka-on-heroku#rotating-credentials&quot;&gt;Rotate Heroku Kafka credentials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For other add-on partners, please check &lt;a href=&quot;https://devcenter.heroku.com/categories/add-on-documentation&quot;&gt;the specific add-on documentation&lt;/a&gt; for instructions on how to rotate credentials.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;how-does-the-exploit-work&quot; href=&quot;#how-does-the-exploit-work&quot;&gt;How does the exploit work?&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Released in &lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-3760&quot;&gt;CVE-2018-3760&lt;/a&gt;, the directory traversal vulnerability was discovered by &lt;a href=&quot;https://twitter.com/orange_8361&quot;&gt;Orange Tsai&lt;/a&gt;  from DEVCORE. To exploit the traversal, the app needs to have their assets set to compile at runtime.&lt;/p&gt;

&lt;p&gt;When runtime compilation is enabled, a Sprockets server will dynamically check to see if an asset is rendered or not when it receives a request. If Sprockets can't find an already rendered asset, it will try to find and compile an asset that matches the request. The search process is very slow, and as such it is not a recommended best practice for this type of asset management software.&lt;/p&gt;

&lt;p&gt;The directory traversal vulnerability exploits this search process to fool the Sprockets server. Using a known absolute path to the directory of a source asset, an attacker can craft a URL that convinces Sprockets it is rendering an asset inside one of its permitted paths.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;heroku-39-s-involvement&quot; href=&quot;#heroku-39-s-involvement&quot;&gt;Heroku's Involvement&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;The Sprockets issue was reported via the Rails security bug tracker on &lt;a href=&quot;https://hackerone.com/rails&quot;&gt;HackerOne&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/orange_8361&quot;&gt;Orange Tsai&lt;/a&gt;. It was passed to the Rails maintainers, who forwarded the issue to Richard Schneeman, the current Sprockets maintainer; Richard Schneeman is also a Heroku employee, and the author of this post.&lt;/p&gt;

&lt;p&gt;A patch was prepared for all three of the currently supported Sprockets versions: 2, 3, and 4. The patches were reviewed privately by the vulnerability reporter and other Rails core members. When the threat was determined to be sufficiently mitigated, a CVE was drafted, and there was a coordinated release of the CVE and the security patches.&lt;/p&gt;

&lt;p&gt;When a severe security release that affects customers is announced, the CVE is passed to the Heroku security team and the vulnerability is given a score. Based on that score, the rest of the company determines what steps to take to best protect our customers. At the time of the CVE release, the knowledge of the security vulnerability by a Sprockets core member allowed us to quickly give it a score, and immediately begin developing a plan to communicate mitigation instructions to customers.&lt;/p&gt;

&lt;p&gt;On June 19, we took the following actions to help ensure that customers likely to be affected were notified of the issue:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Updated the &lt;a href=&quot;https://github.com/heroku/heroku-buildpack-ruby&quot;&gt;Ruby Buildpack&lt;/a&gt; to &lt;a href=&quot;https://github.com/heroku/heroku-buildpack-ruby/pull/776&quot;&gt;fail builds for applications with runtime asset compilation enabled&lt;/a&gt; that are running an affected version of &lt;a href=&quot;https://github.com/rails/sprockets&quot;&gt;Sprockets&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Contacted customers we know to be running Ruby on Rails applications that depend on Sprockets, as determined by our internal dependency tracking tool. Note that this tool may not always generate a complete list of affected applications; even if you did not receive an email, we urge you to carefully check your own dependencies to determine if you are affected.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you cannot upgrade an affected Ruby application at this time but need to deploy, it is possible to &lt;a href=&quot;https://devcenter.heroku.com/changelog-items/1437#regain-deploy-ability-on-a-vulnerable-application&quot;&gt;regain deploy ability&lt;/a&gt;. This method is not recommended as it allows you to continue deploying a vulnerable application. &lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;accolades&quot; href=&quot;#accolades&quot;&gt;Accolades&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Thank you to the original reporter of the security issue Orange Tsai, for patiently working with the core team while a release was coordinated. A safe and responsible disclosure ensured that our customers, and the entire Rails ecosystem, were not caught unaware without a security patch.&lt;/p&gt;

&lt;p&gt;Thank you to the Rails security team for their tireless efforts reproducing and patching security bugs. This is difficult work that goes largely unseen by the general public, and as a result earns far too little appreciation.&lt;/p&gt;

&lt;p&gt;Thank you to Heroku‚Äôs security team, for responding to the threat swiftly to protect our customers.&lt;/p&gt;

&lt;p&gt;Finally, a thank you in advance to all of our affected customers, for upgrading their Sprockets versions and rotating their credentials.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Rails 5.2 Active Storage: Previews, Poppler, and Solving Licensing Pitfalls</title>
      <link>https://blog.heroku.com/rails-active-storage</link>
      <pubDate>Thu, 10 May 2018 15:58:00 GMT</pubDate>
      <guid>https://blog.heroku.com/rails-active-storage</guid>
      <description>&lt;p&gt;Rails 5.2 was just released last month with a major new feature: Active Storage. Active Storage provides file uploads and attachments for Active Record models with a variety of backing services (like AWS S3). While libraries like &lt;a href=&quot;https://github.com/thoughtbot/paperclip&quot;&gt;Paperclip&lt;/a&gt; exist to do similar work, this is the first time that such a feature has been shipped with Rails. At Heroku, we consider cloud storage a best practice, so we've ensured that it works on our platform. In this post, we'll share how we prepared for the release of Rails 5.2, and how you can deploy an app today using the new Active Storage functionality.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;trust-but-verify&quot; href=&quot;#trust-but-verify&quot;&gt;Trust but Verify&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;At Heroku, trust is our number one value. When we learned that Active Storage was shipping with Rails 5.2, we began experimenting with all its features. One of the nicest conveniences of Active Storage is its ability to preview PDFs and videos. Instead of linking to assets via text, a small screenshot of the PDF or Video will be extracted from the file and rendered on the page.&lt;/p&gt;

&lt;p&gt;The beta version of Rails 5.2 used the popular open source tools FFmpeg and MuPDF to generate video and PDF previews. We vetted these new binary dependencies through both our security and legal departments, where we found that MuPDF licensed under AGPL and requires a commercial license for some use. Had we simply added MuPDF to Rails 5.2+ applications by default, many of our customers would have been unaware that they needed to purchase MuPDF to use it commercially.&lt;/p&gt;

&lt;p&gt;The limiting AGPL license was brought to &lt;a href=&quot;https://github.com/rails/rails/pull/30667#issuecomment-332276198&quot;&gt;public attention&lt;/a&gt; in September 2017. To prepare for the 5.2 release, our engineer &lt;a href=&quot;https://twitter.com/hone02&quot;&gt;Terence Lee&lt;/a&gt; worked to update Active Storage so that this PDF preview feature could also use an open-source backend without a commercial license. We opened a PR to Rails &lt;a href=&quot;https://github.com/rails/rails/pull/31906&quot;&gt;introducing the ability to use poppler PDF as an alternative to MuPDF&lt;/a&gt; in February of 2018. The PR was merged roughly a month later, and now any Rails 5.2 user - on or off Heroku - can render PDF previews without having to purchase a commercial license.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;active-storage-on-heroku-example-app&quot; href=&quot;#active-storage-on-heroku-example-app&quot;&gt;Active Storage on Heroku Example App&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;If you've already got an app that implements Active Storage you can &lt;a href=&quot;https://devcenter.heroku.com/articles/active-storage-on-heroku?preview=1&quot;&gt;jump over to our DevCenter documentation on Active Storage&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Alternatively, you can use our example app. Here is a Rails 5.2 app that is a digital bulletin board allowing people to post videos, pdfs, and images. You can &lt;a href=&quot;https://github.com/heroku/active_storage_with_previews_example&quot;&gt;view the source on GitHub&lt;/a&gt; or deploy the app with the Heroku button:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://heroku.com/deploy?template=https://github.com/heroku/active_storage_with_previews_example&quot;&gt;
  &lt;img src=&quot;https://www.herokucdn.com/deploy/button.svg&quot; alt=&quot;Deploy&quot;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: This example app requires a paid S3 add-on.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here's a video example of what the app does.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1526664857-active-storage.gif&quot; alt=&quot;active-storage&quot;&gt;&lt;/p&gt;

&lt;p&gt;When you open the home page, select an appropriate asset, and then submit the form. In the video, the &lt;code&gt;mp4&lt;/code&gt; file is uploaded to S3 and then a preview is generated on the fly by Rails with the help of &lt;code&gt;ffmpeg&lt;/code&gt;. Pretty neat.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;active-storage-on-heroku&quot; href=&quot;#active-storage-on-heroku&quot;&gt;Active Storage on Heroku&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;If you deployed the example app using the button, it's already configured to work on Heroku via the &lt;code&gt;app.json&lt;/code&gt;, however if you've got your own app that you would like to deploy, how do you set it up so it works on Heroku?&lt;/p&gt;

&lt;p&gt;Following the &lt;a href=&quot;https://devcenter.heroku.com/articles/active-storage-on-heroku?preview=1&quot;&gt;DevCenter documentation for Active Storage&lt;/a&gt;, you will need a file storage service that all your dynos can talk to. The example uses a Heroku add-on for S3 called &lt;a href=&quot;https://elements.heroku.com/addons/bucketeer&quot;&gt;Bucketeer&lt;/a&gt;, though you can also use existing S3 credentials.&lt;/p&gt;

&lt;p&gt;To get started, add the AWS gem for S3 to the Gemfile, and if you‚Äôre modifying images as well add Mini Magick:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;gem &quot;aws-sdk-s3&quot;, require: false
gem 'mini_magick', '~&amp;gt; 4.8'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Don't forget to &lt;code&gt;$ bundle install&lt;/code&gt; after updating your Gemfile.&lt;/p&gt;

&lt;p&gt;Next up, add an &lt;code&gt;amazon&lt;/code&gt; option in your &lt;code&gt;config/storage.yml&lt;/code&gt; file to point to the S3 config, we are using config set by Bucketeer in this example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;amazon:
  service: S3
  access_key_id: &amp;lt;%= ENV['BUCKETEER_AWS_ACCESS_KEY_ID'] %&amp;gt;
  secret_access_key: &amp;lt;%= ENV['BUCKETEER_AWS_SECRET_ACCESS_KEY'] %&amp;gt;
  region: &amp;lt;%= ENV['BUCKETEER_AWS_REGION'] %&amp;gt;
  bucket: &amp;lt;%= ENV['BUCKETEER_BUCKET_NAME'] %&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then make sure that your app is set to use the &lt;code&gt;:amazon&lt;/code&gt; config store in production:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;config.active_storage.service = :amazon
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you forget this step, the default store is to use &lt;code&gt;:local&lt;/code&gt; which saves files to disk. This is not a scalable way to handle uploaded files in production. If you accidentally deploy this to Heroku, it will appear that the files were uploaded at first, but then they will disappear on random requests if you're running more than one dyno. The files will go away altogether when the dynos are restarted. You can get more information about &lt;a href=&quot;https://devcenter.heroku.com/articles/active-storage-on-heroku?preview=1#ephemeral-disk&quot;&gt;ephemeral disk of Heroku in the DevCenter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, the last thing you'll need to get this to work in production is to install a custom buildpack that installs the binary dependencies &lt;code&gt;ffmpeg&lt;/code&gt; and &lt;code&gt;poppler&lt;/code&gt; which are used to generate the asset previews:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ heroku buildpacks:add -i 1 https://github.com/heroku/heroku-buildpack-activestorage-preview
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you‚Äôre done you can deploy to Heroku!&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;adding-active-storage-to-an-existing-app&quot; href=&quot;#adding-active-storage-to-an-existing-app&quot;&gt;Adding Active Storage to an Existing App&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;If your app doesn't already have Active Storage, you can add it. First, you'll need to enable Active Storage blob storage by running:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ bin/rails active_storage:install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will add a migration that lets Rails track the uploaded files.&lt;/p&gt;

&lt;p&gt;Next, you'll need a model to &quot;attach&quot; files onto. You can use an existing model, or create a new model. In the example app a mostly empty &lt;code&gt;bulletin&lt;/code&gt; model is used:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ bin/rails generate scaffold bulletin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, run the migrations on the application:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-term&quot;&gt;$ bin/rails db:migrate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After the database is migrated, update the model to let Rails know that you intend to be able to attach files to it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;class Bulletin &amp;lt; ApplicationRecord
  has_one_attached :attachment
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once that's done, we will need three more pieces: a form for uploading attachments, a controller to save attachments, and then a view for rendering the attachments.&lt;/p&gt;

&lt;p&gt;If you have an existing form you can add an attachment field via the &lt;code&gt;file_field&lt;/code&gt; view helper like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-erb&quot;&gt;    &amp;lt;%= form.file_field :attachment %&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see an example of a form with an attachment in &lt;a href=&quot;https://github.com/heroku/active_storage_with_previews_example/blob/ab0370f77f35f8eb0813727b8d49758926450f5e/app/views/welcome/_upload.html.erb#L14&quot;&gt;the example app&lt;/a&gt;. Once you have a form, you will need to save the attachment.&lt;/p&gt;

&lt;p&gt;In this example app, the home page contains the form and the view. In the &lt;a href=&quot;https://github.com/heroku/active_storage_with_previews_example/blob/ab0370f77f35f8eb0813727b8d49758926450f5e/app/controllers/bulletins_controller.rb#L26-L32&quot;&gt;bulletin controller&lt;/a&gt; the attachment is saved and then the user is redirected back to the main bulletin list:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;def create
  @bulletin = Bulletin.new()
  @bulletin.attachment.attach(params[:bulletin][:attachment])
  @bulletin.save!

  redirect_back(fallback_location: root_path)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, in the &lt;a href=&quot;https://github.com/heroku/active_storage_with_previews_example/blob/ab0370f77f35f8eb0813727b8d49758926450f5e/app/views/welcome/index.erb&quot;&gt;welcome view&lt;/a&gt; we iterate through each of the bulletin items and, depending on the type of attachment we have, render it differently.&lt;/p&gt;

&lt;p&gt;In Active Storage the &lt;code&gt;previewable?&lt;/code&gt; method will return true for PDFs and Videos provided the system has the right binaries installed. The &lt;code&gt;variable?&lt;/code&gt; method will return true for images if &lt;code&gt;mini_magick&lt;/code&gt; is installed. If neither of these things is true then, the attachment is likely a file that is best viewed after being downloaded. Here's &lt;a href=&quot;https://github.com/heroku/active_storage_with_previews_example/blob/ab0370f77f35f8eb0813727b8d49758926450f5e/app/views/welcome/index.erb#L24-L37&quot;&gt;how we can represent that logic&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-erb&quot;&gt;&amp;lt;ul class=&quot;no-bullet&quot;&amp;gt;
  &amp;lt;% @bulletin_list.each do |bulletin| %&amp;gt;
    &amp;lt;li&amp;gt;
      &amp;lt;% if bulletin.attachment.previewable? %&amp;gt;
        &amp;lt;%= link_to(image_tag(bulletin.attachment.preview(resize: &quot;200x200&amp;gt;&quot;)),  rails_blob_path(bulletin.attachment, disposition: &quot;attachment&quot;))
        %&amp;gt;
      &amp;lt;% elsif bulletin.attachment.variable? %&amp;gt;
        &amp;lt;%= link_to(image_tag(bulletin.attachment.variant(resize: &quot;200x200&quot;)), rails_blob_path(bulletin.attachment, disposition: &quot;attachment&quot;))%&amp;gt;
      &amp;lt;% else %&amp;gt;
        &amp;lt;%= link_to &quot;Download file&quot;, rails_blob_path(bulletin.attachment, disposition: &quot;attachment&quot;) %&amp;gt;
      &amp;lt;% end %&amp;gt;
    &amp;lt;/li&amp;gt;
  &amp;lt;% end %&amp;gt;
&amp;lt;/ul&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you've got all these pieces in your app, and configured Active Storage to work in production, your users can enjoy uploading and downloading files with ease.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Using Heroku's Expensive Query Dashboard to Speed up your App</title>
      <link>https://blog.heroku.com/expensive-query-speed-up-app</link>
      <pubDate>Tue, 11 Jul 2017 15:39:00 GMT</pubDate>
      <guid>https://blog.heroku.com/expensive-query-speed-up-app</guid>
      <description>&lt;p&gt;I recently &lt;a href=&quot;https://schneems.com/2017/06/22/a-tale-of-slow-pagination/&quot;&gt;demonstrated how you can use Rack Mini Profiler to find and fix slow queries&lt;/a&gt;. It‚Äôs a valuable tool for well-trafficked pages, but sometimes the slowdown is happening on a page you don't visit often, or in a worker task that isn't visible via Rack Mini Profiler. How can you find and fix those slow queries?&lt;/p&gt;

&lt;p&gt;Heroku has a feature called &lt;a href=&quot;https://devcenter.heroku.com/articles/expensive-queries&quot;&gt;expensive queries&lt;/a&gt; that can help you out.  It shows historical performance data about the queries running on your database: most time consuming, most frequently invoked, slowest execution time, and slowest I/O.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1499377005-expensive_queries.png&quot; alt=&quot;expensive_queries&quot;&gt;&lt;/p&gt;

&lt;p&gt;Recently, I used this feature to identify and address some slow queries for a site I run on Heroku named &lt;a href=&quot;https://www.codetriage.com&quot;&gt;CodeTriage&lt;/a&gt; (the best way to get started contributing to open source). Looking at the expensive queries data for CodeTriage, I saw this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1499374279-Screenshot%202017-06-22%2015.16.40.png&quot; alt=&quot;Code Triage Project Expensive Query Screenshot&quot;&gt; &lt;/p&gt;

&lt;p&gt;On the right is the query, on the left are two graphs; one graph showing the number of times the query was called, and another beneath that showing the average time it took to execute the query. You can see from the bottom graph that the average execution time can be up to 8 seconds, yikes! Ideally, I want my response time averages to be around 50 ms and perc 95 to be sub-second time, so waiting 8 seconds for a single query to finish isn't good.&lt;/p&gt;

&lt;p&gt;To find this on your own apps you can follow directions on the &lt;a href=&quot;https://devcenter.heroku.com/articles/expensive-queries&quot;&gt;expensive queries documentation&lt;/a&gt;. The documentation will direct you to &lt;a href=&quot;https://data.heroku.com/&quot;&gt;your database list page&lt;/a&gt; where you can select the database you‚Äôd like to optimize. From there, scroll down and find the expensive queries near the bottom.&lt;/p&gt;

&lt;p&gt;Once you've chosen a slow query, you‚Äôll need to determine why it's slow. To accomplish this use &lt;code&gt;EXPLAIN ANALYZE&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;sql&quot;&gt;issuetriage::DATABASE=&amp;gt; EXPLAIN ANALYZE
issuetriage::DATABASE-&amp;gt; SELECT &quot;issues&quot;.*
issuetriage::DATABASE-&amp;gt; FROM &quot;issues&quot;
issuetriage::DATABASE-&amp;gt; WHERE &quot;issues&quot;.&quot;repo_id&quot; = 2151
issuetriage::DATABASE-&amp;gt;         AND &quot;issues&quot;.&quot;state&quot; = 'open'
issuetriage::DATABASE-&amp;gt; ORDER BY  created_at DESC LIMIT 20 OFFSET 0;
                                                                       QUERY PLAN
---------------------------------------------------------------------------------------------------------------------------------------------------------
Limit  (cost=27359.98..27359.99 rows=20 width=1232) (actual time=82.800..82.802 rows=20 loops=1)
   -&amp;gt;  Sort  (cost=27359.98..27362.20 rows=4437 width=1232) (actual time=82.800..82.801 rows=20 loops=1)
         Sort Key: created_at
         Sort Method: top-N heapsort  Memory: 31kB
         -&amp;gt;  Bitmap Heap Scan on issues  (cost=3319.34..27336.37 rows=4437 width=1232) (actual time=27.725..81.220 rows=5067 loops=1)
               Recheck Cond: (repo_id = 2151)
               Filter: ((state)::text = 'open'::text)
               Rows Removed by Filter: 13817
               -&amp;gt;  Bitmap Index Scan on index_issues_on_repo_id  (cost=0.00..3319.12 rows=20674 width=0) (actual time=24.293..24.293 rows=21945 loops=1)
                     Index Cond: (repo_id = 2151)
Total runtime: 82.885 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case, I'm using &lt;a href=&quot;https://www.codetriage.com/kubernetes/kubernetes&quot;&gt;Kubernetes&lt;/a&gt; because they currently have the highest issue count, so querying on that page will likely give me the worst performance.&lt;/p&gt;

&lt;p&gt;We see the total time spent was 82 ms, which isn't bad for one of the &quot;slowest&quot; queries, but we've seen that some can be way worse. Most single queries should be aiming for around a 1 ms query time.&lt;/p&gt;

&lt;p&gt;We see that before the query can be made it has to sort the data, this is because we are using an &lt;code&gt;order&lt;/code&gt; on an &lt;code&gt;offset&lt;/code&gt; clause. Sorting is a very expensive operation, you can see that it says the &quot;actual time&quot; can take between &lt;code&gt;27.725&lt;/code&gt; ms and &lt;code&gt;81.220&lt;/code&gt; ms just to sort the data, which is pretty tough. If we can get rid of this sort then we can drastically improve our query.&lt;/p&gt;

&lt;p&gt;One way to do this is... you guessed it, add an index. Unlike &lt;a href=&quot;https://schneems.com/2017/06/22/a-tale-of-slow-pagination/&quot;&gt;last week&lt;/a&gt; though, the issues table is HUGE. While the table we indexed last week only had around 2K entries, each of those entries can have a virtually unbounded number of issues. In the case of Kubernetes there are 5K+ issues, and that's only the &lt;code&gt;state=open&lt;/code&gt; ones. The closed issue count is much larger than that, and it will only grow over time. We want to be mindful of taking up too much database size, so instead of indexing ALL the data, we can instead apply a partial index. I'm almost never querying for &lt;code&gt;state=closed&lt;/code&gt; when it comes to issues, so we can ignore those while building our index. Here's the migration I used to add a partial index:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;ruby&quot;&gt;class AddCreatedAtIndexToIssues &amp;lt; ActiveRecord::Migration[5.1]
  def change
    add_index :issues, :created_at, where: &quot;state = 'open'&quot;
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What's the result of adding this index? Let's look at that same query we analyzed before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;sql&quot;&gt;
issuetriage::DATABASE=&amp;gt; EXPLAIN ANALYZE
issuetriage::DATABASE-&amp;gt; SELECT &quot;issues&quot;.*
issuetriage::DATABASE-&amp;gt; FROM &quot;issues&quot;
issuetriage::DATABASE-&amp;gt; WHERE &quot;issues&quot;.&quot;repo_id&quot; = 2151
issuetriage::DATABASE-&amp;gt;         AND &quot;issues&quot;.&quot;state&quot; = 'open'
issuetriage::DATABASE-&amp;gt; ORDER BY  created_at DESC LIMIT 20 OFFSET 0;
                                                                         QUERY PLAN
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Limit  (cost=0.08..316.09 rows=20 width=1232) (actual time=0.169..0.242 rows=20 loops=1)
   -&amp;gt;  Index Scan Backward using index_issues_on_created_at on issues  (cost=0.08..70152.26 rows=4440 width=1232) (actual time=0.167..0.239 rows=20 loops=1)
         Filter: (repo_id = 2151)
         Rows Removed by Filter: 217
Total runtime: 0.273 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wow, from 80+ ms to less than half a millisecond. That's some improvement. The index keeps our data already sorted, so we don't have to re-sort it on every query. All elements in the index are guaranteed to be &lt;code&gt;state=open&lt;/code&gt; so the database doesn't have to do more work there. The database can simply scan the index removing elements where &lt;code&gt;repo_id&lt;/code&gt; is not matching our target.&lt;/p&gt;

&lt;p&gt;For this case it is EXTREMELY fast, but can you imagine a case where it isn't so fast?&lt;/p&gt;

&lt;p&gt;Perhaps you noticed that we still have to iterate over issues until we're able to find ones matching a given Repo ID. I'm guessing that since this repo has the most issues, it's able to easily find 20 issues with &lt;code&gt;state=open&lt;/code&gt;. What if we pick a different repo?&lt;/p&gt;

&lt;p&gt;I looked up the oldest open issue and found it in &lt;a href=&quot;https://www.codetriage.com/rails/journey&quot;&gt;Journey&lt;/a&gt;. Journey has an ID of 10 in the database. If we do the same query and look at Journey:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;sql&quot;&gt;issuetriage::DATABASE=&amp;gt; EXPLAIN ANALYZE
issuetriage::DATABASE-&amp;gt; SELECT &quot;issues&quot;.*
issuetriage::DATABASE-&amp;gt; FROM &quot;issues&quot;
issuetriage::DATABASE-&amp;gt; WHERE &quot;issues&quot;.&quot;repo_id&quot; = 10
issuetriage::DATABASE-&amp;gt;         AND &quot;issues&quot;.&quot;state&quot; = 'open'
issuetriage::DATABASE-&amp;gt; ORDER BY  created_at DESC LIMIT 20 OFFSET 0;
                                                                     QUERY PLAN
----------------------------------------------------------------------------------------------------------------------------------------------------
 Limit  (cost=757.18..757.19 rows=20 width=1232) (actual time=21.109..21.110 rows=6 loops=1)
   -&amp;gt;  Sort  (cost=757.18..757.20 rows=50 width=1232) (actual time=21.108..21.109 rows=6 loops=1)
         Sort Key: created_at
         Sort Method: quicksort  Memory: 26kB
         -&amp;gt;  Index Scan using index_issues_on_repo_id on issues  (cost=0.11..756.91 rows=50 width=1232) (actual time=11.221..21.088 rows=6 loops=1)
               Index Cond: (repo_id = 10)
               Filter: ((state)::text = 'open'::text)
               Rows Removed by Filter: 14
 Total runtime: 21.140 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yikes. Previously we're only using 0.27 ms, now we're back up to 21 ms. This might not have been the &quot;8 second&quot; query we were seeing before, but it's definitely slower than the first query we profiled. &lt;/p&gt;

&lt;p&gt;Even though we've got an index on &lt;code&gt;created_at&lt;/code&gt; Postgres has decided not to use it. It's reverting back to a sorting algorithm and using an index on &lt;code&gt;repo_id&lt;/code&gt; to pull the data. Once it has issues then it iterates over each to remove where the state is not open.&lt;/p&gt;

&lt;p&gt;In this case, there are only 20 total issues for Journey, so grabbing all the issues and iterating and sorting manually was deemed to be faster. Does this mean our index is worthless? Well considering this repo only has 1 subscriber, it's not the case we need to be optimizing for. Also if lots of people visit that page (maybe because of this article), then Postgres will speed up the query by using the cache. The second time I ran the exact same explain query, it was much faster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; Total runtime: 0.092 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Postgres already had everything it needed in the cache. Does this mean we're totally out of the woods then? Going back to my expensive queries page after a few days, I saw that my 8 second worst case is gone, but I still have a 2 second query every now and then.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1499377612-Screenshot%202017-06-26%2010.47.20.png&quot; alt=&quot;Expensive Queries Screenshot 2&quot;&gt;&lt;/p&gt;

&lt;p&gt;This is still a 75% performance increase (in worst case performance) so the index is still useful. One really useful feature of Postgres is the ability to &lt;a href=&quot;https://www.postgresql.org/docs/8.3/static/indexes-bitmap-scans.html&quot;&gt;combine multiple indexes&lt;/a&gt;. In this case, even though we have an index on &lt;code&gt;created_at&lt;/code&gt; and an index on &lt;code&gt;repo_id&lt;/code&gt;, Postgres does not seem to think it's faster to combine the two and use that result. To fix this issue we can add an index that has both &lt;code&gt;created_at&lt;/code&gt; and &lt;code&gt;repo_id&lt;/code&gt;, which maybe I'll explore in the future.&lt;/p&gt;

&lt;p&gt;Before we go, I want to circle back to how we found our slow query test case. I had to know a bit about the data and make some assumptions about the worst case scenarios. I had to guess that &lt;a href=&quot;https://www.codetriage.com/kubernetes/kubernetes&quot;&gt;Kubernetes&lt;/a&gt; was our worst offender, which ended up not being true. Is there a better way than guess and check?&lt;/p&gt;

&lt;p&gt;It turns out that Heroku will &lt;a href=&quot;https://devcenter.heroku.com/articles/postgres-logs-errors#log-duration-3-565-s&quot;&gt;output slow queries into your app's logs&lt;/a&gt;. Unlike the expensive queries, these logs also contain the parameters used in the query, and not just the query. If you have a logging addon such as &lt;a href=&quot;https://elements.heroku.com/addons/papertrail&quot;&gt;Papertrail&lt;/a&gt;, you can search your logs for &lt;code&gt;duration&lt;/code&gt; and get a result like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jun 26 06:36:54 issuetriage app/postgres.29339:  [DATABASE] [39-1] LOG:  duration: 3040.545 ms  execute &amp;lt;unnamed&amp;gt;: SELECT COUNT(*) FROM &quot;issues&quot; WHERE &quot;issues&quot;.&quot;repo_id&quot; = $1 AND &quot;issues&quot;.&quot;state&quot; = $2 
Jun 26 06:36:54 issuetriage app/postgres.29339:  [DATABASE] [39-2] DETAIL:  parameters: $1 = '696', $2 = 'open' 
Jun 26 08:26:25 issuetriage app/postgres.29339:  [DATABASE] [40-1] LOG:  duration: 9087.165 ms  execute &amp;lt;unnamed&amp;gt;: SELECT COUNT(*) FROM &quot;issues&quot; WHERE &quot;issues&quot;.&quot;repo_id&quot; = $1 AND &quot;issues&quot;.&quot;state&quot; = $2 
Jun 26 08:26:25 issuetriage app/postgres.29339:  [DATABASE] [40-2] DETAIL:  parameters: $1 = '1245', $2 = 'open' 
Jun 26 08:49:40 issuetriage app/postgres.29339:  [DATABASE] [41-1] LOG:  duration: 2406.615 ms  execute &amp;lt;unnamed&amp;gt;: SELECT  &quot;issues&quot;.* FROM &quot;issues&quot; WHERE &quot;issues&quot;.&quot;repo_id&quot; = $1 AND &quot;issues&quot;.&quot;state&quot; = $2 ORDER BY created_at DESC LIMIT $3 OFFSET $4 
Jun 26 08:49:40 issuetriage app/postgres.29339:  [DATABASE] [41-2] DETAIL:  parameters: $1 = '1348', $2 = 'open', $3 = '20', $4 = '760' 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case, we can see that our 2.4 second query (the last query in the logs above) is using a repo id of &lt;code&gt;1348&lt;/code&gt; and an offset of &lt;code&gt;760&lt;/code&gt;, which brings up another important point. As the offset goes up, the cost of scanning our index will also go up, so it turns out that we had a worse case than my initial guess (Kubernetes) and my second guess (Journey). It is likely that this repo has lots of issues that are old, and this query isn't made often, so that the data is not in cache. By using the logs we can find the exact worst case scenario without all the guessing.&lt;/p&gt;

&lt;p&gt;Before you start writing that comment message, yes, I know that offset pagination is broken and &lt;a href=&quot;https://www.citusdata.com/blog/2016/03/30/five-ways-to-paginate/&quot;&gt;there are other ways to paginate&lt;/a&gt;. I may start to look at alternative pagination options, or even getting rid of some of the pagination on the site altogether. &lt;/p&gt;

&lt;p&gt;I did go back and &lt;a href=&quot;https://github.com/codetriage/codetriage/commit/ce3ac6a59f92891e4a42b85927c852f074a0be3f&quot;&gt;add an index to both the &lt;code&gt;created_at&lt;/code&gt; and &lt;code&gt;repo_id&lt;/code&gt; columns&lt;/a&gt;. With the addition of those two indexes my &quot;worst case&quot; of 2.4 seconds is now down to 14 ms:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;sql&quot;&gt;issuetriage::DATABASE=&amp;gt; EXPLAIN ANALYZE SELECT  &quot;issues&quot;.*
issuetriage::DATABASE-&amp;gt; FROM &quot;issues&quot;
issuetriage::DATABASE-&amp;gt; WHERE &quot;issues&quot;.&quot;repo_id&quot; = 1348
issuetriage::DATABASE-&amp;gt; AND &quot;issues&quot;.&quot;state&quot; = 'open'
issuetriage::DATABASE-&amp;gt; ORDER BY created_at DESC
issuetriage::DATABASE-&amp;gt; LIMIT 20 OFFSET 760;
                                                                                QUERY PLAN
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit  (cost=1380.73..1417.06 rows=20 width=1232) (actual time=14.515..14.614 rows=20 loops=1)
   -&amp;gt;  Index Scan Backward using index_issues_on_repo_id_and_created_at on issues  (cost=0.08..2329.02 rows=1282 width=1232) (actual time=0.061..14.564 rows=780 loops=1)
         Index Cond: (repo_id = 1348)
 Total runtime: 14.659 ms
(4 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here you can see that we're able to use our new index directly and find only the issues that are open and belonging to a specific repo id.&lt;/p&gt;

&lt;p&gt;What did I learn from this experiment?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can find &lt;a href=&quot;https://devcenter.heroku.com/articles/expensive-queries&quot;&gt;slow queries using Heroku's expensive queries&lt;/a&gt; feature.&lt;/li&gt;
&lt;li&gt;The exact arguments matter a lot when profiling queries. Don't assume that you know the most expensive thing your database is doing, use metrics.&lt;/li&gt;
&lt;li&gt;You can find the exact parameters that go with those expensive queries by &lt;a href=&quot;https://devcenter.heroku.com/articles/postgres-logs-errors#log-duration-3-565-s&quot;&gt;grepping your logs for the exact parameters of those queries&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Indexes help a ton, but you have to understand the different ways your application will use them. It's not enough to profile with 1 query before and after, you need to profile a few different queries with different performance characteristics. In my case not only did I add an index, I went back to the expensive index page which let me know that my queries were still taking a long time (~2 seconds). &lt;/li&gt;
&lt;li&gt;Performance tuning isn't about magic fixes, it's about finding a toolchain you understand, and iterating on a process until you get the results you want.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Richard Schneeman is an Engineer for Heroku who also &lt;a href=&quot;https://www.schneems.com&quot;&gt;writes posts on his own blog&lt;/a&gt;. If you liked this post, you can &lt;a href=&quot;https://schneems.com/mailinglist&quot;&gt;subscribe to his mailing list to get more like it for free&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>N+1 Queries or Memory Problems: Why not Solve Both?</title>
      <link>https://blog.heroku.com/solving-n-plus-one-queries</link>
      <pubDate>Tue, 28 Mar 2017 15:32:00 GMT</pubDate>
      <guid>https://blog.heroku.com/solving-n-plus-one-queries</guid>
      <description>&lt;p&gt;This post is going to help save you money if you're running a Rails server. It starts like this: you write an app. Let's say you're building the next hyper-targeted blogging platform for medium length posts. When you login, you see a paginated list of all of the articles you've written. You have a &lt;code&gt;Post&lt;/code&gt; model and maybe for to do tags, you have a &lt;code&gt;Tag&lt;/code&gt; model, and for comments, you have a &lt;code&gt;Comment&lt;/code&gt; model. You write your view so that it renders the posts:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;% @posts.each do |post| %&amp;gt;
  &amp;lt;%= link_to(post, post.title) %&amp;gt;
  &amp;lt;%= teaser_for(post) %&amp;gt;
  &amp;lt;%= &quot;#{post.comments.count} comments&quot;
&amp;lt;% end %&amp;gt;

&amp;lt;%= pagination(@posts) %&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See any problems with this?  We have to make a single query to return all the posts - that's where the &lt;code&gt;@posts&lt;/code&gt; comes from.  Say that there are N posts returned.  In the code above, as the view iterates over each post, it has to calculate &lt;code&gt;post.comments.count&lt;/code&gt; - but &lt;em&gt;that&lt;/em&gt; in turn needs another database query.  This is the N+1 query problem - our initial single query (the 1 in N+1) returns something (of size N) that we iterate over and perform yet another database query on (N of them). &lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;introducing-includes&quot; href=&quot;#introducing-includes&quot;&gt;Introducing Includes&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;If you've been around the Rails track long enough you've probably run into the above scenario before. If you run a Google search, the answer is very simple -- &quot;use includes&quot;. The code looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# before
@posts = current_user.posts.per_page(20).page(params[:page])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and after&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;@posts = current_user.posts.per_page(20).page(params[:page])
@posts = @posts.includes(:comments)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is still textbook, but let's look at what's going on. Active Record uses lazy querying so this won't actually get executed until we call &lt;code&gt;@posts.first&lt;/code&gt; or &lt;code&gt;@posts.all&lt;/code&gt; or &lt;code&gt;@posts.each&lt;/code&gt;. When we do that two queries get executed, the first one for posts makes sense:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;select * from posts where user_id=? limit ? offset ?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Active Record will pass in user_id and limit and offset into the bind params and you'll get your array of posts.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: we almost always want all queries to be scoped with a limit in production apps.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The next query you'll see may look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;select * from comments where post_id in ?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice anything wrong? Bonus points if you found it, and yes, it has something to do with memory.&lt;/p&gt;

&lt;p&gt;If each of those 20 blog posts has 100 comments, then this query will return 2,000 rows from your database. Active Record doesn't know what data you need from each post comment, it'll just know it was told you'll eventually need them. So what does it do? It creates 2,000 Active Record objects in memory because that's what you told it to do. That's the problem, you don't need 2,000 objects in memory. You don't even need the objects, you only need the count.&lt;/p&gt;

&lt;p&gt;The good: You got rid of your N+1 problem.&lt;/p&gt;

&lt;p&gt;The bad: You're stuffing 2,000 (or more) objects from the database into memory when you aren't going to use them at all. This will slow down this action and balloon the memory use requirements of your app.&lt;/p&gt;

&lt;p&gt;It's even worse if the data in the comments is large. For instance, maybe there is no max size for a comment field and people write thousand word essays, meaning we'll have to load those really large strings into memory and keep them there until the end of the request even though we're not using them.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;n-1-is-bad-unneeded-memory-allocation-is-worse&quot; href=&quot;#n-1-is-bad-unneeded-memory-allocation-is-worse&quot;&gt;N+1 Is Bad, Unneeded Memory Allocation Is Worse&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Now we've got a problem. We could &quot;fix&quot; it by re-introducing our N+1 bug. That's a valid fix, however, you can easily benchmark it. Use &lt;code&gt;rack-mini-profiler&lt;/code&gt; in development on a page with a large amount of simulated data. Sometimes it's faster to not &quot;fix&quot; your N+1 bugs.&lt;/p&gt;

&lt;p&gt;That's not good enough for us, though -- we want no massive memory allocation spikes and no N+1 queries.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;counter-cache&quot; href=&quot;#counter-cache&quot;&gt;Counter Cache&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;What's the point of having Cache if you can't count it? Instead of having to call &lt;code&gt;post.comments.count&lt;/code&gt; each time, which costs us a SQL query, we can store that data directly inside of the &lt;code&gt;Post&lt;/code&gt; model. This way when we load a &lt;code&gt;Post&lt;/code&gt; object we automatically have this info. From &lt;a href=&quot;http://edgeguides.rubyonrails.org/association_basics.html#options-for-belongs-to-counter-cache&quot;&gt;the docs for the counter cache&lt;/a&gt; you'll see we need to change our model to something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;class Comment &amp;lt; ApplicationRecord
   belongs_to :post , counter_cache: count_of_comments
  #‚Ä¶
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now in our view, we can call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;  &amp;lt;%= &quot;#{post.count_of_comments} comments&quot;  %&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Boom! Now we have no N+1 query and no memory problems. But...&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;counter-cache-edge-cases&quot; href=&quot;#counter-cache-edge-cases&quot;&gt;Counter Cache Edge Cases&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;You cannot use a counter cache with a condition. Let's change our example for a minute. Let's say each comment could either be &quot;approved&quot;, meaning you moderated it and allow it to show on your page, or &quot;pending&quot;. Perhaps this is a vital piece of information and you MUST show it on your page.  Previously we would have done this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;  &amp;lt;%= &quot;#{ post.comments.approved.count } approved comments&quot;  %&amp;gt;
  &amp;lt;%= &quot;#{ post.comments.pending.count } pending comments&quot;  %&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case the &lt;code&gt;Comment&lt;/code&gt; model has a &lt;code&gt;status&lt;/code&gt; field and calling &lt;code&gt;comments.pending&lt;/code&gt; is equivalent to adding &lt;code&gt;where(status: &quot;pending&quot;)&lt;/code&gt;. It would be great if we could have a &lt;code&gt;post.count_of_pending_comments&lt;/code&gt; cache and a &lt;code&gt;post.count_of_approved_comments&lt;/code&gt; cache, but we can't. There are some ways to hack it, but there are edge cases, and not all apps can safely accommodate for all edge cases. Let's say ours is one of those.&lt;/p&gt;

&lt;p&gt;Now what? We could get around this with some view caching because if we cache your entire page, we only have to render it and pay that N+1 cost once. Maybe fewer times if we are re-using view components and are using &quot;Russian doll&quot; style view caches .&lt;/p&gt;

&lt;p&gt;If view caching is out of the question due to &amp;lt;reasons&amp;gt;, what are we left with? We have to use our database the way the original settlers of the Wild West did, manually and with great effort.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;manually-building-count-data-in-hashes&quot; href=&quot;#manually-building-count-data-in-hashes&quot;&gt;Manually Building Count Data in Hashes&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;In our controller where we previously had this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;@posts = current_user.posts.per_page(20).page(params[:page])
@posts = @posts.includes(:comments)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can remove that &lt;code&gt;includes&lt;/code&gt; and instead build two hashes. Active Record returns hashes when we use &lt;code&gt;group()&lt;/code&gt;. In this case we know we want to associate comment count with each post, so we group by &lt;code&gt;:post_id&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;@posts = current_user.posts.per_page(20).page(params[:page])
post_ids = @posts.map(&amp;amp;:id)
@pending_count_hash   = Comment.pending.where(post_id: post_ids).group(:post_id).count
@approved_count_hash = Comment.approved.where(post_id: post_ids).group(:post_id).count
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can stash and use this value in our view instead:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;  &amp;lt;%= &quot;#{ @approved_count_hash[post.id] || 0  } approved comments&quot;  %&amp;gt;
  &amp;lt;%= &quot;#{ @pending_count_hash[post.id] || 0 } pending comments&quot;  %&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have 3 queries, one to find our posts and one for each comment type we care about. This generates 2 extra hashes that hold the minimum of information that we need.&lt;/p&gt;

&lt;p&gt;I've found this strategy to be super effective in mitigating memory issues while not sacrificing on the N+1 front.&lt;/p&gt;

&lt;p&gt;But what if you're using that data inside of methods.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;fat-models-low-memory&quot; href=&quot;#fat-models-low-memory&quot;&gt;Fat Models, Low Memory&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Rails encourage you to stick logic inside of models. If you're doing that, then perhaps this code wasn't a raw SQL query inside of the view but was instead nested in a method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;def approved_comment_count
  self.comments.approved.count
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or maybe you need to do the math, maybe there is a critical threshold where pending comments overtake approved:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;def comments_critical_threshold?
  self.comments.pending.count &amp;lt; self.comments.approved.count
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is trivial, but you could imagine a more complex case where logic is happening based on business rules. In this case, you don't want to have to duplicate the logic in your view (where we are using a hash) and in your model (where we are querying the database). Instead, you can use dependency injection. Which is the hyper-nerd way of saying we'll pass in values. We can change the method signature to something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;def comments_critical_threshold?(pending_count: comments.pending.count, approved_count: comments.approved.count)
  pending_count &amp;lt; approved_count
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now I can call it and pass in values:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;post.comments_critical_threshold?(pending_count: @pending_count_hash[post.id] || 0 , approved_count: @approved_count_hash[post.id] || 0 )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or, if you're using it somewhere else, you can use it without passing in values since we specified our default values for the keyword arguments.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;BTW, aren't keyword arguments great?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;post.comments_critical_threshold? # default values are used here
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are other ways to write the same code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;def comments_critical_threshold?(pending_count , approved_count )
  pending_count ||= comments.pending.count
  approved_count ||= comments.approved.count
  pending_count &amp;lt; approved_count
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You get the gist though -- pass values into your methods if you need to.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;more-than-count&quot; href=&quot;#more-than-count&quot;&gt;More than Count&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;What if you're doing more than just counting? Well, you can pull that data and group it in the same way by using &lt;code&gt;select&lt;/code&gt; and specifying multiple fields. To keep going with our same example, maybe we want to show a truncated list of all commenter names and their avatar URLs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;@comment_names_hash = Comment.where(post_id: post_ids).select(&quot;names, avatar_url&quot;).group_by(&amp;amp;:post_ids)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The results look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;1337: [
  { name: &quot;schneems&quot;, avatar_url: &quot;https://http.cat/404.jpg&quot; },
  { name: &quot;illegitimate45&quot;, avatar_url: &quot;https://http.cat/451.jpg&quot; }
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;1337&lt;/code&gt; is the post id, and then we get an entry with a name and an avatar_url for each comment. Be careful here, though, as we're returning more data-- you still might not need all of it and making 2,000 hashes isn't much better than making 2,000 unused Active Record objects. You may want to better constrain your query with limits or by querying for more specific information.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;are-we-there-yet&quot; href=&quot;#are-we-there-yet&quot;&gt;Are We There Yet&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;At this point, we have gotten rid of our N+1 queries and we're hardly using any memory compared to before. Yay! Self-five. :partyparrot:. üéâ&lt;/p&gt;

&lt;p&gt;Here's where I give rapid-fire suggestions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;bullet&lt;/code&gt; gem -- it will help identify N+1 query locations and unused &lt;code&gt;includes&lt;/code&gt; -- it's good.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;rack-mini-profiler&lt;/code&gt; in development. This will help you compare relative speeds of your performance work. I usually do all my perf work on a branch and then I can easily go back and forth between that and master to compare speeds.&lt;/li&gt;
&lt;li&gt;Use production-like data in development. This performance &quot;bug&quot; won't show until we've got plenty of posts or plenty of comments. If your prod data isn't sensitive you can clone it using something like &lt;code&gt;$ heroku pg:pull&lt;/code&gt; to test against, but make sure you're not sending out emails or spending real money or anything first.&lt;/li&gt;
&lt;li&gt;You can see memory allocations by using &lt;code&gt;rack-mini-profiler&lt;/code&gt; with &lt;code&gt;memory-profiler&lt;/code&gt; and adding &lt;code&gt;pp=profile-memory&lt;/code&gt; to the end of your URL. This will show you things like total bytes allocated, which you can use for comparison purposes.&lt;/li&gt;
&lt;li&gt;Narrow down your search by focusing on slow endpoints. All performance trackers list out slow endpoints, this is a good place to start. &lt;a href=&quot;https://scoutapp.com&quot;&gt;Scout&lt;/a&gt; will show you memory breakdown per request and makes finding these types of bugs much easier to hunt down. They also have &lt;a href=&quot;https://elements.heroku.com/addons/scout&quot;&gt;an add-on&lt;/a&gt; for Heroku. You can get started for free &lt;code&gt;$ heroku addons:create scout:chair&lt;/code&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want to dig deeper into what's going on with Ruby's use of memory check out the &lt;a href=&quot;https://devcenter.heroku.com/articles/ruby-memory-use&quot;&gt;Memory Quota Exceeded in Ruby (MRI) Dev Center article
&lt;/a&gt;, my &lt;a href=&quot;http://www.schneems.com/2015/05/11/how-ruby-uses-memory.html&quot;&gt;How Ruby Uses Memory&lt;/a&gt;, and also Nate Berkopec's &lt;a href=&quot;https://www.youtube.com/watch?v=kZcqyuPeDao&quot;&gt;Halve your memory use with these 12 Weird Tricks&lt;/a&gt;. &lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Bundler Changed Where Your Canonical Ruby Information Lives: What You Need to Know</title>
      <link>https://blog.heroku.com/bundler-and-canonical-ruby-version</link>
      <pubDate>Tue, 28 Feb 2017 16:47:06 GMT</pubDate>
      <guid>https://blog.heroku.com/bundler-and-canonical-ruby-version</guid>
      <description>&lt;p&gt;Heroku bumped its Bundler version to 1.13.7 almost a month ago, and since then we've had a large number of support tickets opened, many a variant of the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Your Ruby version is &amp;lt;X&amp;gt;, but your Gemfile specified &amp;lt;Y&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I wanted to talk about why you might get this error while deploying to Heroku, and what you can do about it, along with some bonus features provided by the new Bundler version.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;why&quot; href=&quot;#why&quot;&gt;Why?&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;First off, why are you getting this error? On Heroku in our &lt;a href=&quot;https://devcenter.heroku.com/articles/ruby-versions&quot;&gt;Ruby Version docs&lt;/a&gt;, we mention that you can use a Ruby directive in your &lt;code&gt;Gemfile&lt;/code&gt; to specify a version of Ruby. For example if you wanted &lt;code&gt;2.3.3&lt;/code&gt; then you would need this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Gemfile

ruby &quot;2.3.3&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is still the right way to specify a version, however recent versions of Bundler introduced a cool new feature. To understand why this bug happens you need to understand how the feature works.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;ruby-version-specifiers&quot; href=&quot;#ruby-version-specifiers&quot;&gt;Ruby Version Specifiers&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;If you have people on your team who want to use a more recent version of Ruby, for example say Ruby 2.4.0 locally, but you don't want to force everyone to use that version you can use a Ruby version operator.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Gemfile

ruby &quot;~&amp;gt; 2.3&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;I don't recommend you do this since &quot;2.3&quot; isn't a technically valid version of Ruby. I recommend using full Ruby versions in the version specifier; so if you don't have a Ruby version in your Gemfile.lock &lt;code&gt;bundle platform --ruby&lt;/code&gt; will still return a valid Ruby version.&lt;/p&gt;

&lt;p&gt;You can use multiple version declarations just like in a &lt;code&gt;gem&lt;/code&gt; for example: &lt;code&gt;ruby '&amp;gt;= 2.3.3', '&amp;lt; 2.5'&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This says that any version of Ruby up until 3.0 is valid. This feature came in Bundler 1.12 but wasn't made available on Heroku until Bundler 1.13.7. &lt;/p&gt;

&lt;p&gt;In addition to the ability to specify a Ruby version specifier, Bundler also introduced locking the actual Ruby version in the &lt;code&gt;Gemfile.lock&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Gemfile.lock

RUBY VERSION
   ruby 2.3.3p222
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you run the command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle platform --ruby
ruby 2.3.3p222
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You'll get the value from your &lt;code&gt;Gemfile.lock&lt;/code&gt; rather than the version specifier from your &lt;code&gt;Gemfile&lt;/code&gt;. This is to provide you with development/production parity. To get that Ruby version in your &lt;code&gt;Gemfile.lock&lt;/code&gt; you have to run &lt;code&gt;bundle install&lt;/code&gt; with the same version of Ruby locally, which means when you deploy you'll be using a version of Ruby you use locally.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Did you know this is actually how Heroku gets your Ruby version? We run the &lt;code&gt;bundle platform --ruby&lt;/code&gt; command against your app.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So while the version specifier tells bundler what version ranges are &quot;valid&quot; the version in the &lt;code&gt;Gemfile.lock&lt;/code&gt; is considered to be canonical.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;an-error-by-any-other-name&quot; href=&quot;#an-error-by-any-other-name&quot;&gt;An Error By Any Other Name&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;So if you were using the app before with the specifier &lt;code&gt;ruby &quot;~&amp;gt; 2.3&quot;&lt;/code&gt; and you try to run it with Ruby 1.9.3 you'll get an error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Your Ruby version is 1.9.3, but your Gemfile specified ~&amp;gt; 2.3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the primary intent of the bundler feature, to prevent you from accidentally using a version of Ruby that may or may not be valid with the app. However if Heroku gets the Ruby version from &lt;code&gt;bundle platform --ruby&lt;/code&gt; and that comes from the &lt;code&gt;Gemfile&lt;/code&gt; and &lt;code&gt;Gemfile.lock&lt;/code&gt;, how could you ever be running a version of Ruby on Heroku different from the version specified in your &lt;code&gt;Gemfile&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;One of the reasons we didn't support Bundler 1.12 was due to a &lt;a href=&quot;https://github.com/bundler/bundler/issues/4627&quot;&gt;bug that allowed incompatible &lt;code&gt;Gemfile&lt;/code&gt; and &lt;code&gt;Gemfile.lock&lt;/code&gt; Ruby versions&lt;/a&gt;. I reported the issue, and the bundler team did an amazing job patching it and releasing the fix in 1.13. &lt;/p&gt;

&lt;p&gt;What I didn't consider after is that people might still be using older bundler versions locally.&lt;/p&gt;

&lt;p&gt;So what is happening is that people will update the Ruby version specified in their &lt;code&gt;Gemfile&lt;/code&gt; without running &lt;code&gt;bundle install&lt;/code&gt; so their &lt;code&gt;Gemfile.lock&lt;/code&gt; does not get updated. Then they push to Heroku and it breaks. Or they're using an older version of Bundler and their &lt;code&gt;Gemfile.lock&lt;/code&gt; is using an incompatible version of Ruby locally but isn't raising any errors. Then they push to Heroku and it breaks.&lt;/p&gt;

&lt;p&gt;So if you're getting this error on Heroku, run this command locally to make sure your Bundler is up to date:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gem install bundler
Successfully installed bundler-1.13.7
1 gem installed
Installing ri documentation for bundler-1.13.7...
Installing RDoc documentation for bundler-1.13.7...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Even if you haven't hit this bug yet, go ahead and make sure you're on a recent version of Bundler right now. Once you've done that run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you've already got a Ruby version in your &lt;code&gt;Gemfile.lock&lt;/code&gt; you'll need to run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle update --ruby
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will insert the same version of Ruby you are using locally into your &lt;code&gt;Gemfile.lock&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you get the exception locally &lt;code&gt;Your Ruby version is &amp;lt;X&amp;gt;, but your Gemfile specified &amp;lt;Y&amp;gt;&lt;/code&gt; it means you either need to update your &lt;code&gt;Gemfile&lt;/code&gt; to point at your version of Ruby, or update your locally installed version of Ruby to match your &lt;code&gt;Gemfile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Once you've got everything working, make sure you commit it to git&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git add Gemfile.lock
$ git commit -m &quot;Fix Ruby version&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you're ready to &lt;code&gt;git push heroku master&lt;/code&gt; and things should work.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;when-things-go-wrong&quot; href=&quot;#when-things-go-wrong&quot;&gt;When Things Go Wrong&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;When these type of unexpected problems creep up on customers we try to do as much as we can to make the process easy for customers to understand the problem, and the fix. After seeing a few tickets come in, the information was shared internally with our support department (they're great by the way). Recently I &lt;a href=&quot;https://devcenter.heroku.com/articles/ruby-versions#your-ruby-version-is-x-but-your-gemfile-specified-y&quot;&gt;added documentation to Dev Center&lt;/a&gt; to document this explicit problem. I've also added some checks in the &lt;a href=&quot;https://github.com/heroku/heroku-buildpack-ruby/pull/514&quot;&gt;buildpack to give users a warning that points them to the docs&lt;/a&gt;. This is the best case scenario where not only can we document the problem, and the fix, but also add docs directly to the buildpack so you get it when you need it.&lt;/p&gt;

&lt;p&gt;I also wanted to blog about it to help people wrap their minds around the fact that the &lt;code&gt;Gemfile&lt;/code&gt; is no longer the canonical source of the exact Ruby version, but instead the &lt;code&gt;Gemfile.lock&lt;/code&gt; is. While the &lt;code&gt;Gemfile&lt;/code&gt; holds the Ruby version specifier that declares a range of ruby versions that are valid with your app, the &lt;code&gt;Gemfile.lock&lt;/code&gt; holds the canonical Ruby version of your app.&lt;/p&gt;

&lt;p&gt;As Ruby developers we have one of the best (if not the best) dependency managers in bundler. I'm excited for more people to start using version specifiers if the need arises for their app and I'm excited to support this feature on Heroku.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Container-Ready Rails 5</title>
      <link>https://blog.heroku.com/container_ready_rails_5</link>
      <pubDate>Mon, 02 May 2016 15:59:00 GMT</pubDate>
      <guid>https://blog.heroku.com/container_ready_rails_5</guid>
      <description>&lt;p&gt;Rails 5 will be the easiest release ever to get running on Heroku. You can get it going in just five lines:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rails new myapp -d postgresql
$ cd myapp
$ git init . ; git add . ; git commit -m first
$ heroku create
$ git push heroku master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These five lines (and a view or two) are all you need to get a Rails 5 app working on Heroku ‚Äî there are no special gems you need to install, or flags you must toggle. Let's take a peek under the hood, and explore the interfaces baked right into Rails 5 that make it easy to deploy your app on any modern container-based platform.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;production-web-server-as-the-default&quot; href=&quot;#production-web-server-as-the-default&quot;&gt;Production Web Server as the Default&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Before Rails 5, the default web server that you get when you run &lt;code&gt;$ rails server&lt;/code&gt; is &lt;a href=&quot;http://ruby-doc.org/stdlib-2.3.0/libdoc/webrick/rdoc/WEBrick.html&quot;&gt;WEBrick&lt;/a&gt;, which is the only server that ships with the Ruby standard library. For years now Heroku has &lt;a href=&quot;https://devcenter.heroku.com/articles/ruby-default-web-server#why-not-webrick&quot;&gt;recommended against using WEBrick as a production webserver&lt;/a&gt; mostly due to performance concerns, since by default WEBrick cannot handle more than one request at a time. With the addition of ActionCable to Rails 5, the Rails team needed a web server that could handle concurrent requests, so they decided to make &lt;a href=&quot;https://devcenter.heroku.com/articles/deploying-rails-applications-with-the-puma-web-server&quot;&gt;Puma webserver&lt;/a&gt; the new default. Now, when you deploy a Rails 5 app without a &lt;code&gt;Procfile&lt;/code&gt; in your project and Heroku boots your application using &lt;code&gt;$ rails server&lt;/code&gt;, you'll get a performant, production-ready web server &lt;em&gt;by default&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you're upgrading an existing Rails app, you'll want to &lt;a href=&quot;https://devcenter.heroku.com/articles/deploying-rails-applications-with-the-puma-web-server&quot;&gt;manually add Puma to your app&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In addition to shipping with Puma, Rails also generates &lt;a href=&quot;https://github.com/rails/rails/pull/23057&quot;&gt;config/puma.rb&lt;/a&gt; and efforts were made to &lt;a href=&quot;https://github.com/puma/puma/pull/856&quot;&gt;allow Puma to read this config file&lt;/a&gt; when it's booted by the &lt;code&gt;$ rails server&lt;/code&gt; command. This feature is baked into Puma 3.x+, which allows Rails to configure Puma around the number of threads being generated.&lt;/p&gt;

&lt;p&gt;Active Record will generate a pool of five connections by default. These connections are checked out from the pool for the entire duration of the request, so it's critical that for each concurrent request your webserver can handle, you need that many connections in your connection pool. By default, the Puma server starts with up to 16 threads. This means that it can be processing up to 16 different requests at the same time, but since Active Record is limited to five connections, only five of those requests will have access to the database at a time. This means eventually you'll hit this error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ActiveRecord::ConnectionTimeoutError - could not obtain a database connection within 5 seconds. The max pool size is currently 5; consider increasing it
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The solution was to tell Puma that we only want five threads by default. We also wanted a way to re-configure that count without having to commit a change to git, and redeploy for it to take effect. So by default Rails specifies the same number of threads in Puma as Active Record has in its connection pool:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# config/puma.rb

# Puma can serve each request in a thread from an internal thread pool.
# The `threads` method takes a minimum and maximum.
# Any libraries that use thread pools should be configured to match
# the maximum value specified for Puma. Default is set to 5 threads for minimum
# and maximum, this matches the default thread size of Active Record.

threads_count = ENV.fetch(&quot;RAILS_MAX_THREADS&quot;) { 5 }.to_i
threads threads_count, threads_count
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: For a production service there is &lt;a href=&quot;https://github.com/rails/rails/pull/24227#issuecomment-198000472&quot;&gt;little benefit to setting a minimum thread value&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now when you deploy, your Puma thread count will match your Active Record thread count so you won't get timeout errors. Later &lt;a href=&quot;https://github.com/rails/rails/pull/23528&quot;&gt;the default for Active Record was adjusted&lt;/a&gt; to take advantage of the &lt;code&gt;RAILS_MAX_THREADS&lt;/code&gt; environment variable. When you scale your Puma thread count via that environment variable, the Active Record connection pool automatically does the right thing.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;port&quot; href=&quot;#port&quot;&gt;Port&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;On Heroku, we recommend you specify how to run your app via the &lt;a href=&quot;https://devcenter.heroku.com/articles/procfile&quot;&gt;Procfile&lt;/a&gt; ‚Äî if you don't specify a Procfile we will set a default process type for you. Since Heroku apps run inside containers, they need to know which port to connect to, so we set the &lt;code&gt;$PORT&lt;/code&gt; environment variable. The buildpack will specify a web process command if you don't provide one. For example, if you're deploying a Rails 2 app without a &lt;code&gt;Procfile&lt;/code&gt;, by default your app would run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec ruby script/server -p $PORT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In Rails 5 you can now use the &lt;code&gt;$PORT&lt;/code&gt; &lt;a href=&quot;https://github.com/rails/rails/pull/21267&quot;&gt;environment variable to specify what port you want your app to connect to&lt;/a&gt;. This change doesn't really affect how your app runs on Heroku, but if you're trying to run inside of a logic-less build system it can help make it easier to get your application to connect to the right place.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;serving-files-by-default&quot; href=&quot;#serving-files-by-default&quot;&gt;Serving Files by Default&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Prior to Rails 4.2, a Rails app would not serve its own assets. It was assumed that you would always deploy behind some other kind of server such as NGINX that would serve your static files for you. This is still the default behavior, however, new apps can have the &lt;a href=&quot;https://github.com/rails/rails/pull/18100&quot;&gt;static file service turned on via an environment variable&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# config/environments/production.rb

config.public_file_server.enabled = ENV['RAILS_SERVE_STATIC_FILES'].present?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Heroku will set this value when you deploy a Ruby app via the &lt;a href=&quot;https://github.com/heroku/heroku-buildpack-ruby&quot;&gt;Heroku Ruby Buildpack&lt;/a&gt; for Rails 4.2+ apps. Previously you would have to either set this value manually or use the &lt;a href=&quot;https://github.com/heroku/rails_12factor&quot;&gt;rails_12_factor&lt;/a&gt; gem.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;stdout-logging&quot; href=&quot;#stdout-logging&quot;&gt;STDOUT Logging&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;The default logging location in Rails has always been to a file with the name of your environment so production logs go to &lt;code&gt;logs/production.log&lt;/code&gt;. This works well for a traditional deployment but when deploying to a container-based architecture, it makes retrieving and aggregating logs very difficult. Instead, Heroku has advocated for &lt;a href=&quot;http://12factor.net/logs&quot;&gt;logging to STDOUT instead&lt;/a&gt; and treating your logs as streams. These streams can then be directly consumed, fed into a &lt;a href=&quot;https://elements.heroku.com/addons#logging&quot;&gt;logging add-on&lt;/a&gt; for archival, or even used for structured data aggregation.&lt;/p&gt;

&lt;p&gt;The default hasn't changed, but starting in Rails 5, new apps can log to STDOUT via an environment variable&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;if ENV[&quot;RAILS_LOG_TO_STDOUT&quot;].present?
  logger           = ActiveSupport::Logger.new(STDOUT)
  logger.formatter = config.log_formatter
  config.logger = ActiveSupport::TaggedLogging.new(logger)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This value can be set by the container or the platform on which your Rails app runs. In our case, the Ruby buildpack detects your Rails version, and if it's Rails 5 or greater will set the &lt;code&gt;RAILS_LOG_TO_STDOUT&lt;/code&gt; environment variable.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;database_url&quot; href=&quot;#database_url&quot;&gt;DATABASE_URL&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Support for connection to the database specified in &lt;code&gt;$DATABASE_URL&lt;/code&gt; has been around since Rails 3.2, however, there were a large number of &lt;a href=&quot;https://github.com/rails/rails/pull/13578&quot;&gt;bugs and edge cases that weren't completely handled until Rails 4.1&lt;/a&gt;. Prior to Rails 4.1, because the DATABASE_URL integration was not 100% of the way there, Heroku used to write over your &lt;code&gt;config/database.yml&lt;/code&gt; with a file that parsed the environment variable and returned it back as in YAML format. You can see the &lt;a href=&quot;https://github.com/heroku/heroku-buildpack-ruby/blob/a309797a663ca2cf103591aa31caa4bf6dc92e59/lib/language_pack/ruby.rb#L680-L734&quot;&gt;contents of the &quot;magic&quot; database.yml file here&lt;/a&gt;. The biggest problem is that this magic file replacement wasn't expected. People would add config keys for things like &lt;code&gt;pool&lt;/code&gt; which specifies your Active Record connection pool, and it would be silently ignored. So they had to resort to hacks like this code to modify the database configuration&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# Hack, do not use with Rails 4.1+

Rails.application.config.after_initialize do
  ActiveRecord::Base.connection_pool.disconnect!

  ActiveSupport.on_load(:active_record) do
    config = ActiveRecord::Base.configurations[Rails.env] ||
                Rails.application.config.database_configuration[Rails.env]
    config['pool']              = ENV['DB_POOL']      || ENV['MAX_THREADS'] || 5
    ActiveRecord::Base.establish_connection(config)
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Even then, you need to make sure that code gets run correctly in all different ways your app can be booted. For example, if you're preloading your app to take advantage of Copy on Write, you'll need to make sure this code runs in an &quot;after fork&quot; block. While it works around the issue, it normally meant that configuration was spread around an application in many places, and often resulted in different behaviors for different types of dynos.&lt;/p&gt;

&lt;p&gt;After the 4.1 patch, Rails merged configuration from the &lt;code&gt;config/database.yml&lt;/code&gt; and the &lt;code&gt;$DATABASE_URL&lt;/code&gt; environment variable. Heroku no longer needed to over-write your checked-in file, so you can now set pool size directly in your &lt;code&gt;database.yml&lt;/code&gt; file. You can see the &lt;a href=&quot;https://devcenter.heroku.com/articles/rails-database-connection-behavior#configuring-connections-in-rails-4-1&quot;&gt;database connection behavior in Rails 4.1 and beyond explained here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This allows anyone who does not need to configure a database via an environment variable to run exactly as before, but now anyone connecting using the environment variable can keep additional Active Record config in one canonical location. &lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;secret_key_base&quot; href=&quot;#secret_key_base&quot;&gt;SECRET_KEY_BASE&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;At around the time that Rails 4.1 introduced &lt;code&gt;$DATABASE_URL&lt;/code&gt; support, Rails was introducing the secret token store as a new feature. Prior to this feature, there was one secure string that was used to prevent &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-site_request_forgery&quot;&gt;Cross-site request forgery (CSRF)&lt;/a&gt;. Lots of developers forgot that it was in their source, and they would check that into their git repository. It's never a good idea to store secrets in source control, and quite a few applications that were public on GitHub were vulnerable as a result. Now with the introduction of the secret key store, we can &lt;a href=&quot;https://github.com/rails/rails/pull/13703&quot;&gt;set this secret token value&lt;/a&gt; with an environment variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;# Do not keep production secrets in the repository,
# instead read values from the environment.
production:
  secret_key_base: &amp;lt;%= ENV[&quot;SECRET_KEY_BASE&quot;] %&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we do not need to check secure things directly into our application code. With new Rails 4.1+ apps you are required to provide a secret via the &lt;code&gt;SECRET_KEY_BASE&lt;/code&gt; environment variable, or to set the value some other way.&lt;/p&gt;

&lt;p&gt;When deploying a Rails 4.1+ app, Heroku will specify a &lt;code&gt;SECRET_KEY_BASE&lt;/code&gt; on your app by default. It is a good idea to rotate this value periodically. You can see the current value by running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ heroku run bash
Running bash on issuetriage... up, run.8903
~ $ echo $SECRET_KEY_BASE
abcd12345thisIsAMadeUpSecretKeyBaseforThisArticle
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To set a new key you can use&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ heroku config:set SECRET_KEY_BASE=&amp;lt;yournewconfigkeyhere&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: That this may mean that people who are submitting a form in the time between the key change will have an invalid request as the CSRF token will have changed.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;safer-database-actions&quot; href=&quot;#safer-database-actions&quot;&gt;Safer Database Actions&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;One of the scariest things you can say to a co-worker is &quot;I dropped the production database&quot;. While it doesn't happen often, it's a serious enough case to warrant an extra layer of protection. In Rails 5, the database is now aware of the environment that it is run in and by default &lt;a href=&quot;https://github.com/rails/rails/pull/22967&quot;&gt;destructive actions will be prevented on production database&lt;/a&gt;. This means if you are connected to your &quot;production&quot; database and try to run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rake db:drop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or other destructive actions that might delete data from your database you'll get an error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;You are attempting to run a destructive action against your 'production' database
if you are sure you want to continue, run the same command with the environment variable
DISABLE_DATABASE_ENVIRONMENT_CHECK=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While not required to run on Heroku, it's new in Rails 5, and might save you from a minor catastrophe one day. If you're &lt;a href=&quot;https://devcenter.heroku.com/articles/heroku-postgres-plans#plan-tiers&quot;&gt;running on a high enough Postgres plan tier&lt;/a&gt;, you'll also have the ability to &lt;a href=&quot;https://devcenter.heroku.com/articles/heroku-postgres-rollback&quot;&gt;rollback a database to a specific point in time if anything goes wrong&lt;/a&gt;. This is currently available for different durations for all plans Standard and above.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;request-ids&quot; href=&quot;#request-ids&quot;&gt;Request IDs&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Running a Rails app with high traffic can be demanding, especially when you can't even tell which of your log lines go together with a single Request. For example three requests could look something like this in your logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Started GET &quot;/&quot; for 72.48.77.213 at 2016-01-06 20:30:21 +0000
  Rendered welcome/index.html.erb within layouts/application (0.1ms)
Started GET &quot;/&quot; for 72.48.77.213 at 2016-01-06 20:30:22 +0000
Started GET &quot;/&quot; for 72.48.77.213 at 2016-01-06 20:30:23 +0000
  Rendered welcome/index.html.erb within layouts/application (0.1ms)
Processing by WelcomeController#index as HTML
Completed 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms)
Processing by WelcomeController#index as HTML
  Rendered welcome/index.html.erb within layouts/application (0.1ms)
Completed 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms)
  Processing by WelcomeController#index as HTML
Completed 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With Rails 5, the request ID will be logged by default, ensuring each request is tagged with a unique identifier. While they are still interleaved it is possible to figure out which lines belong to which requests. Like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[c6034478-4026-4ded-9e3c-088c76d056f1] Started GET &quot;/&quot; for 72.48.77.213 at 2016-01-06 20:30:21 +0000
[c6034478-4026-4ded-9e3c-088c76d056f1]  Rendered welcome/index.html.erb within layouts/application (0.1ms)
[abuqw781-5026-6ded-7e2v-788c7md0L6fQ] Started GET &quot;/&quot; for 72.48.77.213 at 2016-01-06 20:30:22 +0000
[acfab2a7-f1b7-4e15-8bf6-cdaa008d102c] Started GET &quot;/&quot; for 72.48.77.213 at 2016-01-06 20:30:23 +0000
[abuqw781-5026-6ded-7e2v-788c7md0L6fQ]  Rendered welcome/index.html.erb within layouts/application (0.1ms)
[c6034478-4026-4ded-9e3c-088c76d056f1] Processing by WelcomeController#index as HTML
[c6034478-4026-4ded-9e3c-088c76d056f1] Completed 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms)
[abuqw781-5026-6ded-7e2v-788c7md0L6fQ] Processing by WelcomeController#index as HTML
[abuqw781-5026-6ded-7e2v-788c7md0L6fQ]  Rendered welcome/index.html.erb within layouts/application (0.1ms)
[abuqw781-5026-6ded-7e2v-788c7md0L6fQ] Completed 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms)
[acfab2a7-f1b7-4e15-8bf6-cdaa008d102c]  Processing by WelcomeController#index as HTML
[acfab2a7-f1b7-4e15-8bf6-cdaa008d102c] Completed 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, if you have the logs and you find this unique ID, you can filter to only look at information from that request. So a filtered log output would be very clear:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[c6034478-4026-4ded-9e3c-088c76d056f1] Started GET &quot;/&quot; for 72.48.77.213 at 2016-01-06 20:30:21 +0000
[c6034478-4026-4ded-9e3c-088c76d056f1]  Rendered welcome/index.html.erb within layouts/application (0.1ms)
[c6034478-4026-4ded-9e3c-088c76d056f1] Processing by WelcomeController#index as HTML
[c6034478-4026-4ded-9e3c-088c76d056f1] Completed 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition to this benefit, the request can be set via the &lt;code&gt;X-Request-ID&lt;/code&gt; header so that the same request could be traced between multiple components. For example, a request comes in from the Heroku router which assigns a &lt;a href=&quot;https://devcenter.heroku.com/articles/http-request-id&quot;&gt;request id&lt;/a&gt;. As the request is processed we can log that id, then when the request is passed on to Rails, the same id is used. That way if a problem is determined to be not caused in Rails, it could be traced back to other components with the same ID. This default was added in &lt;a href=&quot;https://github.com/rails/rails/pull/22949&quot;&gt;PR #22949&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is another feature that isn't explicitly required to run on Heroku, however, it will make running an application at scale much easier.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;summary&quot; href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Rails 5 is the easiest to use Rails version on Heroku ever. We also hope that it's the easiest version to run anywhere else. We're happy that the power of &quot;convention over configuration&quot; can be leveraged by container-based deployment platforms to provide a seamless production experience. Many of these features listed such as request IDs and destructive database safeguards are progressive enhancements that will help all app developers regardless of where they deploy or how they run in production. Heroku has been committed to providing the best possible Ruby and Rails experience from its inception, whether that means building out platform features developers need, automating tasks via the buildpack, or working with upstream maintainers. While we want to provide an easy experience, we don't want one that is &lt;a href=&quot;https://blog.codeship.com/programming-magic/&quot;&gt;too &quot;magical&quot;&lt;/a&gt;. By working together in open source we can make software easier to deploy and manage for all developers, not just Heroku customers.&lt;/p&gt;

&lt;p&gt;If you haven't already, try &lt;a href=&quot;https://blog.heroku.com/archives/2016/1/22/rails-5-beta-upgrade&quot;&gt;upgrading to Rails 5 beta&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Check out this Dev Center article for more information on &lt;a href=&quot;https://devcenter.heroku.com/articles/getting-started-with-rails5&quot;&gt;getting started with Rails 5.x on Heroku&lt;/a&gt;.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Speeding up Sprockets</title>
      <link>https://blog.heroku.com/speeding-up-sprockets</link>
      <pubDate>Mon, 22 Feb 2016 00:00:00 GMT</pubDate>
      <guid>https://blog.heroku.com/speeding-up-sprockets</guid>
      <description>&lt;p&gt;The asset pipeline is the slowest part of deploying a Rails app. How slow? On average, it's over 20x slower than installing dependencies via &lt;code&gt;$ bundle install&lt;/code&gt;. Why so slow? In this article, we're going to take a look at some of the reasons the asset pipeline is slow and how we were able to get a 12x performance improvement on some apps with &lt;a href=&quot;https://rubygems.org/gems/sprockets/versions/3.5.2&quot;&gt;Sprockets version 3.3+&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Rails asset pipeline uses the &lt;a href=&quot;https://github.com/rails/sprockets&quot;&gt;sprockets&lt;/a&gt; library to take your raw assets such as javascript or Sass files and pre-build minified, compressed assets that are ready to be served by a production web service. The process is inherently slow. For example, compiling Sass file to CSS requires reading the file in, which involves expensive hard disk reads. Then sprockets processes it, generating a unique &quot;fingerprint&quot; (or digest) for the file before it compresses the file by removing whitespace, or in the case of javascript, running a minifier. All of which is fairly CPU-intensive. Assets can import other assets, so to compile one asset, for example, an &lt;code&gt;app/assets/javascripts/application.js&lt;/code&gt; multiple files may have to be read and stored in memory. In short, sprockets consumes all three of your most valuable resources: memory, disk IO, and CPU.&lt;/p&gt;

&lt;p&gt;Since asset compilation is expensive, the best way to get faster is not to compile. Or at least, not to compile the same assets twice. To do this effectively, we have to store metadata that sprockets needs to build an asset so we can determine which assets have changed and need to be re-compiled. Sprockets provides a cache system on disk at &lt;code&gt;tmp/cache/assets&lt;/code&gt;. If the path and mtime haven't changed for an asset then we can load the entire asset from disk. To accomplish this task, sprockets uses the cache to store a compiled file's digest.&lt;/p&gt;

&lt;p&gt;This code looks something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# https://github.com/rails/sprockets/blob/543a5a27190c26de8f3a1b03e18aed8da0367c63/lib/sprockets/base.rb#L46-L57

def file_digest(path)
  if stat = File.stat(path)
    cache.fetch(&quot;file_digest:#{path}:#{stat.mtime.to_i}&quot;) do
      Digest::SHA256.file(path.to_s).digest
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have a file's digest, we can use this information to load the asset. Can you spot the problem with the code above?&lt;/p&gt;

&lt;p&gt;If you can't, I don't blame you‚Äîthe variables are misleading. &lt;code&gt;path&lt;/code&gt; should have been renamed &lt;code&gt;absolute_path&lt;/code&gt; as that's what's passed into this method. So if you precompile your project from different directories, you'll end up with different cache keys. Depending on the root directory where it was compiled, the same file could generate a cache key of:
&lt;code&gt;&quot;file_digest:/Users/schneems/my_project/app/assets/javascripts/application.js:123456&quot;&lt;/code&gt; 
or: 
&lt;code&gt;&quot;file_digest:/+Other/path/+my_project/app/assets/javascripts/application.js:123456&quot;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;There are quite a few Ruby systems deployed using Capistrano, where it's common to upload different versions to new directories and setup symlinks so that if you need to rollback a bad deploy you only have to update symlinks. When you try to re-use a cache directory using this deploy strategy, the cache keys end up being different every time. So even when you don't need to re-compile your assets, sprockets will go through the whole process only stopping at the very last step when it sees the file already exists:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# https://github.com/rails/sprockets/blob/543a5a27190c26de8f3a1b03e18aed8da0367c63/lib/sprockets/manifest.rb#L182-L187

if File.exist?(target)
  logger.debug &quot;Skipping #{target}, already exists&quot;
else
  logger.info &quot;Writing #{target}&quot;
  asset.write_to target
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sprockets 3.x+ is not using anything in the cache, and as has been reported in &lt;a href=&quot;https://github.com/rails/sprockets/issues/59&quot;&gt;issue #59&lt;/a&gt;, unless you're in debug mode, you wouldn't know there's a problem, because nothing is logged to standard out.&lt;/p&gt;

&lt;p&gt;It turns out it's not just an issue for people deploying via Capistrano. Every time you run a &lt;code&gt;$ git push heroku master&lt;/code&gt; your build happens on a different temp path that is passed into the buildpack. So even though Heroku stores the cache between deploys, the keys aren't reused.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;the-almost-fix&quot; href=&quot;#the-almost-fix&quot;&gt;The (almost) fix&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;The first fix was very straightforward. A &lt;a href=&quot;https://github.com/rails/sprockets/pull/89&quot;&gt;new helper class&lt;/a&gt; called &lt;code&gt;UnloadedAsset&lt;/code&gt; takes care of generating cache keys and converting absolute paths to relative ones:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;UnloadedAsset.new(path, self).file_digest_key(stat.mtime.to_i)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In our previous example we would get a cache key of &lt;code&gt;&quot;file_digest:/app/assets/javascripts/application.js:123456&quot;&lt;/code&gt; regardless of which directory you're in. So we're done, right?&lt;/p&gt;

&lt;p&gt;As it turns out, cache keys were only part of the problem. To understand why we must look at how sprockets is using our 'file_digest_key'.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;pulling-an-asset-from-cache&quot; href=&quot;#pulling-an-asset-from-cache&quot;&gt;Pulling an asset from cache&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Having an asset's digest isn't enough. We need to make sure none of its dependencies have changed. For example, to use the jQuery library inside another javascript file, we'd use the &lt;code&gt;//= require&lt;/code&gt; directive like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;js&quot;&gt;//= require jquery
//= require ./foo.js

var magicNumber = 42;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If either &lt;code&gt;jquery&lt;/code&gt; or &lt;code&gt;foo.js&lt;/code&gt; change, then we must recompute our asset. This is a somewhat trivial example, but each required asset could require another asset. So if we wanted to find all dependencies, we would have to read our primary asset into memory to see what files it's requiring and then read in all of those other files; exactly what we're trying to avoid. So sprockets stores dependency information in the cache.&lt;/p&gt;

&lt;p&gt;Using this cache key:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;&quot;asset-uri-cache-dependencies:#{compressed_path}:#{ @env.file_digest(filename) }&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sprockets will return a set of &quot;dependencies.&quot;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#&amp;lt;Set: {&quot;file-digest///Users/schneems/ruby/2.2.3/gems/jquery-rails-4.0.4&quot;, &quot;file-digest///Users/schneems/app/assets/javascripts/foo.js&quot;}&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To see if either of these has changed, Sprockets will pull their digests from the cache like we did with our first &lt;code&gt;application.js&lt;/code&gt; asset. These are used to &quot;resolve&quot; an asset. If the resolved assets (and their dependencies) have been previously loaded and stored in the cache, then we can pull our asset from cache:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# https://github.com/rails/sprockets/blob/9ca80fe00971d45ccfacb6414c73d5ffad96275f/lib/sprockets/loader.rb#L55-L58

digest = DigestUtils.digest(resolve_dependencies(paths))
if uri_from_cache = cache.get(unloaded.digest_key(digest), true)
  asset_from_cache(UnloadedAsset.new(uri_from_cache, self).asset_key)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But now, our dependencies contain the full path. To fix this, we have to &quot;compress&quot; any absolute paths, so that if they're relative to the root of our project we only store a relative path.&lt;/p&gt;

&lt;p&gt;Of course, it's never that simple.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;absolute-paths-everywhere&quot; href=&quot;#absolute-paths-everywhere&quot;&gt;Absolute paths everywhere&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;In the last section I mentioned that we would get a file digest by resolving an asset from `&quot;file-digest///Users/schneems/app/assets/javascripts/foo.js&quot;. That turns out to be a pretty involved process. It involves a bunch of other data from the cache, which as you guessed, can have absolute file paths. The short list includes: Asset filenames, asset URIs, load paths, and included paths, all of which we handled in &lt;a href=&quot;https://github.com/rails/sprockets/pull/101&quot;&gt;Pull Request #101&lt;/a&gt;. But wait, we're not finished, the list goes on: Stubbed paths, link paths, required paths (not the same as dependencies), and sass dependencies, all of which we handled in &lt;a href=&quot;https://github.com/rails/sprockets/pull/109&quot;&gt;Pull Request #109&lt;/a&gt;, phew.&lt;/p&gt;

&lt;p&gt;The final solution? A pattern of &quot;compressing&quot; URIs and absolute paths, before they were added to the cache and &quot;expanding&quot; them to full paths as they're taken out. &lt;a href=&quot;https://github.com/rails/sprockets/blob/9ca80fe00971d45ccfacb6414c73d5ffad96275f/lib/sprockets/uri_tar.rb&quot;&gt;URITar&lt;/a&gt; was introduced to handle this compression/expansion logic.&lt;/p&gt;

&lt;p&gt;All of this is available in &lt;a href=&quot;https://rubygems.org/gems/sprockets/versions/3.3.3&quot;&gt;Sprockets version 3.3+&lt;/a&gt;.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;portability-for-all&quot; href=&quot;#portability-for-all&quot;&gt;Portability for all&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;When tested with an example app, we saw virtually no change to the initial compile time (around 38 seconds). The second compile? 3 seconds. Roughly a 1,200% speed increase when using compiled assets and deploying using Capistrano or Heroku. Not bad.&lt;/p&gt;

&lt;p&gt;Parts of the &lt;code&gt;URITar&lt;/code&gt; class were not written with multiple filesystems in mind, notably Windows, which was fixed in &lt;a href=&quot;https://github.com/rails/sprockets/pull/125/commits&quot;&gt;Pull Request #125&lt;/a&gt; and released in version 3.3.4. If you're going to write code that touches the filesystems of different operating systems, remember to use a portable interface.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;into-the-future&quot; href=&quot;#into-the-future&quot;&gt;Into the future&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Sprockets was originally authored by one prolific programmer, &lt;a href=&quot;https://github.com/rails/sprockets/graphs/contributors&quot;&gt;Josh Peek&lt;/a&gt;. He's since stepped away from the project and has given maintainership to the Rails core team. Sprockets 4 is being worked on with support for source maps. If you're running a version of Sprockets 2.x you should try to upgrade to Sprockets 3.5+, as Sprockets 3 is intended to be an upgrade path to Sprockets 4. For help upgrading see the &lt;a href=&quot;https://github.com/rails/sprockets/blob/3.x/UPGRADING.md&quot;&gt;upgrade docs in the 3.x branch&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Sprockets version 3.0 beta was released in September 2014; it took nearly a year for a bug report to come in alerting maintainers to the problem. In addition to upgrading Sprockets, I invite you to open up issues at &lt;a href=&quot;https://github.com/rails/sprockets&quot;&gt;rails/sprockets&lt;/a&gt; and let us know about bugs in the latest released version of Sprockets. Without bug reports and example apps to reproduce problems, we can't make the library better.&lt;/p&gt;

&lt;p&gt;This performance patch was much more involved than I could have imagined when I got started, but I'm very pleased with the results. I'm excited to see how this affects overall performance numbers at Heroku‚Äîhopefully you'll be able to see some pretty good speed increases.&lt;/p&gt;

&lt;p&gt;Thanks for reading, now go and upgrade your sprockets.&lt;/p&gt;

&lt;p&gt;Schneems writes code for Heroku and likes working on open source performance patches. You can find him on his &lt;a href=&quot;http://www.schneems.com&quot;&gt;personal site&lt;/a&gt;.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Upgrading to Rails 5 Beta - The Hard Way</title>
      <link>https://blog.heroku.com/rails-5-beta-upgrade</link>
      <pubDate>Fri, 22 Jan 2016 18:25:00 GMT</pubDate>
      <guid>https://blog.heroku.com/rails-5-beta-upgrade</guid>
      <description>&lt;p&gt;Rails 5 has been brewing for more than a year. To take advantage of new features, and stay on the supported path, you'll need to upgrade. In this post, we'll look at the upgrade process for a production Rails app, &lt;a href=&quot;http://www.codetriage.com&quot;&gt;codetriage.com&lt;/a&gt;. The codebase is open source so you &lt;a href=&quot;https://github.com/codetriage/codetriage/pull/435&quot;&gt;can follow along&lt;/a&gt;. Special thanks to &lt;a href=&quot;https://twitter.com/_cha1tanya&quot;&gt;Prathamesh&lt;/a&gt; for his help with this blog post.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;how-stable-is-the-beta&quot; href=&quot;#how-stable-is-the-beta&quot;&gt;How Stable is the Beta?&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;In Rails a beta means the API is not yet stable, and features will come and go. A Release Candidate (RC) means no new features; the API is considered stable, and RCs will continue to be released until all reported regressions are resolved.&lt;/p&gt;

&lt;p&gt;Should you run your production app on the beta? There is value in getting a beta working on a branch and being ready when the RC or upcoming release is available. Lots of companies run Beta and RC releases of Rails in production, but it's not for the faint of heart. You'll need to be pretty confident in your app, make sure your test suite is up to par, and that manual quality control (QC) checks are thorough. It's always a relief to find and fix bugs before they arrive in production. Please report regressions and bugs you encounter -- the faster we uncover and report them, the faster these bugs get fixed, and the more stable Rails becomes. Remember, no one else is going to find and report the regressions in your codebase unless you do it.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;upgrade-your-gemfile&quot; href=&quot;#upgrade-your-gemfile&quot;&gt;Upgrade your Gemfile&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Step zero of the process is changing your Rails dependency, after which you'll want to &lt;code&gt;$ bundle install&lt;/code&gt;, see what is incompatible and update those dependencies.&lt;/p&gt;

&lt;p&gt;If you want to run on Heroku, I recommend avoiding the beta1 release on rubygems.org. It doesn't include a &lt;a href=&quot;https://github.com/rails/rails/pull/22933&quot;&gt;fix to stdout logging&lt;/a&gt; that is available in master. CodeTriage is running with this SHA of Rails from master:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# Gemfile
gem &quot;rails&quot;, github: &quot;rails/rails&quot;, ref: &quot;dbf67b3a6f549769c5f581b70bc0c0d880d5d5d1&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You'll need to make sure that you're running a current Ruby; Rails now requires Ruby 2.2.2 or greater. At the time of writing, I recommend 2.2.4 or 2.3.0. CodeTriage is running 2.3.0.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;# Gemfile
ruby &quot;2.3.0&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you can't bundle, you'll need to modify your dependencies in your Gemfile, or &lt;code&gt;bundle update &amp;lt;dependency&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Once you've completed a &lt;code&gt;$ bundle install&lt;/code&gt; you're well on your way to getting the upgrade started.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;gems-gems-gems&quot; href=&quot;#gems-gems-gems&quot;&gt;Gems Gems Gems&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Just because a gem installs correctly doesn't mean it's compatible with Rails 5. Most libraries that specify Rails as a runtime dependency do not specify an upper bounds in their gemspec (like a Gemfile for gems). If they said &quot;this gem is valid for rails versions 3-4&quot; then bundler would let you know that it couldn't install that gem. Unfortunately since most say something like &quot;this is good for Rails version 3 to infinity,&quot; there's no way by versioning alone. You'll have to investigate which of your dependencies are compatible manually; don't worry, there are some easy ways to tell.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;rails-console&quot; href=&quot;#rails-console&quot;&gt;Rails Console&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;The first thing you'll want to do is boot a console:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;$ rails console
Loading development environment (Rails 5.0.0.beta1)
irb(main):001:0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Did that work? Great, skip to the next step. If not, use backtraces to see where errors came from, and which gems might be having problems.&lt;/p&gt;

&lt;p&gt;When you get a problem with a gem, check &lt;a href=&quot;https://rubygems.org&quot;&gt;https://rubygems.org&lt;/a&gt; and see if you're using the latest. If not, it's a good idea to try using the most recently released version. If you still get an error, try pointing at their master branch, some libraries have fixes for Rails 5 that haven't been released yet. Many libraries that depend on Rack have not released a version that supports Rack 2, however many of them have support for it in master or in another branch. When in doubt ask a gem maintainer.&lt;/p&gt;

&lt;p&gt;Here's an example of an error that CodeTriage saw:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;$ rails console
/Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/failure_app.rb:9:in `&amp;lt;class:FailureApp&amp;gt;': uninitialized constant ActionController::RackDelegation (NameError)
  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/failure_app.rb:8:in `&amp;lt;module:Devise&amp;gt;'
  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/failure_app.rb:3:in `&amp;lt;top (required)&amp;gt;'
  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/mapping.rb:122:in `default_failure_app'
  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/mapping.rb:67:in `initialize'
  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise.rb:325:in `new'
  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise.rb:325:in `add_mapping'
  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/rails/routes.rb:238:in `block in devise_for'
  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/rails/routes.rb:237:in `each'
  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/rails/routes.rb:237:in `devise_for'
  from /Users/richardschneeman/Documents/projects/codetriage/config/routes.rb:9:in `block in &amp;lt;top (required)&amp;gt;'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/actionpack/lib/action_dispatch/routing/route_set.rb:389:in `instance_exec'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/actionpack/lib/action_dispatch/routing/route_set.rb:389:in `eval_block'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/actionpack/lib/action_dispatch/routing/route_set.rb:371:in `draw'
  from /Users/richardschneeman/Documents/projects/codetriage/config/routes.rb:3:in `&amp;lt;top (required)&amp;gt;'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:40:in `block in load_paths'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:40:in `each'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:40:in `load_paths'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:16:in `reload!'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:26:in `block in updater'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/activesupport/lib/active_support/file_update_checker.rb:75:in `execute'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:27:in `updater'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:7:in `execute_if_updated'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/finisher.rb:69:in `block in &amp;lt;module:Finisher&amp;gt;'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/initializable.rb:30:in `instance_exec'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/initializable.rb:30:in `run'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/initializable.rb:55:in `block in run_initializers'
  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:228:in `block in tsort_each'
  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:350:in `block (2 levels) in each_strongly_connected_component'
  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:431:in `each_strongly_connected_component_from'
  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:349:in `block in each_strongly_connected_component'
  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:347:in `each'
  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:347:in `call'
  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:347:in `each_strongly_connected_component'
  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:226:in `tsort_each'
  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:205:in `tsort_each'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/initializable.rb:54:in `run_initializers'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application.rb:350:in `initialize!'
  from /Users/richardschneeman/Documents/projects/codetriage/config/environment.rb:5:in `&amp;lt;top (required)&amp;gt;'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application.rb:326:in `require_environment!'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/commands/commands_tasks.rb:157:in `require_application_and_environment!'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/commands/commands_tasks.rb:77:in `console'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/commands/commands_tasks.rb:49:in `run_command!'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/command.rb:20:in `run'
  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/commands.rb:19:in `&amp;lt;top (required)&amp;gt;'
  from bin/rails:4:in `require'
  from bin/rails:4:in `&amp;lt;main&amp;gt;'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The gem in the backtrace above is devise 3.5.3; we were able to get things working by pointing to master, which may be released by now.&lt;/p&gt;

&lt;p&gt;Repeat this until your &lt;code&gt;rails console&lt;/code&gt; works. If you're already on the master version of a gem and it's still failing to boot, it might not support Rails 5 yet. Try debugging a little, and see if there's any open issues about Rails 5 on the tracker. If you don't see anything, open a new issue giving your error message and description of what's going on, and try to be as instructive as possible. Before you do this, you might want to try using the master version of their gem in a bare-bones &lt;code&gt;rails new&lt;/code&gt; app to see if you can reproduce the problem. If the problem doesn't reproduce it may be an issue with how you're using the gem instead of the gem itself. This will help you uncover differences there. If you do reproduce the problem, then push it up to GitHub and share it in the issue. The maintainer will be able to fix an easily reproducible example much faster.&lt;/p&gt;

&lt;p&gt;When you're not able to upgrade around a fix, sometimes you might not need that gem, or you might want to remove it temporarily so you can work on upgrading the rest of your app while the maintainer works on a fix.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;shim-gems&quot; href=&quot;#shim-gems&quot;&gt;Shim Gems&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Sometimes functionality is taken out of Rails but provided via smaller gems. For example. While upgrading we had to use &lt;code&gt;record_tag_helper&lt;/code&gt; and &lt;code&gt;rails-controller-testing&lt;/code&gt; to get the app working.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;rails-server&quot; href=&quot;#rails-server&quot;&gt;Rails Server&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Once you've got your console working, try your server. Just like with console, get it to boot. Once it's booted try hitting the main page and some other actions. You'll be looking for exceptions as well as strange behavior, though not all bugs cause exceptions. In the case of CodeTriage, a strange thing was happening. An error was getting thrown, but there was nothing in the logs and no debug error page in development. When this happens it's usually a bug in a rack middleware. You can get a list of them by running&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;$ rake middleware
use Rack::Sendfile
use ActionDispatch::LoadInterlock
use ActiveSupport::Cache::Strategy::LocalCache::Middleware
use Rack::Runtime
use Rack::MethodOverride
use ActionDispatch::RequestId
use Rails::Rack::Logger
use ActionDispatch::ShowExceptions
use WebConsole::Middleware
use ActionDispatch::DebugExceptions
use ActionDispatch::RemoteIp
use ActionDispatch::Reloader
use ActionDispatch::Callbacks
use ActiveRecord::Migration::CheckPending
use ActiveRecord::ConnectionAdapters::ConnectionManagement
use ActiveRecord::QueryCache
use ActionDispatch::Cookies
use ActionDispatch::Session::CookieStore
use ActionDispatch::Flash
use Rack::Head
use Rack::ConditionalGet
use Rack::ETag
use Warden::Manager
use ActionView::Digestor::PerRequestDigestCacheExpiry
use Bullet::Rack
use OmniAuth::Strategies::GitHub
run CodeTriage::Application.routes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The request enters at the top and goes to each middleware until finally it hits our application on the bottom &lt;code&gt;CodeTriage::Application.routes&lt;/code&gt;. If the error was being thrown by our app, then the &lt;code&gt;ActionDispatch::DebugExceptions&lt;/code&gt; middleware outputs error logs and would show a nice exception page in development. In my case that wasn't happening, so I knew the error was somewhere before that point. &lt;/p&gt;

&lt;p&gt;I bisected the problem by adding logging to &lt;code&gt;call&lt;/code&gt; methods. If the output showed up in STDOUT then I knew it was being called successfully and I should go higher up the Rack stack (which is confusing, because it is lower on this middleware list). After enough debugging, I finally found the exception was coming from &lt;code&gt;WebConsole::Middleware&lt;/code&gt;, which is naturally the last possible place an exception could occur before it gets caught by the &lt;code&gt;DebugExceptions&lt;/code&gt; middleware. Upgrading to master on web-console gem fixed the error. It turns out that placement is not by accident and web-console inserts itself before the debug middleware. I &lt;a href=&quot;https://github.com/rails/web-console/issues/178&quot;&gt;opened an issue with web-console&lt;/a&gt; and &lt;a href=&quot;https://github.com/gsamokovarov&quot;&gt;@gsamokovarov&lt;/a&gt; promptly added some code that detects when an internal error is thrown by web-console and makes sure it shows up in the logs. My rule of thumb is if something takes me over an hour to fix that could have been made much easier by exposing the errant behavior (such as logging an exception), then I report it as an issue to raise awareness of that failure mode. Sometimes the bug is well known but often maintainers don't know what is difficult or hard to debug. If you have ideas on things to add to make using a piece of software easier, sharing it in a friendly and helpful way is good for everyone. &lt;/p&gt;

&lt;p&gt;Keep manually testing your app until no errors and all errant behavior is gone. Once you've done this, you're on the home stretch.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;testing&quot; href=&quot;#testing&quot;&gt;Testing&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Now the server and console are working, you want to get your test suite green &lt;code&gt;$ rake test&lt;/code&gt;. Rails 5 now includes a nice test runner &lt;code&gt;$ rails test&lt;/code&gt;; you can specify a particular file or a specific line to run &lt;code&gt;$ rails test test/unit/users.rb:58&lt;/code&gt;. &lt;/p&gt;

&lt;p&gt;This step was bad, resulting in cryptic failures. I recommend picking 1 failure or error and focusing on it. Sometimes you'll see 100 failures, but when you fix one, it resolves them all, as they were the same issue.&lt;/p&gt;

&lt;p&gt;For CodeTriage, we were getting errors&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;UserUpdateTest#test_updating_the_users_skip_issues_with_pr_setting_to_true:
NoMethodError: undefined method `normalize_params' for Rack::Utils:Module
    test/integration/user_update_test.rb:50:in `block in &amp;lt;class:UserUpdateTest&amp;gt;'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A grep of my project indicated &lt;code&gt;normalize_params&lt;/code&gt; wasn't being used. The error didn't have a long backtrace. On that line &lt;code&gt;test/integration/user_update_test.rb:50&lt;/code&gt; we are using a &lt;a href=&quot;http://jnicklas.github.io/capybara/&quot;&gt;capybara&lt;/a&gt; helper:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;click_button 'Save'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On a whim, I updated capybara and it resolved the error. Not sure why the backtrace was so short, that might be worth digging into later, as it would have made debugging faster.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;other-gotchas&quot; href=&quot;#other-gotchas&quot;&gt;Other gotchas&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;The Strong Params that you get in your controller have changed and are no longer inheriting from a hash. At the time, there was no deprecation, so I &lt;a href=&quot;https://github.com/rails/rails/pull/23123&quot;&gt;added one&lt;/a&gt;. It's just a deprecation, but you should still make sure you're only using the approved API.&lt;/p&gt;

&lt;p&gt;There was a change to &lt;code&gt;image_tag&lt;/code&gt; where it no longer takes &lt;code&gt;nil&lt;/code&gt; as an argument, this wasn't used in production, but the tests (accidentally) depended on it.&lt;/p&gt;

&lt;p&gt;With all of this together, we were ready for Rails 5 in the prime time. Here's the &lt;a href=&quot;https://github.com/codetriage/codetriage/pull/435&quot;&gt;PR&lt;/a&gt;&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;after-the-upgrade&quot; href=&quot;#after-the-upgrade&quot;&gt;After the Upgrade&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;We're almost done! The last thing we need to do is to look for deprecations. They'll show up in your logs like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;DEPRECATION WARNING: ActionController::TestCase HTTP request methods will accept only
keyword arguments in future Rails versions.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;DEPRECATION WARNING: `redirect_to :back` is deprecated and will be removed from Rails 5.1. Please use `redirect_back(fallback_location: fallback_location)` where `fallback_location` represents the location to use if the request has no HTTP referer information. (called from block in &amp;lt;class:RepoSubscriptionsControllerTest&amp;gt; at /Users/richardschneeman/Documents/projects/codetriage/test/functional/repo_subscriptions_controller_test.rb:63)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A good deprecation will include the fix in the message. While you technically don't &lt;strong&gt;have&lt;/strong&gt; to fix every deprecation, you'll be glad you did when Rails 5.1 is released, because that upgrade will be even easier.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;regressions&quot; href=&quot;#regressions&quot;&gt;Regressions&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;As you rule out bugs coming from gems or your own code, you'll want to report any regressions to &lt;a href=&quot;https://github.com/rails/rails/issues&quot;&gt;https://github.com/rails/rails/issues&lt;/a&gt;. Please check for existing issues first. Unfortunately the tracker is only for bugs and pull requests, we can't use it to help you with &quot;my application won't work&quot; type problems. Take those issues to &lt;a href=&quot;http://stackoverflow.com&quot;&gt;Stack Overflow&lt;/a&gt;.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;deploy-and-you-39-re-done&quot; href=&quot;#deploy-and-you-39-re-done&quot;&gt;Deploy and You're Done&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Once your code works locally and your tests pass, make sure you're happy with manually looking for regressions. I recommend either deploying to a staging app first or using Heroku's &lt;a href=&quot;https://devcenter.heroku.com/articles/github-integration#review-apps&quot;&gt;GitHub Review apps&lt;/a&gt;. -- That way you'll have a production-ish copy of your web app attached to your pull request that everyone involved can review. Once you're happy commit to git and then:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;$ git push heroku master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Watch your logs or bug tracking add-on for exceptions, remember you can always &lt;a href=&quot;https://devcenter.heroku.com/articles/releases#rollback&quot;&gt;rollback&lt;/a&gt; if a critical problem comes up.&lt;/p&gt;

&lt;p&gt;Upgrading takes time and patience but isn't that complex. The earlier you upgrade the earlier bugs get found, reported, and fixed. Give it a try on a branch and see what happens.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Patching Rails Performance</title>
      <link>https://blog.heroku.com/patching-rails-performance</link>
      <pubDate>Thu, 06 Aug 2015 00:00:00 GMT</pubDate>
      <guid>https://blog.heroku.com/patching-rails-performance</guid>
      <description>&lt;p&gt;In a recent &lt;a href=&quot;https://github.com/rails/rails/pull/21057&quot;&gt;patch&lt;/a&gt; we improved Rails response time by &lt;strong&gt;&amp;gt;10%&lt;/strong&gt;, our largest improvement to date. I'm going to show you how I did it, and introduce you to the tools I used, because.. who doesn‚Äôt want fast apps?&lt;/p&gt;

&lt;p&gt;In addition to a speed increase, we see a 29% decrease in allocated objects. If you haven't already, you can &lt;a href=&quot;http://www.schneems.com/2015/05/11/how-ruby-uses-memory.html&quot;&gt;read or watch more about how temporary allocated objects affect total memory use&lt;/a&gt;. Decreasing memory pressure on an app may allow it to be run on a smaller dyno type, or spawn more worker processes to handle more throughput. Let's back up though, how did I find these optimizations in Rails in the first place?&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;A year ago Heroku added &lt;a href=&quot;https://devcenter.heroku.com/articles/metrics&quot;&gt;metrics&lt;/a&gt; to the application dashboard. During the internal beta, one of the employees building the product asked if they could get access to my open source app, codetriage, because it was throwing thousands of &lt;a href=&quot;https://devcenter.heroku.com/articles/error-codes#r14-memory-quota-exceeded&quot;&gt;R14-out of memory&lt;/a&gt; errors a day. This error occurs when you go over your allotted RAM limit on Heroku (512 mb for a hobby dyno). Before metrics, I had no clue. As the feature was made available to customers,  they became  acutely aware that their apps were slow, used lots of swap memory, and threw errors right and left. What should they do to get their apps back in shape?&lt;/p&gt;

&lt;p&gt;Initially we recommended reducing the number of web worker processes. For example if you were running 3 &lt;a href=&quot;https://devcenter.heroku.com/articles/rails-unicorn&quot;&gt;Unicorn workers&lt;/a&gt; (our recommended webserver at the time) we might suggest you decrease it to 2. This solved the problem for most people. Exposing the RAM usage helped tremendously. Still, customers reported &quot;memory leaks&quot; and overall they weren‚Äôt happy with their memory use. When our largest customers started to ask questions, they landed on my desk.&lt;/p&gt;

&lt;p&gt;It wasn't long before a customer came forward with an app that performed normally on the surface but started to swap quickly. With the owner's permission I was able to use their Gemfile locally, and started to write a &lt;a href=&quot;https://github.com/schneems/derailed_benchmarks&quot;&gt;series of re-useable benchmarks&lt;/a&gt; to reproduce the problem. A concept similar to my &lt;a href=&quot;http://engineering.heroku.com/blogs/2014-11-03-benchmarking-rack-middleware/&quot;&gt;benchmarking rack middleware&lt;/a&gt; article, since Rails is a Rack app. I worked on the original concept with &lt;a href=&quot;https://github.com/ko1&quot;&gt;Koichi&lt;/a&gt;. We were able to isolate that particular issue to &lt;a href=&quot;http://www.schneems.com/2014/11/07/i-ram-what-i-ram.html&quot;&gt;a few problematic gems&lt;/a&gt; that retained a large amount of memory on boot. Meanwhile &lt;a href=&quot;http://samsaffron.com/&quot;&gt;Sam Saffron was writing some amazing posts&lt;/a&gt; about debugging memory in &lt;a href=&quot;http://www.discourse.org&quot;&gt;discourse&lt;/a&gt; which eventually spawned &lt;a href=&quot;https://rubygems.org/gems/memory_profiler&quot;&gt;memory_profiler&lt;/a&gt;. I added the memory_profiler to &lt;a href=&quot;https://github.com/schneems/derailed_benchmarks#dissecting-a-memory-leak&quot;&gt;derailed benchmarks&lt;/a&gt;, this is eventually what I used to find hot spots for this performance patch.&lt;/p&gt;

&lt;p&gt;I'm responsible for maximizing Ruby developer happiness on Heroku. This can mean writing documentation, patching the buildpack to stop pain points before they happen, or working upstream with popular open source libraries to resolve problems before they hit production. The longer I look at slow code or code with a large memory footprint, the more I see these things as reproducible and ultimately fixable bugs. As I was seeing issues in customer's reported apps and in some of my own, I went to the source to try to fix the issues.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ derailed exec perf:objects
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I ran this benchmark against my Rails app, identified a line that was allocating a large amount of memory, and refactored. In some cases we were using arrays only to join them into strings in the same method, other times we were duplicating a hash before it was merged. I slowly whittled down the allocated object count. Allocating objects takes time. If we modify an object in place without creating a duplicate, we can speed up program execution. Alternatively, we can use a pooled object like a frozen string that never needs to be re-allocated. The test app, codetriage.com, uses &lt;a href=&quot;https://github.com/codetriage/codetriage/blob/630149788431e629e3b8a261c3cb9a8e1e11da5a/config/routes.rb#L34-L45&quot;&gt;github style routes&lt;/a&gt; which requires constraints and a &quot;catch all&quot; &lt;a href=&quot;http://guides.rubyonrails.org/routing.html#route-globbing-and-wildcard-segments&quot;&gt;glob route&lt;/a&gt;. Applying these optimizations resulted in a 31% speed increase in url generation for a route with a constraint:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;require 'benchmark/ips'
repo = Repo.first

Benchmark.ips do |x|
  x.report(&quot;link_to speed&quot;) {
    helper.link_to(&quot;foo&quot;, app.repo_path(repo))
  }
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The routing improvements combined with all the other savings gives us a speed boost of more than 10%. The best part is, we don't have to change the way we write our Rails app, you get these improvements for free by upgrading to the next version of Rails. If you're interested in where the savings came from, look at the &lt;a href=&quot;https://github.com/rails/rails/pull/21057/commits&quot;&gt;individual commits&lt;/a&gt; which have the methodology and object allocation savings for my test app recorded.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1488278435-Screenshot%2525202015-08-03%25252011.38.33.png%253Fdl%253D1&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Working with Rails has made me a better developer, more capable of debugging library internals, and helped our Ruby experience on the platform. Likewise having a huge number of diverse Rails apps running on the platform helps us be aware of actual pain points developers are hitting in production. It feels good when we can take these insights and contribute back to the community. Be on the lookout for the Rails 5 pre-release to give some of these changes a try. Thanks for reading, enjoy the speed.&lt;/p&gt;

&lt;p&gt;If you like hording RAM, maximizing iterations per second, or just going plain fast follow &lt;a href=&quot;https://twitter.com/schneems&quot;&gt;@schneems&lt;/a&gt;.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Debugging Super Methods with Ruby 2.2</title>
      <link>https://blog.heroku.com/debugging-support-methods-ruby22</link>
      <pubDate>Wed, 14 Jan 2015 00:00:00 GMT</pubDate>
      <guid>https://blog.heroku.com/debugging-support-methods-ruby22</guid>
      <description>&lt;p&gt;Debugging a large codebase is hard. Ruby makes debugging easier by exposing &lt;a href=&quot;http://www.ruby-doc.org/core-2.2.0/Method.html&quot;&gt;method metadata&lt;/a&gt; and &lt;a href=&quot;http://www.ruby-doc.org/core-2.2.0/Kernel.html#method-i-caller&quot;&gt;caller stack&lt;/a&gt; inside Ruby's own process. Recently in Ruby 2.2.0 this meta inspection got another useful feature by exposing &lt;a href=&quot;https://bugs.ruby-lang.org/issues/9781&quot;&gt;super method metadata&lt;/a&gt;. In this post we will look at how this information can be used to debug and why it needed to be added.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;One of the first talks I ever wrote was &quot;Dissecting Ruby With Ruby&quot; all about inspecting and debugging Ruby processes using nothing but Ruby code. If you've never heard of the &lt;a href=&quot;http://ruby-doc.org/core-2.2.0/Method.html&quot;&gt;Method  method&lt;/a&gt; it's worth a watch.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/UYVUSoNrM-c&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;In short, Ruby knows how to execute your code, as well as where your code was defined. For example, with this small class:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;class Dog
  def bark
    puts &quot;woof&quot;
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can see exactly where &lt;code&gt;Dog#bark&lt;/code&gt; is defined:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;puts Dog.new.bark
# =&amp;gt; &quot;woof&quot;
puts Dog.new.method(:bark).source_location.inspect
# =&amp;gt; [&quot;/tmp/dog.rb&quot;, 2]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Even if someone did some crazy metaprogramming or you accidentally over-wrote the method, Ruby will always tell you the location of the method it will call.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;super-problems&quot; href=&quot;#super-problems&quot;&gt;Super problems&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;If you've &lt;a href=&quot;https://www.youtube.com/watch?v=UYVUSoNrM-c&quot;&gt;seen the &quot;Dissecting Ruby&quot; talk&lt;/a&gt;, you'll know that there is a big problem with the super method. It's almost impossible to tell where the final method location being called is written.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;class SchneemsDog &amp;lt; Dog
  def bark
    super
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I ended up using some metaprogramming to figure this out:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;cinco = SchneemsDog.new
cinco.class.superclass.instance_method(:bark)
# =&amp;gt; [&quot;/tmp/dog.rb&quot;, 6]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works, but it wouldn't if we did certain types of metaprogramming. For example, we would get the wrong answer if we did this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;module DoubleBark
  def bark
    super
    super
  end
end
cinco = SchneemsDog.new
cinco.extend(Doublebark)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case, &lt;code&gt;cinco.bark&lt;/code&gt; will call the method defined in the &lt;code&gt;Doublebark&lt;/code&gt; module:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;cinco.bark
# =&amp;gt; bark
# =&amp;gt; bark

puts cinco.method(:bark)
#&amp;lt;Method: SchneemsDog(DoubleBark)#bark&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The actual &quot;super&quot; being referred to is defined in the &lt;code&gt;SchneemsDog&lt;/code&gt; class. However, the code tells us that the method is in the &lt;code&gt;Dog&lt;/code&gt; class, which is incorrect.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;puts cinco.class.superclass.instance_method(:bark)
# =&amp;gt; #&amp;lt;UnboundMethod: Dog#bark&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is because our &lt;code&gt;Doublebark&lt;/code&gt; module isn't an ancestor of the &lt;code&gt;cinco.class&lt;/code&gt;. How can we solve this issue?&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;super-solutions&quot; href=&quot;#super-solutions&quot;&gt;Super solutions&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;In feature request &lt;a href=&quot;https://bugs.ruby-lang.org/issues/9781&quot;&gt;#9781&lt;/a&gt;, I proposed adding a method to allow Ruby to give you this information directly. Shortly after, one of my co-workers, &lt;a href=&quot;https://bugs.ruby-lang.org/users/4&quot;&gt;Nobuyoshi Nakada&lt;/a&gt;, A.K.A. &quot;The Patch Monster&quot;, attached a working patch, and it was accepted into the Ruby trunk (soon to become 2.2.0) around July.&lt;/p&gt;

&lt;p&gt;If you are debugging in Ruby 2.2.0 you can now use &lt;a href=&quot;http://ruby-doc.org/core-2.2.0/Method.html#method-i-super_method&quot;&gt;Method#super_method&lt;/a&gt;. Using the same code we mentioned previously:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;cinco = SchneemsDog.new
cinco.method(:bark).super_method
# =&amp;gt; #&amp;lt;Method: Dog#bark&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see this returns the method on the &lt;code&gt;Dog&lt;/code&gt; class rather than the &lt;code&gt;SchneemsDog&lt;/code&gt; class. If we call &lt;code&gt;source_location&lt;/code&gt; in the output, we will get the correct value:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;module DoubleBark
  def bark
    super
    super
  end
end
cinco = SchneemsDog.new
cinco.extend(Doublebark)

puts cinco.method(:bark)
# =&amp;gt; #&amp;lt;Method: SchneemsDog(DoubleBark)#bark&amp;gt;
puts cinco.method(:bark).super_method
# =&amp;gt; #&amp;lt;Method: SchneemsDog#bark&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not only is this simpler, it's now correct. The return of &lt;code&gt;super_method&lt;/code&gt; will be the same method that Ruby will call when &lt;code&gt;super&lt;/code&gt; is invoked, regardless of whatever craziness is done with metaprogramming. Even though this is a simple example, I hope you'll find this useful in the wild.&lt;/p&gt;

&lt;p&gt;Follow &lt;a href=&quot;https://twitter.com/schneems&quot;&gt;@schneems&lt;/a&gt; for Ruby articles and pictures of his dogs. Note that Cinco was not harmed in the making of this blog post&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1488278430-Screenshot%2525202015-01-12%25252011.15.27.png%253Fdl%253D1&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Benchmarking Rack Middleware</title>
      <link>https://blog.heroku.com/benchmarking-rack-middleware</link>
      <pubDate>Mon, 03 Nov 2014 00:00:00 GMT</pubDate>
      <guid>https://blog.heroku.com/benchmarking-rack-middleware</guid>
      <description>&lt;p&gt;Performance is important, and if we can't measure something, we can't make it fast. Recently, I've had my eye on the &lt;code&gt;ActionDispatch::Static&lt;/code&gt; middleware in Rails. This middleware gets put at the front of your stack when you set &lt;code&gt;config.serve_static_assets = true&lt;/code&gt; in your Rails app. This middleware has to compare &lt;strong&gt;every&lt;/strong&gt; request that comes in to see if it should render a file from the disk or return the request further up the stack. This post is how I was able to benchmark the middleware and give it a crazy speed boost.&lt;/p&gt;

&lt;!-- more --&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;how-actiondispatch-static-works&quot; href=&quot;#how-actiondispatch-static-works&quot;&gt;How ActionDispatch::Static Works&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Right now to serve static files Action Dispatch hits the disk, basically doing this on every request:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;Dir[&quot;#{full_path}#{ext}&quot;].detect { |m| File.file?(m) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My gut said there had to be a better way, but how do we measure a singular rack middleware's performance? I couldn't find any really good posts on it, so I improvised using &lt;code&gt;benchmark/ips&lt;/code&gt; and &lt;code&gt;Rack::MockRequest&lt;/code&gt; to simulate traffic.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;bootstrap-your-middleware&quot; href=&quot;#bootstrap-your-middleware&quot;&gt;Bootstrap your Middleware&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;First you need to load the file where your middleware is defined:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;require 'rack/file'
require 'action_controller'
load '/Users/schneems/Documents/projects/rails/actionpack/lib/action_dispatch/middleware/static.rb'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we need to load our test capabilities:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;require 'rack/test'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can instantiate a new object that we can call in isolation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;noop       = Proc.new {[200, {}, [&quot;hello&quot;]]}
middleware = ActionDispatch::Static.new(noop, &quot;/my_rails_app/public&quot;)`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we wrap it up in a mock request:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;request = Rack::MockRequest.new(middleware)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I wanted to compare the speed of the middleware with the speed of the proc that it hits, so I made a no-op mock request as well:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;noop_request = Rack::MockRequest.new(noop)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, if we want to exercise a request against our singular middleware we can call&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;request.get(&quot;/path_i_want_to_hit&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;run-your-benchmarks&quot; href=&quot;#run-your-benchmarks&quot;&gt;Run your Benchmarks&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;To do the comparison you'll need the &lt;a href=&quot;https://github.com/evanphx/benchmark-ips&quot;&gt;benchmark ips&lt;/a&gt; gem installed&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;shell&quot;&gt;$ gem install benchmark-ips
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The gem works by running different blocks of code for variable amounts of time to record how many iterations per second they can achieve. The higher the number, the faster the code.&lt;/p&gt;

&lt;p&gt;We set up our benchmark:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;require 'benchmark/ips'

Benchmark.ips do |x|
  x.config(time: 5, warmup: 5)
  x.report(&quot;With ActionDispatch::Static&quot;) { request.get(&quot;/&quot;)  }
  x.report(&quot;With noop&quot;)                   { noop_request.get(&quot;/&quot;) }
  x.compare!
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should get an output similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Calculating ----------------------------------With ActionDispatch::Static
                          1525 i/100ms
           With noop      2667 i/100ms
----------------------------------------------With ActionDispatch::Static
                        15891.2 (¬±11.6%) i/s -      79300 in   5.056266s
           With noop    28660.9 (¬±11.7%) i/s -     141351 in   5.009789s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Higher iterations are better, so the blank no-op middleware was &lt;code&gt;(28660.9 - 15891.2)/15891.2 * 100 #=&amp;gt; 80&lt;/code&gt; roughly 80% faster or ran 80% more operations than with the default &lt;code&gt;ActionDispatch::Static&lt;/code&gt;. This is expected, but only gives us a baseline. So, we still need to test our new code.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;comparing-benchmarks&quot; href=&quot;#comparing-benchmarks&quot;&gt;Comparing Benchmarks&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;I ran the tests a few times to ensure I wasn't getting any flukes. Then, I set it up so that the middleware had some optimizations to not hit the disk were incorporated:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Calculating ----------------------------------Modified ActionDispatch::Static
                          2330 i/100ms
           With noop      2422 i/100ms
----------------------------------------------Modified ActionDispatch::Static
                        24490.9 (¬±7.9%) i/s -     123490 in   5.081158s
           With noop    26870.1 (¬±8.7%) i/s -     135632 in   5.093423s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here you can see that our no-op code ran &lt;code&gt;(26870.1 - 24490.9)/24490.9 * 100 # =&amp;gt; 9.71&lt;/code&gt; roughly 10% faster than the default &lt;code&gt;ActionDispatch::Static&lt;/code&gt;. Here the closer the better as the &lt;code&gt;nooop&lt;/code&gt; is the fastest possible case.&lt;/p&gt;

&lt;p&gt;When we graph the results&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://heroku-blog-files.s3.amazonaws.com/posts/1488278426-Screenshot%2525202014-08-08%25252014.05.21.png%253Fdl%253D1&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;You can see that my slight optimizations got us pretty close to the optimal state. The tick marks on each bar show the standard deviation (the ¬±) to make sure that the numbers are somewhat sane.&lt;/p&gt;

&lt;p&gt;If we do the math, we can see that my new middleware is &lt;code&gt;(24490.9 - 15891.2)/15891.2 * 100 # =&amp;gt; 54.11&lt;/code&gt; or roughly 54% faster than the original &lt;code&gt;ActionDispatch::Static&lt;/code&gt; in the case when we're making a request that is not requesting a file.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;keep-it-in-context&quot; href=&quot;#keep-it-in-context&quot;&gt;Keep it in Context&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Make sure not to get tunnel vision when you're benchmarking, in this case my optimizations made all non-asset requests faster, but since that required more logic, it actually makes asset requests (i.e. &lt;code&gt;request.get(&quot;assets/application-a71b3024f80aea3181c09774ca17e712.js&quot;)&lt;/code&gt;) slightly slower. Luckily, I did some benchmarking there and found the difference to not really be measurable. Either way, don't just benchmark your happy path, make sure to benchmark all common conditions.&lt;/p&gt;

&lt;p&gt;Right now you might also be thinking &quot;Holy cow 54% speed improvement in Rails, zOMG!&quot; but you have to remember that this is a middleware tested in isolation. The performance improvement isn't as much when we compare it to the whole Rails application stack, which I had to benchmark as well (and is a whole different blog post). The end result came to a ~2.6% overall speedup with the new middleware. Not bad. Here's the PR to Rails: &lt;a href=&quot;https://github.com/rails/rails/pull/16464&quot;&gt;https://github.com/rails/rails/pull/16464&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Go forth and benchmark your Rack middleware! If you have any feedback or know of a different/better way to do this, find me on the internet &lt;a href=&quot;https://twitter.com/schneems&quot;&gt;@schneems&lt;/a&gt;.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>A Patch in Time: Securing Ruby</title>
      <link>https://blog.heroku.com/a_patch_in_time_securing_ruby</link>
      <pubDate>Thu, 05 Dec 2013 21:05:00 GMT</pubDate>
      <guid>https://blog.heroku.com/a_patch_in_time_securing_ruby</guid>
      <description>&lt;p&gt;There have been thousands of reported security vulnerabilities in 2013 alone, often with language that leaves it unclear if you're affected. Heroku's job is to ensure you can focus on building your functionality, as part of that we take responsibility for the security of your app as much as we're able. On Friday, November 22nd a security vulnerability was disclosed in &lt;a href=&quot;https://www.ruby-lang.org/en/news/2013/11/22/heap-overflow-in-floating-point-parsing-cve-2013-4164/&quot;&gt;Ruby (MRI): CVE-2013-4164 &lt;/a&gt;. Our team moved quickly to identify the risk to anyone using the Heroku platform and push out a fix.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;the-vulnerability&quot; href=&quot;#the-vulnerability&quot;&gt;The vulnerability&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.ruby-lang.org/en/news/2013/11/22/heap-overflow-in-floating-point-parsing-cve-2013-4164/&quot;&gt;disclosed Ruby vulnerability&lt;/a&gt; contains a &lt;a href=&quot;http://en.wikipedia.org/wiki/Denial-of-service_attack&quot;&gt;denial-of-service vector&lt;/a&gt; with the possibility of arbitrary code execution as it involves a &lt;a href=&quot;http://en.wikipedia.org/wiki/Heap_overflow&quot;&gt;heap overflow&lt;/a&gt;. In a denial-of-service attack, a malicious individual sends your app requests that either causes the system to lock up or become unresponsive. When multiple people or systems do this, it becomes a distributed denial-of-service attack (or DDoS). &lt;/p&gt;

&lt;p&gt;A denial-of-service attack can vary in damage depending on how crucial uptime is to your app. For example if your app brings in a significant amount of money every hour, an attacker could cost you a large sum by bringing your service down.&lt;/p&gt;

&lt;p&gt;In this case, the denial-of-service was particularly easy to execute, making this a potentially devastating attack to some users. In addition, because this attack triggers a heap overflow, there is also a slim theoretical possibility of a much more serious vulnerability, an &lt;a href=&quot;http://en.wikipedia.org/wiki/Arbitrary_code_execution&quot;&gt;arbitrary code execution&lt;/a&gt;. We could not rule out the possibility of arbitrary code execution even though there is no known way to achieve it through this vulnerability.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;the-fix&quot; href=&quot;#the-fix&quot;&gt;The fix&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;When the vulnerability was announced, patched versions of Ruby 1.9.3, 2.0.0, and 2.1.0 were made available from Ruby core. Heroku‚Äôs Ruby Task Force pulled in the latest versions, compiled them and after running tests to confirm their compatibility with the platform released &lt;a href=&quot;https://devcenter.heroku.com/articles/ruby-support#ruby-versions&quot;&gt;updated versions of 1.9.3, 2.0.0, and 2.1.0&lt;/a&gt;. Any new pushes to the platform receive these patched versions. &lt;/p&gt;

&lt;p&gt;Some have asked why we didn‚Äôt automatically force-update all Ruby apps. First, Heroku will not re-deploy a user‚Äôs app without their direct knowledge or action. Additionally, since some upstream dependencies may have changed since an app‚Äôs last deploy, we also want to maintain &lt;a href=&quot;https://devcenter.heroku.com/articles/erosion-resistance&quot;&gt;erosion resistance&lt;/a&gt; by only changing components on an explicit push.&lt;/p&gt;

&lt;p&gt;In addition to building and deploying the fixed versions of Ruby, we:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;a href=&quot;https://devcenter.heroku.com/changelog-items/361&quot;&gt;Released a Changelog entry&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Sent out an email to all Ruby users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also did something unprecedented in the history of Heroku: patched and released two unmaintained language versions, Ruby 1.8.7 and 1.9.2. &lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;unmaintained-ruby-versions&quot; href=&quot;#unmaintained-ruby-versions&quot;&gt;Unmaintained Ruby versions&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.ruby-lang.org/en/news/2013/06/30/we-retire-1-8-7/&quot;&gt;Ruby 1.8.7 version has been at the End of Life&lt;/a&gt; for months, which means that the Ruby Core team will no longer issue bug or security fixes for the language. However, developers are still using it actively in production. &lt;a href=&quot;https://twitter.com/ayumin/status/340699195134595072/photo/1&quot;&gt;Ruby 1.9.2 is currently unmaintained&lt;/a&gt;, though this status has not been formally announced.&lt;/p&gt;

&lt;p&gt;While security patches were not made available for Ruby 1.8.7 and 1.9.2, Ruby engineer &lt;a href=&quot;https://twitter.com/hone02&quot;&gt;Terence Lee&lt;/a&gt; discovered that he could cleanly apply the &lt;a href=&quot;https://github.com/ruby/ruby/commit/46cd2f463c5668f53436076e67db59fdc33ff384&quot;&gt;security fix&lt;/a&gt; to both versions. The source for these are available on Heroku's fork of Ruby:  &lt;a href=&quot;https://github.com/heroku/ruby/releases/tag/v1_8_7_375&quot;&gt;1.8.7p375&lt;/a&gt; and &lt;a href=&quot;https://github.com/heroku/ruby/releases/tag/v1_9_2_321&quot;&gt;1.9.2p321&lt;/a&gt;. He then built, tested, and released these versions on Heroku‚Äôs Cedar stack. Terence recently got &lt;a href=&quot;https://twitter.com/_zzak/status/405513295781588992&quot;&gt;commit access to Ruby core&lt;/a&gt; and is currently working on pushing the changes upstream even though the two versions are technically unmaintained.&lt;/p&gt;

&lt;p&gt;Terence‚Äôs actions give you a longer runway but developers using Ruby 1.8.7 or 1.9.2 must upgrade as soon as possible. Heroku recommends upgrading to Ruby 2.0.0 or at very minimum 1.9.3. &lt;a href=&quot;http://en.wikipedia.org/wiki/Yukihiro_Matsumoto&quot;&gt;Matz&lt;/a&gt; has stated that support for 1.9.3 will likely be dropped within a year. Heroku‚Äôs Bamboo stack runs Ruby Enterprise Edition which has been at &lt;a href=&quot;http://blog.phusion.nl/2012/02/21/ruby-enterprise-edition-1-8-7-2012-02-released-end-of-life-imminent/&quot;&gt;end of life since early 2012&lt;/a&gt; and was not patched. Bamboo users should &lt;a href=&quot;https://devcenter.heroku.com/articles/cedar-migration&quot;&gt;upgrade to the Cedar stack&lt;/a&gt; to stay secure. &lt;/p&gt;

&lt;p&gt;Staying on a current version is crucial to being able to iterate quickly and respond to vulnerabilities. For example, the Chrome web browser auto updates in the background, and recently Apple software has moved their Mac and iOS app stores to this model. Using updated software provides benefits to a maintainer: less fragmentation and less time spent supporting legacy versions. This means more time for features, performance, and compatibility.&lt;/p&gt;

&lt;p&gt;The Rails web framework only supports Ruby 1.9.3+ for their 4.0.0 release, and it is rumored that they will be supporting only 2.0.0+ for their 4.1.0 release. Without dropping support for older syntaxes, developers cannot utilize new ones. As a language user, staying current means you have access to the latest features, latest security updates, and most active language support. For all these reasons, we want to encourage Heroku users to regularly upgrade and stay up-to-date.&lt;/p&gt;

&lt;p&gt;The ending of support for Ruby 1.8.7 and 1.9.2 are interesting events for Heroku: they represent the first time a technology used on the platform became unmaintained by its core developers. While we have extended the period you are secure on these unmaintained versions, we have not made a commitment to maintain either Ruby 1.8.7 or 1.9.2 indefinitely. We know that these environments are still in use, and want to make sure customers have ample time to upgrade. We‚Äôre working to make Heroku‚Äôs Ruby version support commitments and timelines explicit, and will publish documentation to that effect.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;security-matters&quot; href=&quot;#security-matters&quot;&gt;Security Matters&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;For Heroku‚Äôs security team, communication is as much a concern as technical fixes. When communication breaks down, so does security. This year we‚Äôve seen several large vulnerabilities that required notifications for languages, frameworks and tools including &lt;a href=&quot;https://blog.heroku.com/archives/2013/4/4/heroku_postgres_databases_patched&quot;&gt;a Postgres  patch&lt;/a&gt; and the &lt;a href=&quot;http://www.sitepoint.com/anatomy-of-an-exploit-an-in-depth-look-at-the-rails-yaml-vulnerability/&quot;&gt;Rails YAML vulnerability&lt;/a&gt;. We‚Äôre working on better ways to notify affected application owners. In this incident, we sent notifications to all Ruby application owners even if they were running JRuby or Rubinius, runtimes that were not affected. In the future we aim to be able to dial up our signal-to-noise ratio on security notifications, and even be able to provide app-specific information quickly and securely. If this sounds like fun, our &lt;a href=&quot;http://jobs.heroku.com/&quot;&gt;Security team is hiring&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you‚Äôre using Ruby on the platform, be sure to &lt;a href=&quot;https://devcenter.heroku.com/changelog-items/361&quot;&gt;take advantage of the fix&lt;/a&gt; as soon as possible. If you‚Äôre depending on an older, unmaintained version of Ruby, upgrade as soon as possible. If you‚Äôre maintaining your own versions of Ruby, make sure you update and re-compile. &lt;/p&gt;

&lt;p&gt;If you‚Äôre running on Heroku, sleep well knowing that we care about your security.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
    <item>
      <title>Ruby 2.0.0 Now Default on All New Ruby Applications</title>
      <link>https://blog.heroku.com/ruby-2-default-new-aps</link>
      <pubDate>Mon, 17 Jun 2013 19:36:55 GMT</pubDate>
      <guid>https://blog.heroku.com/ruby-2-default-new-aps</guid>
      <description>&lt;p&gt;Heroku provides an opinionated platform in order to help you build better applications. We give you a default version of Ruby to get you started, and give you a way to declare your version for total control. In the past creating an application would give you 1.9.2, starting today the default is 2.0.0.&lt;/p&gt;

&lt;p&gt;Ruby 2.0.0 is fast, stable, and works out of the box with Rails 4. Applications running on 2.0.0 will have a longer shelf life than 1.9.3, giving you greater &lt;a href=&quot;https://blog.heroku.com/archives/2011/6/28/the_new_heroku_4_erosion_resistance_explicit_contracts&quot;&gt;erosion resistance&lt;/a&gt;.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;default-behavior&quot; href=&quot;#default-behavior&quot;&gt;Default Behavior&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;If you have a previously deployed app it will continue to use Ruby 1.9.2, any new applications will run on 2.0.0. Heroku is an &lt;a href=&quot;https://blog.heroku.com/archives/2011/6/28/the_new_heroku_4_erosion_resistance_explicit_contracts&quot;&gt;erosion resistant&lt;/a&gt; platform, which means we will not change a major or minor version of Ruby on your app without you taking action.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;setting-your-ruby-version&quot; href=&quot;#setting-your-ruby-version&quot;&gt;Setting your Ruby Version&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;In addition to providing a default version of Ruby, you have the ability to &lt;a href=&quot;https://blog.heroku.com/archives/2012/5/9/multiple_ruby_version_support_on_heroku&quot;&gt;specify your version of Ruby&lt;/a&gt; in your &lt;code&gt;Gemfile&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruby '2.0.0'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While you can prototype on the default Ruby, we recommend explicitly setting your version on all production applications. When you specify the Ruby version in your codebase, you get the exact same version: across every developer and across every app. This means any new developers on your team, any new staging apps you set up on Heroku and any &lt;a href=&quot;https://devcenter.heroku.com/articles/fork-app&quot;&gt;forked apps&lt;/a&gt; will have the same version. If your app needs consistency: define your Ruby version. &lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;ruby-2-0&quot; href=&quot;#ruby-2-0&quot;&gt;Ruby 2.0&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Ruby 2.0 includes copy on write friendly garbage collection which can reduce memory usage in a forking server &lt;a href=&quot;https://devcenter.heroku.com/articles/rails-unicorn&quot;&gt;such as Unicorn&lt;/a&gt;. Ruby 2.0.0 has faster code loading which means large frameworks such as &lt;a href=&quot;http://blog.tddium.com/2012/11/20/profiling-ruby/&quot;&gt;Rails start much faster&lt;/a&gt;. Ruby 2.0.0 is mostly backwards compatible with 1.9.3 and at Heroku our developers already run Ruby 2.0.0.&lt;/p&gt;

&lt;p&gt;Using the latest stable version of Ruby has advantages for the community as well as for application's performance. In the past some Rubyists have resisted upgrading. This resulted in libraries needing to support multiple versions of Ruby for long periods of time, and creating factions within the community. For instance while Ruby 1.9.3 was &lt;a href=&quot;http://www.ruby-lang.org/en/news/2011/10/31/ruby-1-9-3-p0-is-released/&quot;&gt;released in 2011&lt;/a&gt; there are many developers who are just now upgrading from Ruby 1.8.7 which, was &lt;a href=&quot;http://www.ruby-lang.org/en/news/2008/05/31/ruby-1-8-7-has-been-released/&quot;&gt;released in 2008&lt;/a&gt;. We have encouraged developers to run Ruby 2.0.0 on our platform by making the &lt;a href=&quot;https://blog.heroku.com/archives/2012/11/5/ruby-2-preview-on-heroku&quot;&gt;preview available&lt;/a&gt;, and the GA version available on launch day. By setting the default version to 2.0.0 we hope to encourage more developers to run on the most recent stable Ruby version.&lt;/p&gt;

&lt;p&gt;Stability, speed, and community are all good aspects to support, but we also care about application maintainability. At the end of this month &lt;a href=&quot;http://www.ruby-lang.org/en/news/2011/10/06/plans-for-1-8-7/&quot;&gt;Ruby 1.8.7 will reach end-of-life&lt;/a&gt;. Maximize the life of your application and simplify your upgrade to 2.1.0, coming in December, by using the most recent release.&lt;/p&gt;
&lt;h2 class=&quot;anchored&quot;&gt;
  &lt;a name=&quot;conclusion&quot; href=&quot;#conclusion&quot;&gt;Conclusion&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Ruby at Heroku provides both defaults and flexible choices. You can explicitly declare a Ruby version or accept a stable, default version. Sometimes you may just need something to work, and others you want to enforce &lt;a href=&quot;https://www.12factor.net/dev-prod-parity&quot;&gt;dev/prod parity&lt;/a&gt; between developers. By supporting a default Ruby you can do either.&lt;/p&gt;

&lt;p&gt;Thanks to Japan based Herokai: &lt;a href=&quot;https://twitter.com/ayumin&quot;&gt;Ayumu Aizawa&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/yukihiro_matz&quot;&gt;Yukihiro &quot;Matz&quot; Matsumoto&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/_ko1&quot;&gt;Koichi Sasada&lt;/a&gt;, and &lt;a href=&quot;https://twitter.com/n0kada&quot;&gt;Nobuyoshi Nakada&lt;/a&gt;, for working with us to push our defaults forward. Ruby Core is excited to see Heroku support the new 2.0 default, we hope you are too. Try it on Heroku and let us know what you think: &lt;a href=&quot;https://twitter.com/heroku&quot;&gt;@heroku&lt;/a&gt;.&lt;/p&gt;
</description>
      <author>Richard Schneeman</author>
    </item>
  </channel>
</rss>
