<?xml version="1.0" encoding="UTF-8"?>
<!--Generated by Site-Server v6.0.0-b09f90d0d369e09f14d7ef9dcae79576bde1b2e1-1 (http://www.squarespace.com) on Wed, 16 Nov 2022 18:14:25 GMT
--><rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://www.rssboard.org/media-rss" version="2.0"><channel><title>Engineering Blog</title><link>https://engineering.appfolio.com/</link><lastBuildDate>Fri, 11 Nov 2022 22:26:15 +0000</lastBuildDate><language>en-US</language><generator>Site-Server v6.0.0-b09f90d0d369e09f14d7ef9dcae79576bde1b2e1-1 (http://www.squarespace.com)</generator><description><![CDATA[Technical Blog]]></description><item><title>Understanding Invoices with Document AI</title><dc:creator>Christfried Focke</dc:creator><pubDate>Fri, 11 Nov 2022 23:01:56 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2022/11/11/understanding-invoices-with-document-ai</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:636ecc07d3bd9b0db6288039</guid><description><![CDATA[<p class="">At AppFolio, we specialize in helping Property Management Companies to automate their most repetitive, time-consuming, and tedious processes. Accounting-related tasks often check all of these boxes – and one of them is to maintain an accurate system of record for accounts payable and accounts receivable. This is a critical task due to strict federal and state regulations, and to ensure it is carried out accurately our customers use AppFolio’s Property Management (APM) software to enter invoices. However, these bills must be manually entered –&nbsp; a time-consuming and error-prone process.</p><p class="">To help, we created <strong><em>Smart Bill Entry</em></strong>, a tool powered by state-of-the-art Machine Learning (ML) based models and our Document AI technology. Smart Bill Entry automatically extracts the most important information from the multitude of invoices a Property Manager receives in a given month - and in 2022 alone, we processed over 10 Million invoices. Here we will explore the tools enabling this technology and explain how it works behind the scenes.</p><h2><strong>Smart Bill Entry</strong></h2><p class="">Smart Bill Entry enables Property Managers to upload invoices in PDF format to APM directly via drag and drop, or by forwarding an email to a specific inbox. Once the invoice is uploaded, our ML models automatically extract key information, such as the amount, vendor name, the property’s address, and the invoice number associated with the bill.</p><p class="">We know Property Managers place a heavy emphasis on accuracy to ensure that bills get paid correctly. Based on our estimations, we have found that when manually entering invoices humans are roughly 95-99% accurate, depending on the field. Our model is designed to mirror this accuracy, and we compute calibrated <em>confidence scores</em> for each field to get our models to a state where they are as close to human accuracy as possible.</p><p class="">The model outputs a number between 0 and 1, where a score closer to 1 indicates that the model is confident that the class is correct. However, ML models tend to be over- or under-confident in their predictions, meaning that a given score doesn’t correspond to the <em>probability</em> that the prediction is correct - as demonstrated by the diagram below.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/2270e9a2-f02d-4efa-ad71-14a379eb5807/Frame+4+%281%29.png" data-image-dimensions="382x421" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="636ed1464b2ef83a87ec3f9d" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/2270e9a2-f02d-4efa-ad71-14a379eb5807/Frame+4+%281%29.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">ML models tend to be over or underconfident, such that their confidence scores don’t exactly correspond to the <em>probability</em> that the prediction is correct. we fit additional calibration models To account for this, such that we can make consistent decisions of whether or not to show predictions. Image credit <a href="https://arxiv.org/pdf/1706.04599.pdf">“On Calibration of Modern Neural Networks”</a>.&nbsp;</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">Calibration is the process of transforming the confidence score predicted by the model into a reliable probability that can be used to make decisions, like showing or not showing the prediction to the user. We do this as an added layer of precision. If our calibrated model is returning a confidence score of 0.95 for the “Bill Amount” field, we can expect that the model will be correct 95 times out of 100 predictions.</p><h2><strong>Document AI</strong></h2><p class="">The technology driving Smart Bill Entry’s ability to extract information from PDFs is <strong><em>Document AI</em></strong>. Document AI is a set of Machine Learning tools that are specifically designed for extracting information from a PDF, independent of the layout. The input data used to train our Document AI models includes two types of information:&nbsp;</p><ol data-rte-list="default"><li><p class="">The Optical Character Recognition (OCR) information of the documents. This is the typed or written text on a document converted into machine-encoded text. Since we are only interested in fields that are critical to the invoice, we use datasets that zero in on this information.</p></li><li><p class="">The bounding box coordinates and content of the target fields we are looking to extract from the documents. We derive <em>labels</em><strong><em> </em></strong>from these coordinates. To get the labels (e.g. “Vendor Name”, “Property Name”, etc.) for the content of each bounding box, human supervision is necessary - and the operators who assess the content of the invoices assign the appropriate label.</p></li></ol><p class="">The image below is a visual representation of an invoice, the OCR information identified by the model for that invoice, and what labels would be generated for each object that is present on the invoice.&nbsp;</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/c3f914ac-9b9f-49e5-93ce-58f8c83870dd/Frame+1+%281%29.png" data-image-dimensions="1213x459" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="636ed194f3ae8f4f8f5229ec" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/c3f914ac-9b9f-49e5-93ce-58f8c83870dd/Frame+1+%281%29.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Original Invoice vs OCR Information vs Labels</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">After collecting the OCR and bounding box dataset, we then apply different techniques that combine Computer Vision, Natural Language Processing, and Tabular Features. In the next section, we’ll discuss two of our approaches to extracting the above information, depending on our customer’s specific needs.&nbsp;&nbsp;</p><h2><strong>Traditional&nbsp;vs Deep Learning</strong></h2><p class="">Smart Bill Entry combines two approaches to extracting information from invoices: traditional Machine Learning solutions and advanced Deep Learning solutions. For example, when extracting the ”Property Address” and the “Vendor Name” fields, we are using tree-based&nbsp; models customized for each Property Manager. When we extract generic fields, such as the “Amount” and “Invoice Number” we use powerful DL models that can take advantage of layout and text using state-of-the-art <a href="https://jalammar.github.io/illustrated-transformer/">Transformer</a> architectures.</p><h3><strong>Traditional Machine Learning&nbsp;</strong></h3><p class="">We extract the “Property Address” and “Vendor Name” fields from an invoice using traditional ML models. The input data for training the model is a simple <a href="https://en.wikipedia.org/wiki/Bag-of-words_model"><span><em>Bag-of-words</em></span></a> representation of the invoices, together with other engineered features relating to the layout. After extensive benchmarking we landed on a multi-class <em>Random Forest Classifier</em> as our base estimator<em>.</em> It fits several decision tree classifiers on various sub-samples of the dataset and uses averaging to improve predictive accuracy and prevent over-fitting.</p><p class="">Because AppFolio is a Property Management software used by thousands of companies (each with a large number of vendors and properties in their database), training only one multi-class Random Forest Classifier for all the vendors and properties in our database is a very challenging feat due to the high cardinality in the target variable and the challenge of deduplicating entities.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/29f2cd56-bd4a-4fb1-9bc1-c15eb8ea6de9/Frame+2+%281%29.png" data-image-dimensions="915x398" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="636ed1ca7478803661aa4b78" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/29f2cd56-bd4a-4fb1-9bc1-c15eb8ea6de9/Frame+2+%281%29.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">We train separate Random Forest ClassifierS for fields that have a unique set of target classes for each customer.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">To tackle this, we decided to train one model per Property Management company. Not surprisingly, the drawback of this approach is that it requires a comprehensive and robust ML infrastructure to efficiently maintain and deploy thousands of models to production, as well as a short cold start phase to learn how to correctly map to specific vendor and property entities. It also means that each individual model needs to be relatively light weight and fast to train.<br>At AppFolio, we combined in-house solutions with third-party <a href="https://en.wikipedia.org/wiki/MLOps"><span>MLOps</span></a> solutions, to create a state-of-the-art ML infrastructure that helps every team quickly train, deploy and monitor ML models at scale in production. We will talk more about our infrastructure in future posts in this blog.&nbsp;&nbsp;&nbsp;</p><h3><strong>Deep Learning</strong></h3><p class="">For fields that have well defined labels across all customers such as the “Amount” and “Invoice Number”, we opted for a solution that implements a single Deep Learning model across <em>all</em> Property Management companies as opposed to one model per company. This approach generalizes well to unseen layouts and eliminates cold start issues.</p><p class="">Due to its very high learning capacity we were able to leverage almost all available training data in our database - which lends to our solution producing much more accurate predictions than traditional ML models. We also benchmarked against off the shelf Document AI solutions and found that our models significantly outperform them when evaluated on <em>our</em> holdout data.</p><h3><strong>Deep Learning Architecture</strong></h3><p class="">When deciding which DL architecture to implement for our model, we tried numerous approaches including:&nbsp;</p><ul data-rte-list="default"><li><p class="">Computer Vision models that use the image of an invoice as their input and output a bounding box for each class</p></li><li><p class="">Natural Language Processing models that start from the OCR and classify each bounding box according to one of the given classes, and</p></li><li><p class="">Multimodal models that use as input both the OCR and the image of the invoice.&nbsp;</p></li></ul><p class="">We implemented our models in such a way that we can exchange the architecture without modifying the input data and output format. In the image below, we show how different architectures (the yellow boxes in the diagram) can be exchanged without affecting the first and last layers of the model. This gives us the flexibility and agility to test different solutions and optimize our metrics.&nbsp;</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/7e7fde1e-c572-4823-9aa0-33ca74cbcc84/Frame+3+%282%29.png" data-image-dimensions="900x261" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="636ed21e4b2ef83a87ec5b1a" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/7e7fde1e-c572-4823-9aa0-33ca74cbcc84/Frame+3+%282%29.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Keeping the input and output fixed we can easily switch between Deep learning architectures to optimize metrics such as accuracy, training and inference time. This also gives us the flexibility to quickly try out new approaches as the field advances.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">When considering which architecture to deploy to production, we chose the solution that best balances accuracy with training time and inference speed. We used two processes to evaluate the performance of each model:</p><p class="">1. <em>Offline</em> Evaluation<br>After training a new model, we compare its performance against a frozen and static dataset that we use as a benchmark.</p><p class="">2. <em>Online</em> Evaluation<br>After training a new model, we deploy it in shadow-mode, where we do not show the predictions to the user, but rather just record them and compare metrics.&nbsp;</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/3177a7df-cd4c-42ae-872f-d83ee568efc2/Smart+Bill+Entry+and+Document+AI+%40+AppFolio.png" data-image-dimensions="1280x960" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="636ed25449c25d0577256070" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/3177a7df-cd4c-42ae-872f-d83ee568efc2/Smart+Bill+Entry+and+Document+AI+%40+AppFolio.png?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  




<p class="">Stay tuned for a future blog post where we will discuss the details of our ML infrastructure, and how we train, evaluate, deploy and monitor each model!</p>




<p class="">Authors and contributors: Ezequiel Esposito, Ari Polakof, Christfried Focke, Tony Froccaro</p>]]></description><media:content type="image/png" url="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1668284573237-W53I26SZC745FMX2JQHP/Frame+3+%282%29.png?format=1500w" medium="image" isDefault="true" width="900" height="261"><media:title type="plain">Understanding Invoices with Document AI</media:title></media:content></item><item><title>Lisa: Conversational AI</title><dc:creator>Christfried Focke</dc:creator><pubDate>Wed, 31 Aug 2022 19:03:25 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2022/8/31/lisa-conversational-ai</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:630f89e4c23b243108e6c798</guid><description><![CDATA[<figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/cc51e3c8-a91c-4dab-bf58-1b0eff39696a/DALL%C2%B7E+2022-08-30+21.03.00+-+Dog+sitting+on+a+chair+in+front+of+a+computer+with+one+paw+on+the+keyboard%2C+he+is+turning+to+the+right+to+talk+to+another+dog+next+that+sits+on+the+gr.jpg" data-image-dimensions="1024x834" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="630fb469d9c20a241b67a00d" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/cc51e3c8-a91c-4dab-bf58-1b0eff39696a/DALL%C2%B7E+2022-08-30+21.03.00+-+Dog+sitting+on+a+chair+in+front+of+a+computer+with+one+paw+on+the+keyboard%2C+he+is+turning+to+the+right+to+talk+to+another+dog+next+that+sits+on+the+gr.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Inspired by PETER STEINER/The New Yorker magazine (1993) “On the internet, nobody knows you’re a dog.” Generated with <a href="https://openai.com/dall-e-2/">DALL-E 2</a> prompt “Dog sitting on a chair in front of a computer with one paw on the keyboard. Comic style, black and white.”</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">In our <a href="https://engineering.appfolio.com/appfolio-engineering/2022/8/10/lisa-inquiry-parser"><span>last post</span></a> we discussed the first step in the leasing process driven by Lisa, the Inquiry Parser. Once a message from an Internet Listing Service has been parsed, Lisa’s conversational AI is ready to chat with the prospective resident, either via text or email (we politely decline phone calls 🙂).</p><h2>Lisa’s “Galaxy Brain”</h2><p class="">The primary driver behind Lisa’s conversational AI is what we refer to colloquially as <em>Galaxy Brain</em> or <em>Galaxy </em>(<a href="https://galaxybraindesign.com/our-story/why-galaxy-brain-design/"><span>the evolution, and constant intelligence gathering of <em>Brain</em></span></a>). Galaxy’s task is framed as multi-label text classification, and it works by converting the conversation into a structured response. We then use this response in Lisa’s logic layer to drive the conversation forward.</p><p class="">The structured response, pictured below, is a set of labels that are accompanied by confidence scores. The labels included in our model are:</p><ul data-rte-list="default"><li><p class=""><strong><em>Intents</em></strong><em> - </em>Tasks prospective residents want to accomplish</p><ul data-rte-list="default"><li><p class="">“This Thursday works for me”, “Can we do Friday instead?”, “I can no longer make the showing”<br>(e.g. accept or counteroffer, reschedule, cancel)</p></li></ul></li></ul><ul data-rte-list="default"><li><p class=""><strong><em>Categorical slot values </em>- A piece of information that can be categorized</strong></p><ul data-rte-list="default"><li><p class="">&nbsp;(e.g. “I’m looking for a 1 bedroom”, “Can I do a virtual showing?”)</p></li></ul></li></ul><ul data-rte-list="default"><li><p class=""><strong><em>Requested slots</em></strong> - A piece of information the prospective residents requests from us</p><ul data-rte-list="default"><li><p class="">(e.g. “What's the rent?”, “Do you accept Section 8?”)</p></li></ul></li></ul><ul data-rte-list="default"><li><p class=""><strong><em>Acknowledgements</em></strong> - Cordial responses</p><ul data-rte-list="default"><li><p class="">(e.g. “You’re welcome!”, “Thank you for your time.”)</p></li></ul></li></ul><ul data-rte-list="default"><li><p class=""><strong><em>Miscellaneous labels</em></strong> - Actions that change the back-end behavior of Lisa</p><ul data-rte-list="default"><li><p class="">(e.g. Mark the thread as spam, have the thread skip the operators’ inbox)</p></li></ul></li></ul>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/392847c6-e641-4c78-be5e-8b5278eb8f04/Screen+Shot+2022-08-31+at+9.53.23+AM.png" data-image-dimensions="1089x347" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="630f923ab1753908a81e901c" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/392847c6-e641-4c78-be5e-8b5278eb8f04/Screen+Shot+2022-08-31+at+9.53.23+AM.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Galaxy Input text (top blue blob) and output response (bottom blue blob). The input text includes current inbound message (red) and conversation history (blue), and the output response includes confidence scores for each label.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">The confidence score is a value between 0 and 1 that represents the likelihood that the output of the model is correct, with 1 being the highest. In this instance, because the confidence in <code>SET_TOUR_TYPE_VIRTUAL</code> is high, we would first mark the prospective resident’s preference for virtual tours, and then offer to schedule them a virtual tour. If this score were low, it may be handed off to an operator for review.</p><p class="">While highly accurate, a common problem with deep learning models is that they tend to be <em>overconfident </em>in their predictions. This means that they output a very high (or low) confidence score even if there is high uncertainty associated with the accuracy of the prediction. To adjust for this our model is fit with a set of <a href="https://pypi.org/project/netcal/"><span>calibration models</span></a>, one per label, to map the confidence scores in such a way that they correspond more closely to the <em>probability</em> that the prediction is correct. </p><p class="">For non-categorical slot values, such as names, we use a separate Seq2Seq model similar to <a href="https://engineering.appfolio.com/appfolio-engineering/2022/8/10/lisa-inquiry-parser"><span>Lisa's Inquiry Parser</span></a>.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/bc8a32cd-8889-4f9d-ab81-f5ba953f8e61/Screen+Shot+2022-08-31+at+9.26.02+AM.png" data-image-dimensions="1072x355" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="630f8fbc1779ad2ba153bcb6" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/bc8a32cd-8889-4f9d-ab81-f5ba953f8e61/Screen+Shot+2022-08-31+at+9.26.02+AM.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">NLP models transform the conversation history into a structured conversation state. A logic layer combines the conversation state with information from a Knowledge Base (KB) to compute the next action or response.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<h2>How Lisa Responds&nbsp;</h2><p class="">Lisa uses a state-of-the-art <a href="https://github.com/huggingface/transformers"><span>Transformers</span></a> based classifier to map natural language into a structured <em>Conversation State</em>. There is a limit on input text length stemming from the quadratic complexity of the attention mechanism in Transformers, as each token (sub word unit) is queried against all other tokens in the input text. A common limit of Transformers-based models is 512 tokens, and to accommodate for this we simply truncate the beginning of the conversation history, as this portion is typically less relevant to the current turn. Recently, linear attention mechanisms have been developed to greatly increase this length limit, but we haven’t found any significant performance gains.</p><p class="">We also include special tokens indicating the speaker, as well as the local timestamp of when each message was sent. This helps Galaxy, by enabling it to infer information from the pacing of messages, as well as the time of day, day of the week and current month. This can also help overcome ambiguities and aid with prioritizing different parts of the input relevant to the current turn.</p><p class="">We generate confidence scores for each label independent of each other to allow an inbound message to have multiple classifications (i.e. a “multi-label” model). This simple setup also allows us to add new labels without touching the model code, we simply modify the data generation, and the new label will show up after the next retraining.</p><p class="">For example, if a prospect asks “I would like a 2 bedroom and do you accept section 8?”, our model will return a score close to 1 for at least two classes – one for asking about “Section 8” (affordable housing) and another for responding with a “2 bedroom” unit type.</p><p class="">Lisa then interprets this state by combining it with external knowledge to generate the natural language response back to the prospect. We refer to the external knowledge as Lisa’s Knowledge Base (KB), and it includes database lookups (e.g. to determine a property’s Section 8 policy) and API calls to external systems (e.g. to Google Calendar for an agent’s availability).</p><p class="">Here is an example of<em> Galaxy</em> in action. Given a message and its conversation history, Galaxy determines a score for each class. Most classes are irrelevant to this message and thus have very low scores. However, Galaxy identified 2 classes of importance here:</p><ol data-rte-list="default"><li><p class="">Updating the unit type to 2 bedrooms</p></li><li><p class="">The question pertaining to Section 8</p></li></ol><p class="">When deciding whether the prospect would like a 1 or 2 bedroom apartment, Galaxy paid strong attention to “2 bedroom” in the prospect’s message, but also gave weight to the “1BR” portion in the conversation history. These weights give the class of updating unit type a high score. When judging if there is a Section 8 question, Galaxy gets a strong positive signal from “accept section 8”, but negative signals from the conversations about unit type. This is because prospects don’t tend to mention unit type and Section 8 at the same time. In the end the classifier assigns a positive yet small score to Section 8 class.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1ee1cc59-c960-4c2b-b17f-5dd875ff3f31/Screen+Shot+2022-08-31+at+9.39.43+AM.png" data-image-dimensions="1733x502" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="630f8fefb35bee1df7791e43" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1ee1cc59-c960-4c2b-b17f-5dd875ff3f31/Screen+Shot+2022-08-31+at+9.39.43+AM.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Output of the <a href="https://shap.readthedocs.io/en/latest/">SHAP</a> explainer package for the label SET_UNIT_TYPE_BR2. It shows the importance of each word in the input relating to generating the output score for this label. The colors indicate which parts of the input the model deemed to have positive (red) and negative (blue) contributions. It mostly focuses on the words “2 bedroom” in the last prospect message, but also considers the unit types that Lisa said were available.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/478672ac-a777-4499-9485-a7eda99608a6/Screen+Shot+2022-08-31+at+9.39.21+AM.png" data-image-dimensions="1762x502" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="630f900c110a6d4cd5041921" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/478672ac-a777-4499-9485-a7eda99608a6/Screen+Shot+2022-08-31+at+9.39.21+AM.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Output of the <a href="https://shap.readthedocs.io/en/latest/">SHAP</a> explainer package for the label SECTION8. It shows the importance of each word in the input relating to generating the output score for this label. The colors indicate which parts of the input the model deemed to have positive (red) and negative (blue) contributions. In this case it mostly focuses on the words “accept section 8?”.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">With Lisa’s KB integration we can carry out subsequent actions with our logic layer, such as</p><ul data-rte-list="default"><li><p class="">Mark the desired unit type in the database</p></li><li><p class="">Answer the question about the Section 8 policy</p></li><li><p class="">Look up and offer showing slots for the desired unit type</p></li><li><p class="">Cross sell to a different property if the unit type is not available</p></li></ul><p class="">The logic layer employs a non-ML, template-based approach to generating responses, instead of letting a ML model decide what template to choose or even generate text end-to-end. We chose this methodology because it gives us more control – without having to re-train the model, we can change how Lisa replies to messages or change the conversation flow, just by making adjustments to the logic. Without this, we would need to retrain operators to continuously correct the model’s behavior until enough data is collected to retrain the model, making iterations slow, error prone, and taxing on operators.</p><h2>Teaching Lisa</h2><p class="">To teach Lisa about the leasing process, we need to collect structured training data from the product – one of the greatest challenges underlying all ML products. We carefully designed Lisa’s logic layer to obtain high-quality data without adding much to operators’ workload. Training a classification model usually requires a labeled dataset, one that has annotated classes for each data point.</p><p class="">In our application, this would mean labeling all the desired classes for each inbound message, several hundred labels per message. Instead of asking our operators to create annotations explicitly, we instead infer labels from their behavior. Our operators’ main job is to reply to prospective residents, and to correct our model’s mistakes if needed.&nbsp;</p><p class="">We implemented a convenient user interface that can provide structured responses for operators to choose from, so our model can learn directly from what operators do on the job.&nbsp;</p><p class="">One could say that machines learn what we do, not what we say. The user interface needs to account for different categories of classifications, such as question versus intent, and provide operators with easy ways to generate responses by clicking different buttons or navigating the UI with keyboard shortcuts.&nbsp;</p><p class="">This machine-human interface blurs the boundaries between machine and human responses. Sometimes the machine bypasses operators entirely, and other times operators ignore the suggestions. However, most of the time, the response lies somewhere in the middle; it could be that the machine gives a strong suggestion and operators simply approve it, or that operators slightly modify it to better suit the conversation flow.&nbsp;</p><p class=""><br>So are prospective tenants talking to a machine or a human? With Lisa, the line is certainly blurry ¯\_(ツ)_/¯</p>




<p data-rte-preserve-empty="true" class=""></p><p class="">Authors and contributors: Christfried Focke, Shyr-Shea Chang, Tony Froccaro, Miguel Rivera</p>]]></description><media:content type="image/png" url="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1661963047565-RFVT4RZLEON4MJRDN4PJ/DALL%C2%B7E+2022-08-30+21.03.00+-+Dog+sitting+on+a+chair+in+front+of+a+computer+with+one+paw+on+the+keyboard%2C+he+is+turning+to+the+right+to+talk+to+another+dog+next+that+sits+on+the+gr.png?format=1500w" medium="image" isDefault="true" width="1024" height="1024"><media:title type="plain">Lisa: Conversational AI</media:title></media:content></item><item><title>Lisa: Parsing Inquiries with Seq2Seq Transformers</title><dc:creator>Christfried Focke</dc:creator><pubDate>Thu, 11 Aug 2022 18:15:41 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2022/8/10/lisa-inquiry-parser</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:62f48fd713fc18595bfa09d0</guid><description><![CDATA[<p class="">In our previous blog post we introduced AppFolio AI Leasing Assistant, Lisa, giving a high-level overview of her capabilities and insight into the value she can offer. One of the key technical components which enables Lisa to perform so effectively is the Inquiry Parser.</p><h2>What is Lisa’s Inquiry Parser?</h2><p class="">The leasing conversation flow is often initiated by a prospective resident submitting an inquiry via an Internet Listing Service (ILS), such as Zillow, Apartments.com, or a private homepage for the property. The first component of Lisa to spring into action in response is the inquiry parser. We use it to extract information from inquiries, and process the data collected to start and facilitate a productive conversation in hopes it will lead to a showing, an application, and finally a signed lease.</p><p class="">Once an inquiry is submitted, Lisa receives an e-mail and parses it. All PII (Personal Identifiable Information) is processed and stored securely and not disclosed to anyone not directly involved in the leasing process. At a minimum, a phone number or email address is required to begin a text conversation with the prospective resident. However, with more information such as the prospect’s full name, their desired move-in date, and their unit type preference, Lisa can streamline the conversation as she doesn’t have to ask for it again.</p><p class="">Other than parsing data pertaining to basic information, source attribution is another key component of the inquiry parser. Lisa determines the source of each inquiry, enabling us to generate reports showing which ILS is driving the most business for property managers.&nbsp;</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/8d4215f1-1865-4122-9bb5-1c70335604b6/Screen+Shot+2022-08-10+at+10.21.13+PM.png" data-image-dimensions="1781x316" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62f492187455db0b5e5e281c" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/8d4215f1-1865-4122-9bb5-1c70335604b6/Screen+Shot+2022-08-10+at+10.21.13+PM.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">The Regex Parser has close to 100% precision, but over time its recall will drop as new listing sites come online, or existing sites change their format. We continue to run the RegEx parser first and then augment it with fields from the ML parser. The parsed info is then used to create new, or update existing contacts and threads.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<h2>How Does Lisa’s Inquiry Parser work?&nbsp;</h2><p class="">Because there are hundreds of different listing sites, each with different and evolving formats through which they collect their customer’s data, it is a difficult task to parse the wide array of inbound inquiries to Lisa. Prior to the current iteration, our solution was a file with 4,000 lines of RegEx parsing code, that was frequently amended to keep up with formatting changes or addition of new listing sites. This ended up being a significant time sink and chore for our developers.&nbsp;</p><p class="">Instead, we opted for a more effective solution. In addition to the RegEx, we added a Machine Learning powered parser that generalizes much better by drawing upon data collected from past listing emails and their parsed fields. Lisa now utilizes a <a href="https://huggingface.co/docs/transformers/index"><span>Transformers</span></a>-based, Seq2Seq (sequence-to-sequence) model to map a message derived from an inquiry into a structured string that makes the data trivial to parse. Transformers are a state of the art class of model architectures for Natural Language Processing (NLP) tasks. We leverage pre-trained language models and fine tune them to focus on specific tasks.</p><p class="">As its name suggests, Seq2Seq<em> </em>models transform a sequence into another sequence. A simple example is transforming a German sentence into French. The Transformer generates a target sequence by analyzing both the input and output generated so far, to determine the next token (sub word unit). With the information learned from pre-training on a very large corpus of data, it only needs a fairly modest amount of task specific training data to achieve strong performance.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/4daee4f6-53b1-4fb8-8ca4-84197586ad12/transformer_self-attention_visualization.png" data-image-dimensions="437x413" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62f4927cae32804dd9ed4ef0" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/4daee4f6-53b1-4fb8-8ca4-84197586ad12/transformer_self-attention_visualization.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">An illustration of the activations of the attention mechanism that underpins the Transformer architecture. As we are encoding the word "it", part of the attention mechanism is focusing on "The Animal", and baked a part of its representation into the encoding of that word. Source: <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">In our application, we want to extract information from an ILS message. We input the entirety of a message, and have the model output a structured summary sentence of that message. The following is a sample input and output from our model. The input sequence is the ILS message in the top-most text block, the middle text block contains the generated output sequence, and the bottom-most text-block contains the fully parsed output:<strong>&nbsp;</strong></p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/dd8b57b4-d931-46da-a27e-7ec3a962d7a1/Screen+Shot+2022-08-10+at+10.19.45+PM.png" data-image-dimensions="1086x1060" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62f492c6cc1c1e176f32de4f" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/dd8b57b4-d931-46da-a27e-7ec3a962d7a1/Screen+Shot+2022-08-10+at+10.19.45+PM.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">The input sequence consisting of source domain, email subject and body (we remove URLs and HTML tags before passing it into the model) is mapped to a string that resembles natural language and is trivial to parse. We then check whether each value actually exists in the input (e.g. by regex matching phone numbers), and compute confidence scores for each field.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">When generating the text in the middle block, the model decides which word to generate based on its relevance to the input text. To explain the model behavior, this relevance can be visualized as a score from each word in the input, and these scores can be added up to determine the final score for the output word (see images below). For example, to generate a part of the phone number, the model almost exclusively looks at the keyword “Phone” and the number that follows. However, when generating the first name, the model actually looks at multiple sentences in the input that mention the first name, even the email address. By looking at these visualizations we can understand how the model works and when its predictions are likely to be correct or incorrect.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/eb61c210-e293-4fbe-b0cd-11fbd5ee6026/Screen+Shot+2022-08-11+at+10.49.19+AM.png" data-image-dimensions="1912x796" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62f5440e3ed9fa5d34b1b7d4" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/eb61c210-e293-4fbe-b0cd-11fbd5ee6026/Screen+Shot+2022-08-11+at+10.49.19+AM.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Sample output of the <a href="https://shap.readthedocs.io/en/latest/">SHAP</a> explainer package. It shows the distribution of overall importance when generating part of the phone number (substring “555” in the green circle). The colors indicate which parts of the input the model deemed to have positive (red) and negative (blue) contributions. In this case the model mainly looked at the keyword “Phone” and the phone number itself.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/7c3c41e0-5c6b-4d11-b77f-45198edb3aa1/Screen+Shot+2022-08-11+at+10.49.48+AM.png" data-image-dimensions="1927x800" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62f54423f5d9074d657611b1" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/7c3c41e0-5c6b-4d11-b77f-45198edb3aa1/Screen+Shot+2022-08-11+at+10.49.48+AM.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Sample output of the <a href="https://shap.readthedocs.io/en/latest/">SHAP</a> explainer package. It shows the distribution of overall importance when generating the potential resident’s first name&nbsp; (substring “Jon” in the green circle). The colors indicate which parts of the input the model deemed to have positive (red) and negative (blue) contributions. In this case the model mainly looked at the keyword(s) “first name”, “Jon” and “My name is Jon…”.&nbsp;</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">We chose this model class because the label generation is straight-forward and performance is strong. Lisa simply maps input to target string, and we do not have to annotate exactly where to copy the data from, as would be required of more traditional token classification models. Lisa can read the input and determine the relevant information. There is also no need to post-process the parsed fields to obtain their canonical representation, such as for dates and phone numbers.</p>




<p class="">One important catch during data generation is that we have to ensure that the value we want to parse is actually present in the source. Otherwise, the model will tend to generate information that is not present. We implemented the same safeguard as a post-processing step, in order to avoid returning occasional “typos.”</p><p class="">In addition to the possibility of typos, another drawback of Seq2Seq models is that there is no obvious way to generate confidence scores. Seq2Seq models output a whole sequence, with the confidence of each predicted word depending on all the previously predicted words in the sentence. This makes it difficult to get a confidence score for the generated sequence or subsequences. Lisa generates confidence scores based on the similarity between the new ILS message and the messages previously used for training the model, as well as the score of the words from which we extract the information.</p><p class="">Lisa’s ML parser has reduced the number of unparsed inquiries to nearly zero and greatly improved the accuracy of data when conducting source attribution. Additionally, the parser has significantly reduced the workload of our operators, who would have had to parse them manually, and our developers who had to maintain the complex parsing code.</p><p class="">The inquiry parser is just one of many exciting components that make up Lisa. Stay tuned for the next post, as we deep dive into the main driver of our conversational system that will leave you questioning whether or not you are actually speaking to an AI.</p><p class=""><br>Authors and contributors: Christfried Focke, Shyr-Shea Chang, Tony Froccaro, Miguel Rivera</p>]]></description><media:content type="image/png" url="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1662008318446-3MU5XXTJBGV9AQAM1U1U/Screen+Shot+2022-08-10+at+10.19.30+PM.png?format=1500w" medium="image" isDefault="true" width="1500" height="620"><media:title type="plain">Lisa: Parsing Inquiries with Seq2Seq Transformers</media:title></media:content></item><item><title>AppFolio’s AI Leasing Assistant, Lisa </title><dc:creator>Christfried Focke</dc:creator><pubDate>Fri, 22 Jul 2022 21:28:21 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2022/7/22/appfolios-ai-leasing-assistant-lisa</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:62db0d3d3e48c43aae8dbe03</guid><description><![CDATA[<p class="">Let’s delve into the past and think about the last time you tried to rent an apartment. Hopefully, this doesn’t trigger any painful memories…</p><ol data-rte-list="default"><li><p class="">First, you went to one of the familiar listing sites — Apartments.com, Zillow, or maybe even Craigslist if you’re the type to live dangerously.</p></li><li><p class="">You sent a message or made a phone call to the places that piqued your interest.</p></li><li><p class="">You heard back from some of the inquiries, but from others - silence. You might have even ended up on the phone with a grumpy landlord who acted as if you were wasting their time, or with an intern who couldn’t provide you with an ounce of concrete information.</p></li><li><p class="">Once you made a connection, you scheduled a showing to take a look at the place. Most likely, this was coordinated over text, email, a phone call, or social media.</p></li><li><p class="">You eventually found a place, but you were still left with a lingering sense of doubt. You may have missed out on another fantastic place to rent just because of a communication breakdown.</p></li></ol><p class="">On paper, the process of finding housing sounds simple and relatively straightforward. However, in practice, it’s often a convoluted process –&nbsp; one that includes many hardships and unpredictability. In fact, only 5% of all rental inquiries end with the filing of an application. Put shortly, the leasing process often results in a large amount of wasted effort, both for prospective residents and property managers.&nbsp;</p><p class="">Traditionally, the point of contact for prospective renters has been a leasing agent who engages in time-consuming and repetitive conversations with potential renters. Because so many of these conversations follow a predictable script, AppFolio identified a unique opportunity to streamline the process through automation.&nbsp;</p><p class="">AppFolio AI Leasing Assistant, Lisa, is AppFolio’s response to this problem. Lisa outsources the repetitive and time-consuming parts of leasing conversations while retaining human operators to cover the long tail and provide training data for future automation. Below is a diagram showcasing the processes Lisa is capable of carrying out, and a mock scenario of Lisa at work. </p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/f6740eb1-bf4b-45d1-bd7b-8d0b474c1510/Conversation+Flow.png" data-image-dimensions="1968x737" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62dc2d334f15720fadfa6314" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/f6740eb1-bf4b-45d1-bd7b-8d0b474c1510/Conversation+Flow.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class=""><em>Typical flow of a leasing conversation, Blue boxes indicate steps that are automated by Lisa.</em></p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<h2>Initial Onboarding of a Prospective Resident</h2><p class="">We’ll start with the customer. George is looking for a new place and browses familiar listing sites. Eventually, he sends out a few inquiries. One of these inquiries makes it to Lisa, who automatically parses out George’s profile information, and stores George’s interest in the property’s sales database. Lisa will also use this information to initiate a text conversation with George in less than one or two minutes.</p><h2>Answer Questions, Prompt for Sales Information</h2><p class="">George has come prepared and wants to ask a series of questions most prospective renters ask when in search of a new home:</p><p class=""><em>“Are pets allowed in the apartment? What is the estimated price of utilities per month? What is the floor plan of the available units?”</em></p><p class="">Lisa can detect and answer these common questions automatically by drawing on a standardized set of policies the property sets. Lisa will also nudge the conversation forward. She’ll request additional contact information and suggest times when George might come by the apartment for a tour. </p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/6bfc006c-bf23-4a88-b931-790c97fa91b1/Lisa+Conversation+1.1.png" data-image-dimensions="694x611" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62dc2f652b528652a0f81072" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/6bfc006c-bf23-4a88-b931-790c97fa91b1/Lisa+Conversation+1.1.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">LISA CONVERTS THE NATURAL LANGUAGE INTO A STRUCTURED RESPONSE THAT ALLOWS THE LOGIC LAYER TO COMPUTE THE RESPONSE.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<h2>Find a Showing Time and Handle Scheduling Conflicts</h2><p class="">George would like a showing, but he works long hours and can only attend a showing during the evening or on weekends. This could be a problem, not only for George but also for the property managers, since agents have limited availability on the weekends, and agents are sometimes booked weeks in advance. George tells Lisa his availability and Lisa immediately cross-checks both parties’ schedules to find a time that works.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/a09861ef-4a85-4a9d-85a0-c7934f09f476/Lisa+Counteroffer.png" data-image-dimensions="720x563" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62dc2c274c0284695f41f4ac" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/a09861ef-4a85-4a9d-85a0-c7934f09f476/Lisa+Counteroffer.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">LISA PARSES THE SHOWING TIME PREFERENCE AND CAN RESPOND TO COUNTEROFFERS.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">Lisa’s value proposition should be clear: Lisa can maintain any number of parallel conversations with prospective renters, provide excellent customer service, and free up time for property managers to focus less on the minutiae and more on the big picture.&nbsp;</p><h2>Overview: How Does Lisa Work?&nbsp;</h2><p class="">AppFolio retains a staff of operators, who get a chance to review conversations as they unfold, rapidly re-label messages on the fly, and use tools to handle edge cases or language that our current models fail to understand.&nbsp;</p><p class="">Lisa uses a collection of concepts and models to achieve these outcomes including:</p><ul data-rte-list="default"><li><p class="">A parser for inquiries from Internet Listing Services (ILS)</p></li><li><p class="">A dialog system that combines</p><ul data-rte-list="default"><li><p class="">Natural Language Understanding (NLU)&nbsp;</p></li><li><p class="">Dialogue State Tracking&nbsp;</p></li><li><p class="">Policy (a logic layer that combines conversation state and external knowledge to decide the next step)&nbsp;</p></li><li><p class="">Template-based Natural Language Generation (NLG)</p></li></ul></li><li><p class="">A recommendation system for cross-sells</p></li><li><p class="">A forecasting model to determine hiring goals and set staffing levels for each shift</p></li></ul><p class="">Stay tuned as we will discuss each of these components in greater detail as part of a series of blog posts surrounding the nuances and technologies!</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1120b7f0-f976-489a-910f-80480247e4a1/Lisa+System+Diagram.png" data-image-dimensions="1448x1095" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62dc2d84d29a5377dcc4a417" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1120b7f0-f976-489a-910f-80480247e4a1/Lisa+System+Diagram.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Lisa integrates with several external systems via APIs and Natural language. </p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">Authors and contributors: Christfried Focke, Shyr-Shea Chang, Tony Froccaro, Ian Murray</p>]]></description><media:content type="image/png" url="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1661964640472-LYUGVOKLGDNX7GU13AKI/Lisa+Conversation+1.1.png?format=1500w" medium="image" isDefault="true" width="694" height="611"><media:title type="plain">AppFolio’s AI Leasing Assistant, Lisa</media:title></media:content></item><item><title>Quality Assurance at AppFolio Property Manager 2021</title><dc:creator>Paul Pham</dc:creator><pubDate>Fri, 28 May 2021 05:35:31 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2021/5/27/quality-assurance-at-appfolio-property-manager-2021</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:60b08128e61fad503f8c9161</guid><description><![CDATA[<figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1622185152919-BGSZFWIQAM0G0Y6F7L83/QA-logo.png" data-image-dimensions="385x378" data-image-focal-point="0.5,0.5" alt="QA-logo.png" data-load="false" data-image-id="60b094c071dc6c749deac8c6" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1622185152919-BGSZFWIQAM0G0Y6F7L83/QA-logo.png?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  




<p class="">An AppFolio engineering team typically consists of three to five software engineers, a product manager, a UX designer,  and a QA engineer. Within our agile-based teams, the QA Engineer is in a unique position. The QA engineer can have a wide range of experience across different domains due to the cross-functional responsibilities that the role requires. Depending on the team and situation, the role’s breadth of knowledge overlaps with the product expert, voice of the customer, developer, product manager, agile coach, scrum master, customer support, and site reliability engineer. Within AppFolio Property Manager (APM), a QA Engineer has the potential to impact their teams much more than in an organization where the QA roles are specialized as a Software Tester, Automation Engineer, or QA Analyst. <em>Great people make a great company </em>is one of our company values and keys to continued success, and it works best when a QA Engineer can contribute as a generalist.</p><p class="">In many organizations the definition of QA Engineer defaults to <em>Tester</em> with distinct spots where they contribute to the development process.&nbsp;They find bugs. They wait for discovery and discussions to happen first. They wait for UX designers to create their mockups and prototypes. They wait for the product manager to decide the features and the team to provide the acceptance criteria.&nbsp;They wait for software engineers to write the code. They wait for their <em>turn</em> in the product development process. They wait for the work to come to them. At AppFolio, we prefer to have the entire team collaborate throughout the product development process with QA Engineers on the team promoting the quality mindset from discovery and design through supporting production applications.&nbsp;</p><p class="">While quality assurance at other software companies can operate with a singular testing focus, AppFolio’s entire product development process involves QA Engineers. Quality assurance is not an end of the assembly line task for us, it demands constant questioning, discussion, and considerations throughout each sprint. Quality is always at the forefront of the QA Engineer’s mind, not only during the testing phase. We look for curious and creative individuals with diverse backgrounds and problem-solving abilities, knowing that they will influence and shape our product development process and idea of quality. When it comes to maintaining quality in our engineering organization, our QA Engineers’ domains for quality should pertain to teams, processes, testing, and people.</p><h2>The APM QA Engineer Mindset</h2><p class="">An important part of the role is understanding the QA mindset at AppFolio. It influences how we approach our work and responsibilities. We find that a quality mindset encompasses more than testing. We emphasize curiosity and creativity. Curiosity compels us to ask “why?” and then continue to ask questions in every step of the development process. Creativity can provide a different outlook when identifying risks and assumptions. A curious and creative QA Engineer can help their team succeed beyond testing. Applying quality at a higher level means a QA Engineer has an impact on addressing the needs around teams, culture, and the organization. The way we see it, a QA Engineer is a servant leader within their team.</p><h2>The APM QA Engineer as a Servant Leader</h2><p class="">As a QA Engineer, servant leadership is integral because we are first and foremost a support role within our engineering teams. QA Engineers do their best to help the team succeed in any way possible. This allows for an open mind to approach any number of issues. We focus on supporting the software engineers, UX designers, and product managers within our team. We also mentor other QA Engineers to reinforce the mindset and culture. It is a positive feedback loop that keeps teams, individuals, and the overall engineering organization growing in a healthy way. A QA Engineer is the glue of the team working in all capacities to figure out how to continually improve communication and facilitate the work being done. We have a commitment to the team’s success. Being a servant leader means wearing different hats and adjusting responsibilities that the role may require as priorities and teams change.</p><h2>APM QA Engineer Responsibilities</h2><h3><strong>Product Knowledge:</strong></h3><p class="">The cornerstone of the best QA Engineers lies in their APM product knowledge. As the product’s scale continues to grow with weekly releases, a team’s QA Engineer becomes more indispensable as a product expert even across features distant from their team’s focus. Product knowledge can also include technical understanding of other apps and services that support APM running smoothly. It has a domino effect. Teams have more context in their domain and can better identify the risks that accompany development there. Additionally, the QA Engineer’s product knowledge minimizes assumptions and potential bugs so when features and functionality are discussed, developed, and tested, there is confidence of due diligence done and clear collaborative efforts in quality through the entire process. By leveraging product knowledge during grooming, testing, and demos we can voice possible user concerns and contribute ideas that result in an overall better user experience.</p><h3><strong>Identifying risks and reducing assumptions:</strong></h3><p class="">Mitigating risks and reducing assumptions doesn’t necessarily mean “test better” when the story is ready to test. That would narrow and limit the QA Engineer’s contributions. Since we are involved in testing and most development phases, it is important to stay in the loop every step of the way. Being involved in team discussions and customer calls during product discovery allows us to contribute to grooming stories and setting weekly goals before any code is written and deployed. Bugs will occur and when they do, we have processes in place to ensure we respond and learn from them both as an individual QA as well as during team retrospectives. As an effective communicator, knowing how to write out bugs not only for developers, but for business teams like onboarding, implementation, and services helps to promote trust and set customer expectations by keeping all stakeholders informed with regular updates. Continually learning the product over time in the role provides the team with a more varied perspective regarding risks and assumptions within the current set of stories groomed.</p><h3><strong>Testing:</strong></h3><p class="">When it comes to testing, a curious and creative QA Engineer will excel with exploratory testing. We use it to surface problems, reduce assumptions, and drive our own learning. We combine an understanding of the user experience, product knowledge, technical skills, and automated tests to deliver features and functionality designated as “done.” The test cases, acceptance criteria, and assumptions are verified at the QA engineer’s discretion and choice of tools. Occasionally, test cases, acceptance criteria, and risks are not obvious, so we value a QA Engineer’s curiosity and creativity to snuff them out and bring them to the team’s attention. Sometimes, this can mean demoing the story with the team to review all changes that will be merged. We firmly believe in this creative approach to testing and every QA Engineer has access to the same tools and environments that a software engineer does. Testing is a review of the team’s combined effort from grooming to release, with the goal of assuring that users receive the best possible experience with new features and functions going into the existing product.</p><h3><strong>Releases:</strong></h3><p class="">Releases fall under QA’s purview in terms of tracking when new features and functionalities are delivered to customers. QA Engineers are the team’s source of truth for release dates, tracking sprints, enabling experimental features (what other companies may call feature flags), and determining which customer segments are included in feature rollouts. QA Engineers can exercise judgment and raise concerns with the team when merging features might pose problems with the user experience, affect migrations, or conflict with other teams’ changes. Being aware of releases means staying cognizant of software updates and maintaining constant watch over quality of the product at large. When it comes to bugs, being able to determine the severity, the dates the bug was released and found, when the fix will reach customers, and whether patching is necessary requires QA Engineers to stay on top of the moving parts of our continuous release cycle. Monitoring production post-release is important to track feature usage data, ensure any exception errors logged are addressed, and make sure new features work as expected.</p><h3><strong>Technical:</strong></h3><p class="">Since we have access to the same tools as our software engineers, technical acumen helps. A basic understanding of Ruby, SQL, databases and migrations, internal tools, Git, and automated tests allows us to support the team better. Git experience allows us to manage the process from merging feature branches, creating release branches, and scheduling deployments to customers. We can cherry-pick bug fixes, schedule releases for non-APM apps, write SQL queries to gather data for the team to inform the decision-making process and check assumptions, and use understanding of automated tests to focus exploratory testing. In some cases, we contribute to technical groomings or code reviews. To go further than that, our knowledge of AWS, infrastructure, and APIs allows us to apply the QA process to backend engineering teams. There is no limit for a QA Engineer to how more technical knowledge can help better serve the team.</p><h3><strong>Headlights of the Team:</strong></h3><p class="">This is the one that may register the least initially, but it comes from being a servant leader. It requires time and experience being immersed in a team to understand the people dynamics, process, and blockers that might appear. It might mean bringing up difficult topics in team retrospectives or in one-to-one meetings. Developing strong communication and empathy helps a QA Engineer understand people, situations, and psychological safety on teams. QA Engineers can also be the one to observe their team’s processes to identify where work is slow, if bugs are affecting productivity, if stories aren’t sliced small enough, and if we are addressing customer feedback adequately. Taking these concerns back to team retrospectives for example will let the team tackle these problems sooner rather than later and not let them grow into major blockers.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1622185567675-604T5ECXJLN7F8R7J51Y/modern-agile.jpeg" data-image-dimensions="1153x1153" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="60b0965f858be01626c1504c" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1622185567675-604T5ECXJLN7F8R7J51Y/modern-agile.jpeg?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  




<h3><strong>Keepers of modern agile:</strong></h3><p class="">QA Engineers share the duty to promote a healthy culture and development best practices based on <a href="https://www.infoq.com/articles/modern-agile-intro/">agile guiding principles</a>.<strong> </strong>While trying to embody these principles, we leverage development to deliver value to customers faster, eschew waterfall development tendencies, and promote AppFolio engineering culture.&nbsp;</p><h3><strong>Onboarding and mentoring:</strong></h3><p class="">This is an essential part of growing the QA mindset, AppFolio engineering culture, and setting expectations for all QA Engineers joining our company. QA Engineers are not placed on a team by themselves on day one. Rather they have another QA Engineer as their mentor. They learn the product and their role within the team over a span of one to two months of onboarding, setting weekly goals along the way. They learn about the resources, tools, and help at their disposal through pairing often with their mentors. Multiple QA Engineers can pair and track the progress of the new hire over the onboarding period to make sure they are fully supported. Great QA Engineers support each other so that they can learn how to best support teams when they are on their own. The onboarding and mentoring process is never complete and is regularly evaluated. We constantly try to improve it so that new hires are set for success from their first day and during their entire tenure.</p><h3><strong>Hiring:</strong></h3><p class="">The hiring process for QA Engineers is owned by QA team from managers and leads to engineers. QA Engineers involved in interviews constantly ask themselves, what’s going well, what isn’t, and what can we do better with the interview process. We are not afraid to scrutinize our interview structure and exercises. We receive feedback from interviewers and interviewees, and work hard to ensure the people we hire meet our criteria for culture fit, mindset, and career growth. We have open conversations to examine what we can learn from other companies’ interview practices. The interview process usually has multiple parts that evaluate a candidate’s problem-solving ability, willingness to learn, and general creative thinking.</p>


<hr />

<p class="">Many responsibilities are listed here and the best among us may be able to do all of the above, but more often than not, it is that a QA Engineer here knows how to adapt and tap into different skills depending on their team or the QA organization’s needs at the moment. </p><h2>Who We Are</h2><p class="">In QA, many of us have quite different backgrounds and are united by our ability to think creatively and curiously. We share the QA mindset even with differing experiences and starting points into the role.&nbsp;The varied responsibilities mentioned in the previous section demonstrate how we bring our range and experiences to QA. Here are a few current QA Engineers’ growth and paths in our organization.</p><ol data-rte-list="default"><li><p class=""><strong>Topher</strong> <strong>H.</strong> began his AppFolio career as a QA Engineer, right after graduation from UCSB with a Computer Engineering degree. His technical background and QA mindset allow him to take ownership and direct our entire production release train. Topher functions as a strong individual contributor on his teams while being invaluable to the QA engineering organization managing significant parts of the QA Engineer interview process and invested in growing the careers of the several QA Engineers reporting to him. Topher works on both feature and infrastructure teams, able to adapt to any team’s given needs.&nbsp;</p></li><li><p class=""><strong>Sam P.</strong> worked primarily in the Oil &amp; Gas industry before coming to AppFolio. From his previous career and experiences, Sam has a strong background with risk management, data analysis, managing client expectations, developing internal software tools, and project management. Sam leads his team’s technical groomings, comfortable with both navigating the codebase and exploratory testing for testing, releases, and discussions. He currently supports our engineering teams working on machine learning features within the property maintenance space.</p></li><li><p class=""><strong>Anna G.</strong> started out in APM Customer Success and eventually transitioned into the QA Engineer role after a few years. Anna brings a wealth of product knowledge to engineering after spending years directly supporting our customers and their businesses. She is able to identify risks early in the process, knowing how all moving parts of the software work together from the user interface and database. As a tech lead within the accounting domain, many of our engineering teams focused on accounting initiatives benefit from her expertise and advice.</p></li></ol><h2>So Anyway…</h2><p class="">A QA Engineer has many tools to support other individuals, their teams, and the organization. A successful QA Engineer has not only testing skills, but an aptitude and willingness to improve in other responsibilities and skills, including leadership, culture, ideation, processes, business strategy, computer science, product management, customer service, and hiring to name a few. It is unique and empowering that a QA Engineer at AppFolio has so many avenues for growth.</p><p class="">For a QA Engineer at AppFolio to be successful, a persistent curiosity and willingness to learn are a daily requirement. Each day, week, and month involves a varying degree of responsibility that ranges from mentoring, testing, leading teams, interviewing and hiring, managing projects, collaborating cross-functionally, learning about product improvements, and more. The value of our role lies beyond simply “tester” and has the marks of a generalist that can plunge in and adapt as a specialist depending on the situation. In all instances, quality is our primary concern from top to bottom. We know a little bit of everything and then lean in to where our teams and the organization need us most to ensure we are uplifting those around us to deliver the best they can, including ourselves. Come join us or reach out to learn more.&nbsp;</p><p class="">Books we have read, referenced, or would suggest:</p><ul data-rte-list="default"><li><p class=""><em>Agile Testing Guide</em> by Lisa Crispin and Janet Gregory</p></li><li><p class=""><em>Think Fast and Slow </em>by Daniel Kahneman&nbsp;</p></li><li><p class=""><em>Dynamic Reteaming</em> by Heidi Helfand</p></li><li><p class=""><em>The Lean Startup</em> by Eric Ries</p></li><li><p class=""><em>Creativity, Inc.</em> by Ed Catmull</p></li><li><p class=""><em>Leaders Eat Last</em> by Simon Sinek</p></li><li><p class=""><em>Range</em> by David Epstein</p></li><li><p class=""><em>Inspired: How to Create Products Customers Love</em> by Marty Cagan</p></li><li><p class=""><em>Good Strategy, Bad Strategy</em> by Richard Rumelt</p></li></ul>]]></description><media:content type="image/png" url="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1622185027483-PQLVIYRGC6DEG393HY1Z/QA-logo.png?format=1500w" medium="image" isDefault="true" width="385" height="378"><media:title type="plain">Quality Assurance at AppFolio Property Manager 2021</media:title></media:content></item><item><title>Using code coverage data to speed up continuous integration and reduce costs</title><dc:creator>Zach Walker</dc:creator><pubDate>Mon, 04 May 2020 22:06:18 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2020/5/4/using-code-coverage-data-to-speed-up-continuous-integration-and-reduce-costs</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5eb08d08f78721222d15200f</guid><description><![CDATA[<p class="">One of the disadvantages of having a large monolith is the tendency to have even small changes take a long time to merge.&nbsp; At Appfolio, like many other software providers, we are transitioning from a monolith to smaller consumable web services.&nbsp; However, we have had considerable success in building and maintaining one of the largest monolithic ruby/rails applications that we are aware of and it is not going to disappear anytime soon. &nbsp; In early 2019 we made it a goal to reduce the amount of time it takes for a developer working on our core monolith to:</p><ol data-rte-list="default"><li><p class="">Clone the git repository</p></li><li><p class="">Install dependencies</p></li><li><p class="">Make a trivial change</p></li><li><p class="">Start the development server</p></li><li><p class="">Run a single test locally</p></li><li><p class="">Push a new branch to git</p></li><li><p class="">Wait for continuous integration (CI) to run all tests</p></li></ol><p class="">We call this the “developer loop” and have found that the CI step takes much longer than any other step, initially 53 minutes and 77% of total time.&nbsp; Given that CI takes most of the time we have invested in reducing our CI run time for the average branch over the last year and a half. &nbsp; To this end, we have done many things including:</p><ul data-rte-list="default"><li><p class="">Transitioned from Team City to Circle CI</p></li><li><p class="">Implemented checksum based emulation of tests when the relevant code is unchanged</p></li><li><p class="">Built a profiler to identify which of our 100+ ci build steps (jobs) are on the CI critical path</p></li><li><p class="">Increased parallelism</p></li><li><p class="">Re-thought job dependencies</p></li><li><p class="">Restricted certain integration tests to release branches only</p></li><li><p class="">Run only tests related to the change set for a branch</p></li></ul><p class="">While our success in reducing our developer loop time has come from a combination of all of these factors, the remainder of this article is about the <a href="https://github.com/appfolio/ae_test_coverage">ae_test_coverage</a> gem we have created to collect code coverage data for use in test selection.</p><h2>Overview </h2><p class="">The purpose of this gem is to record per test method code coverage for every test run in the CI environment.&nbsp; The output is a mapping from source file to test methods.&nbsp; At Appfolio we use this mapping to select which tests to run based on which files have been modified on a branch.&nbsp; For a pure ruby application, traditional code coverage using the built in Coverage module would likely be sufficient.&nbsp; In a Rails web application, the Coverage module on its own is likely not enough to correctly select the super set of tests for a changeset due to extensive metaprogramming used in the Rails framework, non Ruby code such as Javascript, and Ruby code found in .erb template files that is not visible to the Coverage module.&nbsp; The main contributions of this gem are hooks into the internals of Rails to get additional code coverage information that will give a better approximation of the true code coverage of a test including file types like erb templates, javascript, and stylesheets in addition to handling some of the more common metaprogrammed rails internals like ActiveRecord model attributes and associations that normal code coverage will not catch.</p><h2>ERB Templates</h2><p class="">Ideally, we would be able to collect line coverage data for .erb template files just like we can for ruby files.&nbsp; Unfortunately, ruby’s built in Coverage module does not collect coverage data for .erb template files.&nbsp; In a large web application like ours, we have a significant number of Selenium tests where the changes to .erb template files are relevant to the test outcome.&nbsp; At first thought, it would seem like the lack of line coverage for .erb files would be a deal breaker for coverage based testing in a Rails application.&nbsp; Fortunately, we can subscribe to <a href="https://github.com/appfolio/ae_test_coverage/blob/5607c20fa9ded701f79a7fbfe99a386075001415/lib/ae_test_coverage/collectors/action_view/rendered_template_collector.rb#L11"><span><strong>ActiveSupport::Notification</strong> for <strong>!render_template.action_view</strong></span></a> to figure out exactly which .erb template files were rendered during the course of a test.</p><h2>Assets (Javascript and Stylesheets)</h2><p class="">Of course, javascript and css files are not ruby, therefore ruby’s Coverage module has little hope of determining whether or not their code is used during the course of a test.&nbsp; For most unit tests, this is not a problem since the javascript and css are not actually evaluated on the server side.&nbsp; However, in our significant number of selenium tests, changes to css and javascript files are important to what the browser renders and the user actually sees.&nbsp; While perhaps less of a problem for coverage based test selection than .erb templates, tracking of the assets used during the course of a test will make test selection more reliable.&nbsp; Similar to how we handled .erb templates, we hooked into the Rails internals to find out when <a href="https://github.com/appfolio/ae_test_coverage/blob/5607c20fa9ded701f79a7fbfe99a386075001415/lib/ae_test_coverage/collectors/action_view/asset_tag_helper.rb#L7"><span><strong>javascript_include_tag</strong></span></a> or <a href="https://github.com/appfolio/ae_test_coverage/blob/5607c20fa9ded701f79a7fbfe99a386075001415/lib/ae_test_coverage/collectors/action_view/asset_tag_helper.rb#L12"><span><strong>stylesheet_link_tag</strong></span></a> was used in the process of rendering a template.&nbsp; This gives us the set of assets rendered during the course of a test.&nbsp; For an application not using the Sprockets assets pipeline directives, that alone may be enough. &nbsp; However, applications using the Sprockets asset pipeline will have used sprockets directives to modularize their javascript and css code.&nbsp; Fortunately, Rails has a way for us to <a href="https://github.com/appfolio/ae_test_coverage/blob/5607c20fa9ded701f79a7fbfe99a386075001415/lib/ae_test_coverage/collectors/sprockets_asset_collector.rb#L10"><span>find the set of asset files that were collected into a single asset</span></a> via sprockets directives and we use this to make sure that we have a complete code coverage mapping for javascript and stylesheet assets to the tests that actually make use of them.</p><h2>Active Record Models</h2><p class="">Consider the Active Record models below</p>


<pre class="source-code">class A &lt; ActiveRecord::Base
    # Attributes:
    # :name
    has_many: :b

    def foo
        return "you called foo"
    end&nbsp;
end

class B &lt; ActiveRecord::Base
    belongs_to: :a
end</pre>

<p class="">Now consider the following test class that instantiates and references an instance of class A.&nbsp;&nbsp;</p>


<pre class="source-code">class ModelReferenceTest &lt; Minitest::Test
    def test_coverage_registers_method_call
        a_instance = A.new(name: 'a')
        assert_equal 'you called foo', a_instance.foo
    end

    def test_coverage_registers_attribute_reference
        a_instance = A.new(name: 'a')
        assert_equal 'a', a_instance.name
    end

    def test_coverage_registers_associate_reference
        b_instance = B.new
        a_instance = A.new(name: 'a', b: b_instance)

        assert_equal b_instance, a.b
    end
end</pre>

<p class="">In this simple case ruby’s built in Coverage module will correctly determine that the source file defining class <em>A</em> is used by the test <strong>test_coverage_registers_method_call</strong>. &nbsp; However, the Coverage module would not include for source file for class A when <strong>test_coverage_registers_attribute_reference </strong>is run, because the source code that actually implements the lookup of the value of <em>A.name</em> from the database and the initializer for class <em>A</em> actually live somewhere in the implementation of <em>ActiveRecord::Base</em>.&nbsp; A test that creates an instance of class <em>A</em> and refers to <em>A.b</em> will have a similar problem because the code that implements <em>has_many</em> is not actually in class <em>A</em>, only the declaration of the <em>has_many</em> relationship.&nbsp; To handle this and other references to <em>ActiveRecord</em> model attributes and associations, we hook into <em>ActiveRecord</em> to record reads and writes of model attributes and associations. See details <a href="https://github.com/appfolio/ae_test_coverage/tree/779ee23cfa2b5751074425ef1ee131b61f10bc2e/lib/ae_test_coverage/collectors/active_record">here</a>.</p><p class="">Our experience thus far has shown this to be most useful for test selection during refactoring.&nbsp; For example, if I were to remove the <em>has_many</em> association from class <em>A</em> to class <em>B</em>, then I would want to make sure that all tests that previously had referred to <em>A.b</em> were run when going through CI so that I could leverage CI to find test failures which would lead me to the code that needs to be fixed.</p><h2>Webpacker Applications</h2><p class="">One of the ways we at Appfolio have been trying to tame the growth of our Ruby monolith is to increasingly decouple the front end and the backend via use of API’s and thick javascript applications built in React.&nbsp; This has led us to use webpacker as part of our asset pipeline which introduces the need to determine whether or not a set of selenium integration tests needs to be run when one of our React applications change.&nbsp; The webpacker gem provides a <strong>javascript_packs_with_chunks_tag</strong> helper that is used in a similar way to <strong>javascript_include_tag</strong>.&nbsp; However, unlike the <strong>javascript_include_tag</strong>, we can’t depend on Rails to give us the collection of all assets rolled up in the pack. &nbsp; To accomplish this we leverage a glob pattern that is generated based on the value that is passed to <strong>javascript_packs_with_chunks_tag</strong> to account for all source files from the javascript application.&nbsp; Admittedly, this casts a pretty wide net.&nbsp; In our CI configuration, we will run all selenium/integration tests that render a link to the javascript app into a response.&nbsp; However, this is far better than what we were doing before which was to run all selenium/integration tests anytime a webpacker javascript application changed.</p><h2>Usage in CI</h2><p class="">Collecting the code coverage data using ae_test_coverage is only part of the recipe to reduce CI run time.&nbsp; The other parts are the automation of the collection of code coverage data and the selection of tests.&nbsp; At this time, the code for this purpose is not part of ae_test_coverage as the code we have written for this purpose is fairly specific to our repository and circle ci workflow.&nbsp; In this section I describe at a high level how we do it.</p><p class="">Each night, we schedule a run of our entire test suite on the latest master branch commit with ae_test_coverage enabled.&nbsp; Each of the jobs instrumented with ae_test_coverage creates a per test JSON artifact that lists all of the source code that was used during the execution of the test. (All of this is included in the gem except for the part about scheduling it to run each night). &nbsp; In our Circle CI config, we have a step that aggregates all of the individual per test code coverage files into a single compressed code coverage artifact which is stored in Circle CI as an artifact of the build.</p><p class="">At the end of the entire workflow, there is an extra job that depends on all other jobs.&nbsp; If all other jobs have passed and this job runs, it uses the circle ci api to find all of the jobs that preceded it in the workflow and downloads the compressed code coverage artifact for each of the jobs where we are using this code coverage test selection technique.&nbsp; In some cases we may have 50+ nodes running in parallel for a job and in these cases, we have produced a compressed coverage artifact for each parallel node in that job meaning 50+ downloads for that job alone.&nbsp; Once all of the compressed per job node coverage artifacts have been downloaded, we decompress and aggregate them together into a reverse mapping where the keys in the map are the paths to source files and the values in the map are the set of test names that use that source file during the course of running the test.&nbsp; This results in a quite large JSON artifact, ~500MB that we compress down to about 2MB and upload to S3.&nbsp; This aggregate artifact is used by our CI workflow on every development branch to select relevant tests.</p><p class="">At the beginning of every feature branch’s run through our CI workflow, we download the above mentioned aggregate code coverage artifact for the whole test suite and store it in the circle ci workspace for use by all jobs that follow.&nbsp; We also take a diff of the feature branch back to where it branched from master and record the set of file names changed on the branch.&nbsp; The list of changed files is also stored to the Circle CI workspace.&nbsp; Each subsequent job and parallel node of parallel jobs takes the intersection of the set of tests that would normally be run on that node and the unique set of tests that are found in the coverage data for the set of changed files.&nbsp; This reduced set of tests is what is actually run during CI.&nbsp; An artifact listing what files would be run without test selection and what files are actually run is stored for each job node so that it is easy to determine whether or not the test selection has or has not run a specific test file.</p><p class="">Since it is our goal to speed up the development process but still prevent any bugs that may slip through our test selection strategy, we still run all tests on our master branch for every merged pull request and every release candidate.</p><h2>New Code and New Tests</h2><p class="">Of course code is changing all the time and this is especially true in a monolith with 100+ engineers committing to it daily.&nbsp; This brings up a few issues with coverage based test selection.&nbsp; First, we don’t have code coverage data for new tests written on a feature branch so we can’t use the techniques described here to select them.&nbsp; In this case, we have decided to simply always run all modified test files on a branch. &nbsp; Another problem to consider is how fast stale coverage data begins to result in code coverage based test selection failures where broken tests get to master. &nbsp; We don’t have great data on this, but anecdotal evidence, from an incident where code coverage data was not updated for nearly a month without anyone noticing, indicates that there was not a significant increase in broken tests getting to master even with a significant amount of new code committed during that time.</p><h2>Conclusion</h2><p class="">Using the technique described in this article, we are able to run the subset of our automated test suite that is most relevant to the changes made on a feature branch. &nbsp; After implementing this technique for the portions of our test suite that take the most CI resources, we have been able to reduce the average CI cost of a development branch by 25%.&nbsp; Measuring how much time this has saved the average developer in a highly parallelized CI build environment is more difficult.&nbsp; We didn’t implement this strategy until after highly optimizing our CI workflow meaning we were already looking for smaller returns by the time we tried this. &nbsp; What I can say is that before we implemented this technique our build timing looked like this:</p><ul data-rte-list="default"><li><p class="">5% of builds took &lt; 15 min</p></li><li><p class="">27% &lt; 20 min</p></li><li><p class="">77% &lt; 25 min</p></li></ul><p class="">And after:</p><ul data-rte-list="default"><li><p class="">5% of builds took &lt; 15 min</p></li><li><p class="">70% &lt; 20 min</p></li><li><p class="">95% &lt; 25 min</p></li></ul><p class="">In our case the CI cost savings may be more beneficial than the time savings, but your mileage may vary.</p>]]></description></item><item><title>How Much Do You Save With Ruby 2.7 Memory Compaction?</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 14 Feb 2020 18:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/12/6/how-much-do-you-save-with-ruby-27-memory-compaction</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5dea2e2553ac1c48c12d3485</guid><description><![CDATA[<p class="">If you read this blog recently, you may have seen that Ruby Memory Compaction in 2.7 doesn’t affect the speed of Rails Ruby Bench much. That’s fair - RRB isn’t a great way to see that because of how it works. Indeed, Nate Berkopec and others have criticised RRB for that very thing.</p><p class="">I think RRB is still a pretty useful benchmark, but I see their point. It is <strong>not</strong> an example of a typical Rails deployment, and you could write a great benchmark centred around the idea of “how many requests can you process at most <strong>without compromising latency at all?”</strong> RRB is <strong><em>not</em></strong> that benchmark.</p><p class="">But that benchmark wouldn’t be perfect for showing off memory compaction either - and this is why we need <a href="https://www.youtube.com/watch?v=kJDOpucaUR4">a variety of performance benchmarks</a>. They show different things. Thanks for coming to my TED talk.</p><p class="">So how <strong><em>would</em></strong> we see what memory compaction does?</p><p class="">If we wanted a really contrived use case, we’d show the “wedged” memory state that compaction fixes - we’d allocate about one page of objects, free all but one, do it over and over and we’d wind up with Ruby having many pages allocated, sitting nearly empty, unfreeable. That is, we could write a sort of bug reproduction showing an error in current (non-compacting) Ruby memory behaviour. “Here’s this bad thing Ruby can do in this specific weird case.” And with compaction, it doesn’t do that.</p><p class="">Or we could look at total memory usage, which is <strong><em>also</em></strong> improved by compaction.</p><h2>Wait, What’s Compaction Again?</h2><p class="">You may recall that <a href="https://docs.google.com/presentation/d/1-WrYwz-QnSI9yeRZfCCgUno-KOMuggiGHlmOETXZy9c/edit#slide=id.p">Ruby divides memory allocation into tiny, small and large</a> objects - each object is one of those. Depending on which type it is, the object will have a reference (always), a Slot (except for <a href="http://engineering.appfolio.com/appfolio-engineering/2019/6/25/how-ruby-encodes-references-ruby-tiny-objects-explained">tiny objects</a>) and a <a href="https://www.geeksforgeeks.org/stack-vs-heap-memory-allocation/">heap allocation</a> (only for large objects.)</p><p class="">The problem is the Slots. They’re <a href="https://en.wikipedia.org/wiki/Slab_allocation">slab-allocated</a> in large numbers. That means they’re cheap to allocate, which is good. But then Ruby has to track them. And since Ruby uses C extensions with old C-style memory allocation, it can’t easily move them around once it’s using them. Ruby deals with this by waiting until you’ve free all the Slots in a page (that’s the slab) of Slots, then freeing the whole thing.</p><p class="">That would be great, except… What happens if you free all but one (or a few) Slots in a page? Then you can’t free it or re-use it. It’s a big chunk of wasted memory. It’s not <strong><em>quite</em></strong> a leak, since it’s tracked, but Ruby can’t free it while there’s even a single Slot being used.</p><p class="">Enter the Memory Compactor. I say you <strong><em>“can’t easily move them around.”</em></strong> But with significant difficulty, a lot of tracking and burning some CPU cycles, actually <strong><em>you totally can</em></strong>. For more details I’d recommend watching <a href="https://www.youtube.com/watch?v=1F3gXYhQsAY">this talk by Aaron Patterson</a>. He wrote the Ruby memory compactor. It’s a really good talk.</p><p class="">In Ruby 2.7, the memory compactor is something you have to run manually by calling “GC.compact”. The plan (as announced in Nov 2019) is that for Ruby 3.0 they’ll have a cheaper memory compactor that can run much more frequently and you won’t have to call it manually. Instead, it would run on certain garbage collection cycles as needed.</p><h2>How Would We Check the Memory Usage?</h2><p class="">A large, complicated Rails app (<strong>cough</strong> <em>Discourse</em> <strong>cough</strong>) tends to have a lot of variability in how much memory it uses. That makes it hard to measure a small-ish change. But a very simple Rails app is much easier.</p><p class="">If you recall, I have a <a href="http://engineering.appfolio.com/appfolio-engineering/2019/4/11/a-simpler-rails-benchmark-puma-and-concurrency">benchmark that uses an extremely simple Rails app</a>. So I added the ability to check the memory usage after it finishes, and <a href="https://github.com/noahgibbs/rsb/blob/master/rails_test_app/config/initializers/compaction.rb">a setting to compact memory at the end of its initialisation</a>.</p><p class="">A tiny Rails app will have a lot less to compact - mostly classes and code, so there’s less in a small Rails app. But it will also have a <strong>lot</strong> less variation in total memory size. Compaction or no, <a href="http://engineering.appfolio.com/appfolio-engineering/2019/2/25/benchmarking-hongli-lais-new-patch-for-ruby-memory-savings">Ruby doesn’t usually free memory back to the operating system</a> (like other dynamic languages), so a lot of what we want to check is whether the total size is smaller after processing a bunch of requests.</p><p class="">A Rails server, if you recall, <a href="https://www.schneems.com/2019/11/07/why-does-my-apps-memory-usage-grow-asymptotically-over-time/">tends to asymptotically approach a memory ceiling as it runs requests</a>. So there’s <strong>still</strong> a lot of variation in the total memory usage. But this is a benchmark, so we all know I’m going to be running it many, many times and comparing statistically. So that’s fine.</p><h2>Methodology</h2><p class="">For this post I’m using Ruby 2.7.0-preview3. That’s because memory compaction was added in Ruby 2.7, so I can’t use a released 2.6 version. And as I write this there’s no final release of 2.7. I don’t have any reason to think compaction will change size later, so these memory usage numbers should be accurate for 2.7 and 3.0 also.</p><p class="">I’m using Rails Simpler Bench (RSB) for this (<a href="https://github.com/noahgibbs/rsb">source link</a>). It’s much simpler than Rails Ruby Bench and far more suitable for this purpose.</p><p class="">For now, <a href="https://github.com/noahgibbs/rsb/blob/master/rails_test_app/config/initializers/compaction.rb">I set an after_initialize hook in Rails to run when RSB_COMPACT is set to YES</a> and I don’t do that when it’s set to NO. I’m using 50% YES samples and 50% NO samples, as you’d expect.</p><p class="">I run the trials in a random order with a <a href="https://github.com/noahgibbs/rsb/blob/master/runners/shell/compaction.rb">simple runner script</a>. It’s running Puma with a single thread and a single process - I was repeatability far more than I want speed for this. It’s hitting an endpoint that just statically renders a single string and never talks to a database or any external service. This is as simple as a Rails app gets, basically.</p><p class="">Each trial gets the process’s memory usage after processing all requests using <a href="https://github.com/schneems/get_process_mem">Richard Schneeman’s get_process_mem gem</a>. This is running on Linux, so it <a href="https://github.com/schneems/get_process_mem/blob/master/lib/get_process_mem.rb#L84">uses the /proc filesystem to check</a>. Since my question is about how Ruby’s internal memory organisation affects total OS-level memory usage, I’m getting my numbers from Linux’s idea of RSS memory usage. Basically, I’m not trusting Ruby’s numbers because I already know we’re messing with Ruby’s tracking - that’s the whole reason we’re measuring.</p><p class="">And then I go through and analyse the data afterward. Specifically, I use a simple script to read through the data files and compare memory usage in bytes for compaction and non-compaction runs.</p><h2>The Trials</h2><p class="">The first and simplest thing I found was this: this was going to take a lot of trials. One thing about statistics is that <a href="https://en.wikipedia.org/wiki/Statistical_significance">detecting a small effect can take a lot of samples</a>. Based on my first fifty-samples-per-config trial, I was looking for a maybe half-megabyte effect in a 71-megabyte memory usage total, and around 350 kilobytes of standard deviation.</p><p class="">Does 350 kilobytes of standard deviation seem high? Remember that I’m measuring total RSS memory usage, which somewhat randomly approaches a memory ceiling, and where a lot of it depends on when garbage collection happened, a bit on memory layout and so on. A standard deviation of 350kb in a 71MB process isn’t bad. Also, that was just initially - the standard deviation goes down as the number of samples goes up, <a href="https://demonstrations.wolfram.com/DistributionOfNormalMeansWithDifferentSampleSizes/">because math</a>.</p><p class="">Similarly, does roughly 500 kilobytes of memory savings seem small? Keep in mind that we’re not changing big allocations like heap allocations, and we’re also not touching cases where Slots are already working well (e.g. large numbers of objects that are allocated together and then either all kept or all freed.) The only case that makes much of a difference is where Rails (very well-tuned Ruby code) is doing something that doesn’t work well with Ruby’s memory system. This is a very small Rails app, and so we’re <strong><em>only</em></strong> getting some of the <strong><em>best-tuned code in Ruby</em></strong>. Squeezing out another half-megabyte for “free” is actually pretty cool, because other similar-sized Ruby programs probably get a lot more.</p><p class="">So I re-ran with 500 trials each for compaction and no compaction. That is, I ran around 30 seconds of constant HTTP requests against a server about a thousand more times, then checked the memory usage afterward. And then another 500 trials each.</p><h2>Yeah, But What Were the Results?</h2><p class="">After doing all those measurements, it was time to check the results again.</p><p class="">You know those pretty graphs I often put here? This wasn’t really conducive to those. Here’s the output of my processing script in all its glory:</p>


<pre class="source-code">Compaction: YES
  mean: 70595031.11787072
  median: 70451200.0
  std dev: 294238.8245869074
--------------------------------
Compaction: NO
  mean: 71162253.14068441
  median: 70936576.0
  std dev: 288533.47219640197
--------------------------------</pre>

<p class="">It’s not quite as pretty, I’ll admit. But with a small amount of calculation, we see that we <strong>save around 554 kilobytes</strong> (exact: 567,222 bytes) per run, with a standard deviation of around 285 kilobytes.</p><p class="">Note that this does not involve ActiveRecord or several other weighty parts of Rails. This is, in effect, the absolute minimum you could be saving with a Rails app. Overall, I’ll take it.</p><p class="">Did you just scroll down here hoping for something easier to digest than all the methodology and caveats? That’s totally fair. I’ll just add a little summary line, my own equivalent of “<a href="http://www.girlgeniusonline.com/comic.php?date=20110502#.XfNTeDL7RTY">and thus, the evil princess was defeated and the wizard was saved</a>.”</p><p class=""><strong>And so you see, with memory compaction, even the very smallest Rails app will save about half a megabyte. And as Aaron says in his talk, the more you use, the more you save!</strong></p>]]></description></item><item><title>How Do I Use Rails Ruby Bench?</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 31 Jan 2020 18:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/11/28/how-do-i-use-rails-ruby-bench</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5ddf870b2104bb50457339f1</guid><description><![CDATA[<p class="">How do I do these explorations with Rails Ruby Bench? How could <strong>you</strong> do them? There’s <a href="https://github.com/noahgibbs/rails_ruby_bench">full source code</a>, but source code is only one piece of the story.</p><p class="">So today, let’s look at that. The most common way I do it is with AWS, so I’m going to describe it that way. Watch this space for a local version in later weeks!</p><h2>An Experiment</h2><p class="">Rails Ruby Bench is a benchmark, which means it’s mostly useful for experiments in the tradition of the scientific method. It exists to answer questions about performance, so it’s important that I have a question in mind. Here’s one: does Ruby’s new compacting GC make a difference in performance currently? I’ve chosen that question partly because it’s subtle - the answer isn’t clear, and Rails Ruby Bench isn’t a perfect tool for exploring it. That means there will be problems, and backtracking, and general difficulties. That’s not the best situation for easy great results, but it’s <strong><em>absolutely perfect</em></strong> for documenting how RRB works. For a benchmark you don’t want to hear about the happy path. You want to hear how to use it when things are normal-or-worse.</p><p class="">My hypothesis is that compacting GC will make a difference in speed but not a large one. Rails Ruby Bench tends to show memory savings as if it were extra speed, and so if compacting GC is doing a good job then it should speed up slightly. I may prove it or not - I don’t know yet, as I write this. And that’s important - you want to follow this little journey when <em>I still don’t know </em>because you’ll be in the same situation if you do this.</p><p class="">(Do I expect you to actually benchmark changes with Rails Ruby Bench? Probably a few of you. But many, many of you will want to do a benchmarking experiment at some point in your career, and those are always uncertain when you’re doing them.)</p><h2>AWS Setup, Building an Image</h2><p class="">RRB’s canonical measurements are always done using AWS. For the last two-ish years, I’ve always used <a href="https://aws.amazon.com/ec2/instance-types/">m4.2xlarge</a> dedicated instances. That’s a way to keep me honest about hardware while giving you access to the same thing I use. It does, however, cost money. I’ll understand if you don’t literally spin up new instances and follow along.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1574935277969-AQTNTB1IA8L1QOZHYV67/packer_start.png" data-image-dimensions="615x206" data-image-focal-point="0.5,0.5" alt="Packer starts to build your image via “packer build ami.json”" data-load="false" data-image-id="5ddf9aede6b0e7013a36af41" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1574935277969-AQTNTB1IA8L1QOZHYV67/packer_start.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Packer starts to build your image via “packer build ami.json”</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">First you’ll need an image. I already have one built where I can just “git pull” a couple of things and be ready to go. But let’s assume you don’t yet, or you don’t want to use one of my public images. I don’t always keep everything up to date - and even when I do, you shouldn’t 100% trust me to. The glory of open source is that if I screw something up, you can find that out and fix it. If that happens, pull requests are appreciated.</p><p class="">To build an image, first check out <a href="https://github.com/noahgibbs/rails_ruby_bench">the Rails Ruby Bench repo</a>, then cd into the packer directory. You’ll need <a href="https://www.packer.io/intro/getting-started/install.html">Packer installed</a>. It’s software to build VM images, such as the AWS Amazon Machine Image you’ll want for Rails Ruby Bench. This lets us control what’s installed and how, a bit like Docker, but without the extra runtime overhead that Docker involves (Docker would, truthfully, be a better choice for RRB if I knew enough about setting it up and also had a canonical hardware setup for final numbers. I know just enough places where it <strong>does</strong> cause problems that I’m not confident I can get rid of all the ones I <strong>don’t</strong> know.)</p><p class="">Got Packer installed? Now “packer build ami.json”. This will go through a long series of steps. It will create a small, cheap AWS instance based on one of the standard Ubuntu AMIs, and then install a lot of software that Rails Ruby Bench and/or <a href="https://github.com/noahgibbs/rsb">RSB</a> want to have available at runtime. It <em>will not</em> install every Ruby version you need. We’ll talk about that later.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1574940207371-GCM4WWEOCCRK0YKICSEP/packer_stop.png" data-image-dimensions="772x413" data-image-focal-point="0.5,0.5" alt="And after around an hour, you have a Packer image. It’ll print the AMI, which you’ll need." data-load="false" data-image-id="5ddfae2e0c91bc6c3ba0f7f3" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1574940207371-GCM4WWEOCCRK0YKICSEP/packer_stop.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">And after around an hour, you have a Packer image. It’ll print the AMI, which you’ll need.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">(If you do Packer builds repeatedly you <strong>will</strong> get transient errors sometimes - a package will fail to download, an old Ubuntu package will be in a broken state, etc. In most cases you can re-run until it works, or wait a day or two. More rarely something is now broken and needs an update.)</p><p class="">If all goes well, you’ll get a finished Packer image. It’ll take in the neighbourhood of an hour but you can re-use the image as often as you like. Mostly you’ll rebuild when the Ubuntu version you’re using gets old enough that it’s hard to install new software, and you find a reason you need to install new software.</p><h2>An Aside: “Old Enough”</h2><p class="">Not every benchmark will have this problem, but Rails Ruby Bench has it in spades: legacy versions. Rails Ruby Bench exists specifically to measure against a baseline of Ruby 2.0.0-p0. Ruby releases a new minor version every Christmas, and so that version of Ruby is about to turn seven years old, or more than five years older than my youngest kid. It is not young software as we measure it, and it’s hard to even get Ruby 2.0 to compile on Mac OS any more.</p><p class="">Similarly, the version of Discourse that I use is quite old and so are all its dependencies. Occasionally I need to do fairly gross code spelunking to get it all working.</p><p class="">If you have ordinary requirements you can avoid this. Today’s article will restrict itself to 2.6- and 2.7-series Ruby versions. But keep in mind that if you want to use RRB for its intended purpose, sometimes you’re going to have an ugly build ahead of you. And if you want to use RRB for modern stuff, you’re going to see a lot of little workarounds everywhere.</p><p class="">If you ask, “why are you using that Ubuntu AMI? It’s pretty old,” the specific answer is “it has an old enough Postgres to be compatible with the ancient Discourse gems, including the Rails version, while it’s new enough that I can install tools I experiment with like <a href="https://locust.io/">Locust</a>.” But the philosophical answer is closer to “I upgrade it occasionally when I have to, but mostly I try to keep it as a simple baseline that nearly never changes.”</p><p class="">In general, Rails Ruby Bench tries not to change because change is a specific negative in a benchmark used as a baseline for performance. But I confess that I’m really looking forward to Christmas of 2020 when Ruby 3x3 gets released and Ruby 2.0 stops being the important baseline to measure against. Then I can drop compatibility with a lot of old gems and libraries.</p><p class="">You’ll also sometimes notice me gratuitously locking things down, such as the version of the Bundler.  It’s the same basic idea. I want things to remain as constant as they can. That’s not 100% possible - for instance, Ubuntu will automatically add security fixes to older distributions, so there’s no equivalent of a Gemfile.lock for Ubuntu. They <strong>won’t let you</strong> install old insecure versions for more compatibility, though you can use an old AMI for a similar result. But where I can, I lock the version of everything to something specific.</p><h2>Starting an Image</h2><p class="">If you built the AMI above then you have an AMI ID. It’ll look something like this: <strong>ami-052d56f9c0e718334</strong>. In fact, that one’s a public AMI I built that I’m using for this post. If you don’t want to build your own AMI you’re welcome to use mine, though it may be a bit old by the time you need to do this.</p><p class="">If you like the AWS UI more than the AWS command-line tools (they’re both pretty bad), then you can just start an instance in the UI. But in case you prefer the command-line tools, here’s the invocation I use:</p><p class="">aws ec2 run-instances --count 1 --instance-type m4.2xlarge --key-name noah-packer-1 --placement Tenancy=dedicated --image-id ami-052d56f9c0e718334 --tag-specifications 'ResourceType=instance,Tags=[]'</p><p class="">Dismal, isn’t it? I also have a script in the RRB repo to launch instances from my most recent AMI. That’s where this comes from. Also, you’ll need your own keypair since your AWS account doesn’t have a key called noah-packer-1.</p><p class="">You’ll need to look up the IP address for the instance, and eventually you’ll want the instance ID in order to terminate it. I’m going to trust you to do those things - <strong><em>do</em></strong> make sure to terminate the instance. Dedicated m4.2xlarges are expensive!</p><h2>Exploration</h2><p class="">Once you have the AMI and you can in theory start the AMI, it’s time to think about the actual experiment: what does GC compaction do relative to Rails Ruby Bench? And how will we tell?</p><p class="">In this case, we’re going to run a number of Ruby versions with compaction on and off and see how it changes the speed of Rails Ruby Bench, which means running it a lot on different Ruby versions with different compaction settings.</p><p class="">To gather data, you generally need a runner script of some kind. You’re going to be running Rails Ruby Bench many times and it would be silly (and error-prone!) to do it all by hand.</p><p class="">First, here’s a not-amazing runner script of the kind I used for awhile:</p>


<pre><code class="lang-bash">#!/bin/bash -l

# Show commands, break on error
set -e
set -x

rvm use 2.6.5
bundle

for i in ; do
  bundle exec ./start.rb -i 10000 -w 1000 -s 0 --no-warm-start -o data/
done

rvm use 2.7.0-preview2
bundle

for i in ; do
  bundle exec ./start.rb -i 10000 -w 1000 -s 0 --no-warm-start -o data/
done
</code></pre>


<p class="">It’s… fine. But it shows you that a runner script doesn’t have to be all that complicated. It runs bash with -l for login so that rvm is available. It makes sure to break on error - modern Ruby doesn’t get a lot of errors in Discourse, but you <strong>do</strong> want to know if it happens. And then it runs 30 trials each on Ruby 2.6.5 and Ruby 2.7.0-preview2, each with 10,000 HTTP requests and 1,000 warmup (untimed) HTTP requests, with the default number of processes (10) and threads per process (6).</p><p class="">With this runner script you’re better off using a small number of iterations (30 is large-ish) and running it repeatedly. That way a transient slowdown doesn’t look like it’s all a difficulty with the same Ruby. In general, you’re better off running everything multiple times if you can, and I often do. All the statistics in the world won’t stop you from doing something stupid, and reproducing everything is one way to make sure you didn’t do some kinds of stupid things. At least, that’s something I do to reduce the odds of <strong>me</strong> doing stupid things.</p><p class="">There’s a <a href="https://github.com/noahgibbs/rails_ruby_bench/blob/master/runner.rb">better runner</a> to start from now in Rails Ruby Bench. The main difference is that it runs all the trials in a random order, which helps with that “transient slowdown” problem. For GC compaction we’ll want to modify it to run with and without GC compaction for Rubies that have it (2.7-series Rubies) and only with no compaction for 2.6-series Rubies. Here’s what the replacement loop for that looks like:</p>


<pre><code class="lang-ruby">commands = []
RUBIES.each do |ruby|
  TESTS.each_with_index do |test, test_index|
    invocation_wc = &quot;rvm use # &amp;&amp; # &amp;&amp; export RUBY_RUNNER_TEST_INDEX=# &amp;&amp; #&quot;
    invocation_nc = &quot;rvm use # &amp;&amp; # &amp;&amp; RUBY_RUNNER_TEST_INDEX=# &amp;&amp; #&quot;
    if ruby[&quot;2.6.&quot;]  # Ruby is 2.6-series?
      commands.concat([invocation_nc] * TIMES)
    else
      commands.concat([invocation_nc,invocation_wc] * TIMES)
    end
  end
end
</code></pre>


<p class="">It’s not simple, but it’s not rocket science. The WITH_COMPACT and NO_COMPACT snippets are already in the runner because it’s not necessarily obvious how to do that - I like to keep that kind of thing around too. But in general you may need some kind of setup code for an experiment, so remember to remove it for the runs that shouldn’t have it. In this case, there’s not a “compaction setting” for Ruby proper, we just run GC.compact manually in an initialiser script. So those snippets create or remove the initialiser script.</p><p class="">The compaction snippets also set an environment variable, RUBY_COMPACT=YES (or NO.) That doesn’t do anything directly. Instead, RRB will remember any environment variable that starts with RUBY for the run so you can tell which is which. I <strong><em>might</em></strong> have done an overnight run and messed that up the first time and had to re-do it because I couldn’t tell which data was which… But in general, if an environment variable contains RUBY or GEM, Rails Ruby Bench will assume it might be an important setting and save a copy with the run data.</p><p class="">For each experiment, you’ll want to either change the runner in-place or create a new one. In either case, it’s just a random script.</p><p class="">I also changed the RUBIES variable to include more Rubies. But first I had to install them.</p><h2>More Rubies</h2><p class="">There are two kinds of Ruby versions you’ll sometimes want to test: prebuilt and custom-built. When I’m testing ordinary Ruby versions like 2.6.0, 2.6.5 or 2.7.0-preview2, I’ll generally just install them with RVM after I launch my AWS instance. A simple “rvm install 2.6.5” and we’re up and running. The new runner script will install the right Bundler version (1.17.3) and the right gems to make sure RRB will run properly. That can be important when you’re testing four or five or eight different Ruby versions - it’s easy to forget to “bundle _1.17.3_ install” for each one.</p><p class="">If you want to custom-build Ruby, there’s slightly more to it. The default Packer build creates one head-of-master custom build, but of course that’s from whenever the Packer image was built. You may want one that’s newer or more specific.</p><p class="">You’ll find a copy of the Ruby source in /home/ubuntu/rails_ruby_bench/work/mri-head. You’ll also find, if you run “rvm list”, that there’s an ext-mri-head the same age as that checkout. But let’s talk about how to make another one.</p><p class="">We’re exploring GC compaction today, so I’m interested in specific changes to Ruby’s gc.c. If you check the <a href="https://github.com/ruby/ruby/commits/master/gc.c">list of commits that changed the file</a>, there’s a lot there. For today, I’ve chosen a few specific ones: <a href="https://github.com/ruby/ruby/commit/8e743fad4e9124bd59bb5f14473cb188db9d3c34#diff-a891bcb4b58dde8242c5f456b2503c19">8e743f</a>, <a href="https://github.com/ruby/ruby/commit/ffd0820ab317542f8780aac475da590a4bdbc7a8#diff-a891bcb4b58dde8242c5f456b2503c19">ffd082</a> and <a href="https://github.com/ruby/ruby/commit/dddf5afb7947f5aba1ff875e9f5eb163f8c3d6c7#diff-a891bcb4b58dde8242c5f456b2503c19">dddf5a</a>. There’s nothing magical about these. They’re changes to gc.c, a reasonable distance apart, that I think might have some kind of influence on Ruby’s speed. I could easily have chosen twenty others - but don’t choose all twenty because the more you choose, the slower testing goes. Also, with GC compaction I know there are some subtle bugs that got fixed so the commits are all fairly recent. I don’t particularly want crashes here if I can avoid them. They’re not complicated to deal with, but they <strong>are</strong> annoying. Worse, frequent crashes usually mean no useful data since “fast but crashy” means that version of Ruby is effectively unusable. Not every random commit to head-of-master would make a good release.</p><p class="">For each of these commits I follow a simple process. I’ll use 8e743f to demonstrate.</p><ol data-rte-list="default"><li><p class="">git checkout 8e743f</p></li><li><p class="">mkdir -p /home/ubuntu/ruby_install/8e743f</p></li><li><p class="">./configure —prefix=/home/ubuntu/ruby_install/8e743f  <em>(you may need to autoconf first so that ./configure is available)</em></p></li><li><p class="">make clean <em>(in case you’re doing this multiple times)</em></p></li><li><p class="">make &amp;&amp; make install</p></li><li><p class="">rvm mount -n mri-pre-8e743f /home/ubuntu/ruby_install/8e743f</p></li></ol><p class="">You could certainly make a script for this, though I don’t currently install one to the Packer image.</p><p class="">And then you’ll need to use these once you’ve built them. Here’s what the top of my runner script looks like:</p>


<pre><code class="lang-ruby">RUBIES = [
  &quot;2.6.0&quot;,
  &quot;2.6.5&quot;,
  &quot;ext-mri-head&quot;,  # Since I have it sitting around
  &quot;ext-mri-pre-8e743f&quot;,
  &quot;ext-mri-pre-ffd082&quot;,
  &quot;ext-mri-pre-dddf5a&quot;,
]
</code></pre>


<p class="">Nothing complicated in RUBIES, though notice that <a href="https://github.com/rvm/rvm/blob/master/help/mount.md">rvm tacks on an “ext-” on the front of mounted Rubies’ names</a>.</p><h2>How Does It Run?</h2><p class="">If all goes well, the next part is underwhelming. Now we actually run it. I’m assuming you’ve done all the prior setup - you have an instance running with Rubies installed, you have a runner script and so on.</p><p class="">First off, you can just run the runner from the command line, something like “./runner.rb”. In fact I’d highly recommend you do that first, possibly set with only an iteration or two of each configuration, just to make sure everything is working fine. If you have a Ruby installation that doesn’t work or a Rails version not working with a gem you added or a typo in code somewhere, you want to find that out <strong><em>before</em></strong> you leave it alone for eight hours to churn. In RRB’s runner you can change TIMES from 30 down to something reasonable like 2 (why not 1? I sometimes get config bugs <strong>after</strong> some piece of configuration is done, so 2 iterations is a bit safer.)</p><p class="">If it works, great! Now you can set TIMES back to something higher. If it doesn’t, now you have something to fix.</p><p class="">You can decide whether to keep the data around from that first few iterations - I usually don’t. If you want to get rid of it then delete /home/ubuntu/rails_ruby_bench/data/*.json so that it doesn’t wind up mixed with your other data.</p><p class="">You can just run the runner from the command line, and it will usually work fine. But if you’re worried about network latency or dropouts (my residential DSL isn’t amazing) then there’s a better way.</p><p class="">Instead, you can run “nohup ./runner &amp;”. That tells the shell <strong>not</strong> to kill your processes if your network connection goes away. It also says to run it in the background, which is a good thing. All the output will go into a file called nohup.out.</p><p class="">If you need to check progress occasionally, you can run “tail -f nohup.out” to show the output as it gets printed. And doing a quick “ls /home/ubuntu/rails_ruby_bench/data/*.json | wc -l” will tell you how many data files have completed. Keep in mind that the runner scripts and RRB itself are designed to crash if anything goes wrong - silent failure is not your friend when you collect benchmark data. But an error like that will generally be in the log.</p><h2>Processing the Result</h2>


<pre class="source-code"># A cut-down version of the JSON raw data format
{
  "version": 3,
  "settings": {
    "startup_iters": 0,
    "random_seed": 16541799507913229037,
    "worker_iterations": 10000,
    (More settings...)
  },
  "environment": {
    "RUBY_VERSION": "2.7.0",
    "RUBY_DESCRIPTION": "ruby 2.7.0dev (2019-11-22T20:42:24Z v2_7_0_preview3~5 8e743fad4e) [x86_64-linux]",
    "rvm current": "ext-mri-pre-8e743f",
    "rails_ruby_bench git sha": "1bba9dbeaa1e02684d8c2ca8a8f9100c90506d5c\n",
    "ec2 instance id": "i-0cf628df3200d5ad5",
    "ec2 instance type": "m4.2xlarge",
    "env-GEM_HOME": "/home/ubuntu/.rvm/gems/ext-mri-pre-8e743f",
    "env-MY_RUBY_HOME": "/home/ubuntu/.rvm/rubies/ext-mri-pre-8e743f",
    "env-rvm_ruby_string": "ext-mri-pre-8e743f",
    "env-RUBY_VERSION": "ext-mri-pre-8e743f",
    "env-RUBYOPT": "-rbundler/setup",
    "env-RUBYLIB": "/home/ubuntu/.rvm/gems/ext-mri-pre-8e743f/gems/bundler-1.17.3/lib",
    (More settings...)
  },
  "warmup": {
    "times": [
      [
        0.177898031,
        0.522202063,
        0.706261902,
        0.372002397,</pre>

<p class="">If you’ve done everything so far, now you have a lot of large JSON files full of data. They’re pretty straightforward, but it’s still easier to use a processing script to deal with them. You’d need a lot of quality time with a calculator to do it by hand!</p><p class="">I do this a lot, so there’s a data-processing script in the Rails Ruby Bench repo that can help you.</p><p class="">First, copy your data off the AWS instance to somewhere cheaper. If you’re done with the instance, this is a decent time to terminate it. Then, copy the RRB script called process.rb to somewhere nearby. You can see <a href="https://github.com/noahgibbs/rrb_datavis/tree/master/posts/2018/jemalloc">this same setup</a> repeatedly in my repository of RRB data. I also have a tendency to copy graphing code into the same place. Copying, not linking, means that the version of the data-processing script is preserved, warts and all, so I know later if something was screwed up with it. The code is small and the data is huge so it’s not a storage problem.</p><p class="">Now, figure out how you’re going to divide up the data. For instance, for this experiment we care which version of Ruby and whether we’re compacting. We can’t use the RUBY_VERSION string because all those pre-2.7.0 Rubies say they’re 2.7.0. But we <strong>can</strong> use ‘rvm current’ since they’re all mounted separately by RVM.</p><p class="">I handle environment variables by prefixing them with “env” - that way there can’t be a conflict between RUBY_VERSION, which is a constant that I save, with an environment variable of the same name.</p><p class="">The processing script takes a lot of data, divides it into “cohorts”, and then shows information for each cohort. In this case, the cohorts will be divided by “rvm current” and “env-RUBY_COMPACT”. To make the process.rb script do that, you’d run “process.rb -c ‘rvm current,env-RUBY_COMPACT’”.</p><p class="">It will then print out a lot of chunks of text to the console while writing roughly the same thing to another JSON file. For instance, here’s what it printed about one of them for me:</p>


<pre class="source-code">Cohort: rvm current: ext-mri-pre-8e743f, env-RUBY_COMPACT: YES, # of data points: 600000 http / 0 startup, full runs: 60
   0%ile: 0.00542679
   1%ile: 0.01045952148
   5%ile: 0.0147234587
  10%ile: 0.0193235859
  50%ile: 0.1217705375
  90%ile: 0.34202113749999996
  95%ile: 0.4023132304000004
  99%ile: 0.53301011523
  100%ile: 1.316529161
--
  Overall thread completion times:
   0%ile: 44.14102196700001
  10%ile: 49.34424536089996
  50%ile: 51.769418454499984
  90%ile: 54.03600075760001
  100%ile: 56.40413652299999
--
  Throughput in reqs/sec for each full run:
  Mean: 187.45566524151448 Median: 188.96162032049574 Variance: 16.072435858651925
  [177.2919614844611, 178.24351344183614, 180.07540051803122, 180.3893011741887, 180.64734390789422, 180.78633357692414, 180.9370756562659, 181.48759316874003, 181.50042200695788, 181.7831931840077, 181.82136366559922, 182.42668523798133, 182.9695378281489, 183.4271937021401, 183.69630166389499, 185.39624590894704, 186.6188358046953, 186.72653137536867, 187.41516559992874, 187.44972315610178, 187.79211195172797, 188.03560095362238, 188.04550491676113, 188.16079648567523, 188.47720218882668, 188.57493052728336, 188.77093032659823, 188.7810661284267, 188.82632914724448, 188.9600070136181, 188.96323362737334, 189.05603777953803, 189.07694018310067, 189.09085709051078, 189.3054218996176, 189.42953673775793, 189.67879103436863, 189.68938987320993, 189.70449808150627, 189.7789255152989, 189.79846786458847, 189.89027249507834, 189.90364836070546, 189.98443889440762, 190.0304216448691, 190.2516551068254, 190.43172176734097, 190.51420115472305, 190.56095325134356, 190.56496123229778, 190.70854487422903, 190.7499088018249, 190.94577669990025, 191.0250241857314, 191.2679317071894, 191.39842651014004, 191.44203815980674, 191.94534584952945, 193.16205400859081, 193.47628839756382]

--
  Startup times for this cohort:
  Mean: nil Median: nil Variance: nil
</pre>

<p class="">What you see there is the cohort for Ruby 8e743f with compaction turned on. I ran start.rb sixty times in that configuration (two batches of 30, random order), which gave 600,000 data points (HTTP requests.) It prints what cohort it is in (the values of “rvm current” and “env-RUBY_COMPACT”). If your window is wide enough you can see that it prints the number of full runs (60) and the number of startups (0). If you check the command lines up above we told it zero startup iterations, so that makes sense.</p><p class="">The top batch of <a href="https://en.wikipedia.org/wiki/Percentile">percentiles</a> are for individual HTTP requests, ranging from about 0.005 seconds to around half a second for very slow requests, to 1.3 seconds for one specific very slow request (the 100th-percentile request.) The next batch of percentiles are called “thread completion times” are because the load tester divides the 10,000 requests into buckets and runs them through in parallel - in this case, each load-tester is running with 30 threads, so that’s about 333 consecutive requests each, normally taking in the neighbourhood of 52 seconds for the whole bunch.</p><p class="">You can also just treat it as one giant 10,000-request batch and time it end-to-end. If you do that you get the “throughput in reqs/sec for each full run” above. Since that happened 60 times, you can take a mean or median for all 60. Data from Rails Ruby Bench generally has a <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal-ish distribution</a>, resulting in the mean and median being pretty close together - 187.5 versus 189.0 is pretty close, particularly with a variance of around 16 (which means the standard deviation is close to 4, since standard deviation is the <a href="https://en.wikipedia.org/wiki/Standard_deviation">square root of variance</a>.)</p><p class="">If you don’t believe me about it being normal-ish, or you just want to check if a particular run was weird, you’ll also get all the full-run times printed out one after the other. That’s sixty of them in this case, so I expect they run off the right side of your screen.</p><p class="">All this information and more also goes into a big JSON file called process_output.json, which is what I use for graphing. But just for eyeballing quickly, I find process.rb’s console output to be easier to skim. For instance, the process_output.json for all of this (ten cohorts including compaction and no-compaction) runs to about six million lines of JSON text and includes the timing of all 600,000 HTTP requests by cohort, among other things. Great for graphing, lousy for quick skimming.</p><h2>But What’s the Answer?</h2><p class="">I said I didn’t know the answer when I started writing this post - and I didn’t. But I also implied that I’d find it out, and I’ve clearly run 600,000 HTTP requests’ worth of data gathering. So what did I find?</p><p class="">Um… That <a href="https://knowyourmeme.com/memes/maybe-the-real-treasure-was-the-friends-we-made-along-the-way">the real memory compaction is the friends we made along the way</a>?</p><p class="">After running all of this for a couple of days, the short answer is “nothing of statistical significance.” I <a href="http://engineering.appfolio.com/appfolio-engineering/2019/10/24/ruby-27preview2-a-quick-speed-update">still see Ruby 2.6.5 being a bit slower than 2.6.0</a>, like before, but close enough that it’s hard to be sure - it’s within about two standard deviations. But the 2.7.0 prereleases are slightly faster than 2.6. And turning compaction on or off makes essentially no difference whatsoever. I’d need to run at least ten times as many samples as this to see statistical significance in these thresholds. <strong>So if there’s a difference between 2.7 Rubies, or with compaction, at all, it’s quite small</strong>.</p><p class="">And that, alas, is the most important lesson in this whole long post. When you don’t get statistical significance, and you’ve checked that you did actually change the settings (I did), the answer is “stop digging.” You can run more samples (notice that I told you to use 30 times and I gave data for 60 times?). You can check the data files (notice that I mentioned throwing away an old run that was wrong?) But in the end, you need to expect “no result” as a frequent answer. I have started many articles like this, gotten “no result” and then either changed direction or thrown them away.</p><p class="">But today I was writing about how to use the tools! And so I get a publishable article anyway. Alas, that trick only works once.</p><p class="">If you say to yourself, “self, this seems like a lot of data to throw away,” you’re not wrong. Keep in mind that there are many tricks that would let you see little or no difference with a <strong>small</strong> run before doing something large like this. Usually you should look for promising results in small sets and <strong>only then</strong> reproduce them as a larger study. There are whole fields of study around how to do studies and experiments.</p><p class="">But today I was showing you the tools. And not infrequently, this is what happens. And so today, this is what you see.</p><p class=""><strong>Does this mean Ruby memory compaction doesn’t help or doesn’t work? Nope. </strong>It means that any memory it saves isn’t enough to show a speed difference in Rails Ruby Bench — but that’s not really what memory compaction is for, even if I wanted to know the result.</p><p class="">Memory compaction solves a weird failure case in Ruby where a single Ruby object can keep a whole page from being freed, resulting in high memory usage for no reason… But Rails Ruby Bench doesn’t hit that problem, so it doesn’t show that case. Basically, memory compaction is still useful in the failure cases it was designed for, even if Rails Ruby Bench is already in pretty good shape for memory density.</p>]]></description></item><item><title>Symbol#to_s Returned a Frozen String in Ruby 2.7 previews - and Now It Doesn’t</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 17 Jan 2020 18:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/10/24/symboltos-returns-frozen-string-in-ruby-27</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5db15b3aa7103e0e94f08681</guid><description><![CDATA[<h2>How a Broken Interface Getting Fixed Showed Us That It's Broken</h2><p class="">One of the things I love about Ruby is the way its language design gets attention from many directions and many points of view. A change in the Ruby language will often come from the JRuby side, such as <a href="https://bugs.ruby-lang.org/issues/16150">this one proposed by Charles Nutter</a>. Benoit Daloze (a.k.a. <a href="https://github.com/eregon">eregon</a>), the now-lead of TruffleRuby is another major commenter. And of course, you’ll see CRuby-side folks including <a href="https://en.wikipedia.org/wiki/Yukihiro_Matsumoto">Matz</a>, who is still Ruby’s primary language designer.</p><p class="">That bug has some interesting implications… So let’s talk about them a bit, and how an interface not being perfectly thought out at the beginning often means that <strong><em>fixing</em></strong> it later can have difficulties. I’m not trying to pick on the .to_s method, which is a fairly good interface in most ways. But all of Ruby started small and has had to deal with more and more users as the language matures. <strong><em>Every</em></strong> interface has this problem at some point, as its uses change and its user base grows. This is just one of many, many good examples.</p><h2>So… What’s This Change, Then?</h2><p class="">You likely know that in Ruby, when you call .to_s on an object, it’s supposed to return itself “translated” to a string. For instance if you call it on the number 7 it will return the string “7”. Or if you call it on a symbol like :bob it will return the string “bob”. A string will just return itself directly with no modifications.</p><p class="">There are a whole family of similar “typecast” methods in Ruby like to_a, to_hash, to_f and to_i. Making it more complicated, most types have two typecast operators, not one. For strings that would be to_s and to_str, which for arrays it’s to_a and to_ary. For the full details of these operators, other ways to change types and how they’re all used, I highly recommend <a href="http://www.confidentruby.com/">Avdi Grimm’s book Confident Ruby,</a> which can be bought, or ‘traded’ for sending him a postcard! In any case, take my word for it that there are a bunch of “type conversion operators,” and to_s is one of them.</p><p class="">In <a href="https://www.ruby-lang.org/en/news/2019/10/22/ruby-2-7-0-preview2-released/">Ruby 2.7-preview2</a>, a random Ruby prerelease, Symbol#to_s started returning a frozen string, which can’t be modified. That breaks a few pieces of code. That’s how I stumbled across the change — I do <a href="http://engineering.appfolio.com/appfolio-engineering/2018/4/2/rails-ruby-bench-what-and-why">speed-testing on pretty ancient Ruby code</a> regularly, so there are a lot of little potential problems that I hit.</p><h2>But Why Is That a Problem?</h2><p class="">When would that break something? When somebody calls #to_s and then messes with the result, mostly. Here’s the <a href="https://github.com/rails/rails/blob/7af44f49dbf221c3b7f0b0d476913a74b6a1d0e4/activesupport/lib/active_support/ordered_options.rb#L42-L43">code that I had trouble with</a>, from an old version of ActiveSupport:</p>


<pre><code class="lang-ruby">    def method_missing(name, *args)
      name_string = name.to_s
      if name_string.chomp!(&quot;=&quot;)
        self[name_string] = args.first
      else
        bangs = name_string.chomp!(&quot;!&quot;)

        if bangs
          self[name_string].presence || raise(KeyError.new(&quot;:# is blank&quot;))
        else
          self[name_string]
        end
      end
    end
</code></pre>


<p class="">So… Was this a perfectly okay way to do it, broken by a new change? Oooooh… <strong><em>That’s a really good question!</em></strong></p><p class="">Here are some more good questions that I, at least, didn’t know the answers to offhand:</p><ul data-rte-list="default"><li><p class="">If a string usually just returns itself, is it okay that modifying the string also modifies the original?</p></li><li><p class="">Is it a problem, optimisation-wise, to keep allocating new strings every time? (<a href="https://github.com/schneems">Schneems</a> <a href="https://github.com/rails/rails/pull/34197">had to work around this</a>)</p></li><li><p class="">If you freeze the string, which freezes the original, is <strong><em>that</em></strong> okay?</p></li></ul><p class="">These are hard questions, not least because fixing question #1 in the obvious way probably breaks question #2 and vice-versa. And question #3 is just kind of weird - is it okay to <strong><em>stop</em></strong> this behaviour part way through? Ruby makes it <strong><em>possible</em></strong>, but that’s not what we care about, is it?</p><p class="">I mention this interface, to_s, “not being perfectly thought out” up at the top of this post. And this is what I mean. to_s is a limited interface that does some things really well, but it simply hasn’t been thought through in this context. That’s true of any interface - there will always be new uses, new contexts, new applications where it either hasn’t been thought about or the original design was wrong.</p><p class=""><strong><em>“Wrong?”</em></strong> Isn’t that a strong statement? Not really. <a href="https://bugs.ruby-lang.org/issues/16150#note-33">Charles Nutter points out that the current design is simply unsafe in the way we’re using it</a> - it doesn’t guarantee what happens if you modify the result, or decide whether it’s legal to do so. And people are, in fact, modifying its result. If they weren’t then we could trivially freeze the result for safety and optimisation reasons and nobody would notice or care (more on that below.)</p><p class="">Also, we’ll know in the future, not just for to_s but for conversion methods in general - it’s not safe to modify their results. I doubt that to_s is the only culprit!</p><h2>Many Heads and a Practical Answer</h2><p class="">In the specific Ruby 2.7 sense, we have an answer. Symbol#to_s returned a frozen string and some code broke. Specifically, the answer to “what broke?” seems to be “<a href="https://bugs.ruby-lang.org/issues/16150#note-29">about six things, some of them old or obscure</a>.” But this is what trying something out in a preview is for, right? If it turns out that there are problems with it, we’re likely to find them before the final release of 2.7 and we can easily roll this back. Such things have happened before, and will again.</p><p class="">(In fact, it <strong>did </strong>happen. The release 2.7.0 won’t do this, and they’re rethinking the feature. It may come back, or may change and come back in a different form. The Ruby Core Team really does try to keep backward compatibility where they can.)</p><p class="">In the mean time, if you’re modifying the result of calling to_s, I recommend you stop! Not only might the language break that (or not) later, but you’re already given no guarantees that it will keep working! In general, don’t trust the result of a duplicated object from a conversion method to be modifiable. It might be frozen, or worse it might modify the original object… And yet, it isn’t guaranteed to, or to <strong><em>keep</em></strong> doing it if it already does.</p><p class="">And so the march of progress digs up another problem for us, and we all learn another little bit of interface design together.</p>]]></description></item><item><title>Ruby 2.7.0's Rails Ruby Bench Speed is Unchanged from 2.6.0</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 03 Jan 2020 18:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/12/27/ruby-270s-rails-ruby-bench-speed-is-unchanged-from-260</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5e05c9031aa65222ba8e7222</guid><description><![CDATA[<p class="">As of the 25th of December, 2019 we have a released version of Ruby 2.7.0. As you can read in the title - it’s basically the same as 2.6.0.</p><p class="">The 2.7.0 series is remarkable in how little the speed has changed. Overall it has been very stable with very little change in performance. I’ve seen a <strong><em>tiny</em></strong> bit of drift in <a href="http://engineering.appfolio.com/appfolio-engineering/2018/4/2/rails-ruby-bench-what-and-why">Rails Ruby Bench</a> results, sometimes as much as 1%-2%, but no more.</p><p class="">The other significant news is also not news: JIT performance is nearly entirely unchanged for Rails apps from 2.6.0. I don’t recommend using <a href="http://engineering.appfolio.com/appfolio-engineering/2019/7/18/jit-and-rubys-mjit">CRuby’s MJIT</a> for Rails, and <a href="https://medium.com/@k0kubun/jit-development-progress-at-ruby-2-7-d6dd62a8c76a">neither does Takashi Kokubun</a>, MJIT’s primary maintainer.</p><p class="">I have a lot of data files to this effect, but… The short version is that, when I run 150 trials of 10,000 HTTP requests each for 2.6.0 versus 2.7.0, the results are well within the margin of error on the measurement. With JIT the results aren’t <strong><em>quite</em></strong> that close, but it’s the same to within a few percent - which means you still shouldn’t turn on JIT for a large Rails app.</p><p class="">I spent some time trying to see if there was a small speedup anywhere in the 2.7 previews that we might have had and missed - there are speed differences of about that size between the fastest and slowest prerelease 2.7 Rubies, which is still very, very small as a span of speeds. And as far as I can tell, no individual change has made a large speed difference, not even 2%. There’s just a very slow drift over time.</p><p class="">Does that mean that Ruby has gotten as fast as it can? Not at all.</p><p class="">Vladimir Makharov (the original author of CRuby’s MJIT) is still working on Mir, a new style of Ruby JIT. Takashi Kokubun is still tuning the existing JIT. I’ve heard interesting things about work from Koichi Sasada on significant reworks of VM subsystems. There are new features happening, and we now have memory compaction.</p><p class="">But I think that at this point, we can reasonably say that the low-hanging performance fruit has been picked. Most speedups from here are going to be more effort-intensive, or require significant architectural changes.</p>]]></description></item><item><title>More Fiber Benchmarking</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 20 Dec 2019 18:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/10/15/more-fiber-benchmarking</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5da5deb5f673d43544981a92</guid><description><![CDATA[<p class="">I’ve been working with Samuel Williams a bit (and on my own a bit) to do more <a href="https://appfolio-engineering.squarespace.com/appfolio-engineering/2019/9/4/benchmark-results-threads-processes-and-fibers">benchmarking Fiber speeds in Ruby</a> and comparing them to processes and threads. There’s always more to do! Not only have I been running more trials for each configuration (get that <a href="https://en.wikipedia.org/wiki/Variance">variance</a> down!), I also tried out a couple more configurations of the test code. It’s always nice to see what works well and what doesn’t.</p><h2>New Configurations and Methodology</h2><p class="">Samuel pointed out that for threads, I could run one thread per worker in the master process, for a total of 2 * workers threads instead of using IO.select in a single thread in the master. True! That configuration is less like processes but more like fibers, and is arguably a fairer representation of a ‘plain’ thread-based solution to the problem. It’s also likely to be slower in at least some configurations since it requires twice as many threads. I would naively expect it to perform worse for lack of a good centralised place to coordinate which thread is working next. But let’s see, shall we?</p><p class="">Samuel also put together a <a href="https://github.com/noahgibbs/fiber_basic_benchmarks/blob/master/benchmarks/williams_fiber_test.rb">differently-optimised benchmark for fibers</a>, one based on <a href="https://apidock.com/ruby/IO/read_nonblock">read_nonblock</a>. This is usually worse for throughput but better for latency. A nonblocking implementation can potentially avoid some initial blocking, but winds up much slower on very old Ruby when read_nonblock was unusably slow. This benchmark, too, has an interesting performance profile that’s worth a look.</p><p class="">I don’t know if you remember from last time, but I was also doing something fairly dodgy with timing - I measured the entire beginning-to-end process time from outside the Ruby process itself. That means that a lot of process/thread/fiber setup got ‘billed’ to the primitive in question. That’s not an <em>invalid</em> way to benchmark, but it’s not obviously the right thing.</p><p class="">As a quick spoiler on that last one: process setup takes between about 0.3 and 0.4 seconds for everything - running Ruby, setting up the IO pipes, spawning the workers and all. And there’s barely any variation in that time between threads vs processes vs fibers. The main difference between “about 0.3” and “about 0.4” seconds is whether I’m spawning 10 workers or 1000 workers. In other words, it basically didn’t turn out to matter once I actually bothered to measure - which is good, and I expected, but it’s always better to measure than to expect and assume.</p><p class="">I also put together a <a href="https://github.com/noahgibbs/fiber_basic_benchmarks/blob/master/comparison_collector.rb">fairly intense runner script</a> to make sure everything was done in a random order - one problem with long tests is that if something changes significantly (the Amazon hardware, some network connection, a background process to update Ubuntu packages…) then a bunch of highly-correlated tests all have the same problem. Imagine if Ubuntu started updating its packages right as the fiber tests began, and then stopped as I switched to thread tests. It would look like fibers were very slow <strong>and</strong> prone to huge variation in results! I handle this problem for my important results by re-running lots of tests when it’s significant… But I’m not always 100% scrupulous, and I’ve been bitten by this before. There’s a reason I can tell you the specifics of the problem, right? A nice random-order runner doesn’t keep background delays from happening, but they keep them from all being in the same <strong>kind</strong> of test. Extra randomly-distributed background noise makes me think, “huh, that’s a lot of variance, maybe this batch of test runs is screwy,” which is <strong>way better</strong> than if I think, “wow, fibers really suck.”</p><p class="">So: the combination of 30 test-runs per configuration rather than 10 and running them in a random order is a great way to make sure my results are basically solid.</p><p class="">I’ve also run with the October 18th prerelease version of Ruby 2.7… And the performance is mostly just like the tested 2.6. A little faster, but barely. You’ll see the graphs.</p><h2>Threaded Results</h2><p class="">Since we have two new configurations, let’s start with one of them. The older thread-based benchmark used IO.select and the newer one uses a lot of threads. In most languages, I’d now comment how the “lot of threads” version needs extra coordination — but <a href="https://www.jstorimer.com/blogs/workingwithcode/8085491-nobody-understands-the-gil">Ruby’s GIL</a> turns out to handle that for us nicely without further work. There are advantages to having a giant, frequently-used lock already in place!</p><p class="">I had a look at the data piecemeal, and yup, on Linux I saw about what I expected to for several of the runs. I saw some different things on my Mac, but <a href="http://engineering.appfolio.com/appfolio-engineering/2019/5/20/why-is-ruby-slower-on-mac-an-early-investigation">Mac can be a little weird for Ruby performance, zigging when Linux zags</a>. Overall we usually treat Linux as our speed-critical deployment platform in the English-speaking world - because who runs their production servers on Mac OS?</p><p class="">Anyway, I put together the full graph… <strong><em>Wait, what?</em></strong></p><p data-rte-preserve-empty="true" class=""></p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1571406465608-EW22AH83NUA9YH9OABZH/threads_multi_master.png" data-image-dimensions="527x404" data-image-focal-point="0.5,0.5" alt="Y Axis is the time in seconds to process 100,000 messages with the given number of threads" data-load="false" data-image-id="5da9c28160d9ba4636dd1d4f" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1571406465608-EW22AH83NUA9YH9OABZH/threads_multi_master.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Y Axis is the time in seconds to process 100,000 messages with the given number of threads</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">That massive drop-off at the end… That’s a good thing, no question, but why is thread contention suddenly not a problem in this case when it was for the previous six years of Ruby?</p><p class="">The standard deviation is quite low for all these samples. The result holds for the other numbers of threads I checked (5 and 1000), I just didn’t want to put eight heavily-overlapped lines on the same graph - but the numbers are very close for those, too.</p><p class="">I knew these were <a href="http://engineering.appfolio.com/appfolio-engineering/2019/1/7/microbenchmarks-vs-macrobenchmarks-ie-whats-a-microbenchmark">microbenchmarks</a>, and those are always a bit prone to large changes from small changes. But, uh, this one surprised me a bit. At least it’s in a good direction?</p><p class="">Samuel is looking into it to try to find the reason. If he gets back to me before this gets published, I’ll tell you what it is. If not, <a href="https://twitter.com/ioquatix">I guess watch his Twitter feed if you want updates?</a></p><h2>Fibrous Results</h2><p class="">Fibers sometimes take a little more code to do what threads or processes manage. That should make sense to you. They’re a higher-performance, lower-overhead method of concurrency. That sometimes means a bit more management and hand-holding, and they <em>allow</em> you to fully control the fiber-to-fiber yield order (manual control) which means you often <em>need</em> to understand that yield order (no clever unpredictable automatic control.)</p><p class="">Samuel Williams, who has done a lot of work on Ruby’s fiber improvements and is the author of the Falcon fiber-based application server, saw a few places to potentially change up my benchmark and how it did things with a little more code. Awesome! The changes are pretty interesting - not so much an obvious across-the-board improvement as a somewhat subtle tradeoff. I choose to interpret that as a sign that my initial effort was pretty okay and there wasn’t an immediately obvious way to do better ;-)</p><p class="">He’s using read_nonblock rather than straight-up read. This reduces latency… but isn’t actually amazing for bandwidth, and I’m primarily measuring bandwidth here. And so his code would likely be even better in a latency-based benchmark. Interesting, read_nonblock had horrifically bad performance in really old Ruby versions, partly because of using exception handling for its flow control - a no-no in nearly any language with exceptions.</p><p class="">You can see <a href="https://github.com/noahgibbs/fiber_basic_benchmarks/blob/master/benchmarks/fiber_test.rb">the code for the original simpler benchmark</a> versus <a href="https://github.com/noahgibbs/fiber_basic_benchmarks/blob/master/benchmarks/williams_fiber_test.rb">his version with changes</a> here.</p><p class="">It turns out that the resulting side by side graph is really interesting. Here, first look for yourself:</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1571406559489-63WAHC9ICE74XEE8ENC1/williams_fiber_testing.png" data-image-dimensions="512x394" data-image-focal-point="0.5,0.5" alt="Red and orange are the optimised version, while blue and green are the old simple one." data-load="false" data-image-id="5da9c2de60d9ba4636dd2474" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1571406559489-63WAHC9ICE74XEE8ENC1/williams_fiber_testing.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Red and orange are the optimised version, while blue and green are the old simple one.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">You already know that read_nonblock is very slow for old Ruby. That’s why the red and orange lines are so high (bad) for Ruby until 2.3, but then suddenly get faster than the blue and green lines for 2.3 and 2.4.</p><p class="">You may remember in <a href="http://engineering.appfolio.com/appfolio-engineering/2019/9/4/benchmark-results-threads-processes-and-fibers">my earlier fiber benchmarks</a> that the fiber performance has a sort of humped curve, with 2.0 being fast, 2.3 being slow and 2.6 eventually getting faster than 2.0. The blue and the green lines are a re-measurement of the exact same thing and so have pretty much exactly the same curve as last week. Good. You can see an echo of the same thing in the way the red and orange lines also get slower for 2.2.10, though it’s obscured by the gigantic speedup to read_nonblock in 2.3.8.</p><p class="">By 2.5, all the samples are basically in a dead heat - close enough that none of them are really outside the range of measurement error of each other. And by 2.6.5, suddenly the simple versions have pulled ahead, but only slightly.</p><p class="">One thing that’s going on here is that read_nonblock has a slight disadvantage compared to blocking I/O in the kind of test I’m doing (bandwidth more than latency.) Another thing that’s going on is that microbenchmarks give large changes with small differences in which operations are fast.</p><p class="">But if I were going to tell one overall story here, it’s that recent Ruby is clearly winning over older Ruby. So our normal narrative applies here too: if you care about the speed of these things, upgrade to the latest stable Ruby or (occasionally, in specific circumstances) later.</p><h2>Overall Results</h2><p class="">The basic <a href="https://appfolio-engineering.squarespace.com/appfolio-engineering/2019/9/4/benchmark-results-threads-processes-and-fibers">conclusions from the previous benchmarks</a> also still hold. In no particular order:</p><ul data-rte-list="default"><li><p class="">Processes get a questionably-fair boost by stepping around the Global Interpreter Lock</p></li><li><p class="">Threads and Fibers are both pretty quick, but Fibers are faster where you can use them</p></li><li><p class="">Processes are <em>extremely</em> quick, but in large numbers will eat all your resources; don’t use too many</p></li><li><p class="">For both threads and fibers, upgrade to a very recent Ruby for best speed</p></li></ul><p class="">I’ll also point out that I’m doing very little here - in practice, a lot of this will depend on your available memory. Processes can get <strong><em>very</em></strong> memory-hungry very quickly. In that case, you may find that having only one copy of your <a href="https://en.wikipedia.org/wiki/Working_set">in-memory data</a> by using threads or fibers is a huge win… At least, if you’re not doing too much calculation and the GIL messes you up.</p><p class="">See why we have multiple different concurrency primitives? There truly isn’t an easy answer to ‘which is best.’ Except, perhaps, that <a href="https://www.jstorimer.com/blogs/workingwithcode/7766069-matz-is-not-a-threading-guy">Matz is “not a threading guy”</a> (still true) - and we don’t prefer threads in CRuby. Processes and Fibers are both better where they work.</p><p class="">(Please note that these numbers, and these attitudes, can be massively different in different Ruby implementations - as they certainly are in <a href="https://www.jruby.org/">JRuby</a>!)</p>]]></description></item><item><title>JIT and Ruby's MJIT</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 06 Dec 2019 18:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/7/18/jit-and-rubys-mjit</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5d30add3f448490001836819</guid><description><![CDATA[<figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485245034-HEURLDK8APM4DMMO43ZF/rip-1.jpg" data-image-dimensions="600x928" data-image-focal-point="0.5,0.5" alt="Arthur Rackham explains Ruby debugging" data-load="false" data-image-id="5d30e43c5729f30001ead0a0" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485245034-HEURLDK8APM4DMMO43ZF/rip-1.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Arthur Rackham explains Ruby debugging</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">If you already know lots about JIT in general and Ruby’s MJIT in particular… you may not learn much new in this post. But in case you wonder “what is JIT?” or “what is MJIT?” or “what’s different about Ruby’s JIT?” or perhaps “why in the world did they decide to do THAT?”…</p><p class="">Well then, perhaps I can help explain!</p><p class="">Assisting me in this matter will be <a href="https://en.wikipedia.org/wiki/Arthur_Rackham">Arthur Rackham</a>, famed early-twentieth-century children’s illustrator whose works are now in the <a href="https://en.wikipedia.org/wiki/Public_domain">public domain</a>. This whole post is adapted from <a href="https://docs.google.com/presentation/d/1KBCX3tQ8YA4qEQU8NAkZPIkwlVZ955fcgQjLBh5RNe4/edit?usp=sharing">slides</a> to a talk I gave at <a href="https://southeastruby.com/">Southeast Ruby</a> in 2018.</p><p class="">I will frequently refer to <a href="https://github.com/oracle/truffleruby">TruffleRuby</a>, which is one of the most complex and powerful Ruby implementations. That’s not because you should necessarily use it, but because it’s a great example of Ruby with a powerful and complicated JIT implementation.</p><h2>What is JIT?</h2><p class="">Do you already know about interpreted languages versus compiled languages? In a <a href="https://en.wikipedia.org/wiki/Compiled_language">compiled language</a>, before you run the program you’re writing, you run the compiler on it to turn it into a native application. Then you run that. In an <a href="https://en.wikipedia.org/wiki/Interpreted_language">interpreted language</a>, the interpreter reads your source code and runs it more directly without converting it.</p><p class="">A compiled language takes a lot of time to do the conversion… once. But afterward, a native application is usually much faster than an interpreted application. The compiler can perform various <a href="https://simple.wikipedia.org/wiki/Optimization_(computer_science)">optimizations</a> where it recognizes that there is an easier or better way to do some operation than the straightforward one and the native code winds up better than the interpreted code - but it takes time for the compiler to analyze the code and perform the optimization.</p><p class="">A language with <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JIT</a> (“Just In Time” compilation) is a hybrid of compiled and interpreted languages. It begins by running interpreted, but then notices which pieces of your program are called many times. Then it compiles just those specific parts in order to optimize them.</p><p class="">The idea is that if you have used a particular method many times, you’ll probably use it again many times. So it’s worth the time and trouble to compile that method.</p><p class="">A JITted language avoids the slow compilation step, just like interpreted languages do. But they (eventually) get the faster performance for the parts of your program that are used the most, like a compiled language.</p><h2>Does JIT Work?</h2><p class="">In general, JIT can be a very effective method. How effective depends on what language you’re compiling and what features of that language - you’ll see <a href="https://codesmithdev.com/v8-now-supports-jitless-javascript-execution/">numbers from 6% to 40% or even more in JavaScript</a>, for instance.</p><p class="">And in fact, there’s an outdated blog post by Benoit Daloze about how <a href="https://eregon.me/blog/2016/11/28/optcarrot.html">TruffleRuby (with JIT) can run a particular CPU-heavy benchmark at 900% the speed of standard CRuby</a>, largely because of its much better JIT (see graph below.) I say “outdated” because TruffleRuby is likely to be <em>even faster</em> now… though so is the latest CRuby.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563472974447-NHL7NYL4R47JZKEUS56K/compare.png" data-image-dimensions="740x200" data-image-focal-point="0.5,0.5" alt="These numbers are from Benoit Daloze in 2016, see link above" data-load="false" data-image-id="5d30b44eee22fc0001a91051" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563472974447-NHL7NYL4R47JZKEUS56K/compare.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">These numbers are from Benoit Daloze in 2016, see link above</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">And in fact, the most recent CRuby with JIT enabled runs this same benchmark about <a href="https://youtu.be/cmOt9HhszCI?t=1360">280% the speed of older interpreted CRuby</a>.</p><h2>JIT Tradeoffs</h2><p class="">Nothing is perfect in all situations. Every interesting decision you make as an engineer is a tradeoff of some kind.</p><p class="">Compared to interpreting your language, JIT’s two big disadvantages are <em>memory usage</em> and <em>warmup time</em>.</p><p class=""><strong>Memory usage</strong> makes sense - if you use JIT, you have to have the interpreted version of your method and the compiled, native version. Two versions, more memory. For complicated reasons, sometimes it’s more than two versions - TruffleRuby often has a lot more than two, which is part of why it’s so fast, but uses lots of memory.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485889709-1Y2DB9PB6U6KQAP04O7P/mischief_you.jpg" data-image-dimensions="670x994" data-image-focal-point="0.5,0.5" alt="A JIT Implementation beset by troubles" data-load="false" data-image-id="5d30e6c1c7c35100010be9f7" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485889709-1Y2DB9PB6U6KQAP04O7P/mischief_you.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">A JIT Implementation beset by troubles</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">In addition to keeping multiple versions of each method, JIT has to track information <em>about</em> the method. How many times was it called? How much time was spent there? With what arguments? Not every JIT keeps all information, but that means a more complicated JIT with better performance will track more information and use more memory.</p><p class="">In addition to memory usage, there’s <strong>warmup time</strong>. With JIT, the interpreter has to recognize that a method is called a lot and then take time to compile it. That means there’s a delay between when the program starts and when it gets to full speed.</p><p class="">Some JITs try to compile optimistically - to quickly notice that a method is called a lot and compile it. If it does that, it will often compile methods that don’t get called again much, sometimes, which wastes its time. The Java Virtual Machine (JVM) is (in)famous for this, and tends to run <em>very slowly</em> until JIT has finished.</p><p class="">Other JITs compile pessimistically - they compile methods slowly, and only after they have been called many times. This makes for less waste by compiling the wrong methods, but more warmup time near program start before the program is running quickly. There’s not a “right” answer, but instead various interesting tradeoffs and situations.</p><p class="">JIT is best for programs that run for a long time, like background jobs or network servers. For long-running programs there’s plenty of time to compile the most-used methods and plenty of time to benefit from that speedup. As a result, JIT is often counterproductive for small, short-running programs. Think of “gem list” or small Rake tasks as examples where JIT may not help, and could easily hurt.</p><h2>Why Didn’t Ruby Get JIT Sooner?</h2>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485524849-5CRHY9LJCLLUPMGWR4SP/exquisite_fairy_dancing.jpg" data-image-dimensions="670x965" data-image-focal-point="0.5,0.5" alt="A Ruby core developer tests a JIT implementation for stability" data-load="false" data-image-id="5d30e55452a6cb00010953a5" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485524849-5CRHY9LJCLLUPMGWR4SP/exquisite_fairy_dancing.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">A Ruby core developer tests a JIT implementation for stability</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">JIT’s two big disadvantages (memory usage, startup/warmup time) are both huge CRuby <em>advantages</em>. That made JIT a tough sell.</p><p class="">Ruby’s current JIT, called MJIT for “Method JIT,” was far from the first attempt. <a href="https://www.youtube.com/watch?v=QaLvtNpoc5o">Evan Phoenix built an LLVM Ruby JIT</a> long ago that wound up becoming Rubinius. Early prototypes have been around long before MJIT or its at-the-time competitors. JIT in other Ruby implementations (LLVM libs in Rubinius, OMR) have been tried out and rejected many times. Memory usage has been an especially serious hangup. The Core Team wants CRuby to run well on the smallest Heroku dynos and (historically) in embedded environments.</p><p class="">And while it’s <em>possible</em> to tune a JIT implementation to be okay for warmup time, most JIT is not tuned that way. The Java Virtual Machine (JVM) is an especially serious offender here. Since JRuby (Ruby written in Java) is the most popular alternate Ruby implementation, most Ruby programmers think of “Ruby with JIT” startup time as “Ruby with JVM” startup time, which is dismal.</p><p class="">Also, a JIT implementation can be quite large and complicated. The Ruby core team didn’t really want to adopt something large and complicated that they didn’t have much experience with into the core language.</p><p class="">Shyouhei Urabe, a core team member, created a “deoptimization branch” for Ruby that basically proved you could write a mini-JIT with limited memory use, fast startup time and minimal complexity. This convinced <a href="https://en.wikipedia.org/wiki/Yukihiro_Matsumoto">Matz</a> that such a thing was possible and opened the door to JIT in CRuby, which had previously seemed difficult or impossible.</p><p class="">Several JIT implementations were developed… And eventually, <a href="https://developers.redhat.com/blog/2018/03/22/ruby-3x3-performance-goal/">Vladimir Makarov created an initial implementation for what would become Ruby’s JIT</a>, one that was reasonably quick, had very good startup time and didn’t use much memory — we’ll talk about how below.</p><p class="">And that was it? No, not quite. MJIT wasn’t clearly the best possibility. Vlad’s MJIT-in-development competed with various other Ruby implementations and with <a href="https://github.com/k0kubun">Takashi Kokubun</a>’s LLVM-based Ruby JIT. After Vlad convinced Takashi that MJIT was better, Takashi found a way to take roughly the simplest 80% of MJIT and integrate it nicely into Ruby in a way that was easy to deactivate if necessary and touched very little code outside itself, which he called “YARV-MJIT.”</p><p class="">And after months of integration work, YARV-MJIT was accepted provisionally into prerelease Ruby 2.6 to be worked on by the other Ruby core members, to make sure it could be extended and maintained.</p><p class="">And that was how Ruby 2.6 got MJIT in its current form, though still requiring the Ruby programmer to opt into using it.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485647502-B3H21WBC8ZV5NWN69YS7/rip-33.jpg" data-image-dimensions="600x407" data-image-focal-point="0.5,0.5" alt="Making fun of Ruby for not having JIT yet" data-load="false" data-image-id="5d30e5cfb12a650001133bb3" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485647502-B3H21WBC8ZV5NWN69YS7/rip-33.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Making fun of Ruby for not having JIT yet</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<h2>MJIT: CRuby’s JIT</h2>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485595112-S297V10G9PHTDT66GT82/ring36.jpg" data-image-dimensions="600x774" data-image-focal-point="0.5,0.5" alt="The MJIT implementation shows early promise" data-load="false" data-image-id="5d30e59ad1c4ff00014a2b49" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485595112-S297V10G9PHTDT66GT82/ring36.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">The MJIT implementation shows early promise</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">MJIT is an unusual JIT implementation: it uses a Ruby-to-C language translator and a background thread running a C compiler. It literally writes out C language source files on the disk and compiles them into shared libraries which the Ruby process can load and use. This is not at all how most JIT implementations work.</p><p class="">When a method has been called a certain number of times (10,000 times in current prerelease Ruby 2.7), MJIT will mark it to be compiled into native code and put it on a “to compile” queue. MJIT’s background thread will pull methods from the queue and compile them one at a time into native code.</p><p class="">Remember how we talked about the JVM’s slow startup time? That’s partly because it rapidly begins compiling methods to native code, using a lot of memory and processor time. MJIT compiles only one method at once and expects the result to take time to come back. MJIT sacrifices time-to-full-speed to get good performance early on. This is a great match for CRuby’s use in small command-line applications that often don’t run for long.</p><p class="">“Normal” JIT compiles inside the application’s process. That means if it uses a lot of memory for compiling (which it nearly always does) then it’s very hard to free that memory back to the system. Ruby’s MJIT runs the compiler as a separate background process - when the compiling finishes, the memory is automatically and fully freed back to the operating system. This isn’t as efficient — it sets up a whole external process for compiling. But it’s wonderful for avoiding extra memory usage.</p><h2>How To Use JIT</h2><p class="">This has mostly been a conceptual post. But how do you <strong><em>actually use</em></strong> JIT?</p><p class="">In Ruby 2.6 or higher, use the “—jit” argument to Ruby. This will turn JIT on. You can also add “—jit” to your RUBYOPT environment variable, which will automatically pass it to Ruby every time.</p><p class="">Not sure if your version of Ruby is high enough? Run “ruby —version”. Need to install a later Ruby? Use <a href="http://rvm.io/">rvm</a>, <a href="https://github.com/rbenv/ruby-build">ruby-build</a> or your version manager of choice. Ruby 2.6 is already released as I write this, with Ruby 2.7 coming at Christmastime of 2019.</p><h2>What About Rails?</h2><p class="">Unfortunately, there is one huge problem with Ruby’s current MJIT. At the time I write this in mid-to-late 2019, <strong><em>MJIT will slow Rails down</em></strong> instead of speeding it up.</p><p class="">That’s a pretty significant footnote.</p><h2>Problems, Worries and Escape Hatches</h2><p class="">If you want to turn JIT off for any reason in Ruby 2.6 or higher, you can use the “—disable-jit” command-line argument to do that. So if you know you don’t want JIT and you may run the same command with Ruby 3, you can explicitly turn JIT off.</p><p class="">Why might you want to turn JIT off?</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485697467-A41124LX9M6JN006UE1T/switch.jpg" data-image-dimensions="550x670" data-image-focal-point="0.5,0.5" alt="Debugging JIT problems" data-load="false" data-image-id="5d30e601cbc46a0001f97801" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485697467-A41124LX9M6JN006UE1T/switch.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Debugging JIT problems</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<ul data-rte-list="default"><li><p class=""><strong>Slowdowns:</strong> you may know you’re running a tiny program like “gem list —local” that won’t benefit from JIT at all.</p></li><li><p class=""><strong>No compiler available:</strong> you’re running on a production machine without GCC, Clang, etc. MJIT won’t work.</p></li><li><p class=""><strong>You’re benchmarking:</strong> you don’t want JIT because you want predictability, not speed.</p></li><li><p class=""><strong>Memory usage:</strong> MJIT is unusually good for JIT, but it’s not free. You may need every byte you can get.</p></li><li><p class=""><strong>Read-Only /tmp Dir:</strong> If you can’t write the .c files to compile, you can’t compile them.</p></li><li><p class=""><strong>Weird platform:</strong> If you’re running Ruby on your old <a href="https://en.wikipedia.org/wiki/Amiga">Amiga</a> or <a href="https://en.wikipedia.org/wiki/Itanium">iTanium</a>, there isn’t going to be a supported compiler. You may want to turn JIT off out of general worry and distrust.</p></li><li><p class=""><strong>Known bug:</strong> you know of some specific un-fixed bug and you want to avoid it.</p></li></ul><p data-rte-preserve-empty="true" class=""></p><p data-rte-preserve-empty="true" class=""></p><h2>What’s My Takeaway?</h2>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485749339-PFV8H12B25AZNLKURKI9/rackham-arthur-goblin-M00025-16.jpg" data-image-dimensions="1367x2000" data-image-focal-point="0.5,0.5" alt="Telling the playfully frightened children of a once-JITless Ruby" data-load="false" data-image-id="5d30e632cbc46a0001f97b97" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1563485749339-PFV8H12B25AZNLKURKI9/rackham-arthur-goblin-M00025-16.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Telling the playfully frightened children of a once-JITless Ruby</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">If you’re running a non-Rails Ruby app and you’d like to speed it up, test it out with “—jit”. It’s likely to do you some good - at least if the CPU is slowing you down.</p><p class="">If you’re running a Rails app or you don’t need better CPU performance, don’t do anything. At some point in the future JIT will become default, and then you’ll use it automatically. It’s already pretty safe, but it will be even safer with a longer time to try it out. And by then, it’s likely to help Rails as well.</p><p class="">If you have a specific reason to turn JIT off (see above,) now you know how.</p><p class="">And if you’ve heard of Ruby JIT and you’re wondering how it’s doing, now you know!</p>]]></description></item><item><title>RubyConf Nashville</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 22 Nov 2019 18:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/11/20/rubyconf-nashville</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5dd5b45f28602074cedd10f4</guid><description><![CDATA[<p class="">Hey, folks! I’d love to call out a little fun Ruby news from <a href="https://rubyconf.org/">RubyConf</a> in Nashville.</p><h2>Ruby 3.0 and Ruby Core</h2><p class="">We’ve been saying for awhile that Ruby 3.0 will ‘probably’ happen next year (2020.) It has now been formally announced that Ruby 3 will <strong><em>definitely</em></strong> happen next year. From the same Matz Q&amp;A, we heard that he’s still not planning to allow emoji operators. 🤷</p><p class="">Additionally, it looks like a lot of “Async” gems for use with <a href="https://ruby-doc.org/core-2.5.0/Fiber.html">Fibers</a> will be pulled into Ruby Core. In general, it looks like there’s a lot of <a href="http://engineering.appfolio.com/appfolio-engineering/2019/9/13/benchmarking-fibers-threads-and-processes">interesting Fiber-related change</a> coming.</p><h2>Artichoke</h2><p class="">I like to follow alternative (non-MRI) Ruby implementations. <a href="https://github.com/artichoke/artichoke">Artichoke</a> is a Ruby interpreter running in Rust and <a href="https://github.com/mruby/mruby">mruby</a> (an embedded lightweight Ruby dialect, different from normal Ruby). It compiles to <a href="https://webassembly.org/">WebAssembly</a>, allowing it to be easily embedded and <a href="https://en.wikipedia.org/wiki/Sandbox_(software_development)">sandboxed</a> in a web page to run untrusted code, or to run under Node.js on a server.</p><p class="">It’s pretty early days for artichoke, but it runs a lot of Ruby code already. They consider any difference in behaviour from MRI to be a bug, which is a good sign. You can play with their <a href="https://artichoke.run/">in-browser version</a> from their demo page.</p><h2>Rubyfmt</h2><p class=""><a href="https://github.com/penelopezone/rubyfmt">Rubyfmt</a>, pronounced “Ruby format,” is a Ruby automatic formatter, similar to “go fmt.” If that doesn’t mean anything to you, imagine that you could run any Ruby source file through a program and it would use absolutely standard spacing to reformat it - there could be exactly one way to arrange spaces to format your source file. The benefit is that you can stop arguing about it and just use the one standard way of spacing.</p><p class="">Rubyfmt is still in progress. <a href="https://penelope.zone/">Penelope Phippen</a> very insistently wants it to be faster before there’s anything resembling a general release. But there’s enough now that it’s possible to contribute and to play with it.</p><p data-rte-preserve-empty="true" class=""></p>]]></description></item><item><title>Intern Experience at Appfolio</title><dc:creator>Ciarra Peters</dc:creator><pubDate>Thu, 14 Nov 2019 18:21:05 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/11/13/intern-experience-at-appfolio</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5dcca3a5a67070630da85821</guid><description><![CDATA[<p class="">My experience at Appfolio for the past 5 months has been nothing short of amazing. TL;DR - I've made more friends and participated in more 'extra-curricular' activities than my other two internships combined. If beautiful weather 24/7, programming in paradise, biking, rock climbing, D&amp;D, dancing, or Disney World catch your fancy, read on.</p><p class=""><strong># Day 1</strong></p><p class="">My mentor greeted me at the door with a peach flavored smoothie to welcome me to the team. Everyone I passed by on my first day said 'hi' or waved to all us new hires on our morning tour. My computer already had most of the necessary technologies installed, and I was ready to start programming immediately! Appfolio has a well-organized onboarding program to get new hires familiar with Ruby on Rails and React - if you have no experience coming in, not to worry! You get to build a mini Rails app from the ground up - it takes about 1-2 weeks, but you can also start working on production code in parallel if you want to get your hands dirty and are comfortable with the product! Appfolio also has "Engineering Academy" sessions that they will enroll new hires in for the first few weeks after starting. They are hour-long sessions to help you get familiar with the product, the technologies, and the market around property management. This was definitely the best onboarding process I've ever experienced; it got me up to speed quickly and was a great intro to meet other new hires in the same boat. Even though I thought it was a smooth process, Appfolio is always iterating on it and trying to make it better. My manager asked specifically for input on how it could be improved, and they took my feedback into account almost immediately.</p><p class=""><strong># Work and Customer Impact</strong></p><p class="">My first day was Monday Jun 17. By the end of my 2nd week, I finished onboarding and made my first change to production code! (Adding CA-friendly verbiage to the Financial Diagnostics page). One week later, our team added rent-calculation columns to the Rent Roll report, which is important for property managers in determining their high-revenue units. Three weeks later, we start-to-finished the spreadsheet importer for Fixed Assets, which customers use to import information about movable items like trucks and refrigerators. This tool more than doubled the number of fixed assets in our system, from 40,000 to near 100,000 now in November! It was a dramatic increase in just a couple months. In August we created PDF Invoicing -- creating PDFs of charges that property managers can print out &amp; deliver to tenants who don't have email. This was serious work -- it added a highly-desired feature to our most-trafficked pages: our accounting pages. It required meeting with other teams working on Corporate Accounting to ensure our work integrated with their own new features. By September, we had rolled it out and were responding to feedback. About a month later, about 10% of all customers have used this feature, generating more than 5,000 PDF Invoices with it. These are serious, legally-binding accounting documents, and we finally implemented after fiddling with PDF CSS and modals all the way to linking them to reports and work-orders in Rails!</p><p class=""><strong># Technical Challenges</strong></p><p class="">For those of you reading this and wanting to learn more about challenging problems my team confronts on a regular basis, this section is for you:</p><p class="">Working with Rails engines: Engines are a Rails design pattern that helps segment pieces of the host application into miniature Rails apps called "Engines". This reduces the main app's complexity. However, engines also add a lot of technical overhead: If engine A defines Rails models required by engine B, you need to explicitly "inject" those models into engine B in a special way. Moreover, this makes testing engine B more complex: to test engine B in isolation, you have to make a dummy object within engine B for each injected model. Luckily Appfolio has a whole ‘Engineering Academy’ session to learn how to use engines!</p><p class="">Selenium tests: We have been moving over to React as a frontend instead of Rails server-side-rendered frontends. This means that when the Rails server needs to serve a React page, the app has to load up a bunch of javascript that comprises the React page. In production, this javascript is pre-compiled to speed up its delivery. In local development however, it is not pre-compiled by default. Consequently, my local browser-automation Selenium tests were failing because they were timing out waiting for certain React-powered elements to appear on the page. After some investigation, we discovered that my local machine wasn't pre-compiling the React pages, but instead waiting until test-time to compile and deliver the javascript, which takes so long that the test times-out. We solved it by configuring our local testing environment to pre-compile the React pages. We also found we could speed up our Continuous Integration testing platform (Circle CI) by adding the new React javascript to a list of assets to precompile.</p><p class=""><strong># Extra Curricular Activities</strong></p><p class="">The best thing about Appfolio is the people that work here! Appfolio has cultivated an amazing culture that is collaborative, encouraging, and so much fun. Whenever I have a question, there are plenty of engineers who are excited to answer and take the time to explain new concepts in detail. Everyone in product and dev is open to trying new things that may even take them out of their comfort zone - such as learning Michael Jackson's Thriller dance to flash mob it in public and all over the Appfolio office! And that's not all. In the time that I've been here, I've been able to participate in more activities than I've ever done at a single point in my life:<br>Appfolio encourages our development teams to visit customers on site. So my team went to visit a customer an hour away in Ventura! We stopped off at a pier and ate some amazing tacos for lunch outside. We have another site visit planned for Camarillo, another town down the coastline.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1573693949030-AOD1M2IRW57EGRKLLYUW/MVIMG_20190724_154405.jpg" data-image-dimensions="2500x1875" data-image-focal-point="0.5,0.5" alt="MVIMG_20190724_154405.jpg" data-load="false" data-image-id="5dcca9faa8f5af77205168d9" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1573693949030-AOD1M2IRW57EGRKLLYUW/MVIMG_20190724_154405.jpg?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  




<p class="">Every Wednesday a group of us Appfolians play soccer during lunch.</p><p class="">We go climbing at the Santa Barbara Rock Gym with other Appfolians after work.</p><p class="">I was able to go to Disneyland for the first time on our tech retreat!</p><p class="">We have hackdays a few times a year where you can make whatever you want.</p><p class="">Tons of Appfolians bike, and there are great trails all around SB. I get to do some fun rides around the area with pro bikers. We've biked downtown to get the best mini cupcakes in the world, we've biked to the next city over to see the beautiful beaches there, and we've biked along the cliffs overlooking the ocean. I also bike to work every day and am always blown away by the view of palm trees and mountains in the distance.</p><p class="">There are plenty of amazing hikes in the area from which you can get a view of the ocean and the mountains all at once!</p><p class="">Some of us also managed to get involved in hip hop and jiu jitsu classes after work at the University of California Santa Barbara campus.</p><p class="">I love Dungeons and Dragons, and another new hire started a group around the same time I started. We fight giants and save (or not) cities every other Monday night.</p><p class="">Once a month, a bunch of Appfolians bring in their favorite board games and play for hours on end after work. Dinner is provided!</p><p class="">Monday nights a group of people join the same server to play Counterstrike Go together. It's a cross-office enterprise - we coordinate with the San Diego office and spend hours in battle. Dinner is also provided!</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1573693985913-I4AH4JD7GS97AQ9VKWQK/MVIMG_20190918_130719_1.jpg" data-image-dimensions="2500x1875" data-image-focal-point="0.5,0.5" alt="MVIMG_20190918_130719_1.jpg" data-load="false" data-image-id="5dccaa1ed17ca03acd480d33" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1573693985913-I4AH4JD7GS97AQ9VKWQK/MVIMG_20190918_130719_1.jpg?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  




<p class="">Our dev team went on an outing to Santa Cruz island, had a picnic, did some hiking, and tried to find tidepools along the beach.</p><p class="">All of these activities really helped me build great relationships with my co-workers and create a little home-away-from-home. I felt so comfortable learning how to climb, playing soccer with people who actually compete in games, and riding around town with competitive bikers, because the people here are so encouraging and accepting, no matter what skill level I am. Even at work, no matter what knowledge you come in with, Appfolio is a great place for cultivating your skills in a supportive environment. The company culture really focuses on being collaborative (I do a lot of pair programming!), encouraging, fun, and blame-free (if you mess up, you aren't fired, you just get to learn from your mistakes). For example, our team unknowingly made a breaking change to part of the app that generates PDF Invoices for charges. This happened because we changed an API in a separate repo to provide an object instead of an object ID, since we were also in the process of changing the code in our app to expect an object instead of the object ID. However, I Code Reviewed and we merged the JSON API change before we finished CRing and testing the code in our app. Another team who was running through a demo of that portion of the app found the break, and told us about it. We then started brainstorming ways to avoid this type of bug in the future!</p><p class=""><strong># Closing Notes</strong></p><p class="">On your first or second week, you'll be introduced to the rest of the company and asked the question "What is one thing that, if you didn't tell us now, we would never know about you?" When I started, I told everyone that I flash mobbed Thriller with my mom every Halloween in middle and high school. Five months later in October, because of some amazing Appfolians and the support of a very excited VP of engineering, the Thriller dance team went on the road and surprised all of Appfolio with our dance skills. We even barged into a board meeting of all the owners of the company on Halloween and did the dance for them! Hopefully this will become a yearly tradition for the product team! Think of your response to your “fun fact” question wisely, because the answer to this question has the potential to have a very large impact.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1573694019239-T168TKC7E053BBO4J0EQ/ciarra-thriller.jpeg" data-image-dimensions="1680x945" data-image-focal-point="0.5,0.5" alt="ciarra-thriller.jpeg" data-load="false" data-image-id="5dccaa3f44277a243026b56c" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1573694019239-T168TKC7E053BBO4J0EQ/ciarra-thriller.jpeg?format=1000w" />
            
          
        
          
        

        
      
        </figure>]]></description></item><item><title>Ruby's Roots and Matz's Leadership</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 08 Nov 2019 18:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/7/3/rubys-roots-and-matzs-leadership</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5d1c99a24ef9a0000107f36b</guid><description><![CDATA[<p class="">I recently had the excellent fortune to be invited to a gathering at <a href="https://cookpad.com">CookPad</a> in Bristol, where <a href="https://en.wikipedia.org/wiki/Yukihiro_Matsumoto">Matz</a>, <a href="https://twitter.com/_ko1">Koichi</a>, <a href="https://twitter.com/mametter?lang=en">Yusuke Endoh</a> (a.k.a. <a href="https://github.com/mame">Mame</a>) and <a href="https://twitter.com/tenderlove">Aaron Patterson</a> all gave great talks about Ruby.</p><p class="">I was especially interested in Matz’s first talk, which was about where he got inspiration for various Ruby features, and about how he leads the language - how and why new features are added to Ruby.</p><p class="">You can find plenty of online speculation about where Ruby is going and how it’s managed. And I feel awkward adding to that speculation — especially since I have great respect for Matz’s leadership. But it seems reasonable to relay his own words about how he chooses.</p><p class="">And I love hearing about how Ruby got where it is. Languages are neat.</p><p class="">You’ll notice that most of Ruby’s influences are old languages, often obscure ones. That’s partly because <a href="https://en.wikipedia.org/wiki/Ruby_(programming_language)">Ruby itself dates back to 1995</a>. A lot of current languages didn’t exist to get these features from!</p><h2>Ruby and Its Features</h2><p class="">Much of what Matz had to say was about Ruby itself, and where particular features came from. This is a long list, so settle in :-)</p><p class="">The <strong>begin</strong> and <strong>end</strong> keywords, and Ruby’s idea of “comb indentation” - the overall flow of if/elsif/else/end - came from the <a href="https://en.wikipedia.org/wiki/Eiffel_(programming_language)">Eiffel</a> language. He mentions that <strong>begin/end versus curly-braces</strong> are about operator precedence, which I confess I’d never even considered.</p><p class="">On that note, why “<strong>elsif</strong>”? Because it was the shortest way to spell “else if” that was still pronounced the same way. “elseif” is longer, and “elif” wouldn’t be pronounced the same way.</p><p class="">“<strong>Then</strong>” is technically a Ruby keyword, but it’s optional, a sort of “soft” keyword as he called it. For instance, you can actually use “then” as a method name and it’s fine. You might be forgiven for asking, “wait, what does that do?” The effective answer is “nothing.”</p><p class="">Ruby’s loop structure, with <strong>continue</strong>, <strong>next</strong>, <strong>break</strong> and so on came from C and Perl. He liked Perl’s “<strong>next</strong>” because it’s shorter than the equivalent C structures.</p><p class="">Ruby’s mixins, mostly embodied by <strong>modules</strong>, came from Lisp’s <a href="https://en.wikipedia.org/wiki/Flavors_(programming_language)">Flavors</a>.</p><p class="">“<strong>Unless</strong>” is from <a href="https://www.perl.org/">Perl</a>. Early on, Ruby was meant as a Perl-style scripting language and many of its early features came from that fact. “<strong>Until</strong>” is the same. Also, when I talk about Perl here I mean Perl 5 and before, which are very different from Perl 6 - Ruby was very mature by the time Perl 6 happened.</p><p class="">He’s actually forgotten <em>where</em> he stole the <strong>for/in loop</strong> syntax from. Perhaps Python? It can’t be JavaScript, because Ruby’s use of for/in is older than JavaScript.</p><p class="">Ruby’s three-part <strong>true/false/nil</strong> with false and nil being the only two <a href="https://gist.github.com/jfarmer/2647362">falsy</a> values is taken from various Lisp dialects. For some of them there is a “false” constant as the only false value, and some use “t” and “nil” in a similar way. He didn’t say so, but I wonder if it might have had a bit of SQL influence. SQL booleans have a similar true/false/NULL thing going on.</p><p class="">Ruby’s <strong>and/or/not</strong> operations come straight from Perl, of course. Matz likes the way they feel descriptive and flow like English. As part of that, they’re often good for avoiding extra parentheses.</p><p class="">Matz feels that <strong>blocks</strong> are the greatest invention of Ruby  (I agree.) He got the idea from a <a href="https://en.wikipedia.org/wiki/CLU_(programming_language)">1970s language called CLU</a> from MIT, which called them “iterators” and only allowed them on certain loop constructs.</p><p class="">He took <strong>rescue/ensure/retry</strong> from Eiffel, but Eiffel didn’t otherwise have “normal” exception handling like similar languages. Ruby’s method of throwing an exception <em>object</em> isn’t like Eiffel, but <em>is</em> like several other older languages. He didn’t mention a single source for that, I don’t think.</p><p class="">He <em>tried</em> to introduce a <strong>different style of error handling</strong> where each call returns an error object along with its return value from a <a href="https://en.wikipedia.org/wiki/Icon_(programming_language)">1970s language called Icon</a> from the University of Arizona. But after early trials of that method, he thought it would be too hard for beginners and generally too weird. It sounds a lot like what I see of <a href="https://8thlight.com/blog/kyle-krull/2018/08/13/exploring-error-handling-patterns-in-go.html">GoLang error handling</a> from his descriptions.</p><p class=""><strong>Return</strong> came from <a href="https://en.wikipedia.org/wiki/C_(programming_language)">C</a>. No surprise. Though of course, not multivalue return.</p><p class="">He got <strong>self</strong> and <strong>super</strong> from <a href="https://en.wikipedia.org/wiki/Smalltalk">SmallTalk</a>, though SmallTalk’s super is different - it’s the parent object, and you can call any parent method you like on it, not just the one that you just received.</p><p class="">He says he regrets <strong>alias</strong> and <strong>undef</strong> a little. He got them from <a href="https://en.wikipedia.org/wiki/Sather">Sather</a> (1980s, UC Berkeley, a derivative language from Eiffel.) Sather had specific labelling for <em>interface inheritance</em> versus <em>implementation inheritance</em>. Ruby took alias and undef without keeping that distinction, and he feels like we often get those two confused. Also, alias and undef tend to be used to break <a href="https://en.wikipedia.org/wiki/Liskov_substitution_principle">Liskov Substitution</a>, where a child-class instance can always be used as if it were a parent-class instance. As was also pointed out, both alias and undef can be done with method calls in Ruby, so it’s not clear you really need to use keywords for them. He says the keywords now mostly exist for historical reasons since you can define them as methods… but that he doesn’t necessarily think you should always use the methods (alias_method, undefine_method) over the keywords.</p><p class=""><strong>BEGIN</strong> and <strong>END</strong> are from Awk originally, though Perl folks know they exist there too. This was also from Ruby’s roots as a system administrator’s scripting language. Matz doesn’t recommend them any more, especially in non-script applications such as Ruby on Rails apps.</p><p class="">C folks already know that <strong>__FILE__</strong> and <strong>__LINE__</strong> are from the <a href="https://en.wikipedia.org/wiki/C_preprocessor">C preprocessor</a>, a standard tool that’s part of the C language (but occasionally used separately from it.)</p><h2>On Matz and Ruby Leadership</h2><p class="">That was a fun trip down memory lane. Now I’ll talk about what Matz said about his own leadership of Ruby. Again, I’m trying to keep this to what Matz actually said rather than putting words in his mouth. But there may be misunderstandings or similar errors - and if so, they are <em>my</em> errors and I apologize.</p><p class="">Matz points out that Ruby uses the <a href="https://en.wikipedia.org/wiki/Benevolent_dictator_for_life">“Benevolent Dictator for Life”</a> model, as Python did until recently. He can’t personally be an expert on everything so he asks other people for opinions. He points out that he has only ever written a single Rails application, for instance, and that was from a tutorial. But in the end, after asking various experts, it <strong><em>is</em></strong> his decision.</p><p class="">An audience member asked him: when you add new features, it necessarily adds entropy to the language (and Matz agreed.) Isn’t he afraid of doing too much of that? <em>No</em>, said Matz, because <em>we’re not adding many different ways of doing the same thing</em>, and that’s what he considers the problem with too many language features causing too much entropy. Otherwise (he implied) a complicated language isn’t a particularly bad thing.</p><p class="">He talked a bit about the new pipeline operator, which is a current controversy - a lot of people don’t like it, and Matz isn’t sure if he’ll keep it. He suggested that he might remove or rework it. He’s thinking about it. (Edit: it has <a href="https://github.com/ruby/ruby/commit/2ed68d0ff9a932efbc4393c869534040dec8f647">since been removed</a>.)</p><p class="">But he pointed out: he <strong><em>does</em></strong> need to be able to experiment, and putting a new feature into a prerelease Ruby to see how he likes it is a good way to do that. The difficulty is with features that make it into a release, because then people are using them.</p><h2>The Foreseeable Future</h2><p class="">Matz also talked about some <em>specific</em> things he does or doesn’t want to do with Ruby.</p><p class="">Matz doesn’t expect he’ll add any new fully-reserved words to the language, but is considering “it” as a sort of “soft”, or context-dependent keyword. In the case of “it” in particular, it would be a sort of self-type variable for blocks. So when he says “no new keywords,” it doesn’t seem to be an absolute.</p><p class="">He’s trying not to expand into emoji or Unicode characters, such as using the Unicode lambda (λ) to create lambdas - for now, they’re just too hard for a lot of users to type. So Aaron’s patch to turn the pipeline operator into a smiley emoji isn’t going in. Matz said he’d prefer the big heart anyway :-)</p><p class="">And in general, he tries hard to keep backward compatibility. Not everybody does - he cites Rails as an example of having lower emphasis on backward compatibility than the Ruby language. But as Matz has said in several talks, he’s really been trying not to break too much since the Ruby 1.8/1.9 split that was so hard for so many users.</p><h2>What Does That Mean?</h2><p class="">Other than a long list of features and where Matz got them, I think the thing to remember is: it’s up to Matz, and sometimes he’s not perfectly expert, and sometimes he’s experimenting or wrong… But he’d love to hear from you about it, and he’s always trying hard and looking around.</p><p class="">As the list above suggests, he’s open to a <em>wide</em> variety of influences if the results look good.</p>]]></description></item><item><title>Ruby 2.7preview2, a Quick Speed Update</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 25 Oct 2019 17:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/10/24/ruby-27preview2-a-quick-speed-update</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5db16c927dc76a3ab97a1162</guid><description><![CDATA[<p class="">As you know, <a href="http://engineering.appfolio.com/appfolio-engineering/2019/3/7/ruby-speed-roundup-20-through-26">I like to check Ruby’s speed for running big Rails apps</a>. Recently, <a href="https://www.ruby-lang.org/en/news/2019/10/22/ruby-2-7-0-preview2-released/">Ruby 2.7 preview2 was released</a>. Normally Ruby releases a new version every Christmas, so it was about time.</p><p class="">I’ve run Rails Ruby Bench on it to check a few things - first, is the speed significantly different? Second, any change in <a href="http://engineering.appfolio.com/appfolio-engineering/2018/8/6/how-can-i-use-ruby-26-jit">Ruby’s JIT</a>?</p><p class="">Today’s is a pretty quick update since there haven’t been many changes.</p><h2>Speed and Background</h2><p class="">Mostly, <a href="http://engineering.appfolio.com/appfolio-engineering/2019/3/7/ruby-speed-roundup-20-through-26">Ruby’s speed jumps are between minor versions</a> - 2.6 is different from 2.5 is different from 2.4, but there’s not much change between 2.4.0 and 2.4.5, for instance. I’ve done some checking of this, and it’s held pretty true over time. It's much less true of prerelease Ruby versions, as you’d expect - they’re often still getting big new optimisations, so 2.5’s prereleases were quite different from each other, and from the released 2.5. That’s appropriate and normal.</p><p class="">But I went ahead and speed-checked 2.6.5 against 2.6.0. While these small changes don’t usually make a significant difference, 2.6.0 was one I checked carefully.</p><p class="">And of course, over time I’ve checked how JIT is doing with Rails. Rails is still too tough for it, but <a href="http://engineering.appfolio.com/appfolio-engineering/2019/5/17/jit-performance-with-a-simpler-benchmark">there’s a difference in how close do breakeven it is</a>, depending on both <strong><em>what code</em></strong> I’m benchmarking and exactly what revision of JIT I’m testing.</p><h2>Numbers First</h2><p class="">While I’ve run a lot of trials of this, the numbers are fairly simple - what’s the median performance of Ruby, running Discourse flat-out, for this number of samples? This is code I’ve benchmarked many times in roughly this configuration, and it turns out to be well-summarised by the median.</p><p class="">In this case, the raw data is small enough that I can just hand it to you. Here’s my data for 90 runs per configuration with 10,000 HTTP requests per run, with everything else how I generally do it:</p>


<table>
  <tr><th>Ruby version</th><th>Median reqs/sec</th><th>Std. Dev.</th><th>Variance</th></tr>
  <tr><td>2.6.0</td><td>174.0</td><td>1.47</td><td>2.17</td></tr>
  <tr><td>2.6.5</td><td>170.1</td><td>1.69</td><td>2.86</td></tr>
  <tr><td>2.7.0</td><td>175.6</td><td>1.63</td><td>2.67</td></tr>
  <tr><td>2.7.0 w/ JIT</td><td>110.4</td><td>1.05</td><td>1.11</td></tr>
</table>

<p class="">One of the first things you’re likely to notice: except for 2.7 with JIT, which we expect to be slow, these are all pretty close together. The difference between 2.6.5 and 2.7.0 is only 5.5 reqs/second, which is a little over three standard deviations - not a huge difference.</p><p class="">I’ve made a few trials, though, and these seem to hold up. 2.6.5 <strong><em>does</em></strong> seem just a touch slower than 2.6.0. The just-about-2% slower that you’re seeing here seems typical. 2.7.0 seems to be a touch faster than 2.6.0, but as you see here, it would take a lot of samples to show it convincingly. One standard deviation apart like this could easily be measurement error, even with the multiple runs I’ve done separately. This is simply too close to call without extensive measurement.</p><h2>Conclusions</h2><p class="">Sometimes when you do statistics, you get the simple result: overall, Ruby 2.7 preview 2 is the same speed as 2.6.0. There might be a regression in 2.6.5, but if so, it’s a small one and there’s a small optimisation in 2.7 that’s balancing it out. Alternately, all these measurements are so close that they may all, in effect, be the same speed.</p>]]></description></item><item><title>How MJIT Generates C From Ruby - A Deep Dive</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 11 Oct 2019 17:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/7/19/how-mjit-generates-c-from-ruby-a-deep-dive</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5d31dff44d0b5b00015965dc</guid><description><![CDATA[<p class="">You probably <a href="http://engineering.appfolio.com/appfolio-engineering/2017/12/26/ruby-3-and-jit-where-when-and-how-fast">already know the basics of JIT in Ruby</a>. CRuby’s JIT implementation, called MJIT, is a really interesting beast.</p><p class="">But what does the C code actually look like? How is it generated? What are all the specifics?</p><p class="">If you’re afraid of looking at C code, this may be a good week to skip this blog. I’m just sayin’.</p><h2>How Ruby Runs Your Code</h2><p class="">I’ll give you the short version here: Ruby parses your code. It turns it into an <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">Abstract Syntax Tree</a>, which is just a tree-data-structure version of the operations you asked it to do. Before Ruby 1.9, Ruby would directly interpret the tree structure to run your code. Current Ruby (1.9 through 2.6-ish) translates it into buffers of <a href="https://en.wikipedia.org/wiki/Bytecode">bytecodes</a>. These buffers are called ISEQs, for “Instruction SEQuences.” There are various tools like <a href="https://github.com/ko1/yomikomu">yomikomu</a> that will let you dump, load and generally examine ISEQs. <a href="https://github.com/Shopify/bootsnap">BootSnap</a>, the now-standard tool to optimize startup for large Rails apps, works partly by loading dumped ISEQs instead of parsing all your code from .rb files.</p><p class="">Also, have I talked up Pat Shaughnessy’s <a href="http://patshaughnessy.net/ruby-under-a-microscope">Ruby Under a Microscope</a> lately? He explains all of this in massive detail. If you’re a Ruby-internals geek (guilty!) this is an amazing book. It’s also surprising how little Ruby’s internals have changed since he wrote it.</p><p class="">In the <a href="https://github.com/ruby/ruby">Ruby source code</a>, there’s a <a href="https://github.com/ruby/ruby/blob/master/insns.def">file full of definitions</a> for all the instructions that go into ISEQs. You can look up trivial examples like <a href="https://github.com/ruby/ruby/blob/master/insns.def#L1065">the optimized plus operator</a> and see how they work. Ruby actually doesn’t call these directly - the source file is written in a weird, not-exactly-C syntax that gets taken apart and used in multiple ways. You can think of it as a C <a href="https://en.wikipedia.org/wiki/Domain-specific_language">DSL</a> if you like. For the “normal” Ruby interpreter, they all wind up in a giant loop which looks up the next operation in the ISEQ, runs the appropriate instructions for it and then loops again to look up the next instruction (and so on.)</p><p class="">A Ruby build script generates the interpreter’s giant loop as a C source file when you build Ruby. It winds up built into your normal ruby binary.</p><p class="">Ruby’s MJIT uses the same file of definitions to generate C code from Ruby. MJIT can take an ISEQ and generate all the lines of C it would run in that loop without actually needing the loop or the instruction lookup. If you’re a compiler geek, yeah, this is a bit like <a href="https://en.wikipedia.org/wiki/Loop_unrolling">loop unrolling</a> since we already know the instruction sequence that the loop would be operating on. So we can just “spell out” the loop explicitly. That also lets the C compiler see where operations would be useless or cancel each other out and just skip them. That’s hard to do in an interpreter!</p><p class="">So what does all this actually look like when Ruby does it?</p><h2>MJIT Options and Seeing Inside</h2><p class="">It turns out that MJIT has some options that let us see behind the curtain. If you have Ruby 2.6 or higher then you have JIT available. Run “ruby —help” and you can see MJIT’s extra options on the command line. Here’s what I see in 2.6.2 (note that some options are changing for not-yet-released 2.7):</p>


<pre class="source-code">JIT options (experimental):
  --jit-warnings  Enable printing JIT warnings
  --jit-debug     Enable JIT debugging (very slow)
  --jit-wait      Wait until JIT compilation is finished everytime (for testing)
  --jit-save-temps
                  Save JIT temporary files in $TMP or /tmp (for testing)
  --jit-verbose=num
                  Print JIT logs of level num or less to stderr (default: 0)
  --jit-max-cache=num
                  Max number of methods to be JIT-ed in a cache (default: 1000)
  --jit-min-calls=num
                  Number of calls to trigger JIT (for testing, default: 5)</pre>

<p class="">Most of these aren’t a big deal. Debugging and warnings can be useful, but they’re not thrilling. But “—jit-save-temps” there may look intriguing to you… I know it did to me!</p><p class="">That will actually save the C source files that Ruby is using and we can see inside them!</p><p class="">If you do this, you may want to set the environment variables TMP or TMPDIR to a directory where you want them - OS X often puts temp files in weird places. I added an extra print statement to mjit_worker.c in the function “convert_unit_to_func” right after “sprint_uniq_filename” so that I could see when it created a new file… But that means messing around in your Ruby source, so you do you.</p><h2>Multiplication and Combinatorics</h2>


<pre><code class="lang-ruby"># multiply.rb
def multiply(a, b)
  a * b
end

1_000_000.times do
  multiply(7.0, 10.0)
end
</code></pre>


<p class="">I decided to start with <strong><em>really</em></strong> simple Ruby code. MJIT will only JIT a method, so you need a method. And then you need to call it, preferably a lot of times. So the code on the right is what I came up with. It is intentionally <strong><em>not complicated</em></strong>.</p><p class="">The “multiply” method multies two numbers and does nothing else. It gets JITted because it’s called many, many times. I ran this code with “ruby —jit —jit-save-temps multiply.rb”, which worked fine for me once I figured out where MacOS was putting its temp files.</p><p class="">The <a href="https://gist.github.com/noahgibbs/55f0dd6c0fd58ed9580bd3152b08ae37">resulting .c file generated by Ruby is 236 lines</a>. Whether you find this astoundingly big or pretty darn small depends a lot on your background. Let me show you a few of the highlights from that file.</p><p class="">Here is a (very) cut-down and modified version:</p>


<pre class="source-code">// Generated by MJIT from multiply.rb
ALWAYS_INLINE(static VALUE _mjit_inlined_6(...));
static inline VALUE
_mjit_inlined_6(rb_execution_context_t *ec, rb_control_frame_t *reg_cfp, const VALUE orig_self, const rb_iseq_t *original_iseq)
{
    // ...
}

VALUE
_mjit0(...)
{
    // ...
    label_6: /* opt_send_without_block */
    {
        // ...
        stack[0] = _mjit_inlined_6(ec, reg_cfp, orig_self, original_iseq);
    }
}
</pre>

<p class="">What I’m showing here is that there is an inlined <strong>_mjit_inlined_6</strong> method (C calls them “functions”) that gets called by a top-level “mjit0” method, which is the MJIT-ted version of the “multiply” method in Ruby. “Inlined” means the C compiler effectively rewrites the code so that it’s not a called method - instead, the whole method’s code, all of it, gets <strong><em>pasted in where the method would have been called</em></strong>. It’s a bit faster than a normal function call. It also lets the compiler optimize it just for that one case, since the pasted-in code won’t be called by anything else. It’s pasted in at that one single call site.</p><p class="">If you look at the full code, you’ll also see that each method is full of “labels” and comments like the one above (“opt_send_without_block”). Below is basically all of the code to that inlined function. If you ignore the dubious indentation (generated code is generated), you have <strong>a chunk of C for each bytecode instruction</strong> and some setup, cleanup and stack-handling in between. <strong>The large</strong> <strong>“cancel” block at the end is all the error handling </strong>that is done if the method does not succeed.</p><p class="">The chunks of code at each label, by the way, are what the interpreter loop would normally do.</p><p class="">And if you examine these specific opcodes, you’ll discover that this is taking two local variables and multiplying them - <strong><em>this</em></strong> is the actual multiply method from the Ruby code above.</p>


<pre class="source-code">static inline VALUE
_mjit_inlined_6(rb_execution_context_t *ec, rb_control_frame_t *reg_cfp, const VALUE orig_self, const rb_iseq_t *orig
inal_iseq)
{
    const VALUE *orig_pc = reg_cfp-&gt;pc;
    const VALUE *orig_sp = reg_cfp-&gt;sp;
    VALUE stack[2];
    static const VALUE *const original_body_iseq = (VALUE *)0x7ff4cd51a080;

label_0: /* getlocal_WC_0 */
{
    MAYBE_UNUSED(VALUE) val;
    MAYBE_UNUSED(lindex_t) idx;
    MAYBE_UNUSED(rb_num_t) level;
    level = 0;
    idx = (lindex_t)0x4;
    {
        val = *(vm_get_ep(GET_EP(), level) - idx);
        RB_DEBUG_COUNTER_INC(lvar_get);
        (void)RB_DEBUG_COUNTER_INC_IF(lvar_get_dynamic, level &gt; 0);
    }
    stack[0] = val;
}

label_2: /* getlocal_WC_0 */
{
    MAYBE_UNUSED(VALUE) val;
    MAYBE_UNUSED(lindex_t) idx;
    MAYBE_UNUSED(rb_num_t) level;
    level = 0;
    idx = (lindex_t)0x3;
    {
        val = *(vm_get_ep(GET_EP(), level) - idx);
        RB_DEBUG_COUNTER_INC(lvar_get);
        (void)RB_DEBUG_COUNTER_INC_IF(lvar_get_dynamic, level &gt; 0);
    }
    stack[1] = val;
}

label_4: /* opt_mult */
{
    MAYBE_UNUSED(CALL_CACHE) cc;
    MAYBE_UNUSED(CALL_INFO) ci;
    MAYBE_UNUSED(VALUE) obj, recv, val;
    ci = (CALL_INFO)0x7ff4cd52b400;
    cc = (CALL_CACHE)0x7ff4cd5192e0;
    recv = stack[0];
    obj = stack[1];
    {
        val = vm_opt_mult(recv, obj);

        if (val == Qundef) {
            reg_cfp-&gt;sp = vm_base_ptr(reg_cfp) + 2;
            reg_cfp-&gt;pc = original_body_iseq + 4;
            RB_DEBUG_COUNTER_INC(mjit_cancel_opt_insn);
            goto cancel;
        }
    }
    stack[0] = val;
}

label_7: /* leave */
    return stack[0];

cancel:
    RB_DEBUG_COUNTER_INC(mjit_cancel);
    rb_mjit_iseq_compile_info(original_iseq-&gt;body)-&gt;disable_inlining = true;
    rb_mjit_recompile_iseq(original_iseq);
    const VALUE current_pc = reg_cfp-&gt;pc;
    const VALUE current_sp = reg_cfp-&gt;sp;
    reg_cfp-&gt;pc = orig_pc;
    reg_cfp-&gt;sp = orig_sp;

    struct rb_calling_info calling;
    calling.block_handler = VM_BLOCK_HANDLER_NONE;
    calling.argc = 2;
    calling.recv = reg_cfp-&gt;self;
    reg_cfp-&gt;self = orig_self;
    vm_call_iseq_setup_normal(ec, reg_cfp, &amp;calling, (const rb_callable_method_entry_t *)0x7ff4cd930958, 0, 2, 2);

    reg_cfp = ec-&gt;cfp;
    reg_cfp-&gt;pc = current_pc;
    reg_cfp-&gt;sp = current_sp;
    *(vm_base_ptr(reg_cfp) + 0) = stack[0];
    *(vm_base_ptr(reg_cfp) + 1) = stack[1];
    return vm_exec(ec, ec-&gt;cfp);

} /* end of _mjit_inlined_6 */</pre>

<p class="">The labels mark where a particular bytecode instruction in the ISEQ starts, and the name is the name of that bytecode instruction. This is doing nearly <em>exactly</em> what the Ruby interpreter would, including lots of Ruby bookkeeping for things like call stacks.</p><h2>What Changes?</h2><p class="">Okay. We’ve multiplied two numbers together. This is a single, small operation.</p><p class="">What changes if we do more?</p><p class="">Well… This is already a fairly long blog post. But first, <a href="https://github.com/noahgibbs/mjit_simple_multiply_output">I’ll link a repository of the output I got when multiplying more than two numbers</a>.</p><p class="">And then after you clone that repo, you can start doing interesting things yourself to see what changes over time. For instance:</p>


<pre class="source-code"># See what's different between multiplying 2 Floats and multiplying 3 Floats
diff -c multiply_2_version_0.c multiply_3_version_0.c
</pre>

<p class="">And in fact, if we multiply three or more Floats, MJIT will realize it can improve some things over time. When multiplying three (or four!) Floats, it will produce three different chunks of C code, not just one, as it continues to iterate. So:</p>


<pre class="source-code"># See what's different between the first and second way to multiply three Floats
diff -c multiply_3_version_0.c multiply_3_version_1.c</pre>

<p class="">I’ll let you have a look. When looking at diffs, keep in mind that the big hexadecimal numbers in the CALL_INFO and CALL_CACHE lines will change for every run, both in my output and in any output you make for yourself — they’re literally hardcoded memory addresses in Ruby, so they’re different for every run. But the other changes are often interesting and substantive, as MJIT figures out how to optimize things.</p><h2>What Did We Learn?</h2><p class="">I like to give you interesting insights, not <strong><em>just</em></strong> raw code dumps. So what’s interesting here?</p><p class="">Here’s one interesting thing: <em>you don’t see any checks for whether operations like multiply are redefined</em>. But that’s not because of excellent JIT optimization - it’s because that all lives inside the vm_opt_mult function call up above. At best, they might be recognized as a repeat check and the compiler <strong><em>might</em></strong> be able to tell that it doesn’t need to check them again. But that’s actually hard — there’s a lot of code here, and it’s hard to verify that none of it could possibly ever redefine an operation… Especially in Ruby!</p><p class="">So: MJIT is going to have a lot of trouble skipping those checks, given the way it structures this code.</p><p class="">And if it can’t skip those checks, it’s going to have a lot of trouble doing optimisations like <a href="https://en.wikipedia.org/wiki/Constant_folding">constant folding</a>, where it multiplies two numbers at compile time instead of every time through the loop. You and I both know that 7 * 10 will always be 70, every time through the loop because nobody is redefining Integer multiply halfway. But MJIT can’t really know that - what if there was a <a href="https://apidock.com/ruby/Kernel/set_trace_func">trace_func</a> that redefined operations constantly? Or a background thread that redefined the operation halfway through? Ruby allows it!</p><p class="">To put it another way, MJIT isn’t doing a lot of interesting language-level optimisation here. Mostly it’s just optimising simple bookkeeping like the call stack and C-level function calls. Most of the Ruby operations, including overhead like checking if you redefined a function, stay basically the same.</p><p class="">That should make sense. Remember how MJIT got written and merged in record time? It’s very hard to make language-level optimizations without a chance of breaking something. MJIT tries not to change the language semantics at all. So it doesn’t make many changes or assumptions. So mostly, MJIT is a simple mechanical transform of what the interpreter was already doing.</p><p class="">If you didn’t already know what the Ruby interpreter was doing under the hood, this is also a fun look into that.</p>]]></description></item><item><title>Benchmark Results: Threads, Processes and Fibers</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 27 Sep 2019 17:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/9/4/benchmark-results-threads-processes-and-fibers</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5d6f660cc5285d0001f0e871</guid><description><![CDATA[<p class="">You may recall me writing an I/O-heavy test for threads, processes and fibers to benchmark their performance. I then ran it a few times on my Mac laptop, wrote the numbers down and called it a day.</p><p class="">While that can be useful, it’s now how we usually do benchmarking around here. So let’s do something with a touch more rigor, shall we?</p><p class="">Also some pretty graphs. I like graphs.</p><h2>Methodology</h2><p class="">If you’re the type to care about methodology (I am!) then this is a great time to review the previous blog post and/or <a href="https://github.com/noahgibbs/fiber_basic_benchmarks/tree/master/benchmarks">the code to the tests</a>.</p><p class="">I’ve written a simple master/worker pattern in (separately) threads, fibers and processes. In each case, the master writes to the worker, which reads, writes a response, and waits for the next write. This is very simple, but heavy on I/O and coordination.</p><p class="">For this post, I’ll be timing the results for not just threads vs fibers vs processes, but also for Rubies 2.0 through 2.6 - specifically, CRuby versions 2.0.0-p0, 2.1.10, 2.2.10, 2.3.8, 2.4.5, 2.5.3 and 2.6.2.</p><p class="">I’ll mention “workers” for all these tests. For thread-based testing, a “worker” is a thread. Same for processes and fibers - one worker is one process or one fiber.</p><h2>First Off, Which is Faster?</h2><p class="">It’s hard to definitively say which of the three methods of concurrency is faster in general. In fact, it’s nearly a meaningless question since they do significantly different things and are often combined with each other.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567582158693-55UN97D9TZ1YTEAWU9E5/t_v_f_v_p_ruby_2_6.png" data-image-dimensions="1030x976" data-image-focal-point="0.5,0.5" alt="t_v_f_v_p_ruby_2_6.png" data-load="false" data-image-id="5d6f67cdc5285d0001f0fb7c" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567582158693-55UN97D9TZ1YTEAWU9E5/t_v_f_v_p_ruby_2_6.png?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  




<p class="">Now, with sanity out of the way, let’s pretend we can just answer that with a benchmark. You know you want to.</p><p class="">The result for Ruby 2.6 is to the right.</p><p class="">It looks as if processes are always faster assuming you don’t use too many of them.</p><p class="">And that’s true, sort of. Specifically, it’s true until you start to hit limits on memory or number of processes, and then it’s false. That’s probably why you’re seeing that rapid rise in processing time for 1,000 processes. These are extremely simple processes - if you’re doing more real work you wouldn’t use 1,000 workers because you’d run out of memory long before that.</p><p class="">However, for a simple task like this, fibers beat threads because they’re lighter-weight, using less memory or CPU. And processes beat both, because they get around Ruby’s use of the GIL, and it’s such a tiny task that we don’t hit memory constraints until we use close to 1,000 processes - a far larger number of workers than is useful or productive.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567584555071-9GXOCT728C5VSF9MZUMS/tfp_ruby_20.png" data-image-dimensions="1000x957" data-image-focal-point="0.5,0.5" alt="tfp_ruby_20.png" data-load="false" data-image-id="5d6f712aa4f09300015b6e1f" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567584555071-9GXOCT728C5VSF9MZUMS/tfp_ruby_20.png?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  




<p class="">In fact, you would normally use multiple of these. You can and should use <a href="http://engineering.appfolio.com/appfolio-engineering/2017/3/22/rails-benchmarking-puma-and-multiprocess">multiple threads or fibers per process with multiple processes in CRuby to avoid GIL issues</a>. Yeah, fine, real-world issues. Let’s ignore them and have more fun with graphs. Graphs are awesome.</p><p class="">You might (and should) reasonably ask, "but is this an artifact of Ruby 2.6?” To the right are the results for Ruby 2.0, for reference. They <strong><em>do not</em></strong> include 1,000 workers because Ruby 2.0 segfaults when you try that.</p><p data-rte-preserve-empty="true" class=""></p><p data-rte-preserve-empty="true" class=""></p><p data-rte-preserve-empty="true" class=""></p><p data-rte-preserve-empty="true" class=""></p><h2>Processes Across the Years</h2>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567582806026-SH59ZR69IH3M0LWJL8D2/forks_by_ruby.png" data-image-dimensions="1015x944" data-image-focal-point="0.5,0.5" alt="forks_by_ruby.png" data-load="false" data-image-id="5d6f6a545487dc0001e37682" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567582806026-SH59ZR69IH3M0LWJL8D2/forks_by_ruby.png?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  




<p class="">How has our Ruby multiprocess performance changed since Ruby 2.0? That’s <a href="http://engineering.appfolio.com/appfolio-engineering/2017/12/4/hows-progress-on-ruby-3x3">the baseline for Ruby 3x3</a>, so it can be our baseline here, too.</p><p class="">If you look to the right, the short answer is that if you use a reasonable number of workers the performance is excellent and very stable. If you use a completely over-the-top number of workers, the performance isn’t amazing. I wouldn’t really call that a bug.</p><p class="">Incidentally, that isn’t just noisy data. While I only ran each test 10 times, the variance is very low on the results. Ruby 2.3.8 and 2.6.2 just seem to be (reliably) extra-bad with far too many processes. Of course, that’s a bad idea on<strong><em> any</em></strong> Ruby, not just the extra-bad versions.</p><p class="">In general, though, Ruby processes are living up to their reputation here - CRuby has used processes for concurrency first and foremost. Their performance is excellent and so is their stability.</p><p class="">Though you’ll notice that the “1,000 processes” line doesn’t go all the way back to Ruby 2.0.0-p0, as mentioned above. That’s because it gets a segmentation fault and crashes the Ruby interpreter. That’s a theme - fibers and threads <strong><em>also</em></strong> crash Ruby 2.0.0-p0 when you try to use far too many of them. But Ruby 2.1 has fixed the problem. I <strong><em>hope</em></strong> that doesn’t mean you need to upgrade, since Ruby 2.1 is almost six years old now…</p><h2>Threads Across the Years</h2>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567583215572-3NKM0HPO08GFCCYAIDTD/threads_by_ruby.png" data-image-dimensions="1025x957" data-image-focal-point="0.5,0.5" alt="threads_by_ruby.png" data-load="false" data-image-id="5d6f6bee5487dc0001e387a9" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567583215572-3NKM0HPO08GFCCYAIDTD/threads_by_ruby.png?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  




<p class="">That was processes. What about threads?</p><p class="">They’re pretty good too. And unlike processes, thread performance has improved pretty substantially between Ruby 2.0 and 2.6. That’s nearly twice as fast for an all-coordination, all-I/O task like this one!</p><p class="">1,000 threads is still far too many for this task, but CRuby handles it gracefully with only a slight performance degradation. It’s a minor tuning error, not a horrible misuse of resources like 1,000 processes would be.</p><p class="">What you’re seeing there, with 5-10 threads being optimal for an I/O-heavy workload, is pretty typical of CRuby. It’s hard to get great performance with a lot of threads because the GIL keeps more than one from running Ruby at once. Normally with 1,000 threads, CRuby’s performance will fall off a cliff - it simply won’t speed up beyond something like 6 threads. But this task is nearly all I/O, and so the GIL does fairly minimal harm here.</p><h2>Fibers Across the Years</h2>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567583507172-GSKECDJ3BNZKYILTRPCH/fibers_by_ruby.png" data-image-dimensions="996x953" data-image-focal-point="0.5,0.5" alt="fibers_by_ruby.png" data-load="false" data-image-id="5d6f6d1252f197000155e98b" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567583507172-GSKECDJ3BNZKYILTRPCH/fibers_by_ruby.png?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  




<p class="">Fibers are the really interesting case here. We know they’ve received some rewriting love in recent Ruby versions, and I’ve seen Fiber.yield times significantly improved from very old to very new CRuby. Their graph is to the right. And <strong><em>it is indeed interesting</em></strong>.</p><p class="">First, 1,000 fibers are clearly too many for this task, as with threads and processes. In fact, threads seem to handle the excess workers better, at least until 2.6.</p><p class="">Also, fibers seem to get worse for performance after 2.0 until 2.6 precipitously fixes them. Perhaps that’s Samuel Williams’ work?</p><p class="">It’s also fair to point out that I only test fibers (or threads or processes, for that matter) with a pure-Ruby reactor. All of this assumes that a simple <a href="https://ruby-doc.org/core-2.6.4/IO.html#method-c-select">IO.select</a> is adequate, when you can get better performance using something like <a href="https://github.com/socketry/nio4r">nio4r</a> to use more interesting system calls, and to do more of the work in optimized C.</p><h2>Addendum: Ruby 2.7</h2><p class="">I did a bit of extra benchmarking of (not yet released) Ruby 2.7, with the September 6th head-of-master commit. The short version is that threads and processes are exactly the same speed as 2.6 (makes sense), while fibers have gained a bit more than 6% speed from 2.6.</p><p class="">So there’s more speed coming for fibers!</p><h2>Conclusions</h2><p class="">Clearly, the conclusion is to only use processes in CRuby, ever, and to max out at 10 processes. Thank you for coming to my TED talk.</p><p class="">No, not really.</p><p class="">Some things you’re seeing here:</p><ul data-rte-list="default"><li><p class="">Fibers got faster in Ruby <strong><em>2.6 specifically</em></strong>. If you use them, consider upgrading to Ruby 2.6+.</p></li><li><p class="">Be careful tuning your number of threads and processes. You’ve <a href="http://engineering.appfolio.com/appfolio-engineering/2019/4/11/a-simpler-rails-benchmark-puma-and-concurrency">seen me</a> say that <a href="http://engineering.appfolio.com/appfolio-engineering/2017/3/22/rails-benchmarking-puma-and-multiprocess">before</a>, and it’s still true.</p></li><li><p class="">Threads, oddly, have gained a bit of performance in recent CRuby versions. That’s unexpected and welcome.</p></li></ul><p class="">Thank you and good night.</p>]]></description></item><item><title>Benchmarking Fibers, Threads and Processes</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 13 Sep 2019 17:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/9/13/benchmarking-fibers-threads-and-processes</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5d69fa5a68abb20001ef2847</guid><description><![CDATA[<p class="">Awhile back, I set out to look at <a href="https://ruby-doc.org/core-2.5.0/Fiber.html">Fiber</a> performance and how it's improved in recent Ruby versions. After all, <a href="http://engineering.appfolio.com/appfolio-engineering/2017/12/4/hows-progress-on-ruby-3x3">concurrency is one of the three pillars of Ruby 3x3</a>! Also, there have been some major speedups in Ruby's Fiber class by <a href="https://github.com/ioquatix">Samuel Williams</a>.</p><p class="">It's not hard to write a microbenchmark for something like <a href="https://ruby-doc.org/core-2.5.0/Fiber.html#method-c-yield">Fiber.yield</a>. But it's harder, and more interesting, to write a benchmark that's useful and representative.</p><h2>Wait, Wait, Wait - What?</h2>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567230852223-R1D5NTFFUWKX98UNW479/concurrency_vs_parallelism.png" data-image-dimensions="959x361" data-image-focal-point="0.5,0.5" alt="And don’t get me started on parallelism…" data-load="false" data-image-id="5d6a0b841c83ed0001cfbe80" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567230852223-R1D5NTFFUWKX98UNW479/concurrency_vs_parallelism.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">And don’t get me started on parallelism…</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">Okay, first a quick summary: what are fibers?</p><p class="">You know how you can fork a process or create a thread and suddenly there’s this code that’s also running, alongside your code? I mean, sure, it doesn’t necessarily literally run at the same time. But there’s another flow of control and sometimes it’s running. This is all called <a href="https://en.wikipedia.org/wiki/Concurrency_(computer_science)">concurrency</a> by developers who are picky about vocabulary.</p><p class="">A fiber is like that. However, when you have multiple fibers running, they don’t automatically switch from one to the other. Instead, when one fiber calls <a href="https://ruby-doc.org/core-2.5.0/Fiber.html#method-c-yield">Fiber.yield</a>, Ruby will switch to another fiber. As long as all the fibers call yield regularly, they all get a chance to run and the result is very efficient.</p><p class="">Fibers, like threads, all run inside your process. By comparison, if you call “fork” for a new process then of course it isn’t in the same process. Just as a process can contain multiple threads, a thread can contain multiple fibers. For instance, you could write an application with ten processes, each with eight threads, and each of those threads could have six fibers.</p><p class="">A thread is lighter-weight than a process, and multiple can run inside a process. A fiber is lighter-weight than a thread, and multiple can run inside a thread. And unlike threads or processes, fibers have to manually switch back and forth by calling “yield.” But in return, they get lower memory usage and lower processor overhead than threads in many cases.</p><p class="">Make sense?</p><p class="">We’ll also be talking about the <strong>Global Interpreter Lock</strong>, or <strong>GIL</strong>, which these days is more properly called the Global VM Lock or GVL - but nobody does, so I’m calling it the GIL here. Basically, multiple Ruby threads or fibers inside a single process can <strong>only have one of them running Ruby at once</strong>. That <a href="http://engineering.appfolio.com/appfolio-engineering/2019/4/11/a-simpler-rails-benchmark-puma-and-concurrency">can make a huge difference in performance</a>. We’re not going to go deeply into the GIL here, but you may want to research it further if this topic interests you.</p><h2>Why Not App Servers?</h2>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567231027728-M1OOX1LBCZ2INCZ0V005/image-asset.png" data-image-dimensions="1318x492" data-image-focal-point="0.0,0.5" alt="It’s a nice logo, isn’t it?" data-load="false" data-image-id="5d6a0c334f45fb0001018a8f" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567231027728-M1OOX1LBCZ2INCZ0V005/image-asset.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">It’s a nice logo, isn’t it?</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">Some of you are thinking, "but comparing threads and fibers isn’t hard at all." After all, I do lots of HTTP benchmarking here. Why not just benchmark <a href="https://github.com/puma/puma">Puma</a>, which uses threads, versus <a href="https://github.com/socketry/falcon">Falcon</a>, which uses fibers, and call it a day?</p><p class="">Several reasons.</p><p class="">One: there are a lot of differences between Falcon and Puma. HTTP parsing, handling of multiple processes, how the reactor is written. And in fact, both of them spend a lot of time in non-Ruby code via <a href="https://github.com/socketry/nio4r">nio4r</a>, which lets Ruby use some (very cool, very efficient) C libraries to do the heavy lifting. That's great, and I think it's a wonderful choice... But it's not really benchmarking Ruby, is it?</p><p class="">No, we need something much simpler to look at raw fiber performance.</p><p class="">Also, Ruby 3x3 uses Ruby 2.0 as its baseline. Falcon, nio4r and recent Puma all very reasonably require more recent Ruby than that. Whatever benchmark I use, I want to be able to compare all the way back to Ruby 2.0. Puma 2.11 can do that, but no version of Falcon can.</p><h2>Some Approaches that Didn't Work</h2><p class="">Just interested in the punchline? Skip this section. Curious about the methodology? Keep reading.</p><p class="">I tried putting together a <strong><em>really</em></strong> simple HTTP client and server. The client was initially <a href="https://github.com/wg/wrk">wrk</a> while the server was actually three different servers - one threads, one processes, one fibers. <a href="https://github.com/noahgibbs/fiber_basic_benchmarks/tree/master/http">I got it partly working</a>.</p><p class=""><strong>But it all failed</strong>. Badly.</p><p class="">Specifically, wrk is intentionally picky and finicky. If the server closes the socket on it too soon, it gives an error. Lots of errors. Read errors and write errors both, depending. Just writing an HTTP server with Ruby's TCPSocket is harder than it looks, basically, if I want a picky client to treat it as reasonable. <a href="https://curl.haxx.se/">Curl</a> thinks it's fine. Wrk wants clean benchmark results, and says no.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567231114815-6U1V9X36WRP2L6E8U15Q/stages-of-failure-explained.jpg" data-image-dimensions="1400x1000" data-image-focal-point="0.5,0.5" alt="If I avoid strategy and vision, I can narrow the scope of my failures. That’s the right takeaway, I’m sure of it." data-load="false" data-image-id="5d6a0c89e4f17a00017b9189" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567231114815-6U1V9X36WRP2L6E8U15Q/stages-of-failure-explained.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">If I avoid strategy and vision, I can narrow the scope of my failures. That’s the right takeaway, I’m sure of it.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">Yeah, okay, fine. I guess I <strong><em>do</em></strong> want clean benchmark results. Maybe.</p><p class="">Okay, so then, maybe just a TCP socket server? Raw, fast C client, three different TCPServer-based servers, one threads, one processes, one fibers? It took some doing, but <a href="https://github.com/noahgibbs/fiber_basic_benchmarks/tree/master/socket">I did all that</a>.</p><p class="">That also failed.</p><p class="">Specifically, I got it all working with threads - they're often the easiest. And a 10,000-request run took anything from 3 seconds to 30 seconds. That... seems like a lot. I thought, okay, maybe threads are bad at this, and I tried it with fibers. Same problem.</p><p class="">So I tried it with straight-line non-concurrent code for the server. Same problem. What about a simple select-based <a href="https://en.wikipedia.org/wiki/Reactor_pattern">reactor</a> for the fiber version to see if some concurrency helps? Nope. Same problem.</p><p class="">It turns out that just opening a TCP/IP socket, even on localhost, adds a <strong>huge</strong> amount of variation to the time for the trial. So much variation that it swamps what I'm trying to measure. I <em>could</em> have just run many, many trials to (mostly) average out the noise. But having more measurement noise than signal to measure is a really bad idea.</p><p class="">So: <strong>back to the drawing board</strong>.</p><p class="">No HTTP. No TCP. No big complicated app servers, so I couldn't go <em>more</em> complicated.</p><p class="">What was next?</p><h2>Less Complicated</h2>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567231321410-GOTGODI4BW8OIDGKMYLV/empirical_usage.png" data-image-dimensions="469x168" data-image-focal-point="0.5,0.5" alt="I’m starting to enjoy how tremendously bad the visual explanations of shell pipes are. Maybe that’s a form of Stockholm Syndrome?" data-load="false" data-image-id="5d6a0d594f45fb0001019b6c" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567231321410-GOTGODI4BW8OIDGKMYLV/empirical_usage.png?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">I’m starting to enjoy how tremendously bad the visual explanations of shell pipes are. Maybe that’s a form of Stockholm Syndrome?</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">What's more predictable and less variable than TCP/IP sockets? Local process-to-process sockets with no network protocol in the middle. In Ruby, one easy way to do that is <a href="https://ruby-doc.org/core-2.5.1/IO.html#method-c-pipe">IO.pipe</a>.</p><p class="">You can put together a pretty nice simple master/worker pattern by having the master set up a bunch of workers, each with a shell-like pipe. It's very fast to set up and very fast to use. This is the same way that <a href="https://www.gnu.org/software/bash/">shells like bash</a> sets up pipe operators for "cat myfile | sort | uniq" to run output through several programs before it's done.</p><p class="">So that's what I did. I used threads as workers for the first version. <a href="https://github.com/noahgibbs/fiber_basic_benchmarks/blob/master/pipe/thread_test.rb#L29">The code for that is pretty simple</a>.</p><p class="">Basically:</p><ul data-rte-list="default"><li><p class="">Set up read and write pipes</p></li><li><p class="">Set up threads as workers, ready to read and write</p></li><li><p class="">Start the master/controller code in Ruby’s main process and thread</p></li><li><p class="">Keep running until finished, then clean up</p></li></ul><p class="">There’s some <a href="https://github.com/noahgibbs/fiber_basic_benchmarks/blob/master/pipe/thread_test.rb#L64">brief reactor code for master</a> to make sure it only reads and writes to pipes that are currently ready. But it’s very short, certainly under ten lines of “extra.”</p><p class="">The multiprocess version is barely different - it's so similar that <a href="https://gist.github.com/noahgibbs/caa891aba8ceeabd25ef0fac3752e0c6">there are about fives lines of difference between them</a>.</p><h2>And Now, Fibers</h2><p class="">The fiber version is a little more involved. Let's talk about that.</p><p class="">Threads and processes both have <a href="https://en.wikipedia.org/wiki/Preemption_(computing)#Preemptive_multitasking">pre-emptive multitasking</a>. So if you set one of them running and mostly forget about it, roughly the right thing happens. Your master and your workers will trade off pretty nicely between them. Not everything works perfectly all the time, but things basically tend to work out okay.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567231557907-YPZQ4ZUCVMMX0UFJN7J5/getty_525935645_183646.jpg" data-image-dimensions="970x450" data-image-focal-point="0.5,0.5" alt="In cooperative multitasking, he keeps the goofy grin on his face and switches when he feels like. In preemptive multitasking he can’t spend too long on the cellphone or the hand with the book slaps him." data-load="false" data-image-id="5d6a0e451c83ed0001cfe330" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567231557907-YPZQ4ZUCVMMX0UFJN7J5/getty_525935645_183646.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">In cooperative multitasking, he keeps the goofy grin on his face and switches when he feels like. In preemptive multitasking he can’t spend too long on the cellphone or the hand with the book slaps him.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">Fibers are different. A fiber has to manually yield control when it's done. If a fiber just reads or writes at the wrong time, it can block your whole program until it’s done. That's not <em>as severe</em> a problem with IO.pipe as with TCP/IP. But it's still a good idea to use a pattern called a <a href="https://en.wikipedia.org/wiki/Reactor_pattern">reactor</a> to make sure you're only reading when there's data available and only writing when there's space in the pipe for it.</p><p class="">Samuel Williams has <a href="https://www.codeotaku.com/journal/2018-11/fibers-are-the-right-solution/index">a presentation about Ruby fibers</a> that I used heavily as a source for this post. He includes a simple reactor pattern for fibers there that I'll use to sort my workers out. Like the master in the earlier code, this reactor uses IO.select to figure out when to read and write and how to transfer control between the different fibers. The reactor pattern can be used for threads or processes as well, but Samuel's code is written for fibers.</p><p class="">So initially, I <a href="https://github.com/noahgibbs/fiber_basic_benchmarks/blob/master/pipe/fiber_test.rb#L87">put all the workers into a reactor in one thread, and the master with an IO.select reactor in another thread</a>. That's very similar to how the thread and process code is set up, so it's clearly comparable. But as it turned out, the performance for that version isn't great.</p><p class="">But it seems silly to say it's testing fibers while using threads to switch back and forth... So I wrote a <a href="https://github.com/noahgibbs/fiber_basic_benchmarks/blob/master/pipe/remastered_fiber_test.rb#L88">"remastered" version of the code, with the master code using a fiber per worker.</a> Would this be really slow since I was doubling the number of fibers...? Not so much.</p><p class="">In fact, using just fibers and a single reactor doubled the speed for large numbers of messages.</p><p class="">And with that, I had some nice comparable thread, process and fiber code that's nearly all I/O.</p><h2>How’s It Perform?</h2><p class="">I put it through its paces locally on my Macbook Pro with Ruby 2.6.2. Take this as “vaguely suggestive” performance, in other words, not “heavily vetted” performance. But I think it gives a reasonable start. I’ll be validating on larger Linux EC2 instances before you know it - we’ve met me before.</p><p class="">Here are numbers of workers and requests along with the type of worker, and how long it takes to process that number of requests:</p>


<table>
  <thead>
    <tr><th></th><th>Threads</th><th>Processes</th><th>Fibers w/ old-style Master</th><th>Fibers w/ Fast Master</th></tr>
  </thead>
  <tbody>
    <tr><th>5 workers w/ 20,000 reqs each</th><td>2.6</td><td>0.71</td><td>4.2</td><td>1.9</td></tr>
    <tr><th>10 workers w/ 10,000 reqs each</th><td>2.5</td><td>0.67</td><td>4.0</td><td>1.7</td></tr>
    <tr><th>100 workers w/ 1,000 reqs each</th><td>2.5</td><td>0.76</td><td>3.9</td><td>1.6</td></tr>
    <tr><th>1000 workers w/ 100 reqs each</th><td>2.8</td><td>2.5</td><td>5.0</td><td>2.4</td></tr>
    <tr><th>&nbsp;</th></tr>
    <tr><th>10 workers w/ 100,000 reqs each</th><td>25</td><td>5.8</td><td>41</td><td>16</td></tr>
  </tbody>
</table>

<p class="">Some quick notes: Processes give an <strong><em>amazing</em></strong> showing, partly because they have no GIL. Threads beat out Fibers with a threaded master, so <em>combining</em> threads and fibers too closely seems to be dubious. But with a proper fiber-based master they’re faster than threads, as you’d hope and expect.</p><p class="">You may also notice that processes <strong><em>do not</em></strong> scale gracefully to 1000 workers, while threads and fibers do much better at that. That’s normal and expected, but it’s nice to see the data bear it out.</p><p class="">That final row has 10 times as many total requests as all the other rows. So that’s why its numbers are about ten times higher.</p><h2>A Strong Baseline for Performance</h2>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567231432199-P22HENX35FBDFMVQ8UNG/personal-success-change-your-life-keys-to-success2.jpg" data-image-dimensions="1280x960" data-image-focal-point="0.5,0.5" alt="This guy has just gotten Ruby Fiber code to work. You can tell by the posture." data-load="false" data-image-id="5d6a0dc74f45fb000101a2e0" data-type="image" src="https://images.squarespace-cdn.com/content/v1/562ea223e4b0e0b9dab0b930/1567231432199-P22HENX35FBDFMVQ8UNG/personal-success-change-your-life-keys-to-success2.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">This guy has just gotten Ruby Fiber code to work. You can tell by the posture.</p>
          </figcaption>
        
      
        </figure>
      

    
  


  




<p class="">This article is definitely long enough, so I won't be testing this from Ruby version 2.0 to 2.7... Yet. You can expect it soon, though!</p><p class="">We want to show that fiber performance has improved over time - and we'd like to see if threads or processes have changed much. So we'll test over those Ruby versions.</p><p class="">We also want to compare threads, processes and fibers at different levels of concurrency. This isn't a perfectly fair test. There's no such thing! But it can still teach us something useful.</p><p class="">And we'd also like a baseline to start looking at various "autofiber" proposals - variations on fibers that automatically yield when doing I/O so that you don't need the extra reactor wrapping for reads and writes. That simplifies the code substantially, giving something much more like the thread or process code. There are at least two autofiber proposals, <a href="https://bugs.ruby-lang.org/issues/13618">one by Eric Wong</a> and <a href="https://github.com/ruby/ruby/pull/1870">one by Samuel Williams</a>.</p><p class="">Don't expect all of that for the same blog post, of course. But the background work we just did sets the stage for all of it.</p>]]></description></item><item><title>How Ruby Encodes References - Ruby Tiny Objects Explained</title><dc:creator>Guest User</dc:creator><pubDate>Fri, 30 Aug 2019 17:00:00 +0000</pubDate><link>https://engineering.appfolio.com/appfolio-engineering/2019/6/25/how-ruby-encodes-references-ruby-tiny-objects-explained</link><guid isPermaLink="false">562ea223e4b0e0b9dab0b930:564c01e8e4b09fc032ff180b:5d12263609ef9500018e088f</guid><description><![CDATA[<p class="">When you’re using Ruby and you care about performance, you’ll hear a specific recommendation: “use small, fast objects.” As a variation on this, people will suggest you use symbols (“they’re faster than strings!”), prefer nil to the empty string and a few similar recommendations.</p><p class="">It’s usually passed around as hearsay and black magic, and often the recommendations are somehow wrong. For instance, some folks used to say “don’t use symbols! They can’t be garbage collected!”. But nope, now they can be. And the strings versus symbols story gets a lot more complicated if you use frozen strings…</p><p class="">I’ve <a href="https://bit.ly/kaigi2018-gibbs">explained how Ruby allocates tiny, small and large objects</a> before, but this will be a deep dive into tiny (reference) objects and how they work. That will help you understand the current situation and what’s likely to change in the future.</p><p class="">We’ll also talk a bit about how C stores objects. <a href="https://www.ruby-lang.org/en/downloads/">CRuby</a> (aka Matz’s Ruby or “plain” Ruby) is written in C, and uses C data structures to store your Ruby objects.</p><p class="">And along the way you’ll pick up a common-in-C trick that can both be used in Ruby (Matz does!) and help you understand the deeper binary underpinnings of a lot of higher-level languages.</p><h2>How Ruby Stores Objects</h2><p class="">You may recall that Ruby has three different objects sizes, which I’ll call “tiny,” “small” and “large.” For deeper details on that, <a href="https://bit.ly/kaigi2018-gibbs">the slides from my 2018 RubyKaigi talk are pretty good</a> (or: <a href="https://www.youtube.com/watch?v=Z4nBjXL-ymI">video link</a>.)</p><p class="">But the short version for Ruby on 64-bit architectures (such as any modern processor) is:</p><ul data-rte-list="default"><li><p class="">A Ruby 8-byte “reference” encodes tiny objects directly inside it, or points to…</p></li><li><p class="">A 40-byte RVALUE structure, which can fully contain a small object or the starting 40 bytes of…</p></li><li><p class="">A Large object (anything bigger), which uses an RVALUE <strong><em>and</em></strong> an allocation from the OS.</p></li></ul><p class="">Make sense? Any Ruby value gets a reference, even the smallest ones. Tiny values are encoded directly into the 8-byte reference. Small or large objects (but not tiny) also get a 40-byte RVALUE. Small objects are encoded directly into the 40-bytes RVALUE. And large objects don’t fit in just a reference or just an RVALUE, so they get an extra allocation of whatever size they actually need (plus the RVALUE and the reference.) For the C folks in the audience, that “extra allocation” is the same thing as a call to malloc(), the usual C memory allocation function.</p><p class="">The <a href="http://engineering.appfolio.com/appfolio-engineering/2018/1/2/how-ruby-uses-memory">RVALUE is often called a “Slot”</a> when you’re talking about Ruby memory. Technically Ruby uses the word “slot” for the allocation and “RVALUE” for the data type of the structure that goes <strong><em>in</em></strong> a slot, but you’ll see both words used both ways - treat them as the same thing.</p><p class="">Why the three-level system? Because it gets more expensive in performance as the objects get bigger. 8-byte references are tiny and very cheap. Slots get allocated in blocks of 408 at a time and aren’t that big, so they’re fairly cheap - but a thousand or more of them start to get expensive. And a large object takes a reference and a slot <strong>and</strong> a whole allocation of its own that gets separate tracked - not cheap.</p><p class="">So: let’s look at references. Those are the 8-byte tiny values.</p><h2>Which Values are Tiny?</h2><p class="">I say that “some” objects are encoded into the reference. Which ones?</p><ul data-rte-list="default"><li><p class="">Fixnums between about negative one billion and one billion</p></li><li><p class="">Symbols</p></li><li><p class="">Floating-point numbers (like 3.7 or -421.74)</p></li><li><p class="">The special values true, false, undef and nil</p></li></ul><p class="">That’s a pretty specific set. Why?</p><h2>C: Mindset, Hallucinations and One Weird Trick That Will Shock You</h2><p class="">C really treats all data as a chunk of bits with a length. There are all sorts of operations that <strong>act</strong> on chunks of bits, of course, and some of those operations might be assigned something resembling a “type” by a biased human observer. But C is a big fan of the idea that if you have a chunk of bytes and you want to treat it as a string in one line and an integer the next, that’s fine. Length is the major limitation, and even length is surprisingly flexible if you’re careful and/or you don’t mind the occasional <a href="https://en.wikipedia.org/wiki/Buffer_overflow">buffer overrun</a>.</p><p class="">What’s a pointer? Pointers are how C tracks memory. If you imagine numbering all the bytes of memory starting at zero, and the next byte is one, the next byte two and so on, you get exactly how old processors addressed memory. Some very simple embedded processors still do it that way. That’s exactly what a C pointer is - an index for a location in memory, if you were to treat all of memory as one giant vector of bytes. Memory addressing is more complicated in newer processors, OSes and languages, but they still present your program with that same old abstraction. In C, you use it very directly.</p><p class="">So when I say that in C a pointer is a memory address, you might ask, “is that a separate type from integer with a bunch of separate operations you can do on it?” and I might answer “it’s C, so I just mean there are a bunch of pointer operations that you can do with any piece of data anywhere inside your process.” The theme here is “C doesn’t track your stuff for you at runtime, who do you think C is, your mother?” The other, related theme is “C assumes when you tell it something you know what you’re doing, whether you actually do or not.” And if not, eh, <a href="https://en.wikipedia.org/wiki/Segmentation_fault">crashes happen</a>.</p><p class="">One bit related to this mindset: allocating a new “object” (really a chunk of bytes) in C is simple: you call a function and you get back a pointer to a chunk of bytes, guaranteed to hold at least the size you asked for. Ask it for 137 bytes, get back a pointer to a buffer that is at least 137 bytes big. That’s what “<a href="https://en.wikipedia.org/wiki/C_dynamic_memory_allocation">malloc</a>” does. When you’re done with the buffer you call “free” to give it back, after which it may become invalid or be handed back to somebody else, or split up and <em>parts of it</em> handed back to somebody else. Data made of bits is weird.</p><p class="">A side effect of all of this “made of bits” and “track it yourself” stuff is that often you’ll do <a href="https://en.wikipedia.org/wiki/Tagged_union">type tagging</a>. You keep one piece of data that says what type another piece of data is, and then you interpret the second one completely differently depending on the first one. <strong><em>Wait, what? </em></strong>Okay, so, an example: if you know you could have an integer or a string, you keep a <strong>tag</strong>, which is either 0 for integer or 1 for string. When you read the object, first you check the tag for how to interpret the second chunk of bits. When you set a new value (which could be either integer or string) you also set the tag to the correct value. Does this all sound disorganized and error-prone? Good, you’re understanding a bit of what C is like.</p><p class="">One last oddity: because of how processor alignment and memory tracking works, due to a weird quirk of history, pointers are essentially always even. In fact, values returned by a memory allocator on a modern processor is always a multiple of 8, because most processors don’t like accessing an 8-bytes value on an address that isn’t a multiple of 8. The memory allocator can’t just tell you not to use any 8-byte values. Processors are weird, yo.</p><p class="">Which means if you looked at the representation of your pointer in binary, the smallest three bits would always be zero. Because, y’know, multiple of 8. Which means <em>you could use those three bits</em> for something. Keep that in mind for the next bit.</p><h2>Okay, So What Does Ruby Do?</h2><p class="">If this sounds like I’m building up to explaining some type-tagging… Yup, well spotted!</p><p class="">It turns out that a <strong><em>reference</em></strong> is normally a C pointer under the hood. Basically every dynamic language does this, with different little variations. So all references to <em>small</em> and <em>large</em> Ruby objects are pointers. The exception is for <em>tiny</em> objects, which live completely in the reference.</p><p class="">Think about the last three bits of Ruby’s 8-byte references. You know that <strong><em>if</em></strong> those last bits are all zeroes, the value is (or could be) a pointer to something returned by the memory allocator - so it’s a <em>small</em> or <em>large</em> object. But if they’re <strong><em>not</em></strong> zero, the value lives in the reference and it’s a <em>tiny</em> object.</p><p class="">And Ruby is going to pass around a lot of values that you’d like to be small and fast… Numbers, say, or symbols. Heck, you’d like nil to be pretty small and fast too.</p><p class="">So: CRuby has a few things that it calls “immediate” values in its source code. And the list of those immediate values look exactly like the list above - values you can store as tiny objects directly in a reference.</p><p class="">Let’s get back to those last three bits of the reference again.</p><p class="">If the final bit is a “1” then the reference contains a Fixnum. If the final two bits are “10” then it’s a Float. And if the last <strong><em>four</em></strong> bits are “1100” then it’s a Symbol. But the last three of “1100” are still illegal for an allocated pointer, so it works out.</p><p class="">The four “special” values (true, false, undef, nil) are all represented by small numbers that will <strong>also</strong> never be returned by the memory allocator. For completeness, here they are:</p>


<table>
  <tr><th>Value</th><th>Hexadecimal value</th><th>Decimal value</th></tr>
  <tr><td>true</td><td>0x14</td><td>20</td></tr>
  <tr><td>false</td><td>0x00</td><td>0</td></tr>
  <tr><td>undef</td><td>0x34</td><td>52</td></tr>
  <tr><td>nil</td><td>0x08</td><td>8</td></tr>
</table>

<h2>So Every Integer Ends in 1, Then?</h2><p class="">You might reasonably ask… but what about even integers?</p><p class="">I mean, “ends in 1” is a reasonable way to distinguish between pointers and not-pointers. But what if you want to store the number 4 at some point? Its binary representation ends in “00,” not “1.” The number 88 is even worse - like a pointer, it’s a multiple of 8!</p><p class="">It turns out that CRuby stores your integer in just the top 63 bits out of 64. The final “1” bit isn’t part of the integer’s value, it’s just a sign saying, “yup, this is an integer.” So if type-tagging is two values with one tagging the other, then the bottom bit is the tag and the top 63 bits are the “other” piece of data. They’re both crunched together, but… Well, this is C. If you want to crunch up “multiple” pieces of data into one chunk… C isn’t your mother, and it won’t stop you. In fact, that’s what C does with all its arrays anyway. And in this case it makes for pretty fast code, so that’s what CRuby does.</p><p class="">If you’re up for it, here’s the C code for immediate Fixnums - all this code makes heavy use of <a href="https://en.wikipedia.org/wiki/Bitwise_operation">bitwise operations</a>, as you’d expect.</p>


<pre>
// Check if a reference is an immediate Fixnum
#define RB_FIXNUM_P(f) (((int)(SIGNED_VALUE)(f))&amp;RUBY_FIXNUM_FLAG)

// Convert a C int into a Ruby immediate Fixnum reference
#define RB_INT2FIX(i) (((VALUE)(i))&lt;&lt;1 | RUBY_FIXNUM_FLAG)

// Convert a Ruby immediate Fixnum into a C int - RSHIFT is just &gt;&gt;
#define RB_FIX2LONG(x) ((long)RSHIFT((SIGNED_VALUE)(x),1))
</pre>

<h2>So It’s All That Simple, Then?</h2><p class="">This article can’t cover everything. If you think about symbols for a moment, you’ll realize they have to be a bit more complicated than that - what about a symbol like :thisIsAParticularlyLongName? You can’t fit that in 8 bytes! And yet it’s still an immediate value. Spoiler: Ruby keeps a table that maps the symbol names to fixed-length keys. This is another very old trick, often called <a href="https://en.wikipedia.org/wiki/String_interning">String Interning</a>.</p><p class="">And as for what it does to the Float representation… I’ll get into a lot more detail about that, and about what it does to Ruby’s floating-point performance, in a later post.</p><p data-rte-preserve-empty="true" class=""></p>]]></description></item></channel></rss>