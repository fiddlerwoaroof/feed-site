<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Ruby in Source Diving on Medium]]></title>
        <description><![CDATA[Latest stories tagged with Ruby in Source Diving on Medium]]></description>
        <link>https://sourcediving.com/tagged/ruby?source=rss----217db81c7a01--ruby</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Ruby in Source Diving on Medium</title>
            <link>https://sourcediving.com/tagged/ruby?source=rss----217db81c7a01--ruby</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 17 Nov 2022 13:02:10 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/sourcediving/tagged/ruby" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <item>
            <title><![CDATA[How we improved our Rails app’s performance with Conditional Get Requests]]></title>
            <link>https://sourcediving.com/how-we-improved-our-rails-apps-performance-with-conditional-get-requests-35a7a472a0b9?source=rss----217db81c7a01--ruby</link>
            <guid isPermaLink="false">https://medium.com/p/35a7a472a0b9</guid>
            <category><![CDATA[ruby]]></category>
            <category><![CDATA[ruby-on-rails]]></category>
            <category><![CDATA[caching]]></category>
            <category><![CDATA[web-performance]]></category>
            <category><![CDATA[rails-performance]]></category>
            <dc:creator><![CDATA[Gavin Morrice]]></dc:creator>
            <pubDate>Tue, 31 May 2022 11:17:21 GMT</pubDate>
            <atom:updated>2022-06-01T14:52:40.422Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZqmemPKq8b1PJByRfMnaGg.jpeg" /><figcaption>Photo by <a href="https://unsplash.com/@veri_ivanova?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Veri Ivanova</a> on <a href="https://unsplash.com/collections/yVu6sldRUVI/stopwatch?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></figcaption></figure><p>HTTP provides a method of client-side caching known as <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Conditional_requests">Conditional Get Requests</a>. This style of caching allows a client to cache the content of a response locally (in your browser cache or mobile device). When the client makes a subsequent request to your application for the same resource, it includes a token or timestamp from their previous request. Based on this token or timestamp, if your application determines that the response body would be the same as the last request, the server may respond with a short, head-only, 304 Not Modified response. This instructs the requesting client to fetch the response body from its local cache store instead.</p><p>This method of caching can reduce latency and server burden, since it saves our application from having to fetch many records from our database and from having to render view templates when a resource hasn’t changed. For resources that tend not to change too often, or are requested multiple times by the same client without changing, <em>Conditional Get Requests</em> can be a simple and powerful tool for improving performance.</p><p>In May 2022, we conducted an experiment to determine if we could use this method of caching at <a href="https://cookpad.com/">Cookpad</a> to improve response times on selected API endpoints. This post documents how we used this feature to shave up-to 150ms off our response times.</p><h4>Beyond default behaviour</h4><p>Modern web frameworks and networking libraries are quite full-featured. Our mobile clients (<a href="https://play.google.com/store/apps/details?id=com.mufumbo.android.recipe.search&amp;hl=en_GB&amp;gl=US">Android</a> and <a href="https://apps.apple.com/us/app/cookpad-find-share-recipes/id585332633">iOS</a>) were already sending the required headers for <em>Conditional Get Requests</em> with every GET request to our backend, but our API—a Ruby on Rails app—was effectively ignoring these and rendering responses afresh with each request.</p><p>Rails does, of course, support <em>Conditional Get Requests</em> straight out of the box, and our application would respond with a 304 Not Modified if it recognised a resource hadn&#39;t changed since the previous request. But our default implementation was still doing the work of fetching the same records from the database and rendering the same view template, regardless of the <em>Conditional Get</em> headers, so the performance benefit was minimal. To reap the benefits of client-side caching required some thoughtful configuration and refactoring.</p><h4>Choosing our test subject</h4><p>To find good candidates for client-side caching, we used <a href="https://www.charlesproxy.com">Charles Proxy</a> to monitor our own personal use of the mobile application. Certain endpoints stood out as being likely to be visited by a browsing user more than once in a short period of time, without the response content having changed. This made them a good candidate for client-side caching.</p><p>We nominated the /user/:user_id/recipes endpoint as a test subject. This is an endpoint in our API that presents all of the recipes belonging to a given user. Next, we discussed <em>which method</em> we would use to determine if a resource was changed or not.</p><p>HTTP supports more than one way of validating whether a resource has changed or not (called <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Conditional_requests#validators">validators</a>), and more than one validation method can be used in the same request. In this particular case, this recipes endpoint can present different data based on query parameters (such as page and per_page), so we decided that <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag">ETags</a> were a more accurate way to validate if a resource had changed than a <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Last-Modified">last modified</a> timestamp.</p><p>Rails sends an ETag header with each response by default, which represents a hash digest (like a fingerprint) of the response body that is essentially unique. As mentioned before, our mobile clients were already sending this ETag back to the server in subsequent requests (as a header named HTTP-IF-NONE-MATCH). All that was required then, was that we update our controllers to only load resources and render views <em>if</em> the hash digest was different from the one expected.</p><h4>Updating our controllers</h4><p>To add this behaviour, we used two Rails methods: <a href="https://api.rubyonrails.org/classes/ActionController/ConditionalGet.html#method-i-stale-3F">#stale?</a> and <a href="https://api.rubyonrails.org/classes/ActionController/ConditionalGet/ClassMethods.html#method-i-etag">#etag</a>.</p><h4>#stale?</h4><p>The #stale? method provided by Rails will compare the hash digest of a given value or object with the value provided in the HTTP-IF-NONE-MATCH request header. If the two values are the same, #stale? will return false and the action can respond with a 304 Not Modified without having to load and render additional data. If the two values differ, #stale? returns true and the page should be loaded and rendered in full.</p><p>Here’s an example to help you visualise how this might look:</p><pre>def index<br>  @user = User.find(params[:user_id])<br>  if stale?(etag: @user.updated_at)<br>    @recipes = @user.recipes<br>  else<br>    # head 304<br>  end<br>end</pre><p>In this example, the ETag validator includes the current user’s updated_at timestamp. <a href="https://github.com/rails/rails/blob/cb52350f3829803b7408c230268224ef8b92daef/actionpack/lib/action_dispatch/http/cache.rb#L134-L136">Behind the scenes</a>, Rails will add this (and potentially other) values to an array and then hash all of those values to generate a unique hexadecimal string-the ETag. If that string matches the value in the HTTP-IF-NONE-MATCH header, #stale? will return false and the controller will respond 304 Not Modified.</p><h4>#etag</h4><p>Calculating whether a response matches an ETag or not is not always as straightforward as checking just a simple timestamp. Some endpoints include query parameters (such as pagination), and some endpoints render a resource differently depending on which user is making this request.</p><p>To factor these variable values into the entire digest for a response ETag, Rails provides the etag method. Simply call this method in the controller, with a block, to add any additional validators to the array of values for the final ETag digest.</p><p>For example:</p><pre>etag { params[:page] }<br><br>etag { Current.user.id }<br><br>def index<br>  # ...<br>end</pre><p>To make our lives easier, we added an additional class method to our controllers .include_params_in_etags to simplify adding parameters and their values to the validators array.</p><pre>module ETagParamKeys<br>  extend ActiveSupport::Concern<br><br>  module ClassMethods<br>    private<br><br>    def include_params_in_etags(*param_names)<br>      Set.new(param_names).each { |p| include_param_in_etags(p) }<br>    end<br><br>    def include_param_in_etags(param_name)<br>      etag { params.key?(param_name) &amp;&amp; &quot;#{param_name}-#{params[param_name]}&quot; }<br>    end<br>  end<br>end</pre><h4>A complete example</h4><p>Here’s a complete example that connects the three concepts described above:</p><pre># The value of these parameters will be included as part of<br># the array used to create the ETag digest<br>include_params_in_etags :page, :per_page<br><br># The current User&#39;s ID will be added to the ETag, making it unique<br># per authenticated User<br>etag { Current.user&amp;.id }<br><br>def show<br>  if stale?(etag: etag_value)<br>    # Slow query we&#39;d rather not make if we can avoid it<br>    @recipes = user.recipes.published.page(params[:page]).per(params[:per_page])<br>  else<br>    # head 304<br>  end<br>end<br><br>private<br><br>  def etag_value<br>    # This value is store in an in-memory cache and is super quick!<br>    @_etag_value ||= user.latest_recipe_updated_at<br>  end<br><br>  def user<br>    @_user ||= User.find(params[:user_id])<br>  end</pre><p>Our implementation in this experiment was essentially the same as described above, but with some additional lines of code for collecting performance metrics and other such things.</p><h4>To invalidate or not to invalidate?</h4><blockquote><em>There are only two hard things in Computer Science: cache invalidation and naming things.</em></blockquote><blockquote><em>— Phil Karlton</em></blockquote><p>Implementing these changes in the code was fairly straightforward. The real difficulty came in determining the most effective yet reliable way of invalidating our cache.</p><p>Our initial PR was blocked, because other engineers started to spot potential cases where the response data might have changed and our ETag value wouldn’t detect it. This led to quite a long and detailed discussion in which we tried to identify as many potential cases as possible. In the end, we opted for a cached timestamp that would be invalidated every time a user or any of their recipes changed. This additional method call added an extra few milliseconds to the response time, but not enough to outweigh the overall benefit.</p><h4>Results</h4><p>We used Prometheus to record the response times for each request to this endpoint, and also to track whether the ETag was stale or not (a cache “miss” or a cache “hit”).</p><p>The results of this experiment are shown in the charts below:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jxfyDeJVftmzdnQHOMbwhg.png" /><figcaption>Chart showing the “hit”, “miss”, and average response times (ms) for this endpoint</figcaption></figure><p>In this first chart, the Y-axis shows the response time (ms), and the X-axis is the time. The three lines indicate cache hits, cache misses, and the average response time for the endpoint (both hits and misses).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JoM-3nTYbx-_rq1a4o7VlQ.png" /><figcaption>Chart showing the cache hit rate (%) for this endpoint</figcaption></figure><p>In this second chart, the Y-axis shows the % cache hits, and the X-axis is the time.</p><p>As you can see from these results, cache <em>miss</em> responses tended to range between 220ms and 310ms. Whereas cache <em>hits</em> were consistently around 95ms. The cache hit rate was around 25%, but we saw this growing steadily over time. This had the effect of bringing the <strong>average response time down between 30ms and 70ms</strong>, roughly 10–20%, and that improvement has increased with the cache hit rate.</p><p>Adding <em>Conditional Get Requests</em> improved response time <em>dramatically</em> for about 25% of requests, and improved the average response time for all requests to this endpiont. Not bad for a few hours of work!</p><h4>Caveats</h4><p>Implementing Conditional Get Requests in Rails is surprisingly simple.</p><p><em>However</em>, it is important to ensure that the logic for determining a stale vs fresh cache is sound, and covers all scenarios. It is very easy to miss certain edge cases or certain combinations of variables, and doing so could mean serving stale, out of date content to a user.</p><p>We ended up writing several tests to thoroughly cover the cases we thought of, and to provide a layer of insurance that future changes wouldn’t break the cache behaviour.</p><h4>Cache lifespan</h4><p>Cached responses may live on a device for a long time. This means that improperly configured ETag values could result in users experiencing bugs or viewing out-of-date information for several days or weeks. Since these errors take place on the client-side, and not server-side, it is very difficult to know if problems arise unless our users start to complain!</p><p>This issue can be mitigated by customising the :cache_control option in the #stale? method to set a shorter cache expiry time, but that is outside of the scope of this article.</p><h4>Proxies and privacy</h4><p>It’s possible that, depending on the cache-control headers you set, caching a response body may also happen in one of the proxies along a network. You should ensure you understand the cache-control headers, and how these are configured for your application. Make sure this also aligns with your company’s privacy policy and other legal requirements too; if in doubt, err on the side of caution.</p><h4>YMMV</h4><p>The above described experiment demonstrates an average improvement of 10%-20% in response times for this endpoint. However, the impact of conditional get requests is likely to vary significantly for each endpoint. <em>Your Mileage May Vary</em>. I would encourage teams to A/B test the effectiveness of their implementation as part of their workflow, and compare the results with the default implementation, to prove that the overall benefit justifies developing and managing this feature.</p><h4>Conclusion</h4><p><em>Conditional Get Requests</em> offer an easy and potentially powerful way to improve the response times for a given endpoint. In our experiment, we were impressed by how high the cache hit rate was, and how much faster cached responses were. However, it is essential that you ensure the conditions you have identified for cache validity are realistic and comprehensive.</p><p>Teams are advised to measure and benchmark their implementation to gain a deeper understanding of the impact (if any) their changes have made.</p><p>Remember to share your findings with others 🤓</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=35a7a472a0b9" width="1" height="1" alt=""><hr><p><a href="https://sourcediving.com/how-we-improved-our-rails-apps-performance-with-conditional-get-requests-35a7a472a0b9">How we improved our Rails app’s performance with Conditional Get Requests</a> was originally published in <a href="https://sourcediving.com">Source Diving</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Debugging Deadlocks (in Ruby on Rails)]]></title>
            <link>https://sourcediving.com/debugging-deadlocks-in-ruby-on-rails-c16bd1a91721?source=rss----217db81c7a01--ruby</link>
            <guid isPermaLink="false">https://medium.com/p/c16bd1a91721</guid>
            <category><![CDATA[ruby]]></category>
            <category><![CDATA[sql]]></category>
            <category><![CDATA[ruby-on-rails]]></category>
            <dc:creator><![CDATA[Ollie Haydon-Mulligan]]></dc:creator>
            <pubDate>Thu, 24 Mar 2022 10:40:06 GMT</pubDate>
            <atom:updated>2022-03-24T10:40:06.540Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*g1l4MIZiu4jaKq83uOJqUQ.jpeg" /></figure><p>Until recently, I hadn’t dug into the causes and implications of database deadlocks. Then one day, an endpoint I’d been working on suffered several thousand ActiveRecord::Deadlocked errors in a few hours 😬.</p><p>In this post, I’ll share some things I learnt about how deadlocks happen, how to avoid them, and why they are a problem. (Scroll to the bottom for those takeaways).</p><p>But in the main I’m going to focus on <em>how</em> I went about investigating them. Here’s the TL;DR of techniques that might help you debug a deadlock:</p><ul><li>get a snapshot of what locks have been granted and denied to each running transaction with ActiveRecord::Base.connection.execute(&quot;SHOW ENGINE INNODB STATUS&quot;).each{|row| puts row} and innodb_status_output_locks enabled</li><li>replicate overlapping transactions (in specs) to see how they impact each other. Turn off use_transactional_tests and start separate transactions in separate Threads</li><li>after a deadlock has happened, check the “LAST DEADLOCK” section from the “SHOW ENGINE INNODB STATUS” output to get a detailed report on the cause</li></ul><h4>Background</h4><p>It’ll help to have an idea of the endpoint I’d been working on.</p><p>It was responsible for creating or updating a Device record, by</p><p>a) setting some attributes on the Device itself (i.e on a row in the devices table) including token and platform, and</p><p>b) setting a list of push notification &#39;subscriptions&#39; for the Device; each &#39;subscription&#39; is a record stored in a separate push_notification_subscriptions table with a device_id referring to the Device and a public_alias which identifies a push notification type</p><p>The endpoint is called by apps when they’ve fetched a token from Firebase. The up-to-date Device record - with the Firebase token set - can later be used to send push notifications (via Firebase) to that device 🎉.</p><p>The change I had made was to insert all of the PushNotificationSubscription records for the Device in a single database insert, after a single delete operation to clear the out-of-date subscriptions. This cut the number of inserts significantly, and sped up the endpoint a lot.</p><p>But it also prompted <em>a lot</em> of deadlocks.</p><p>These were raised during the insert operation (for inserting all the PushNotificationSubscriptions), and we realised a lot of the time this insert was being carried out unnecessarily. So we added a check that massively cut the number of inserts, thereby cutting the number of deadlocks.</p><p>But still we were getting a steady-ish stream of deadlocks… and wanted to understand why, and what the implications were.</p><h4>The Investigation 🕵️</h4><h4>1. Getting better visibility</h4><p>We already suspected part of the cause of the deadlocks was rapid requests coming from the same device to the same endpoint. We confirmed (by adding some logging) that the apps <em>do</em> send requests less than 100ms apart from the same user. But how might these overlapping requests cause deadlocks?</p><p>I <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html">read up on locking in MySQL</a> and some Stack Overflow advice about avoiding deadlocks, but I was struggling to apply the information to this particular problem. Reasoning about what locks <em>should</em> or <em>might</em> be requested and denied felt way too speculative. I needed to get some reliable and detailed information about what locks were <em>actually</em> being requested.</p><p>So I switched from looking into <em>how locks work</em> to finding out how to get <strong>visibility</strong> on what locks are being requested/granted/released/blocked.</p><p>The key tool that really helped lift the veil of mystery is <a href="https://dev.mysql.com/doc/refman/8.0/en/show-engine.html">SHOW ENGINE INNODB STATUS</a>. Execute it in MySQL and it gives a detailed snapshot about what’s going on in the database. Turn on the innodb_status_output_locks option and that snapshot tells you exactly which locks each transaction currently holds and is waiting for.</p><p>To log out the report at a given point in a Rails app, run:</p><pre>ActiveRecord::Base.connection.execute(&quot;SHOW ENGINE INNODB STATUS&quot;).each{|row| puts row}</pre><p>To turn on innodb_status_output_locks, start MySQL with --innodb_status_output_locks. If you run MySQL using docker-compose, add --innodb_status_output_locks to the service command. Or you can turn the option <a href="https://dev.mysql.com/doc/refman/5.6/en/dynamic-system-variables.html">on at runtime</a>, after MySQL has already started.</p><h4>2. Replicating the scenario</h4><p>I wanted to replicate the “rapid requests” scenario locally so I could dig into what locks were being requested and try to understand how they could result in deadlocks.</p><p>After a bit of trial and error, I had the following:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bb12b709710698fc3407ab072ef03bae/href">https://medium.com/media/bb12b709710698fc3407ab072ef03bae/href</a></iframe><p>This triggers two transactions in quick succession, each of which attempts to</p><ul><li>update the existing device with a new token</li><li>delete the device’s existing push notification subscriptions</li><li>insert new push notification subscriptions for the device</li></ul><h4>3. Exploring “SHOW ENGINE INNODB STATUS”</h4><p>I then executed “SHOW ENGINE INNODB STATUS” at various points in the DeviceCreationService.</p><p>The key section in the output is the list of TRANSACTIONS. When I ran it after the second transaction had begun, that section included:</p><pre>---TRANSACTION 1133883, ACTIVE 0 sec starting index read<br>mysql tables in use 1, locked 1<br>LOCK WAIT 2 lock struct(s), heap size 1136, 1 row lock(s)<br>MySQL thread id 212, OS thread handle 140640384386816, query id 8100 172.19.0.1 root updating<br>UPDATE `devices` SET `devices`.`token` = &#39;new token&#39;, `devices`.`updated_at` = &#39;2022-01-24 11:25:15.671074&#39; WHERE `devices`.`id` = 54<br>Trx read view will not see trx with id &gt;= 1133883, sees &lt; 1133882<br>------- TRX HAS BEEN WAITING 0 SEC FOR THIS LOCK TO BE GRANTED:<br>RECORD LOCKS space id 34394 page no 3 n bits 72 index PRIMARY of table `global_test`.`devices` trx id 1133883 lock_mode X locks rec but not gap waiting<br>Record lock, heap no 2 PHYSICAL RECORD: n_fields 12; compact format; info bits 0</pre><p>(Reading these reports takes a bit of getting used to 😬 😅, and it definitely helped studying the MySQL docs <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html">here</a> and <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-locks-set.html">here</a> first…)</p><p>Note that transaction 1133883 is waiting for a RECORD LOCK for a PRIMARY INDEX on table global_test.devices. This means the transaction is waiting for a lock on a specific row in the devices table.</p><p>Further down we see this:</p><pre>---TRANSACTION 1133882, ACTIVE 0 sec<br>4 lock struct(s), heap size 1136, 5 row lock(s), undo log entries 4<br>MySQL thread id 211, OS thread handle 140640384657152, query id 8101 172.19.0.1 root starting<br>SHOW ENGINE INNODB STATUS<br>Trx read view will not see trx with id &gt;= 1133882, sees &lt; 1133882<br>TABLE LOCK table `global_test`.`devices` trx id 1133882 lock mode IX<br>RECORD LOCKS space id 34394 page no 3 n bits 72 index PRIMARY of table `global_test`.`devices` trx id 1133882 lock_mode X locks rec but not gap<br>Record lock, heap no 2 PHYSICAL RECORD: n_fields 12; compact format; info bits 0</pre><p>Transaction 1133882 <em>already has</em> a RECORD LOCK for a PRIMARY INDEX on table global_test.devices.</p><p>In fact, 1133882 has exactly the lock needed by the other transaction, which makes sense. Both transactions want to update the same device record. The first transaction gets a lock on the record so it can carry on and carry out the update operation as well as the other changes in the push_notification_subscriptions table. The second transaction has to <em>wait</em> for the first transaction to be committed, at which point the lock is released and the second transaction can carry on.</p><p>So here we see the second transaction <em>waiting</em> for a lock, but there is no deadlock. The first transaction can carry on, commit, and release the lock the second transaction needs.</p><p>I was getting some understanding of how to read the “SHOW ENGINE INNODB STATUS” report and what locks were needed as part of DeviceCreationService. But I didn&#39;t understand how the rapid requests could result in a deadlock. If both transactions want to update the same device record, there is no chance of a deadlock - the second simply waits for the first transaction to complete.</p><h4>4. Testing a Theory</h4><p>I therefore wondered if the deadlock might happen when neither request has to update the device record itself, so a lock on the device record isn’t needed.</p><p>Prompted by this advice <a href="https://stackoverflow.com/a/68730444">in this Stack Overflow answer</a></p><blockquote>keep the order in which you delete items consistent</blockquote><p>I came up with a theory that might account for deadlocks when one of the transactions called DELETE WHERE push_notification_subscriptions.device_id = X.</p><p>I was imagining this sequence of events:</p><pre>TRANSACTION 1: `DELETE WHERE push_notification_subscriptions.device_id = X`. Transaction holds lock on first record it wants to delete (record 1, say).<br><br>TRANSACTION 2: `DELETE WHERE push_notification_subscriptions.device_id = X`. Transaction holds lock on first record it wants to delete (record 2, say).<br><br>TRANSACTION 1: Attempts to get lock on the next record it wants to delete, which happens to be record 2. Must wait for Transaction 2 to finish in order for it to release its lock on record 2.<br><br>TRANSACTION 2: Attempts to get lock on the next record it wants to delete, which happens to be record 1. Must wait for Transaction 1 to finish in order for it to release its lock on record 1.</pre><p>Deadlock!</p><p>This scenario relied upon different transactions processing the same records in a different order when they each carry out the same DELETE WHERE push_notification_subscriptions.device_id = X operation.</p><p>I updated my spec so neither transaction needed the initial lock on the device record:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/69b920d724d160eed104322e1598f71f/href">https://medium.com/media/69b920d724d160eed104322e1598f71f/href</a></iframe><p>But no deadlock happened.</p><p>I tried to force the DELETE WHERE push_notification_subscriptions.device_id = X to delete the rows in different orders by changing it to device.push_notification_subscriptions.order(“rand()”).delete_all. No deadlocks. I experimented with different fixed orders, but whatever I did, the output of “SHOW ENGINE INNODB STATUS” always showed that the same push_notification_subscriptions row was being locked first. The second transaction <em>always</em> had to wait for the cooksnap_reminder record to be unlocked:</p><pre>---TRANSACTION 1133922, ACTIVE 0 sec starting index read<br>mysql tables in use 1, locked 1<br>LOCK WAIT 2 lock struct(s), heap size 1136, 1 row lock(s)<br>MySQL thread id 216, OS thread handle 140640384657152, query id 8257 172.19.0.1 root updating<br>DELETE FROM `push_notification_subscriptions` WHERE `push_notification_subscriptions`.`device_id` = 55<br>Trx read view will not see trx with id &gt;= 1133922, sees &lt; 1133921<br>------- TRX HAS BEEN WAITING 0 SEC FOR THIS LOCK TO BE GRANTED:<br>RECORD LOCKS space id 34420 page no 3 n bits 72 index PRIMARY of table `global_test`.`push_notification_subscriptions` trx id 1133922 lock_mode X waiting<br>Record lock, heap no 2 PHYSICAL RECORD: n_fields 7; compact format; info bits 32<br> 0: len 8; hex 8000000000000132; asc        2;;<br> 1: len 6; hex 000000114d61; asc     Ma;;<br> 2: len 7; hex 3b000001e72c7f; asc ;    , ;;<br> 3: len 8; hex 8000000000000037; asc        7;;<br> 4: len 17; hex 636f6f6b736e61705f72656d696e646572; asc cooksnap_reminder;;<br> 5: len 8; hex 99abf0bdd9061322; asc        &quot;;;<br> 6: len 8; hex 99abf0bdd9061322; asc        &quot;;;</pre><p>In general, if two concurrent transactions need locks on the same records and they process those records in <em>different</em> orders, there will be a deadlock. But here it seemed that whatever order was given to the DELETE operation, each transaction first needed a lock on the same record. That means the second transaction has to wait for the first to commit and release the lock - but there isn’t a deadlock, because the first transaction can carry on and isn’t blocked by a lock already held by the second transaction.</p><h4>5. Testing Another Theory</h4><p>At this point, it occurred to me that a deadlock doesn’t only happen when transactions both need a lock on records in the same table. It can happen when two transactions need locks on records in different tables in different orders.</p><p>That got me wondering whether our two rapid requests might each start by locking records in <em>different</em> tables. And then I thought: what if the first request doesn’t need to update the device record but the second request <em>does</em>? What if the two requests aren’t duplicates?</p><p>I now had another theory to test.</p><pre>TRANSACTION 1: doesn’t need to update the device because the existing device record is up-to-date with the supplied params. Goes straight to locking a subscription record for deletion.<br><br>TRANSACTION 2: does need to update the device because the supplied params are different from the existing attributes. Starts by locking the device row.<br><br>TRANSACTION 1: for some reason, now needs to get a lock on the device row. Cannot get the lock because it is held by TRANSACTION 2.<br><br>TRANSACTION 2: now wants to start deleting subscription records so needs a lock on the first subscription record. Cannot get the lock because it is held by TRANSACTION 1.</pre><p>At this point, neither transaction can progress. They each need the other to finish in order to get the lock they are waiting for. It’s a deadlock!</p><p>I recalled — having just read through some old discussions about the same endpoint — that the rapid requests sent by the apps <em>don’t</em> necessarily contain the same params.</p><blockquote>The app sends the /me/devices request on launch, then it seems that about 150–180ms after the FCM library explicitly tells us didReceiveRegistrationToken and we POST it again.</blockquote><p>And, crucially, the firebase token can be different in the second request.</p><p>So a realistic scenario (simulated in the spec below) is that the first request includes the firebase token already set on the existing device record, then a follow-up request is sent to update that firebase token.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f133a1b25ddba840980216a8286e441e/href">https://medium.com/media/f133a1b25ddba840980216a8286e441e/href</a></iframe><p>And every time I ran this spec, I got a deadlock.</p><p>Now “SHOW ENGINE INNODB STATUS” was even more useful. If there has been a deadlock recently, it includes a section called “LAST DEADLOCK”. So I put a “SHOW INNODB STATUS” after the ensure, and it returned:</p><pre>------------------------<br>LATEST DETECTED DEADLOCK<br>------------------------<br>2022-01-24 14:10:24 0x7fe9642b2700<br>*** (1) TRANSACTION:<br>TRANSACTION 1134002, ACTIVE 1 sec starting index read<br>mysql tables in use 1, locked 1<br>LOCK WAIT 4 lock struct(s), heap size 1136, 2 row lock(s), undo log entries 1<br>MySQL thread id 224, OS thread handle 140640384116480, query id 8565 172.19.0.1 root updating<br>DELETE FROM `push_notification_subscriptions` WHERE `push_notification_subscriptions`.`device_id` = 57<br>*** (1) WAITING FOR THIS LOCK TO BE GRANTED:<br>RECORD LOCKS space id 34420 page no 3 n bits 72 index PRIMARY of table `global_test`.`push_notification_subscriptions` trx id 1134002 lock_mode X waiting<br>Record lock, heap no 2 PHYSICAL RECORD: n_fields 7; compact format; info bits 32<br> 0: len 8; hex 800000000000013b; asc        ;;;<br> 1: len 6; hex 000000114db1; asc     M ;;<br> 2: len 7; hex 670000054921dd; asc g   I! ;;<br> 3: len 8; hex 8000000000000039; asc        9;;<br> 4: len 17; hex 636f6f6b736e61705f72656d696e646572; asc cooksnap_reminder;;<br> 5: len 8; hex 99abf0e29709c5fd; asc         ;;<br> 6: len 8; hex 99abf0e29709c5fd; asc         ;;<br><br>*** (2) TRANSACTION:<br>TRANSACTION 1134001, ACTIVE 1 sec inserting<br>mysql tables in use 1, locked 1<br>5 lock struct(s), heap size 1136, 6 row lock(s), undo log entries 4<br>MySQL thread id 223, OS thread handle 140640384657152, query id 8566 172.19.0.1 root update<br>INSERT INTO `push_notification_subscriptions` (`public_alias`,`created_at`,`updated_at`,`device_id`) VALUES (&#39;comment&#39;, &#39;2022-01-24 14:10:23.556665&#39;, &#39;2022-01-24 14:10:23.556665&#39;, 57) ON DUPLICATE KEY UPDATE `public_alias`=`public_alias`<br>*** (2) HOLDS THE LOCK(S):<br>RECORD LOCKS space id 34420 page no 3 n bits 72 index PRIMARY of table `global_test`.`push_notification_subscriptions` trx id 1134001 lock_mode X<br>Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0<br> 0: len 8; hex 73757072656d756d; asc supremum;;<br><br>Record lock, heap no 2 PHYSICAL RECORD: n_fields 7; compact format; info bits 32<br> 0: len 8; hex 800000000000013b; asc        ;;;<br> 1: len 6; hex 000000114db1; asc     M ;;<br> 2: len 7; hex 670000054921dd; asc g   I! ;;<br> 3: len 8; hex 8000000000000039; asc        9;;<br> 4: len 17; hex 636f6f6b736e61705f72656d696e646572; asc cooksnap_reminder;;<br> 5: len 8; hex 99abf0e29709c5fd; asc         ;;<br> 6: len 8; hex 99abf0e29709c5fd; asc         ;;<br><br>Record lock, heap no 3 PHYSICAL RECORD: n_fields 7; compact format; info bits 32<br> 0: len 8; hex 800000000000013c; asc        &lt;;;<br> 1: len 6; hex 000000114db1; asc     M ;;<br> 2: len 7; hex 67000005492213; asc g   I&quot; ;;<br> 3: len 8; hex 8000000000000039; asc        9;;<br> 4: len 20; hex 6d656e74696f6e65645f696e5f636f6d6d656e74; asc mentioned_in_comment;;<br> 5: len 8; hex 99abf0e2970a95ef; asc         ;;<br> 6: len 8; hex 99abf0e2970a95ef; asc         ;;<br><br>Record lock, heap no 4 PHYSICAL RECORD: n_fields 7; compact format; info bits 32<br> 0: len 8; hex 800000000000013d; asc        =;;<br> 1: len 6; hex 000000114db1; asc     M ;;<br> 2: len 7; hex 67000005492249; asc g   I&quot;I;;<br> 3: len 8; hex 8000000000000039; asc        9;;<br> 4: len 18; hex 6d6f6465726174696f6e5f6d657373616765; asc moderation_message;;<br> 5: len 8; hex 99abf0e2970b60c5; asc       ` ;;<br> 6: len 8; hex 99abf0e2970b60c5; asc       ` ;;<br><br>*** (2) WAITING FOR THIS LOCK TO BE GRANTED:<br>RECORD LOCKS space id 34394 page no 3 n bits 72 index PRIMARY of table `global_test`.`devices` trx id 1134001 lock mode S locks rec but not gap waiting<br>Record lock, heap no 2 PHYSICAL RECORD: n_fields 12; compact format; info bits 0<br> 0: len 8; hex 8000000000000039; asc        9;;<br> 1: len 6; hex 000000114db2; asc     M ;;<br> 2: len 7; hex 680000021414f6; asc h      ;;<br> 3: len 4; hex 80000039; asc    9;;<br> 4: len 15; hex 646966666572656e7420746f6b656e; asc different token;;<br> 5: len 3; hex 696f73; asc ios;;<br> 6: len 8; hex 99abf0e297087e79; asc       ~y;;<br> 7: len 8; hex 99abf0e2970d59e7; asc       Y ;;<br> 8: SQL NULL;<br> 9: SQL NULL;<br> 10: SQL NULL;<br> 11: SQL NULL;<br><br>*** WE ROLL BACK TRANSACTION (1)</pre><p>This tells the story I predicted, but, confusingly, the first transaction (the one that began first) is called (2) in the report.</p><p>That transaction <em>holds</em> a lock on the cooksnap_reminder subscription record, because it went straight ahead to destroy the subscription records. It is waiting for a lock on the device record.</p><p>The other transaction (the second one to begin) holds a lock on the device record, because it started by updating that record. It is waiting for a lock on the cooksnap_reminder subscription record, because it wants to destroy that record.</p><p>But <em>why</em> does the first transaction need a lock on the device record if it didn’t actually need to update the device record? The output tells us it is stuck waiting for the lock at this operation:</p><pre>INSERT INTO `push_notification_subscriptions` (`public_alias`,`created_at`,`updated_at`,`device_id`) VALUES (&#39;comment&#39;, &#39;2022-01-24 14:10:23.556665&#39;, &#39;2022-01-24 14:10:23.556665&#39;, 57) ON DUPLICATE KEY UPDATE `public_alias`=`public_alias`</pre><p>So inserting into the push_notification_subscriptions stable requires a lock on the device record 🤯. But it isn’t waiting for an exclusive lock - it needs an S lock, or Shared lock, which means the lock isn’t for updating or deleting the record. Then it dawned on me - the transaction is <em>reading</em> the device record to make sure that record exists, and continues to exist, while new push_notification_subscriptions records are inserted, because those new record <em>refer to</em> the device record. Sure enough, there is a foreign key constraint which means a subscription record can only be inserted if its device_id has a corresponding device record.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sS-ytdh8zY6B82F3pWJJcg.png" /></figure><p>Another thing I realised: the transaction that errors is the one that starts by updating the device record. This is actually the one we want to succeed. So the result isn’t just a deadlock and failed request. The device ends up with an out-of-date token.</p><h4>Avoiding the deadlock 👷</h4><p>I then thought about how this deadlock could be avoided.</p><h4>Option 1: Split up the transactions</h4><p>The deadlock happens when the first transaction wants a lock on the device record that the second transaction is holding onto (and the second transaction wants a lock on a subscription record that the first transaction is holding onto). We can avoid this circular dependency by moving the device update operation into a separate transaction — this separate transaction can release the lock on the device without waiting for the first transaction to finish.</p><p>We’d end up with three transactions.</p><pre>TRANSACTION 1: doesn’t need to update the device because the existing device record is up-to-date with the supplied params. Goes straight to locking a subscription record for deletion.<br><br>TRANSACTION 2: does need to update the device because the supplied params are different from the existing attributes. Locks the device row.<br><br>TRANSACTION 1: needs to get a lock on the device row in order to insert new subscription records. Cannot get the lock because it is held by TRANSACTION 2.<br><br>TRANSACTION 2: finishes updating the device, commits and releases its lock.<br><br>TRANSACTION 1: is free to get a lock on the device record, finishes inserting the new subscriptions and completes.<br><br>TRANSACTION 3: new transaction is opened to refresh subscription records. Wants a lock on the first subscription record to delete. Transaction 1 has released its locks so Transaction 3 can delete the persisted subscription records and insert new ones.</pre><p>In code, this change amounts to removing the the transaction that currently wraps the whole DeviceCreationService.</p><p>Instead of:</p><pre>Device.transaction do<br>  if device.update(device_params)<br>    refresh_push_notification_subscriptions<br>  end<br>end</pre><p>We can have</p><pre>if device.update(device_params)<br>  refresh_push_notification_subscriptions<br>end</pre><p>And indeed, the spec no longer results in a deadlock.</p><p>The possible downside is that now if an error happens after updating the device, the device will still have been updated. It may be set up to receive push notifications, but not the correct types of notification.</p><h4>Option 2: Ensure both transactions request the same lock first</h4><p>Another solution is to stick with two transactions, but ensure the first begins by requesting a lock on the device record even though it doesn&#39;t need to update that record.</p><p>Instead of:</p><pre>Device.transaction do<br>  if device.update(device_params)<br>    refresh_push_notification_subscriptions<br>  end<br>end</pre><p>We can have</p><pre>device.with_lock do<br>  if device.update(device_params)<br>    refresh_push_notification_subscriptions<br>  end<br>end</pre><p>device.with_lock opens a transaction and does the following as an initial operation:</p><pre>SELECT `devices`.* FROM `devices` WHERE `devices`.`id` = X LIMIT 1 FOR UPDATE</pre><p>This SELECT FOR UPDATE locks the row. If another concurrent transaction wants to open the same lock, it waits for the first transaction to finish.</p><p>The first transaction gets the lock on the device row first, and the second transaction (which first of all wants to update the device) cannot do anything until that lock is released. The second transaction waits for the first transaction to commit, then it can progress.</p><p>The important thing is that the second transaction doesn’t begin by getting some other lock that will block the first transaction in future. Both transactions begin by needing the same lock, which means the second waits without causing problems for the first.</p><p>And indeed, with this implementation, the spec no longer results in a deadlock, and the device ends up with the updated token.</p><h4>Takeaways 🎁</h4><h4>1. Deadlocks don’t necessarily mean slow requests</h4><p>Whenever I’d heard about “database deadlocks” in the past, I’d vaguely assumed they brought the database to a standstill and resulted in very slow requests. But when I was able to replicate a deadlock, the error happened <em>as soon as</em> the second transaction needed the lock it couldn’t get. There was no delay. With a deadlock, the database (or more precisely INNODB, the storage engine) knows immediately that it’s impossible for the conflicting transactions to progress. So deadlocks don’t themselves slow requests down.</p><h4>2. So what’s the problem?</h4><p>Firstly, a deadlock can still indicate a slow transaction, because deadlocks are more <em>likely</em> when there are slow transactions. They only happen when transactions overlap, and that is more likely the longer each transaction takes. More deadlocks may indicate missing indexes, for example, or slow joins, or large transactions with lots of operations.</p><p>Secondly, a deadlock results in a transaction being rolled back, which <em>can</em> result in an attempted database change not being made, though this isn’t always the case — the transaction that does succeed may make the desired change anyway.</p><p>Thirdly, a deadlock is an error, and if that error is surfaced as a failed request to an API, it could prompt clients to retry requests, resulting in more traffic, which can have an effect on overall response times.</p><h4>3. Avoiding deadlocks</h4><p>In general, we can reduce the likelihood of deadlocks by trying to avoid slow transactions and by reducing the number of operations in transactions. The more a transaction does, the more likely a deadlock is, because the order of operations is more likely to be a problem: the longer the sequence of locks a transaction requires, the less likely that sequence is to be in the ‘right order’ (i.e. compatible with other transactions). It can be very important to combine operations in the same transaction — but when there <em>isn’t</em> a reason, we should keep in mind that there is a cost to doing so. (For some more general advice on how to minimise deadlocks, check out the MySQL docs <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-deadlocks-handling.html">here</a>.)</p><p>Understanding a specific deadlock means working out which transactions required the same locks in a different order. Fixing the problem can then mean splitting up the transactions into smaller parts, or reordering operations, or <em>adding</em> a lock to one or both to ensure they both start by requiring the same lock.</p><p>I haven’t uncovered a simple silver bullet to deal with all deadlocks… but I have peeled back some of the mystery I felt surrounding them — and I hope sharing the process and my learnings helps you do the same.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c16bd1a91721" width="1" height="1" alt=""><hr><p><a href="https://sourcediving.com/debugging-deadlocks-in-ruby-on-rails-c16bd1a91721">Debugging Deadlocks (in Ruby on Rails)</a> was originally published in <a href="https://sourcediving.com">Source Diving</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making a Single Page Search with Turbo]]></title>
            <link>https://sourcediving.com/making-a-single-page-search-with-turbo-ff96160362a8?source=rss----217db81c7a01--ruby</link>
            <guid isPermaLink="false">https://medium.com/p/ff96160362a8</guid>
            <category><![CDATA[ruby-on-rails]]></category>
            <category><![CDATA[ruby]]></category>
            <dc:creator><![CDATA[Tony Rowan]]></dc:creator>
            <pubDate>Tue, 14 Dec 2021 14:19:08 GMT</pubDate>
            <atom:updated>2021-12-20T17:41:10.591Z</atom:updated>
            <content:encoded><![CDATA[<p><a href="https://turbo.hotwire.dev">Turbo</a> (part of <a href="https://hotwire.dev">Hotwire</a>) is a supercharged version of Turbolinks. If you’re used to immediately turning off Turbolinks in any new project, you might be surprised to learn that you don’t need React or Vue to build a rich and interactive web app.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_VnbTSnOPYRG0lAQUGtbBQ.jpeg" /><figcaption>Photo by <a href="https://unsplash.com/@sl33vo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Danny Sleeuwenhoek</a> on <a href="https://unsplash.com/s/photos/turbo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></figcaption></figure><h4>What We’re Building</h4><p>We’re going to be adding a single page search to a simple Single Page App (SPA) that allows users to view information about some Movies.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*YIVbi0XZ1JkZ2h9bWYNdEQ.gif" /></figure><h4>Starting Point</h4><p>The starting point is heavily based on ‘<a href="https://www.mikewilson.dev/posts/using-hotwire-with-rails-for-a-spa-like-experience">Using Hotwire with Rails for a SPA like experience</a>’ by Mike Wilson. If you want to follow along with this post you can use the starting point <a href="https://github.com/tony-rowan/turbo-movies/tree/single-page-search-start">here on Github</a> or follow along through that post. You should have a simple app where you can select a movie without reloading the page, with only a very simple rails controller.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*idOwlO4kyh5H-d_XLRXWoA.png" /></figure><pre>class MoviesController &lt; ApplicationController<br>  def index<br>    @movies = Movie.all<br>  end<br><br>  def show<br>    @movie = Movie.find(params[:id])<br>  end<br>end</pre><h4>Adding Search</h4><p>The first step is to add search support to the controller.</p><pre>class MoviesController &lt; ApplicationController<br>  def index<br>    @movies = movies<br>    @query = query<br>  end<br><br>  def show<br>    @movie = Movie.find(params[:id])<br>  end<br><br>  private<br><br>  def movies<br>    if query<br>      Movie.where(&quot;title ILIKE ?&quot;, &quot;%#{query}%&quot;)<br>    else<br>      Movie.all<br>    end<br>  end<br><br>  def query<br>    params[:query]<br>  end<br>end</pre><p>We can verify this is working by simply adding the query parameter to the URL and visiting the page again. Only the movies matching the query you made are present in the list.</p><p>Not very user friendly though. Let’s add a search form to the top of the list of movies, within the turbo-frame-tag named :index - the same turbo-frame with the list of movies.</p><pre>&lt;%= form_with(url: movies_path, method: :get) do |f| %&gt;<br>  &lt;%= f.text_field(:query, value: @query, placeholder: &quot;Search&quot;) %&gt;<br>&lt;% end %&gt;</pre><p>Now when you make a search and hit enter, the list of movies is filtered. You’ll also notice that, if you have a movie selected, it stays selected. Searching doesn’t cause a full page refresh, only the list of movies updates.</p><p>This happens because each turbo-frame creates its own navigational context. By default all links and form submissions from within a given turbo-frame only effect that turbo-frame. Under the hood, turbo is hijacking the submission event, making an AJAX call for the form submission and interpreting the result. When the submission is successful, turbo reads the response, looking for a turbo-frame-tag that has the same name as the turbo frame that caused the submission. If it finds a match, it swaps out the contents.</p><p>In our case, the form submission returns the same page so it is pretty clear that there will be a matching turbo-frame — the :index frame.</p><p>I said this was the default behaviour. By adding a turbo-target attribute to a form or link turbo will instead look for, and swap the contents in, the turbo frame with the name you passed - this is how clicking the movies changes the :details turbo frame and not the :index frame.</p><h4>Come Alive with Stimulus</h4><p>This is pretty good, but what if the user <em>didn’t</em> have to press enter when they were done searching? We can search as the user types and we can do this with very little Javascript thanks to <a href="https://stimulus.hotwire.dev">Stimulus</a> — another part of the Hotwire suite.</p><p>We can watch for input changes on the text field of the search form and call an action on a Stimulus controller to submit the form for the user. This can achieved with a small amount of markup on the form and the text field.</p><pre>&lt;%= form_with(<br>  url: movies_path,<br>  method: :get,<br>  data: {<br>    controller: &quot;submit-form&quot;,<br>    submit_form_target: &quot;form&quot;,<br>  }<br>) do |f| %&gt;<br>  &lt;%= f.text_field(<br>    :query,<br>    value: @query,<br>    placeholder: &quot;Search&quot;,<br>    data: {<br>      action: &quot;input-&gt;submit-form#submit&quot;<br>    }<br>  ) %&gt;<br>&lt;% end %&gt;</pre><p>On the text field we’ve added data-action=&quot;input-&gt;submit-form#submit&quot;. The value assigned to the data-action attribute encodes which event to listen for and which controller and action (method) to invoke when the event happens. The generic markup is event-&gt;controller-name#action. In our example,event is input, controller is submit-form and our action is submit.</p><p>The input event name comes from the <a href="https://www.w3schools.com/js/js_htmldom_events.asp">JS DOM events</a>, and we want to listen to any changes in the input. The controller name submit-form is just the name we gave the controller we want to invoke here - it is expected to be defined in a file named submit_form_controller.js. Its name in Javascript land will then be SubmitFormController. The submit action is just a public method defined on that controller.</p><p>On the form element we’ve added data-controller=&quot;submit-form&quot; and data-submit-form-target=&quot;form&quot;. The former simply tells Stimulus to attach a new instance of the SubmitFormController to the form element, allowing us to invoke its actions from the form element or any of its children. The latter attribute makes the form element available from the controller as a &#39;target&#39; called form - from the controller we will be able to access the element by calling this.formTarget.</p><p>The SubmitFormController is very simple.</p><pre>import { Controller } from &quot;stimulus&quot;;<br><br>export default class extends Controller {<br>  static targets = [&quot;form&quot;];<br><br>  submit() {<br>    clearTimeout(this.timeout);<br>    this.timeout = setTimeout(() =&gt; {<br>      this._submit();<br>    }, 300);<br>  }<br><br>  _submit() {<br>    this.formTarget.requestSubmit();<br>  }<br>}</pre><p>When the submit action is called it submits the target, which is assumed to be a form. The action itself is debounced. This stops us from generating too many simultaneous requests which makes the submissions overlap.</p><p>The this.formTarget.requestSubmit(); is very important, if we used submit() rather than requestSubmit(), the event that Turbo hooks into the intercept the form submission would not happen and you would have a normal form submission - with a full page refresh.</p><h4>A Small Problem</h4><p>This works really well — until the first time the form is submitted — then the focus in the text field is lost and we can’t keep typing. That’s definitely a bug!</p><p>What’s happening here is that when the response is returned, we’re replacing the whole contents of the :index turbo frame - including the search form itself. To avoid this we can move the search form outside of the turbo frame, and then add turbo-target: :index to the form element. This will mean when the form is submitted, turbo will replace the contents of the :index turbo-frame and leave the search field completely untouched.</p><h4>Conclusion</h4><p>And there we have it. We’ve added a single page search to an app and all we needed was a small amount of markup and very small amount of Javascript. That’s the power of Turbo.</p><p>The resulting code can be found on <a href="https://github.com/tony-rowan/turbo-movies">Github</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ff96160362a8" width="1" height="1" alt=""><hr><p><a href="https://sourcediving.com/making-a-single-page-search-with-turbo-ff96160362a8">Making a Single Page Search with Turbo</a> was originally published in <a href="https://sourcediving.com">Source Diving</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Custom Form Handling With Turbo]]></title>
            <link>https://sourcediving.com/custom-form-handling-with-turbo-29e5525ff4c3?source=rss----217db81c7a01--ruby</link>
            <guid isPermaLink="false">https://medium.com/p/29e5525ff4c3</guid>
            <category><![CDATA[cookpad]]></category>
            <category><![CDATA[ruby-on-rails]]></category>
            <category><![CDATA[rails]]></category>
            <category><![CDATA[ruby]]></category>
            <dc:creator><![CDATA[Ollie Haydon-Mulligan]]></dc:creator>
            <pubDate>Tue, 28 Sep 2021 10:57:54 GMT</pubDate>
            <atom:updated>2022-01-31T09:42:39.975Z</atom:updated>
            <content:encoded><![CDATA[<p>Turbo will be a default part of Rails from Rails 7, replacing <a href="https://github.com/turbolinks/turbolinks">Turbolinks</a> and <a href="https://github.com/rails/rails/tree/main/actionview/app/assets/javascripts">rails-ujs</a>. This post is a result of time I spent digging into Turbo, in particular its implications for forms that don’t seem to fit what Turbo is designed for: that is, forms that don’t necessarily (or only) trigger a redirect or DOM changes. I don’t have a definitive answer for what we <em>should</em> do in these cases, but I’ll explain some options that might be useful if or when Turbo’s constraints feel a bit awkward.</p><h4>Introducing Turbo…</h4><p>Most of this post is about <a href="https://turbo.hotwired.dev/handbook/drive">Turbo Drive</a>, one of the four techniques that together constitute <a href="https://turbo.hotwired.dev">Turbo</a>.</p><p>Turbo Drive is the bit that intercepts link clicks and form submissions to avoid full page reloads. It’s the new incarnation of <a href="https://github.com/turbolinks/turbolinks">Turbolinks</a>, which has been a default part of Rails apps for a long time. Turbolinks only intercepted link clicks, not form submissions — but now, if you have Turbo installed, a form without any data attributes will automatically be handled and ultimately submitted by <a href="https://github.com/hotwired/turbo/blob/efbad251b3a08efb6db61d33b6114c4b45da36b6/src/observers/form_submit_observer.ts#L16">Turbo’s javascript</a>. This means form submissions are by default ajax requests, which don’t result in a full page load when the browser gets a response.</p><p>So what does happen with the response after Turbo submits a form?</p><ol><li>If the response is a redirect, Turbo will follow that redirect, navigating to the new page (without a full page load) as if the user had clicked a link. This is equivalent to the <a href="https://github.com/turbolinks/turbolinks-rails/blob/master/lib/turbolinks/redirection.rb">redirect support</a> in Turbolinks-Rails when a form is submitted as an ajax request — in other words, we did have a way pre-Turbo to submit a form and redirect without a full page load.</li><li>If the response is html and the status is 4XX or 5XX, Turbo will render that html (without changing the URL). Turbolinks-Rails didn’t do this. Previously, if a POST request returned some html, nothing would happen without custom javascript to swap that html into the page or simulate a Turbolinks visit.</li><li>If the response is a ‘Turbo Stream’ response, Turbo will process it… A what? Turbo Streams are a new kind of response. Their content-type header is text/vnd.turbo-stream.html and they contain one or more Turbo Stream elements, which are custom html elements. Turbo automatically appends these elements to the DOM and whenever such an element is added, it triggers DOM changes (such as appending or replacing or removing html as specified by the markup in the Turbo Stream element.</li></ol><p>Those three alternatives are the <em>only</em> things Turbo is designed to do after a form is submitted:</p><ol><li>follow a redirect,</li><li>render html if the http status is 4XX or 5XX, or</li><li>process Turbo Streams, which can trigger only a limited range of DOM changes</li></ol><h4>Doing what Turbo isn’t designed for…</h4><p>These constraints are deliberate and there’s no reason to debate them. But it is important to understand them and what they mean in practice. If we want to do something Turbo <em>isn’t</em> really designed for, what should we do? What <em>can</em> we do?</p><p>I was learning about Turbo soon after implementing a checkout flow in <a href="https://cookpad.com">Cookpad</a> using <a href="https://stripe.com/docs/js">stripe js</a>. It works by creating a Payment Method in Stripe, then submitting the Payment Method’s id in a form to our server. If all goes well processing the purchase, the user is redirected to a success page. But the purchase might fail because the user needs to authorise the payment with their bank. In that scenario, our server returns the data needed to call stripe&#39;s <a href="https://stripe.com/docs/js/payment_intents/confirm_card_payment">confirmCardPayment</a> function. And that function launches the authorisation flow for the user’s bank. [1]</p><p>Calling javascript functions using data returned by the server doesn’t feel like one of the Three Things Turbo is designed to do after submitting a form. So as I read about Turbo, I kept asking myself this: what if we need (or want) to do something else? Or, being a bit more specific:</p><blockquote>With Turbo set up, (how) can we submit a form then handle the response — in particular an error response — in a custom way, without only redirecting or inserting and/or removing some html?</blockquote><h4>Option 1…</h4><p>One option is to use Turbo up to a point, then, at that point, take over from it. Let Turbo submit the request, let Turbo handle a redirect, but prevent Turbo handling the response if, instead of rendering html or appending Turbo Stream elements, we want to do “other stuff” like call some javascript functions.</p><p>This is doable by listening for the turbo:before-fetch-response event, emitted on the document after the request has been made but before the response has been used.</p><p>We can put this stimulus action on a form:</p><pre>&lt;form data-action=&quot;turbo:before-fetch-response@document-&gt;prevent-default#preventDefault&quot;&gt;<br>  ...<br>&lt;/form&gt;</pre><p>Then define preventDefault in a prevent-default stimulus controller:</p><pre>export default class extends Controller {<br> // Let Turbo make the request.<br> // Check the response. If it&#39;s unsuccessful, stop Turbo attempting to handle the response.<br> async preventDefault(event) {<br>   // The response is available here and we can block Turbo&#39;s default behaviour.<br>   if (!event.detail.fetchResponse.succeeded) {<br>     event.preventDefault()<br>     const json = await event.detail.fetchResponse.response.clone().json()<br>     console.log(&quot;Do stuff with the json...&quot;, json)<br>   }<br> }<br>}</pre><p>Now, if the server responds with an error, we can do whatever we want. See how the response doesn’t even have to be html.</p><p>But t̶h̶e̶r̶e̶’̶s̶ ̶a̶ ̶p̶r̶o̶b̶l̶e̶m̶ 👈. UPDATE: The problem I describe below no longer exists. <a href="https://github.com/hotwired/turbo/pull/367">This update</a> means the turbo:before-fetch-response event is fired on the form itself, not the document. Our stimulus action can be turbo:before-fetch-response-&gt;prevent-default#preventDefault instead of turbo:before-fetch-response@document-&gt;prevent-default#preventDefault.</p><p>Because the event target i̶s̶ was document, I couldn’t find a nice way to be sure it corresponds to the correct form on the page. I could have checked the URL the request was sent to, or put a DOM identifier in the response, but neither is ideal. Now that the target is the element that triggered the request, we can listen for the event on the specific form we want to handle. That gives us a convenient way to let Turbo make the request then optionally &#39;take over&#39; when the response is ready.</p><p>(Also note that if the response content-type is text/vnd.turbo-stream.html, or the response status is 4XX or 5XX, we can take over from Turbo without <em>needing</em> to call event.preventDefault(). Turbo’s default behaviour won’t get in the way. Turbo only raises a “Form responses must redirect to another location” error if the response status is 200 and the content-type is something other than text/vnd.turbo-stream.html. And if the response body is JSON, Turbo won’t render it on the page or do anything else with it.)</p><h4>Option 2a…</h4><p>Another option is to trigger the ‘other stuff’ (the stuff that isn’t inserting and/or removing html <em>by</em> inserting some html.</p><p>For example, if we want to trigger stripe’s card authorisation flow, we can return a Turbo Stream element that appends a block of html that attaches a stimulus controller that triggers the card authorisation flow.</p><p>The Turbo Stream element could be rendered like this:</p><pre>&lt;%= turbo_stream.update &quot;stripe-authentication-container&quot; do %&gt;<br>  &lt;%= render &quot;shared/payment/authentication&quot;,<br>    client_secret: error_payload[:data][:client_secret],<br>    payment_method_id: error_payload[:data][:payment_method_id] %&gt;<br>&lt;% end %&gt;</pre><p>When it’s added to the DOM, it will update the contents of the stripe-authentication-container with an authentication partial.</p><p>The authentication partial could look like this:</p><pre>&lt;div<br>  data-controller=&quot;stripe-authentication&quot;<br>  data-stripe-authentication-public-key-value=&quot;&lt;%= Rails.configuration.x.stripe.public_key %&gt;&quot;<br>  data-stripe-authentication-client-secret-value=&quot;&lt;%= client_secret %&gt;&quot;<br>  data-stripe-authentication-payment-method-id-value=&quot;&lt;%= payment_method_id %&gt;&quot;&gt;<br>&lt;/div&gt;</pre><p>And the stimulus controller’s connect function could look like this:</p><pre>async connect() {<br>  this.stripe = await loadStripe(this.publicKeyValue)<br>  const result = await this.stripe.confirmCardPayment(this.clientSecretValue, {<br>    payment_method: this.paymentMethodIdValue,<br>  })<br>  // ...handle the result by showing errors or sending another request to fulfil the purchase<br>}</pre><p>I think using a Turbo Stream to insert html as a way to do other things - things that <em>could</em> be done without inserting html at all - is in line with what the Turbo docs advocate <a href="https://turbo.hotwired.dev/handbook/streams">here</a>:</p><blockquote>Turbo Streams consciously restricts you to seven actions: append, prepend, (insert) before, (insert) after, replace, update, and remove. If you want to trigger additional behavior when these actions are carried out, you should attach behavior using Stimulus controllers.</blockquote><h4>Option 2b…</h4><p>In the above example, the ‘other behaviour’ is triggered when a stimulus controller connects, which happens when an element is added to the DOM. In that sense, the additional behaviour is triggered by the DOM change.</p><p>But we could also use a Turbo Stream to trigger behaviour in a more roundabout way: the Turbo Stream could cause a stimulus controller (A) to connect, which could emit an event, which we could listen for in some other stimulus controller (B). Stimulus controller B would then perform the action not because it has just connected, making the resulting behaviour a bit more removed from the thing the Turbo Stream is designed for: making a DOM change.</p><p>We could render a Turbo Stream like this:</p><pre>&lt;%= turb_stream.append &quot;form&quot; do %&gt;<br>  &lt;div data-controller=&quot;pass-error&quot; data-pass-error-payload-value=&quot;&lt;%%= @object.errors.to_json %&gt;&quot;&gt;&lt;/div&gt;<br>&lt;% end %&gt;</pre><p>The ‘pass-error’ stimulus controller could connect like this:</p><pre>connect() {<br>  this.element.dispatchEvent(<br>    new CustomEvent(&quot;error&quot;, { bubbles: true, detail: { payload: this.payloadValue }})<br>  )<br>  this.element.remove()<br>}</pre><p>And we could listen for the custom error event in the same way we can listen for rails-ujs ajax:error events:</p><pre>&lt;form data-action=&quot;error-&gt;error-handler#handleError&quot;&gt;<br>  ...<br>&lt;/form&gt;</pre><p>This effectively means using a Turbo Stream to simulate the standard way we (at Cookpad) <em>currently</em> handle a response payload. It feels a bit like hacking Turbo Streams to let us handle non-html responses, and isn&#39;t really in the spirit of Turbo… but it could be useful, especially if you want to switch to Turbo but continue acting on events similar to ajax:error.</p><h4>Option 3…</h4><p>Finally, even with Turbo installed (and Turbolinks removed), we don’t <em>have</em> to use it. We can disable Turbo on an individual form by adding a data-turbo=false attribute. This will result in a standard non-ajax form submission. Or we can add a data-remote=true attribute to the form. As long as we still have rails-ujs installed, the &#39;data-remote&#39; attribute will stop Turbo handling the submission because rails-ujs will intercept it first.</p><p>This is definitely a way to have Turbo set up while handling a form response in ways Turbo isn’t designed for. Submit the form with rails-ujs instead and act on the events it emits to do whatever needs to be done. Great.</p><p>Except that by default we then <em>lose</em> the option of responding to the submission with a redirect. Without Turbolinks-Rails installed, if you try to redirect in response to a rails-ujs form submission, nothing will happen...</p><p>What we need for rails-ujs to be viable in a non-Turbolinks setup is a way to redirect <em>with Turbo</em> when a <em>non</em>-Turbo ajax form is submitted.</p><p>And here it is, <a href="https://github.com/hotwired/turbo-rails/blob/main/UPGRADING.md#2-replace-the-turbolinks-gem-in-gemfile-with-turbo-rails">in the Turbo docs</a>. A Turbo version of the <a href="https://github.com/turbolinks/turbolinks-rails/blob/master/lib/turbolinks/redirection.rb">Turbolinks-Rails redirect_to method</a>. Drop this into your ApplicationController, and you can redirect with Turbo even when Turbo didn&#39;t submit the form.</p><h4>Conclusions…</h4><p>I don’t know how many others are or will be asking themselves the question I found <em>myself</em> asking, and I haven’t found a definitive answer to that question anyway… But hopefully I have explained a few approaches that might help as we adapt to Rails without Turbolinks and without rails-ujs.</p><p>I’ll finish with a bit of practical advice, because something that <em>has</em> become clear as I’ve tried out these approaches is a way to make the leap to Turbo a bit calmer and more gradual.</p><p>If your existing application submits remote: true forms, there&#39;s no need to rewrite them all straight away. Let rails-ujs continue intercepting the submissions. Let it continue emitting the convenient ajax:error and ajax:success hooks. Start by letting Turbo take over the <em>other</em> forms: Turbo will seamlessly [2] turn them into ajax submissions and handle them without a full page load. <em>Then</em> consider each &#39;remote&#39; form individually, either removing remote: true and refactoring to deliver the necessary behaviour with Turbo, or keeping remote: true, or using <em>neither</em> rails-ujs nor Turbo.</p><p>Thanks for reading, and feel free to get in touch with me <a href="https://twitter.com/olliedoodleday">@olliedoodleday</a>. 👋</p><p>[1] Similarly, after submitting the form, we call this <a href="https://stripe.com/docs/js/payment_request/events/on_paymentmethod">complete</a> function to finish processing an Apple or Google pay purchase. The argument we pass to complete depends on the server response.</p><p>[2] Assuming the response is either a redirect or html with a 4XX or 5XX status.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=29e5525ff4c3" width="1" height="1" alt=""><hr><p><a href="https://sourcediving.com/custom-form-handling-with-turbo-29e5525ff4c3">Custom Form Handling With Turbo</a> was originally published in <a href="https://sourcediving.com">Source Diving</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Fight to Deliver Apps to the Globe Faster]]></title>
            <link>https://sourcediving.com/a-fight-to-deliver-apps-to-the-globe-faster-97e5760956ce?source=rss----217db81c7a01--ruby</link>
            <guid isPermaLink="false">https://medium.com/p/97e5760956ce</guid>
            <category><![CDATA[continuous-delivery]]></category>
            <category><![CDATA[ruby]]></category>
            <category><![CDATA[app-store-localization]]></category>
            <category><![CDATA[fastlane]]></category>
            <category><![CDATA[ios]]></category>
            <dc:creator><![CDATA[ainame]]></dc:creator>
            <pubDate>Wed, 18 Nov 2020 14:32:29 GMT</pubDate>
            <atom:updated>2020-11-18T14:32:29.470Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*DvbvfreGqPTo9q4Na368Zg.jpeg" /><figcaption><a href="https://pixabay.com/photos/courier-night-panning-warsaw-1214227/">https://pixabay.com/photos/courier-night-panning-warsaw-1214227/</a></figcaption></figure><p>At Cookpad, we build a global community to make everyday cooking fun. We deliver an iOS app to the worldwide market that supports 26 languages and about 74 regions as of today. It is always challenging to develop and ship apps on that scale. Release management is vital to be able to deliver new features or bug fixes to our customers quickly and continuously. In order to do this, we automate <a href="https://sourcediving.com/continuous-ios-app-delivery-1a158f1f3d33">our release flow with using </a><a href="https://sourcediving.com/continuous-ios-app-delivery-1a158f1f3d33">fastlane, </a><a href="https://sourcediving.com/continuous-ios-app-delivery-1a158f1f3d33">Jenkins, and </a><a href="https://sourcediving.com/continuous-ios-app-delivery-1a158f1f3d33">Slack</a>. Thanks to this automation, we have been able to do weekly release cycles for a while.</p><p>However, we have recently faced an issue where submission by fastlane&#39;s deliver was getting unacceptably slow during a couple of releases, which made it take about 4 hours and sometimes ended up failing altogether. After spending some days to investigate it, I was able to improve the performance of deliver to complete submission within a reasonable time. <a href="https://github.com/fastlane/fastlane/pull/16972">fastlane/fastlane#16972</a></p><p>Let’s have a look at what happened and how I overcame the issue.</p><h4>Big Update Comes with a Pain</h4><p>Just a week before WWDC 2020, on 16th June, Apple renewed its App Store Connect portal, and it appeared to be causing errors in fastlane&#39;s deliver. This meant that we couldn&#39;t use fastlane to submit apps anymore for the time being. (Technically we could upload the binary .ipa file but couldn&#39;t update metadata and screenshots.) <a href="https://github.com/fastlane/fastlane/issues/16621">fastlane/fastlane#16621</a></p><p>This issue impacted us a lot and we had to figure out what we can or can’t do with the existing fastlane lanes in our Fastfile and then follow necessary steps to manually release in the first week. At that moment, <a href="https://twitter.com/joshdholtz">@joshdholtz</a> the lead maintainer of fastlane was the hero that eagerly migrated old APIs to the latest APIs. As we waited for the migration to be done we tested RC versions locally for a few weeks.</p><p>On 2nd July, fastlane v2.150.0 was finally released followed by some patch version bumps. Although we saw errors while uploading screenshots, that turned out to be our fault as some errors were not caught by old APIs; e.g. filename extensions didn&#39;t match the expected file format. All seemed to work when we first tried the new version.</p><h4>Weekends didn’t come free</h4><p>v2.150.0 resolved most of our submission issues, however some additional work was still required to support the new APIs for asset uploading. This was eventually resolved in <a href="https://github.com/fastlane/fastlane/pull/16842">fastlane/fastlane#16842</a>.</p><p>In our weekly release cycle, we submit the app every Friday afternoon, and as the release manger I was trying to submit the app with CI and waiting for CI jobs to complete for hours on Friday afternoon. When it failed, I tried it again. Each trial took up around 3–4 hours and my working hours were over, but I kept repeating that from my iPhone. This continued until midnight on Saturday. I wasn’t that exhausted doing it but felt like my work extended forever.</p><p>I decided to fight against this to get back my Fridays and weekends.</p><h4>Test it, Measure it</h4><p>For our case it was apparent that a specific part of the app submission was incredibly slow and problematic. We always run fastlane&#39;s command with --verbose option on Jenkins so that we could check those logs quickly.</p><p>We saw two issues:</p><ol><li>Uploading screenshots was too slow</li><li>The results when making bad entries were shown as errors in the screenshots section in the App Store Console</li></ol><p>Although they were different issues, both were around screenshots uploading. To test and measure screenshot uploads, firstly, I set up a dummy app and a lane to run deliver like this. In general, you need to be able to measure how fast or slow it is if you want to improve performance, otherwise you can&#39;t even tell if the performance improved or not by your change.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4f36a071e8e072ad068314c7fc142132/href">https://medium.com/media/4f36a071e8e072ad068314c7fc142132/href</a></iframe><p>That didn’t finish within a reasonable time initially as we had about 380 images for production.</p><blockquote>[03:36:31]: fastlane.tools just saved you 132 minutes! 🎉</blockquote><p>So I left one region’s screenshots and removed the rest locally for testing. That made it easy to do trial and error until it finally finished in 10 minutes.</p><h4>Ruby working with IO</h4><p>I had a hunch that parallelisation would help to improve the performance, and I had already made similar performance improvements for other parts of our workflow; i.e. importing and exporting translations from and to an external service. The more languages or regions we support, the more data we need to handle, and if it comes over the Internet, IO — HTTP requests would be likely to be the bottlenecks.</p><p>The Ruby language that fastlane uses can make multiple Thread objects easily. In Ruby, you can&#39;t let multiple threads run at a time due to GIL (Global Interpreter Lock), which secures Ruby&#39;s thread safety, you can still multiplex IO with them.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c19b9295c0b8ca3c3d82e7ade191b2be/href">https://medium.com/media/c19b9295c0b8ca3c3d82e7ade191b2be/href</a></iframe><p>Let’s say downloading an image takes up to 10 seconds and we need to download 10 images sequentially. It is done by 10 secs * 10 images = 100 seconds. If Ruby multiplexes the network request to download images, that is done in nearly 10 secs with Ruby’s Thread in theory. It&#39;s ten times faster than without using threads.</p><p>This, of course, applies to uploads as well. So I used Thread to parallelise deliver. This change seemed to make it run about two times faster than before with the mini data set.</p><h4>Find Pattern, Solve Problem</h4><p>After reading through fastlane deliver and assessing logs, I noticed deletions of screenshots (driven by overwrite_screenshots flag) was already running on multi-threads but it was not as fast as I imagined. So I added a micro-benchmark additionally around the uploading and deleting code to investigate its elapsed time.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8b69da32525184ca51218795ece59e31/href">https://medium.com/media/8b69da32525184ca51218795ece59e31/href</a></iframe><p>This change appeared to be quite helpful for me to identify how the real problem was occurring. Most of the deletion requests completed quickly, but some of them could take longer than others. This became much more obvious when observing upload requests.</p><pre>Uploading ‘./fastlane/screenshots/ar-SA/5.5_1.jpg’...<br>Uploaded &#39;./fastlane/screenshots/ar-SA/5.5_1.jpg&#39;... (2.637683 secs)<br>...<br>Uploading ‘./fastlane/screenshots/ar-SA/iPad Pro (12.9-inch) (3rd generation)_3.jpg’...<br>Uploaded &#39;./fastlane/screenshots/ar-SA/iPad Pro (12.9-inch) (3rd generation)_3.jpg’... (125.733078 secs)</pre><p>This slows down the most straightforward approach to use multi-threads. For example, deliver&#39;s upload operation was like this semi-pseudo code.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5d8e3d163a06b21e586fa996cca1cdcd/href">https://medium.com/media/5d8e3d163a06b21e586fa996cca1cdcd/href</a></iframe><p>Look at this picture below, and imagine you have three threads and three images to upload on each region; ar-SA and en-US. The above pseudo-code reluctantly has to wait for the longest response time to move on to the next locale. Let’s sort it out.</p><figure><img alt="The simplest approach to upload images in parallel is to parallelise uploading for each region" src="https://cdn-images-1.medium.com/max/596/1*Cp978oENGvWhDaW-5d1Obg.png" /><figcaption>The simplest approach to upload images in parallel is to parallelise uploading for each region</figcaption></figure><p>In a nutshell, this can be solved with the Queue-Worker pattern, which is commonly known and used in a variety of places where scalability matters but often its name varies; Thread Pool, Job Queue, or Work Queue. iOS developers will recognise it as DispatchQueue or OperationQueue.</p><p>We can expect more speed in uploading generally as it won’t be blocked by slow responses in each iteration. In the following picture the graph shows when the Queue-Worker pattern is used with three threads. Even if each response time is the same, the latter can use a free thread efficiently.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/619/1*rrgAdxaQWdRtwjbygCNZ1A.png" /><figcaption>How much “Queue-Worker” pattern can minimise the overall execution time in the case</figcaption></figure><p>From this point of view, Ruby is powerful. It has a class Thread::Queue (or just Queue) cooperating with Thread.</p><blockquote>The Queue class implements multi-producer, multi-consumer queues. It is especially useful in threaded programming when information must be exchanged safely between multiple threads. The Queue class implements all the required locking semantics. The class implements FIFO type of queue. In a FIFO queue, the first tasks added are the first retrieved.</blockquote><p><a href="https://www.rubydoc.info/stdlib/core/Queue">Class: Queue</a></p><p>So thanks to the power of Queue, I was able to implement &quot;Queue-Worker&quot; pattern with a piece of code in Ruby.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c10a73603e769ad08ece76f1559094d0/href">https://medium.com/media/c10a73603e769ad08ece76f1559094d0/href</a></iframe><p>This QueueWorker can be used just like this.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4bb17b334c2ca0185f5faee36ddcada0/href">https://medium.com/media/4bb17b334c2ca0185f5faee36ddcada0/href</a></iframe><p>So I applied this in deliver, and as a result it completed about six times faster locally once the App Store Connect API was responding better. (Remember it used to take over 2 hours initially.)</p><ul><li>With fastlane 2.154.0 19m25.106s</li><li>With my working branch 3m20.148s</li></ul><p>Test environment</p><p>We had 385 images to be uploaded (but four skipped due to exceeding the limit of 10 in each screenshot set 🙈)</p><pre>% du -sh fastlane/screenshots<br>90M  fastlane/screenshots<br>% ls fastlane/screenshots/**/*.{png,jpg} | wc -l<br> 385</pre><p>I used these option to run deliver to upload screenshots.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c0a975d9938f452821210d2a97c3040f/href">https://medium.com/media/c0a975d9938f452821210d2a97c3040f/href</a></iframe><h4>Not exactly saving your time if it’s slow</h4><blockquote>[03:36:31]: fastlane.tools just saved you 132 minutes! 🎉</blockquote><p>This message that fastlane outputs at the end of the command always reminds me that I would be exhausted every week if I were to submit apps manually. I love how fastlane saves our precious time in day-to-day work. However, if there is some bottleneck in it, it may involve your time in the end when you need to check the result of your fastlane action. In other words, automated workflows should run fast enough that you’re still able to retry within your working hours. Failures are a reality, nothing is perfect!</p><p>Fight to save your time and enjoy your weekend.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=97e5760956ce" width="1" height="1" alt=""><hr><p><a href="https://sourcediving.com/a-fight-to-deliver-apps-to-the-globe-faster-97e5760956ce">A Fight to Deliver Apps to the Globe Faster</a> was originally published in <a href="https://sourcediving.com">Source Diving</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Life beyond the cargo cult]]></title>
            <link>https://sourcediving.com/life-beyond-the-cargo-cult-14964dbb6854?source=rss----217db81c7a01--ruby</link>
            <guid isPermaLink="false">https://medium.com/p/14964dbb6854</guid>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[rspec]]></category>
            <category><![CDATA[testing]]></category>
            <category><![CDATA[ruby-on-rails]]></category>
            <category><![CDATA[ruby]]></category>
            <dc:creator><![CDATA[Jens Jakob Balvig]]></dc:creator>
            <pubDate>Tue, 29 Sep 2020 05:46:03 GMT</pubDate>
            <atom:updated>2020-09-29T05:00:23.813Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*lYhsnq7-oov7g8Zwn3BuoA.jpeg" /></figure><p>This isn’t your typical tech post evangelizing a cool new pattern or tool, or picking a fight with a popular convention.</p><p>Well, maybe a <em>touch</em> of the latter, but mainly it’s a tale of coming to terms with past decisions, making the best of what one is given, and finding peace with living in the uncanny valley.</p><p>So, put on your rubber boots as we get into the weeds and take a look at why and how we switched from RSpec to TestUnit, without <em>actually</em> switching.</p><h3>The early days</h3><p>I recall the time I discovered Rails (version 1.2) as an extremely exciting period of my life, spending many a happy moment scouring the internet for any knowledge I could find about this new amazing framework.</p><p>Once I’d gotten past the sheer joy of <a href="https://www.youtube.com/watch?v=Gzj723LkRJY">“whoops”</a>ing together one toy app after another, eager to use it for larger projects and hungry for more knowledge, I began learning about testing.</p><p>Curiously, all the material and tutorials I came across on the subject told me to first remove the built-in /test folder and install RSpec instead.</p><p>I duly complied, without ever questioning why one wouldn’t just use the testing framework that came with Rails itself.</p><p>Thus began a journey of getting better at testing <em>using RSpec</em> and a series of apps all initiated with the ritual of replacing /test with /spec - to the point of eventually having it automated.</p><p>I don’t know if the people I’ve worked with since share a similar story, but every person I came into contact with seemed only to validate the choice: <em>“everyone”</em> was using RSpec.</p><p>It was only years later, starting a new gem from scratch, that I thought to actually give Minitest/TestUnit a whirl and…what a breath of fresh air it was!</p><p>…even if it was a little difficult to breathe through the palm that was thoroughly stuck to my face.</p><h3>The revelation</h3><p>Let me be clear: This <em>isn’t</em> an RSpec-bashing post.</p><p>It’s an impressive piece of software, and I know people who swear by it and love the unique DSL.</p><p>For my part, it might have been that I (and the people I worked with) just weren’t very good at “RSpeccing,” but the results always felt a little too elaborate and somewhat <em>foreign</em> when compared with code written to drive the app itself.</p><p>It would probably take an entirely separate blog post to go into details about what I mean by that, but to try to sum it up, the tests we used to write looked <em>something</em> like this:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/afa37035b9e1268f1b02d23d57a33970/href">https://medium.com/media/afa37035b9e1268f1b02d23d57a33970/href</a></iframe><p>Writing that using TestUnit might look like this:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/cfc90a6e3844e0ed7f2c32466d78dd20/href">https://medium.com/media/cfc90a6e3844e0ed7f2c32466d78dd20/href</a></iframe><p>Now, I don’t know if everyone will necessarily agree that the second example looks better, and either way, I’m being unfair…to both frameworks!</p><p>The difference becomes more apparent when seen across an entire test suite, and I can’t think of any single example I can write without either doing too much or too little.</p><p>What struck me, however, is the main thing I’m trying to illustrate above:</p><p>TestUnit code looks just like any other Ruby class.</p><p>Not only that, but the same core principles that go into writing solid business logic can, for the most part, be applied to writing good test cases — albeit in a slightly more restrained manner to keep the risk of bugs in the tests themselves low.</p><p>Once I’d seen the alternative, I couldn’t escape the feeling that all the extra DSL, nesting, and features were all just a little bit…well, <em>unnecessary</em>.</p><h3>The fix</h3><p>So…what is one to do when the project you’re working on already has a gem &quot;rspec-rails&quot; entry that’s several years old?</p><p>How long would it take to rewrite tens of thousands of lines of test code?</p><p>Thankfully, the team had begun to suspect that maybe there was a better way early on and therefore had been fairly conservative with using the full RSpec toolbox.</p><p>Therefore, ultimately what we ended up doing was <em>keeping</em> the framework, but <em>adopting a “TestUnit-like” style to writing tests</em>.</p><p>In practice, this meant:</p><ul><li>Discouraging some of the more advanced features in <a href="https://github.com/cookpad/global-style-guides/tree/379ddec187f7a0168b8db9083370e23bcbaff3a9/rspec">our style guides</a>.</li><li>Flattening contexts.</li><li>Applying a “What would TestUnit do?”-mentality.</li></ul><p>The good thing was that for the last item, more often than not, it <em>mostly</em> boiled down to “do what you would do writing a normal Ruby class.”</p><p>For the example above, that equates to something like this:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/68e349c3f4f553ffc93fa816fc36432c/href">https://medium.com/media/68e349c3f4f553ffc93fa816fc36432c/href</a></iframe><p>Yes, it’s <em>a little awkward!</em></p><p>That dangling private method is looking particularly out of place (at least until you get used to it), and we still have a handful of RSpec conventions that would be difficult to break while also keeping things somewhat consistent.</p><p>We’ve considered rewriting the suite in TestUnit (hard to justify the time investment), using it only for <em>new</em> tests (the context switching would probably be a nightmare), and even mixing/matching using assert in place of expect <em>within</em> the RSpec blocks.</p><p>None of those solutions felt realistic or even desirable, and so instead, we found a middle ground.</p><p>I still feel a twinge of sadness whenever I have to write something like:</p><pre>expect(user.like(recipe)).to be_true</pre><p>…knowing that it could simply be:</p><pre>assert user.like(recipe)</pre><p>…and I still haven’t given up hope that maybe one day we’ll have a big hackathon and leave the uncanny valley for good.</p><p>Until that day comes, nothing stops us from trying to make our livelihoods as comfortable as possible within the world we’ve been born into.</p><p><a href="https://dhh.dk/2012/rails-is-omakase.html">“Rails is omakase”</a> not only made for a <a href="https://www.youtube.com/watch?v=E99FnoYqoII">great meme</a>, but also, whatever you might think of it, in the spirit of the original message I would encourage anyone who’s diverged from the menu to revisit the Rails defaults.</p><p>You might just find something you didn’t know you were missing.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=14964dbb6854" width="1" height="1" alt=""><hr><p><a href="https://sourcediving.com/life-beyond-the-cargo-cult-14964dbb6854">Life beyond the cargo cult</a> was originally published in <a href="https://sourcediving.com">Source Diving</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Keeping it 100]]></title>
            <link>https://sourcediving.com/keeping-it-100-a65e65c2bd86?source=rss----217db81c7a01--ruby</link>
            <guid isPermaLink="false">https://medium.com/p/a65e65c2bd86</guid>
            <category><![CDATA[ruby]]></category>
            <category><![CDATA[software-development]]></category>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[ruby-on-rails]]></category>
            <category><![CDATA[code-review]]></category>
            <dc:creator><![CDATA[Jens Jakob Balvig]]></dc:creator>
            <pubDate>Thu, 24 Sep 2020 00:43:34 GMT</pubDate>
            <atom:updated>2020-09-25T00:21:50.206Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RC5mKNAM9ezSH39KDRLkqQ.jpeg" /></figure><p>We’ve written in the past about how we <a href="https://sourcediving.com/a-practical-guide-to-small-and-easy-to-review-pull-requests-a7f04a01d5d5">encourage pull requests with less than 100 additions</a>. In this blog, we take a closer look at where the guideline comes from and how it works in practice.</p><h3>Origins</h3><p>It’s worth noting that managing the line count in a pull request itself is not the goal, but more of a means to an end.</p><p>We are mainly concerned with the number of <em>concepts</em>.</p><p>While a pull request should have enough context to be understandable, we have found that limiting the overall number of concepts has several benefits:</p><h4>Reduced risk of getting blocked</h4><p>The more things going on in a pull request, the greater the likelihood of getting blocked.</p><p>For example, a pull request containing a refactoring, a novel pattern introduction, <em>and </em>a new feature implementation all at once risks getting stuck should questions arise around <em>any</em> of those.</p><p>By splitting out the refactoring into a new pull request it can be submitted, reviewed, and merged separately. Similarly, on a different pull request the new pattern could be discussed.</p><h4>Safer deployments</h4><p>In our setup merging to the main branch triggers an automatic deployment to production.</p><p>Limiting the amount of code that gets released at one time reduces the number of things that can go wrong and ensures that a targeted revert is easy to do should it become necessary.</p><h4><strong>Better review quality</strong></h4><p>Reviewers’ time and bandwidth are limited, and the more they need to take in and give feedback on at a time, the less able they will be to do so in a meaningful way.</p><p>This might lead to missing key details or even giving the review a pass and “doing it later.”</p><h4><strong>Early course correction</strong></h4><p>Reviewing a pull request containing a fully completed feature and noticing a better path that could have been taken several commits earlier is a painful situation to be in for all.</p><p>Sharing (even unfinished) results <em>early</em> helps detect problems or alternate solutions and build consensus early on.</p><h4><strong>Review speed</strong></h4><p>A tightly scoped pull request with less code to read helps reviewers get through it more quickly and makes it more appealing to pick up in the first place.</p><h3><strong>Relying on constraints</strong></h3><p>Working in this way requires discipline and being mindful of how and when to share code with the team.</p><p>The &lt;100 additions constraint works as a guide and <em>reminder</em>, that should be just enough to make you stop and think.</p><p>100 has shown itself to be the sweet spot for checking whether there <em>might </em>be an issue or better alternative to uncover.</p><p>That’s not to say there aren’t times where a larger pull request is warranted (and it is equally possible to stuff a pull request with multiple concepts in few lines of code).</p><p>This is also why we ultimately <em>encourage</em> the behavior, rather than having it as a strict rule.</p><p>We subtly incentivize this behavior by <a href="https://github.com/cookpad/cp8/blob/2ae41a475070826232220b38e6ff1d94b1f0f8ca/lib/notifications/ready_for_review_notification.rb#L16-L30">having our review bot notify reviewers for small pull requests</a> in a dedicated chat channel:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/820/1*bVpRIUKeffs8vC11urCsfQ.png" /></figure><p>Larger pull requests will have the mentions omitted and rely on <a href="https://sourcediving.com/how-to-use-github-notifications-for-fun-profit-dd76857a843a">being found through the GitHub inbox</a>.</p><p>It doesn’t mean you won’t get reviewed if you have more than 100 additions but sets the expectation that while a &lt;100 pull request will get reviewed within hours (even minutes at times), it might take a little longer for anything bigger.</p><h3>Staying on course</h3><p>As can be the case with any metric, people may lose sight of the <em>intent</em> and overly focus on the metric itself.</p><p>A few common indicators of this are as follows:</p><p><strong>Problem:</strong> Artificially splitting <em>already written code</em> to keep under the 100 addition mark in a way that omits the required context for reviewing (an example could be submitting just the migration of an already written model).</p><p><strong>Solution:</strong> Consider revisiting development style, f.ex using the <a href="https://sourcediving.com/a-practical-guide-to-small-and-easy-to-review-pull-requests-a7f04a01d5d5">top-down approach</a> or <a href="https://sourcediving.com/progressive-enhancement-as-a-valuable-philosophy-in-the-age-of-javascript-aac2e26364d2">progressive enhancement</a> to avoid the problem in the first place, or diverge from the guideline when it makes sense.</p><p><strong>Problem:</strong> Committing inappropriate workarounds, such as removing line breaks that were helping readability in order to stay within the constraint.</p><p><strong>Solution:</strong> Code quality and readability should always take priority over any guideline. Just as with our <a href="https://github.com/cookpad/global-style-guides/blob/379ddec187f7a0168b8db9083370e23bcbaff3a9/.rubocop.ruby.yml#L12">120 character line length limit</a>, the intent is to put enough pressure on the code to come up with <em>better code</em>, not to make compromises to hit a numerical target.</p><p><strong>Problem:</strong> Chain of unreviewed pull requests depending on each other piling up.</p><p><strong>Solution:</strong> Both submitters and reviewers have a responsibility to <a href="https://medium.com/@balvig/its-a-good-question-efb510db70ff">ensure a tight review loop</a>. Submitters sharing their work early in a well-presented manner helps expedite the work of reviewers, who on their end commit to reviewing in a timely manner.</p><p>However, it <em>is</em> a balancing act to limit interruption while also unblocking others, so should one find themselves in a situation of starting a third dependent pull request, consider manually pinging the team for help.</p><p>While it does require a particular mindset and discipline to follow, the simple presence of this constraint has led to a noticeable improvement in review turnaround, quality, and several innovations in how we work.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a65e65c2bd86" width="1" height="1" alt=""><hr><p><a href="https://sourcediving.com/keeping-it-100-a65e65c2bd86">Keeping it 100</a> was originally published in <a href="https://sourcediving.com">Source Diving</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Testing external dependencies using dependency injection]]></title>
            <link>https://sourcediving.com/testing-external-dependencies-using-dependency-injection-ad06496d8cb6?source=rss----217db81c7a01--ruby</link>
            <guid isPermaLink="false">https://medium.com/p/ad06496d8cb6</guid>
            <category><![CDATA[cookpad]]></category>
            <category><![CDATA[ruby-on-rails]]></category>
            <category><![CDATA[rails]]></category>
            <category><![CDATA[ruby]]></category>
            <dc:creator><![CDATA[Christian Bruckmayer]]></dc:creator>
            <pubDate>Wed, 17 Jun 2020 13:31:40 GMT</pubDate>
            <atom:updated>2020-06-17T13:31:40.420Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*P120Y1TuFYP9U_KV2UD-UQ.png" /></figure><p>If you work on a bigger Ruby (on Rails) project, chances are that you need to interact with some external resource like a database, cloud API or message queue. Testing these external resources can sometimes be tricky, let’s have a look at different approaches.</p><h3>Example</h3><p>As an example, we will use the Dropbox API gem to upload a file.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1b5dd403ce16fa4cac68dc4198bcf129/href">https://medium.com/media/1b5dd403ce16fa4cac68dc4198bcf129/href</a></iframe><p>We have an Uploader class which accepts the file content and file name. Finally it uses the Dropbox client to upload the data to the cloud.</p><h3>WebMock</h3><p>An approach we often see is to use <a href="https://github.com/bblimke/webmock">WebMock</a> to stub the HTTP requests.</p><p>From the official documentation, it is described as a</p><blockquote>Library for stubbing and setting expectations on HTTP requests in Ruby.</blockquote><p>Our test code could look like this</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b9acacf138d66d26d9c242eddf0d4878/href">https://medium.com/media/b9acacf138d66d26d9c242eddf0d4878/href</a></iframe><p>With this approach, we test that the correct Dropbox API URLs are called. This has the advantage that we can refactor our code and even use a different library and as long as we still call the same URLs this test would pass.</p><p>However, if Dropbox decides to update their API we would need to change our specs. If we’re treating the <em>gem</em> as our public interface, and trusting it to be maintained and respond to API changes, then having to update those specs every time seems like an unnecessary step. We could argue our test might have too much low-level details and this should get tested in the gem instead.</p><h3><strong>PORO fake</strong></h3><p>An alternative could be to build our own Plain Old Ruby Object (PORO) fake.</p><p>In case we want to swap out Dropbox for another cloud storage, we would need to change the Uploader class (<a href="https://en.wikipedia.org/wiki/Open%E2%80%93closed_principle">open-closed principle</a>). Instead, Uploader should accept a client parameter which is called dependency injection.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f0c2d31b674a61f23446e3142c5e9ee1/href">https://medium.com/media/f0c2d31b674a61f23446e3142c5e9ee1/href</a></iframe><p>As we now only need to provide an object which responds to <em>upload_by_chunks</em> we can implement a simple test client fake too.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/af180a8401000b1f407a278c0f072f26/href">https://medium.com/media/af180a8401000b1f407a278c0f072f26/href</a></iframe><p>The <a href="https://github.com/rails/rails/blob/master/activesupport/lib/active_support/core_ext/module/attribute_accessors.rb#L71">cattr_accessor</a> is a handy helper to create a getter and setter for a class attribute. In our <em>upload_by_chunks</em> method, we just store the parameters in this class attribute. We can now use this test client in our specs like this.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d75a8e6f3eaa1750554163fd19dd890d/href">https://medium.com/media/d75a8e6f3eaa1750554163fd19dd890d/href</a></iframe><p>However, our Uploader class is now tied to the public API of the Dropbox gem. If the <em>upload_by_chunks</em> method is renamed for example, the class would have to be modified even though from its perspective nothing has changed.</p><p>A way to avoiding this is sometimes referred to as the “dependency inversion principle”, that suggests to “depend upon abstractions — not concretions”. We can fix this by wrapping our DropboxApi::Client into a DropboxClient adapter with a common ‘interface’ upload.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2ff692f14e6865b27b80f170ed224575/href">https://medium.com/media/2ff692f14e6865b27b80f170ed224575/href</a></iframe><p>With this change we also make the public interface of the uploader client smaller (<a href="https://en.wikipedia.org/wiki/Interface_segregation_principle">interface segregation</a>) and easier to support more clients (e.g. Box) in the future as we only need to write a new adapter.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/67d0dda2721335274d5885f9a7f464c1/href">https://medium.com/media/67d0dda2721335274d5885f9a7f464c1/href</a></iframe><p>We don’t couple our specs to the Dropbox API or gem anymore as we test against our adapter interface. As always, there is however a trade-off: We need to maintain our PORO fake and keep it in sync with our adapter and specs might still pass even though the Dropbox gem or API changed. We should test our wrapper class too.</p><h3>Summary</h3><p>By using dependency injection with a PORO fake, our specs and implementation not only became more extensible but also more robust against changes. It might not be needed for <em>every</em> situation, but used at the right moment can be a useful tool.</p><p>Some examples where we use this pattern at Cookpad include sending SMS messages, <a href="https://github.com/cookpad/streamy/blob/master/lib/streamy/test_dispatcher.rb">Kafka messages</a>, push notifications and even <a href="https://github.com/guilleiguaran/fakeredis">Redis</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ad06496d8cb6" width="1" height="1" alt=""><hr><p><a href="https://sourcediving.com/testing-external-dependencies-using-dependency-injection-ad06496d8cb6">Testing external dependencies using dependency injection</a> was originally published in <a href="https://sourcediving.com">Source Diving</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ruby 2.7 NEWS: Commentary by Cookpad’s Full Time Ruby Comitters]]></title>
            <link>https://sourcediving.com/ruby-2-7-news-commentary-by-cookpads-full-time-ruby-comitters-bdbaacb36d0c?source=rss----217db81c7a01--ruby</link>
            <guid isPermaLink="false">https://medium.com/p/bdbaacb36d0c</guid>
            <category><![CDATA[ruby]]></category>
            <category><![CDATA[cookpad]]></category>
            <dc:creator><![CDATA[Miles]]></dc:creator>
            <pubDate>Fri, 03 Jan 2020 13:14:51 GMT</pubDate>
            <atom:updated>2020-01-03T13:49:28.461Z</atom:updated>
            <content:encoded><![CDATA[<h3>Ruby 2.7 NEWS: Commentary by Cookpad’s Full Time Ruby Committers</h3><p>We are Koichi Sasada (<a href="https://twitter.com/_ko1">ko1</a>) and Yusuke Endoh (<a href="https://twitter.com/mametter">mame</a>) from Cookpad Inc. tech team. Cookpad sponsors us to work full time developing the Ruby interpreter (MRI: Matz Ruby Implementation).</p><figure><img alt="Koichi Sasada (ko1) and Yusuke Endoh (mame)" src="https://cdn-images-1.medium.com/max/1024/1*geL6I7JlOTgxAidqWfTFPQ.jpeg" /></figure><p>We released a Japanese article “<a href="https://techlife.cookpad.com/entry/2019/12/25/121834">Ruby 2.7 NEWS explained by Ruby Professionals</a>” when Ruby 2.7 was released on 25th Dec. 2019. This is an English translation of the article with help from <a href="https://twitter.com/tapster">Miles Woodroffe</a>.</p><p>“<a href="https://github.com/ruby/ruby/blob/v2_7_0/NEWS">NEWS</a>” is a text file that lists all new features and changes of the Ruby interpreter. Compared to a few years ago, we are making an effort to make the file easier to read, for example, by adding many examples. Some of the code in the article is quoted from the NEWS file. In this article, in addition to a description of the new features and changes, we will explain the background of “why and how the changes have been introduced” as much as possible to hopefully make it easier to understand.</p><p>The Ruby 2.7 release has a lot of changes leading to Ruby 3, which is scheduled for release next year, in 2020. Also, as always, many useful new features and performance improvements have been introduced. We hope you will enjoy them.</p><h3>Language Changes</h3><p><em>Changes to the grammar, semantics, etc. of the Ruby programming language.</em></p><h4>1. Pattern Matching</h4><p><em>Pattern matching</em> has been experimentally added <a href="https://bugs.ruby-lang.org/issues/14912">[Feature #14912]</a>. The feature is to check and deconstruct a data structure nicely.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d33225e8924a621441374d5582a8edd0/href">https://medium.com/media/d33225e8924a621441374d5582a8edd0/href</a></iframe><p><strong>What is a pattern match?</strong></p><p>At first glance, it looks like the case/when statement that you are familiar with, but the new syntax is case/in. The semantics are also similar to case/when; it will find a pattern that matches the value of {a: 0, b: 1, c: 2}, from the top to the bottom.</p><p>in {a: 0, x: 1} means “the key a exists and its value is 0” and “the key x exists and its value is 1”.<br> {a: 0, b: 1, c: 2} satisfies the first condition, but does not the second condition because there is no x. So the pattern does not match the value.</p><p>Then, it tries the next pattern in {a: 0, b: var}. This means “the key a exists and its value is 0” and “the key b exists and its value is anything, and substitute it for the variable var”.<br> The value satisfies both conditions, so the pattern matches the value, assigns 1 to var, and executes the corresponding clause, i.e., p var in this case.</p><p>If no pattern matches, a NoMatchingPatternError exception is thrown. Note that this is different from case/when syntax.</p><p>As a concrete use case, you can use it to check that the JSON data has the expected structure, and then take out the necessary data at once. You don’t have to use #dig anymore.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3b0eec193f378f7357d1ec1befcd92eb/href">https://medium.com/media/3b0eec193f378f7357d1ec1befcd92eb/href</a></iframe><p>It will take a long time to explain the full details of pattern matching. For more on this, see the material by Kazuki Tsujimoto who designed and implemented Ruby’s pattern matching (may be a little old unfortunately).</p><ul><li><a href="https://speakerdeck.com/k_tsj/pattern-matching-new-feature-in-ruby-2-dot-7">Pattern matching-New feature in Ruby 2.7</a></li></ul><p><strong>What was difficult in introducing pattern matching?</strong></p><p>Here is the background of the change.</p><p>Pattern matching is a feature often used in statically typed functional programming languages. It has been anticipated for Ruby for a long time: Many people tried to emulate it in Ruby, proposed it for Ruby, or prototyped it with Ruby:</p><ul><li><a href="https://mametter.hatenablog.com/entry/20100522/p1">Pattern matching in Ruby</a>: Attempt to mimic with existing Ruby syntax</li><li><a href="https://rubykaigi.org/2017/presentations/yotii23.html">Pattern Matching in Ruby</a>: Proposal to introduce %p{} and prototype</li><li><a href="https://bugs.ruby-lang.org/issues/14709">Feature #14709-Proper pattern matching</a>: Prototype ticket above</li><li><a href="https://github.com/baweaver/qo">Qo-Query Object-Pattern matching and fluent querying in Ruby</a>: A library that mimics in Ruby syntax</li></ul><p>However, it was difficult to propose a suitable language-builtin syntax. This is because Ruby’s syntax is too flexible and has little room for expansion. It is almost impossible to introduce a new keyword because of backwards compatibility. In addition, usually, a pattern syntax for pattern matching is similar to the construction syntax: [x, y, z] for an array and {a: x, b: y} for a hash.</p><p>Kazuki Tsujimoto settled this situation. He suggested reusing the keyword in. Ruby has a iteration syntax for ... in (that is rarely used these days), and for this syntax in was already a keyword, so there is no need to introduce a new one. It might not be the best keyword to express a pattern matching, but the case/in syntax is reasonably intuitive. Using this idea, the introduction of pattern matching became a reality.</p><p>Kazuki made the prototype proposal of grammar and semantics for pattern matching in 2018, triggered the discussion, implemented in 2019, committed at the time of RubyKaigi 2019, and repeated the experiments and discussions for more than half a year. Finally it is released in 2.7.</p><p>However, it is still experimental, and if you use it, you will get the following warning.</p><pre>$ ./miniruby -e &#39;case 1; in 1; end&#39;<br>-e: 1: warning: Pattern matching is experimental, and the behavior may change in future versions of Ruby!</pre><p>In the future, I think that it will be stabilized through finer improvements from using it more widely. Putting it into production code may be a risk, but I’d like you to give it a try and give us feedback.</p><p>(Credit: mame)</p><h4>2. Warning for keyword argument separation in Ruby 3</h4><p>In Ruby 3, an incompatibility called “keyword argument separation” is planned. Ruby 2.7 now warns about code that will not work in Ruby 3. <a href="https://bugs.ruby-lang.org/issues/14183">[Feature #14183]</a></p><p><strong>Keyword arguments of Ruby 2</strong></p><p>Ruby 2 keyword arguments are passed as just hash arguments. This is a legacy of the Ruby 1 era, and I thought it was a natural extension at the time. However, this design was a trap that created many non-intuitive behaviors.</p><p>The problem is that the called method cannot tell whether it was a keyword or a hash. This is a concrete example:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/af12b99afc61f48fae46a6ada1ebbfab/href">https://medium.com/media/af12b99afc61f48fae46a6ada1ebbfab/href</a></iframe><p>We define two very similar methods. Call them by passing them a hash:</p><pre>foo({}) #=&gt; [{}, {}]<br>bar({}) #=&gt; [1, {}]</pre><p>Surprised? Since the methods do not know whether the last argument was a hash object or a keyword, they interpret the argument with a ad-hoc priority of “required arguments &gt; keyword arguments &gt; optional arguments”. At the time of 2.0 release, it was “keyword argument &gt; required argument &gt; optional argument”, but it has been changed after bugs were reported.</p><p>How can we pass {} as an optional argument to bar? You might come up with bar({}, **{}). However, in Ruby 2.6 this is a disappointing result.</p><pre>bar({}, **{}) #=&gt; [1, {}]</pre><p>**{} is considered “the same as nothing”, and then, the first {} is interpreted as a keyword. To pass the argument {} to bar, it was correct to call it bar({}, {}). I don’t know.</p><p>In the initial version of 2.0, it meant “**{}consistently passes {}&quot;, but a bug report stating “**{}should be the same as nothing” was submitted. Was changed later. Ruby 2’s keyword arguments keep repeating that fixing something creates new intuition.</p><p><strong>Keyword arguments in Ruby 3</strong></p><p>The problem with Ruby 2 stems from the basic design of passing keyword arguments as a hash. Ruby 3 will fix this fundamentally. That is, it separates keyword arguments from positional arguments.</p><p>In Ruby 3, foo({}) always passes positional arguments, and foo(**{}) always passes keyword arguments. Sounds perfect.</p><pre># in Ruby 3<br>foo({}) #=&gt; [{}, {}]<br>bar({}) #=&gt; [{}, {}]</pre><pre>foo(**{}) #=&gt; wrong number of arguments (given 0, expected 1)<br>bar(**{}) #=&gt; [1, {}]</pre><p>However, this would break any code that was written as foo(opt) and intended to pass keyword arguments. Here you need to rewrite foo(**opt).</p><p>So Ruby 2.7 works in principle the same as Ruby 2.6, but it warns you if you make a call that doesn’t work this way.</p><pre>def foo(**kw)<br>end </pre><pre>foo({}) #=&gt; test.rb: 4: Using the last argument as keyword parameters is deprecated; maybe ** should be added to the call<br>        #   test.rb: 1: warning: The called method `foo&#39; is defined here</pre><p>If this warning appears, it means that the code will not work in Ruby 3 unless you take appropriate measures. Add ** in this case.</p><p><strong>More about keyword argument separation</strong></p><p>While this is easy, the migration of keyword argument separation is sometimes more difficult, especially in cases involving delegation. Check out the migration guide on Ruby’s official website.</p><ul><li><a href="https://www.ruby-lang.org/en/news/2019/12/12/separation-of-positional-and-keyword-arguments-in-ruby-3-0/">https://www.ruby-lang.org/en/news/2019/12/12/separation-of-positional-and-keyword-arguments-in-ruby-3-0/</a></li></ul><p><strong>Background of keyword argument separation</strong></p><p>I confess. I (Yusuke Endoh) implemented the keyword argument in Ruby 2. As an excuse, I knew that there were some corner cases that were somewhat uncomfortable. However, Ruby 1 already had the omission of hash braces in a method call (foo(:a =&gt; 1, :b =&gt; 2)). Ruby 2’s keyword arguments were a natural extension. So I thought it was a good compromise if we cared about compatibility.</p><p>However, a number of non-intuitive behaviors have been reported since the 2.0 release, and each time they made the semantics increasingly complex. What I learnt is that the language design must not be based on bug reports.</p><p>I’ve always regretted this and hoped to fix it with Ruby 3. When I became a full-time committer in Cookpad, I told Matz about this, and Matz thought the same way. He declared to change at RubyWorld Conference 2017 and RubyConf 2017.</p><p>Since late 2017, Matz, akr, ko1, and I have been working on this issue; what semantics we should aim for in Ruby 3, and how we can provide a migration path. I repeated the process of creating a design proposal, performing an experiment with a Cookpad internal Rails app, and measuring the impact. Some of the results were also reported in the ticket <a href="https://bugs.ruby-lang.org/issues/14183">[Feature #14183]</a>.</p><p>At first, we aimed for complete separation, but the incompatibilities were too big (too much code passes foo(k: 1) to the method def foo(opt = {}); end). This was claimed by Jeremy Evans on the ticket, and we agreed on the ticket to give up separation on this case. This was around April 2019.</p><p>Jeremy Evans became the committer following this discussion. He did not only insist on the suitable semantics, but also implemented and experimented. Ruby 2.7 couldn’t be released without him. I really appreciate his efforts.</p><p>(mame)</p><h4>3. Numbered parameters</h4><p>A feature called Numbered parameters has been introduced that allows you to omit the declaration of block parameters. <a href="https://bugs.ruby-lang.org/issues/4475">[Feature #4475]</a></p><p>We can convert the elements of the array ary = [1, 2, 10] into a string expressed in hexadecimal by ary.map {|e| e.to_s(16) }. The variable name of this block parameter e means “element”, and it is a variable that I often choose when I don’t want to think of a good name. However, since it is an integer, it is hard to discard the integer |i|. No, the number |n| might be nice too. Which should I do? Hmm. Naming is cumbersome.</p><p>Naming can be a hassle. It’s a simple program (converting the contents of an array to hexadecimal notation would be a very simple program in Ruby), and I don’t want to spend time to think about it.</p><p>Therefore, a new feature called numbered parameter was introduced (<a href="https://bugs.ruby-lang.org/issues/4475">Feature #4475: default variable name for parameter</a>). This feature allows you to refer to block arguments as _1 and_2 without naming them. The example of conversion to hexadecimal notation above can be written as ary.map {_1.to_s(16)}.</p><pre>ary.map {|e| e.to_s(16)}<br>ary.map {_1.to_s(16)} </pre><p>Looking the two, you can see that three characters have been omitted. Well, I think it’s a nice thing that you don’t have to think about names when writing, rather than the number of characters.</p><p><strong>Detailed story of Numbered parameters</strong></p><p>If you use a lot of _1 and_2, your program will be a chaos. You should be careful to use them only for really small bits of code. Maybe someone will write a plugin for RuboCop. Also, it is better not to use it when you need to make your code understandable to others. An appropriate variable name is fairly important for easy-to-understand programs.</p><p>It is difficult to understand even if blocks are nested, so only one block can use the numbered parameter in block nesting. For example, if you use it in the outer block and the innermost block, you will get an error:</p><pre>3.times do<br>  _1.times do<br>    p _1<br>  end<br>end<br><br># =&gt;<br># t.rb: 3: numbered parameter is already used in<br># t.rb: 2: outer block here<br>#      p _1<br># ^ ~~~~~~~</pre><p>The error message is easy to understand.</p><p>When using only the first argument in a block, you can use two similar notations, iter {|e|} and iter {|e,|}. The difference is |e| and |e,| in the declaration of block arguments, that is, whether or not there is a comma. Well, because it’s a hassle, I avoid a detailed explanation, but if only one _1 is written in the block, it means|e|.</p><p>_1 was an ordinary variable that could be used for local variables and method names. However, it is better not to use it in the future. If you already have a program which uses _1 like names, we recommend you rename it.</p><p>If there is a local variable _1 outside the block, it will be used as a variable in the outer scope, not an implicit block argument. However, a warning is issued for the expression _1 = ....</p><pre>_1 = 0<br>#=&gt; warning: `_1&#39; is reserved for numbered parameter; consider another name<br>[1].each {p _1} # prints 0 instead of 1</pre><p><strong>Discussion on Numbered parameters</strong></p><p>This feature has been discussed for quite some time. However, it was hard to decide. The main issues are notation and function.</p><p>For example, Groovy had a feature called it, equivalent to_1 this time. However, in Ruby, an identifier it is already used in RSpec etc. There was also a debate as to whether only the first argument was sufficient. Personally, &lt;&gt; of Scheme (SRFI 26) was good, but there were some opinions that it looks like !=.</p><p>Suddenly, Matz once decided @1, @2, … at a Ruby developer meeting (a monthly meeting to review the specifications with Matz). At this time, using only @1 had the same meaning as|e,|.</p><p>After that, there were various discussions about notation. For example, the ticket <a href="https://bugs.ruby-lang.org/issues/15723">Misc #15723: Reconsider numbered parameters-Ruby master-Ruby Issue Tracking System</a> has 129 comments.</p><p>In addition, mame-san performed an experiment of mechanically changing the block argument to @1, … and checking the notation (<a href="https://twitter.com/mametter/status/1159346003536838656">https://twitter.com/mametter/status/1159346003536838656</a>). Looking at this, Matz said that @1 is too similar to an instance variable.</p><p>And then, Matz changed his mind and chose _1, _2, … Also, |e| is very often used than |e,|, so (lonely)_1 was changed to have the same meaning as |e|. It took very long until it was decided.</p><p>This feature is all about how to write blocks easily. In the above conversion to hexadecimal notation, for example, if you have Integer#to_s16 to convert to hexadecimal notation, you can write ary.map(&amp;:to_s16). Everyone loves it. But we have no to_s16. So, numbered parameters were introduced because it meets the needs of writing such a block. Another suggestion was to provide a mechanism to generate a Proc with an argument “16”, for example, ary.map(&amp;:to_s(16)). Requests of various notations like this have been proposed. We thought that numbered parameter will solve almost all of them, so this numbered parameter (_1, …) was introduced.</p><p>If someone designs a different mechanism that allows us to use this kind of functional language-like features in Ruby, it may be introduced in future.</p><p>(ko1)</p><h4>4. Calling proc/lambda without blocks is deprecated/prohibited</h4><ul><li>Proc.new andKernel#proc with no block in a method called with a block gets a warning now.</li><li>Kernel#lambda with no block in a method called with a block raises an exception.</li></ul><p>proc {...} creates a Proc object. Well, do you know what happens if you don’t pass the block?</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2c15b37cc68f2aea27b997845ab81bc3/href">https://medium.com/media/2c15b37cc68f2aea27b997845ab81bc3/href</a></iframe><p>If you don’t specify a block, it returns a block that is passed to the method that calls proc as a Proc object.</p><p>The feature was designed in very old era, when there were no block arguments (&amp;block) in Ruby. This change quits this feature. If you are using this feature, you will get a warning: warning: Capturing the given block using Kernel#proc is deprecated; use &#39;&amp; block&#39; instead.</p><p>lambda had been warned for a few years, and now throws an exception (ArgumentError) in 2.7.</p><p><strong>How to write without using </strong><strong>proc / lambda without block</strong></p><p>From now on, please rewrite by using a block argument:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/026113a899cc2a5e773ba23d2f531a39/href">https://medium.com/media/026113a899cc2a5e773ba23d2f531a39/href</a></iframe><p>If a method accepts a block not only as a Proc but also via a block argument, proc without a block was useful.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/20a7a582de8c1d36a9d1025af25f4c44/href">https://medium.com/media/20a7a582de8c1d36a9d1025af25f4c44/href</a></iframe><p>This cannot be reproduced with only a block argument.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e425fcaadf45fa687ba23e31881a3540/href">https://medium.com/media/e425fcaadf45fa687ba23e31881a3540/href</a></iframe><p>In this way, it is possible to respond by adding a conditional statement one extra line. Well, it’s hard to read, so it’s better to reconsider the API of receiving both.</p><p><strong>No block </strong><strong>proc / lambda prohibition background</strong></p><p>The reason for this fix was that Matz suggested how to define a method that does not accept a block as def foo(&amp;nil). (Note that this proposal is not included in 2.7.) This proposal was aimed at alerting the mistake of wrongly passing a block to a method that does not accept a block. (Have you ever written p {...} by mistake?) However, I was opposed with this feature because it would bring to Ruby a diligent custom that people add &amp;nil to all the method definitions that accept no block, i.e., most method definitions.</p><p>Instead, I think that the interpreter should display a warning or an error against passing a block to a method that seems not to use a block. After a few tries, I actually found two bugs (<a href="https://bugs.ruby-lang.org/issues/15554">[Feature #15554]</a>). However, this proposal has some problems and is not included in 2.7. This is because, in real-world programs, I found some reasonable usages that intentionally pass a block to a method that does not use the given block. Let’s expect Ruby 3.</p><p>There were some obstacles to determine whether a method uses a block or not. One of them is “proc without a block&quot;. proc uses a block, but it looks just like a normal method call to the interpreter. The interpreter has no way to determine at compile time whether the method call named proc is really Kernel#proc. So the interpreter overlooks a usage of a given block.</p><p>In addition to this background, some people says that proc (and lambda) without blocks should be prohibited in recent years. So it became obsolete.</p><p>(ko1)</p><h4>5. Beginless range</h4><ul><li>A beginless range has been experimentally introduced. It might not be as useful as the endless range, but could be good for DSL purpose. <a href="https://bugs.ruby-lang.org/issues/14799">[Feature #14799]</a></li></ul><p>You may know that the endless range was introduced in Ruby 2.6. The next addition is its beginless counterpart.</p><pre>ary = [1, 2, 3, 4, 5]<br>p ary[..2] #=&gt; [1, 2, 3]</pre><p>Unlike endless range, beginless range cannot use #each, so it will be limited to use for expressing a range of values.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ea5451a90b435f77108f735ecb60222c/href">https://medium.com/media/ea5451a90b435f77108f735ecb60222c/href</a></iframe><p>(mame)</p><h4>6. Deprecation of special variables $; and $,</h4><ul><li>Setting $; to non-nil value will generate a warning now. Use of it in String#split will also be warned. <a href="https://bugs.ruby-lang.org/issues/14240">[Feature #14240]</a></li><li>Setting $, to non-nil value will generate a warning now now. Use of it in Array#join will be warned too. <a href="https://bugs.ruby-lang.org/issues/14240">[Feature #14240]</a></li></ul><p>As part of the move away from the special variables inherited from Perl, the use of $; and $, will display a warning. I don’t know what the variables are, but according to NEWS, they seem to be related to String#split andArray#join.</p><p>Few people use them in this way. In addition, they are potentially dangerous since changing them affects some libraries that use String#split. This is why they are deprecated.</p><p>(mame)</p><h4>7. Line break is prohibited as an identifier of the quote here document</h4><ul><li>Quoted here-document identifier must end within the same line.</li></ul><p>In here-documents, you can enclose identifiers in quotes. (FYI: You can prohibit string interpolation by writing like &lt;&lt;&#39;EOS&#39;.) Now, this part of EOS was actually allowed to contain line breaks. I think you don’t know what I’m saying.</p><pre>&lt;&lt; &quot;EOS<br>&quot;# This had been warned since 2.4; Now it raises a SyntaxError<br>EOS</pre><p>We could write this kind of code. In this case, EOS\n was the delimiter. Since Ruby 2.4, such a program has been warned, but in Ruby 2.7 it will be changed to an error. I wonder if this has been used in any program.</p><p>(ko1)</p><h4>8. Flip-flop is back</h4><ul><li>The flip-flop syntax deprecation is reverted. <a href="https://bugs.ruby-lang.org/issues/5400">[Feature #5400]</a></li></ul><p>Flip-flop became obsolete in Ruby 2.6, but came back (probably) because the voice of “I’m still using it!” was so strong. Congratulations to the fans. Saying your idea is important.</p><p>(ko1)</p><h4>9. Some method chains like .bar can be commented out</h4><ul><li>Comment lines can be placed between fluent dot now.</li></ul><pre>foo<br>  # .bar<br>  .baz #=&gt; foo.baz</pre><p>When the method chain foo.bar.baz is described with a line feed before the., you may want to comment out only the .bar part, for example, during debugging. Previously, commenting out like this resulted in a syntax error, but in Ruby 2.7 it now means foo.baz.</p><p>Note, blank line will generate a syntax error.</p><pre>foo<br><br>  .bar<br>  #=&gt; syntax error, unexpected &#39;.&#39;, expecting end-of-input</pre><p>(ko1)</p><h4>10. Private method can be called even with self.</h4><ul><li>Calling a private method with a literal self as the receiver is now allowed. <a href="https://bugs.ruby-lang.org/issues/11297">[Feature #11297]</a> <a href="https://bugs.ruby-lang.org/issues/16123">[Feature #16123]</a></li></ul><p>Private methods could not be called with a receiver. That is, the private method foo could not be called as recv.foo. Also, self.foo failed as well. However, for some reason, we want to attach self., and Ruby 2.7 allows it.</p><pre>self.p 1<br># =&gt;<br># Ruby 2.6: t.rb: 1: in `&lt;main&gt;&#39;: private method `p&#39; called for main: Object (NoMethodError)<br># Ruby 2.7: 1</pre><p>Note that the receiver must be written exactly as self.; private methods cannot be called ass.foo using variables such as s = self.</p><p>With this feature, if you call self.private_method until now, method_missing would be called, but now you can call the called method, so there is a tiny incompatibility that method_missing will not be called. I don’t want to think that there is a program depending on this behavior.</p><p>(ko1)</p><h4>11. Changing the precedence of postfix rescue in multiple assignment</h4><ul><li>Modifier rescue now operates the same for multiple assignment as it does for single assignment. <a href="https://bugs.ruby-lang.org/issues/8279">[Bug #8279]</a></li></ul><pre>a, b = raise rescue [1, 2]<br># Previously parsed as: (a, b = raise) rescue [1, 2]<br># Now parsed as: a, b = (raise rescue [1, 2])</pre><p>As you can see in the comments, the postfix rescue has changed when it is used in multiple assignment expressions. Was the position of the parentheses the same as you expected?</p><p>Personally, I don’t use postfix rescue because it’s difficult. I don’t know what exception to catch.</p><p>(ko1)</p><h4>12. yield in singleton class is deprecated</h4><ul><li>yield in singleton class syntax is warned and will be deprecated later <a href="https://bugs.ruby-lang.org/issues/15575">[Feature #15575]</a>.</li></ul><p>I suspect you cannot understand what this is saying. Even if you do, you can probably think of no reason why one would do so. The following code works on Ruby 2.6 and earlier.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5a7adb457629b7a6bfc847a275b2b3f2/href">https://medium.com/media/5a7adb457629b7a6bfc847a275b2b3f2/href</a></iframe><p>But it doesn’t make sense, so it is warned. I believe no one does so. It will be a syntax error in Ruby 3.</p><p>The background of the change is actually the same as the prohibition of proc without a block.</p><p>(ko1)</p><h4>13. Notation for delegating arguments (...)</h4><ul><li>Argument forwarding by (...) is introduced. <a href="https://bugs.ruby-lang.org/issues/16253">[Feature #16253]</a></li></ul><p>A notation for delegation that passes received arguments to other methods as-is has been introduced.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/742d7974b1d8fcf1317de76db6f065f7/href">https://medium.com/media/742d7974b1d8fcf1317de76db6f065f7/href</a></iframe><p>Both the recipient and the passer must be (...). Otherwise, it would be a syntax error.</p><p>Delegation used to be written as follows. It was cumbersome, difficult to optimize, and required to pass **opt in Ruby 3. Thus, the feature was introduced.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8eb87828a1916ad2cd31b0bd39d81398/href">https://medium.com/media/8eb87828a1916ad2cd31b0bd39d81398/href</a></iframe><p>In addition, many people think that “parentheses can be omitted in Ruby method calls”, but parentheses are required for this delegation notation. The reason is that bar ... is interpreted as an endless range.</p><p>(mame)</p><h4>14. Deprecation of $SAFE</h4><ul><li>Access and setting of $SAFE is now always warned.$SAFE will become a normal global variable in Ruby 3.0. <a href="https://bugs.ruby-lang.org/issues/16131">[Feature #16131]</a></li></ul><p>$SAFE, the old security mechanism of Ruby, has been removed. Did you even know about $SAFE?</p><p>$SAFE is security feature based on “taint” flag which are marked to untrusted data (such as strings) generated from IO input, etc. If the code attempts to use the tainted data for dangerous operations like system or open, the interpreter stops it (SecurityError is raised).</p><p>However, modern frameworks do not consider $SAFE and do not add an appropriate taint flags, making $SAFE virtually unusable. In this situation, trusting $SAFE is more dangerous. So deletion of this feature was proposed.</p><p>In 2.7, if you assign something to $SAFE, you will get the following warning.</p><pre>$SAFE = 1<br>#=&gt; t.rb:1: warning: $SAFE will become a normal global variable in Ruby 3.0</pre><p>In Ruby 3.0, $SAFE will became a normal global variable. I was wondering if it would be treated like a “retired number”, though.</p><ul><li>Object#{taint, untaint, trust, untrust} and related functions in the C-API no longer have an effect (all objects are always considered untainted), and are now warned in verbose mode.This warning will be disabled even in non-verbose mode in Ruby 3.0, and the methods and C functions will be removed in Ruby 3.2. <a href="https://bugs.ruby-lang.org/issues/16131">[Feature #16131]</a></li></ul><p>With this change, methods such as Object#taint have no effect. This means that there are no objects with the taint flag. Ruby 3.2 will remove these methods, so let’s deal with them early.</p><p>(ko1)</p><h4>15. Refinements are considered in Object#method etc.</h4><ul><li>Refinements take place at Object#method and Module#instance_method. <a href="https://bugs.ruby-lang.org/issues/15373">[Feature #15373]</a></li></ul><p>Until now, the method objects retrieved by Object#method etc. did not care about refinements, but now they do.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/410478d0f659074811b38a0e22132e98/href">https://medium.com/media/410478d0f659074811b38a0e22132e98/href</a></iframe><h3>Updates To Embedded Classes</h3><p><em>Updates to the core classes of the Ruby language.</em></p><h4>1. Introduction of Array#intersection</h4><ul><li>Added Array#intersection. <a href="https://bugs.ruby-lang.org/issues/16155">[Feature #16155]</a></li></ul><p>Array#intersection has been introduced to extract only the common elements of arrays. It is almost the same as the Array#&amp; (it is slightly different as it can take intersections of three or more arrays).</p><pre>ary1 = [1, 2, 3, 4, 5]<br>ary2 = [1, 3, 5, 7, 9]</pre><pre>p ary1.intersection (ary2) #=&gt; [1, 3, 5]</pre><p>Ruby 2.6 introduced Array#union as a counterpart toArray #|, but intersection was not introduced because it had not been requested. This time, a request came, so we implemented it. Ruby development sometimes runs paranoid on a demand basis.</p><p>(mame)</p><h4>2. Performance improvement of Array#minmax, Range#minmax</h4><ul><li>Added Array#minmax, with a faster implementation than Enumerable#minmax. <a href="https://bugs.ruby-lang.org/issues/15929">[Bug #15929]</a></li></ul><p>When ary.minmax was executed, Enumerable#minmax was executed on Ruby 2.6. However, by preparing Array#minmax separately, it can be executed faster. Because it doesn’t have to call #each.</p><p>Theoretically, Enumerable and each are sufficient for this. But practically, they are not, so this kind of change is needed. As an interpreter developer, I’d really like to be able to make it faster without these kinds of tricks.</p><ul><li>Added Range#minmax, with a faster implementation thanEnumerable#minmax. It returns a maximum that now corresponds to Range#max. <a href="https://bugs.ruby-lang.org/issues/15807">[Bug #15807]</a></li></ul><p>Similarly, Range#minmax has been prepared separately. In addition, since the algorithm for calculating the maximum value uses Range#max instead of Enumerable#max, incompatibilities may appear.</p><p>(ko1)</p><h4>3. Comparable#clamp corresponds to Range argument</h4><ul><li>Comparable#clamp now accepts a Range argument. <a href="https://bugs.ruby-lang.org/issues/14784">[Feature #14784]</a></li></ul><pre>-1.clamp(0..2) #=&gt; 0<br> 1.clamp(0..2) #=&gt; 1<br> 3.clamp(0..2) #=&gt; 2</pre><p>As you can see, we round up and down to fit in the range of 0..2. The same thing could be done with n.clamp(0, 2), but there was a request to write it in a Range, so it has been added. An exception is raised if an exclusive range is given (because the meaning of rounding down when it is exceeded cannot be defined).</p><pre>0.clamp(0...2) #=&gt; ArgumentError (cannot clamp with an exclusive range)</pre><p>(mame)</p><h4>4. Introduction of Complex#&lt;=&gt;</h4><ul><li>Added Complex#&lt;=&gt;. So 0 &lt;=&gt; 0i will not raiseNoMethodError. <a href="https://bugs.ruby-lang.org/issues/15857">[Bug #15857]</a></li></ul><p>Complex is well known for not being able to define comparisons, but a comparison method has been introduced.</p><pre>Complex(1, 0) &lt;=&gt; 3 #=&gt; -1</pre><p>It seems to want to be able to compare when the imaginary part is 0. Note that the comparison of a complex whose imaginary part is not 0 is nil.</p><pre>Complex(0, 1) &lt;=&gt; 1 #=&gt; nil</pre><p>(mame)</p><h4>5. Dir.glob and Dir.[] do not support NUL separate pattern</h4><ul><li>Dir.glob andDir.[] no longer allow NUL-separated glob pattern. Use Array instead. <a href="https://bugs.ruby-lang.org/issues/14643">[Feature #14643]</a></li></ul><p>A feature that nobody knew about has been quietly removed. For example, if there are two files named “foo” and “bar” in the current directory, and if Dir.glob is called with a pattern &quot;f*\0b*&quot;:</p><pre>Dir.glob(&quot;f*\0b*&quot;) #=&gt; [&quot;foo&quot;, &quot;bar&quot;]</pre><p>So, NUL-character (\0) was a “OR” pattern. This behaviour has been removed. If you want to specifically do this, you can use an array.</p><pre>Dir.glob([&quot;f*&quot;, &quot;b*&quot;]) #=&gt; [&quot;foo&quot;, &quot;bar&quot;]</pre><p>(mame)</p><h4>6. Added CESU-8 encoding</h4><ul><li>Added new encoding CESU-8 <a href="https://bugs.ruby-lang.org/issues/15931">[Feature #15931]</a></li></ul><p>An encoding called CESU-8 has been added.</p><p>I don’t know it well, but it seems to be a deprecated encoding (<a href="https://www.unicode.org/reports/tr26/tr26-4.html">UTR#26: Compatibility Encoding Scheme for UTF-16: 8-Bit (CESU-8)</a>). You can ignore it unless it is being used in a system that you need to use.</p><p>(ko1)</p><h4>7. Add Enumerable#filter_map</h4><ul><li>Added Enumerable#filter_map. <a href="https://bugs.ruby-lang.org/issues/15323">[Feature #15323]</a></li></ul><p>A method to do filter and map simultaneously has been added.</p><pre>[1, 2, 3].filter_map {|x| x.odd? ? X.to_s : nil } #=&gt; [&quot;1&quot;, &quot;3&quot;]</pre><p>If the result of the block conversion is falsy (false or nil), it will be deleted. Whether to discard both or only nil has pros and cons, and has been discussed many times, but we decided to discard both due to the intuition of Matz.</p><p>It is roughly equivalent to: filter and map.</p><pre>[1, 2, 3].filter {|x| x.odd? }.map {|x| x.to_s } #=&gt; [&quot;1&quot;, &quot;3&quot;]</pre><p>The following example looks like “map and then filter&quot;.</p><pre># Collect first element of array, but discard false<br>[ary1, ary2, ary3].filter_map {|ary| ary.first }</pre><pre>[ary1, ary2, ary3].map {|ary| ary.first }.filter {|elem| elem }</pre><p>Since we often want to use filter and map in combination, filter_map has been introduced as a dedicated method that eliminates the need to create intermediate arrays.</p><p>(mame)</p><h4>8. Add Enumerable#tally</h4><ul><li>Added Enumerable#tally. <a href="https://bugs.ruby-lang.org/issues/11076">[Feature #11076]</a></li></ul><p>A convenience method for counting the number of elements has been introduced.</p><pre>[&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;, &quot;A&quot;].tally<br>  #=&gt; {&quot;A&quot; =&gt; 2, &quot;B&quot; =&gt; 2, &quot;C&quot; =&gt; 1}</pre><p>It returns a hash with elements as keys and numbers as values. Have you ever implemented it yourself?</p><p>tally is a word that represents the action of counting while writing a line (<a href="https://en.wikipedia.org/wiki/Tally_marks">Tally marks-Wikipedia</a>).</p><p>(mame)</p><h4>9. Add Enumerator.produce</h4><ul><li>Added Enumerator.produce to generate Enumerator from any custom data-transformation. <a href="https://bugs.ruby-lang.org/issues/14781">[Feature #14781]</a></li></ul><p>A class method has been added that is useful for creating infinite sequences as an Enumerator.</p><pre>naturals = Enumerator.produce(0) {|n| n + 1 } # [0, 1, 2, 3, ...]<br>p naturals.take(10) #=&gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</pre><p>The argument is an initial value, and an infinite sequence is created by applying the given block repeatedly to it.</p><p>There was a request from Ruby 2.6, but the name was not easy to determine. In Haskell, the name is iterate, but in Ruby there is the word iterator, which is confusing; generate is a bit too common; recurrence is not well understood; how about from, … After all suggestions were considered, Matz chose produce.</p><p>(mame)</p><h4>10. Add Enumerator::Lazy#eager</h4><ul><li>Added Enumerator::Lazy#eager that generates a non-lazy enumerator from a lazy enumerator. <a href="https://bugs.ruby-lang.org/issues/15901">[Feature #15901]</a></li></ul><p>A method to convert from Enumerator::Lazy toEnumerator. To understand this, you need to understand the background a bit more deeply.</p><p>Ruby has three types of data that represent a sequence of elements: Array, Enumerator, and Enumerator::Lazy.</p><p>Array is a data class in which all elements are arranged in sequential memory, and has the property of consuming memory for each element.<br>On the other hand, Enumerator and Enumerator::Lazy are classes whose internal representation is “calculation to yield the next element”. Even if they represent the same sequence, Enumerator and Enumerator::Lazy can avoid memory consumption. Note that, however, they have disadvantages such as being slow, and cannot be randomly accessed. This is the trade-off.</p><p>The difference between Enumerator and Enumeartor::Lazy is quite subtle; the Enumerator&#39;s methods often enumerate the elements and actually return an Array, whereas Enumerator::Lazy&#39;s methods delay the enumeration of elements until #force method is called.</p><p>For example, for a sequence from 0 to 10,000, you want to double each element and get the first five elements.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/783b83fa84584339fc802db9720ce20e/href">https://medium.com/media/783b83fa84584339fc802db9720ce20e/href</a></iframe><p>Enumerator#map immediately enumerates the elements and returns Array, so it creates an array with a length of 10,000 and consumes a lot of memory.<br> On the other hand, Enumerator::Lazy#map returns Enumerator::Lazy instead of Array because the enumeration of elements is delayed. By doing take(5) and doing force at the end, you can take out the first five elements without creating a 10,000-length array.</p><p>Now, consider a last_odd method that takes an Enumerator and returns the last odd number.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a0667ec80346a8c00feb196e0790d927/href">https://medium.com/media/a0667ec80346a8c00feb196e0790d927/href</a></iframe><p>What if you want to pass Enumerator::Lazy to this method? If you pass it as is, the result of select is also Enumerator::Lazy, which will cause an error because last is not defined. It works if you pass it to an Array produced by force, but it wastes memory. It would be nice if we could convert from Enumerator::Lazy to Enumerator, but there was no API to convert it easily.</p><p>So, to support this case we have introduced Enumerator::Lazy#eager.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b15c991a58322558e71edaf224758a4b/href">https://medium.com/media/b15c991a58322558e71edaf224758a4b/href</a></iframe><p>You can use this to convert Enumerator to Enumerator::Lazy, then you can safely pass it to last_odd.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/781c1b011fa8f4f60eca49313efe0710/href">https://medium.com/media/781c1b011fa8f4f60eca49313efe0710/href</a></iframe><p>Well, it’s complicated. I feel that only Array and Enumerator::Lazy were enough.</p><p>(mame)</p><h4>11. Add Enumerator::Yielder#to_proc</h4><ul><li>Added Enumerator::Yielder#to_proc so that a Yielder object can be directly passed to another method as a block argument. <a href="https://bugs.ruby-lang.org/issues/15618">[Feature #15618]</a></li></ul><p>Enumerator::Yielder can be passed to each with &amp;. I think there are few people who can understand that.</p><p>Enumerator can be used in two ways for this. One is to call each without the block like (0..10000).each shown in the last section, and the other is Enumerator.new {|y| ...}:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5b31739fff797720229e2d152e3d48a2/href">https://medium.com/media/5b31739fff797720229e2d152e3d48a2/href</a></iframe><p>This y is an object of the Enumerator::Yielder class. With the addition of to_proc, you can write something like this:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9a62a4bc6480104f919e060b7c991d91/href">https://medium.com/media/9a62a4bc6480104f919e060b7c991d91/href</a></iframe><p>I wonder how many people are using Enumerator like this.</p><p>(mame)</p><h4>12. Add Fiber#raise</h4><ul><li>Added Fiber#raise that behaves like Fiber#resume but raises an exception on the resumed fiber. <a href="https://bugs.ruby-lang.org/issues/10344">[Feature #10344]</a></li></ul><p>Fiber#raise method has been added. What it does is to resume first, and then raise an exception in the resume destination context.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4041df5fde6a15d883daf35f89673b38/href">https://medium.com/media/4041df5fde6a15d883daf35f89673b38/href</a></iframe><p>When Fiber is doing something like a worker, it can be used to raise an exception that interrupts processing.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/22c6375d9cdcad17e4ce19bd5a82d262/href">https://medium.com/media/22c6375d9cdcad17e4ce19bd5a82d262/href</a></iframe><p>However, because it is difficult (it may generate an unintended exception flow of the fiber creator), it is better to avoid using it if possible. I think that you should do the halt explicitly with resume.</p><p>For example, as in the previous example,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d3fc7851f7eb82b1b9e2d2ac36ab8840/href">https://medium.com/media/d3fc7851f7eb82b1b9e2d2ac36ab8840/href</a></iframe><p>When nil comes, the loop terminates. It is clearer.</p><p>(ko1)</p><h4>13. Change a corner case of File.extname</h4><ul><li>File.extname now returns a dot string at a name ending with a dot on non-Windows platforms. <a href="https://bugs.ruby-lang.org/issues/15267">[Bug #15267]</a></li></ul><p>File.extname, which gets the extension from the file name string, has changed slightly.</p><pre>File.extname(&quot;foo.&quot;) #=&gt; &quot;&quot; in Ruby 2.6<br>                     #=&gt; &quot;.&quot; in Ruby 2.7</pre><p>The reason is that when the result of basename and the result of extname are combined, they should round-trip.</p><pre>f = &quot;foo.&quot;<br>b = File.basename(f, &quot;.*&quot;) #=&gt; &quot;foo&quot;<br>e = File.extname(f)        #=&gt; &quot;.&quot;<br>p b + e == f               #=&gt; true</pre><p>Note that this still returns &quot;&quot; on Windows for profound reasons. I’m not sure, but on Windows it seems that filenames ending with a dot are invalid.</p><p>(mame)</p><h4>14. FrozenError supports receiver</h4><ul><li>Added FrozenError#receiver to return the frozen object that modification was attempted on. To set this object when raising FrozenError in Ruby code, pass it as the second argument to FrozenError.new. <a href="https://bugs.ruby-lang.org/issues/15751">[Feature #15751]</a></li></ul><p>Attempting to update a frozen object will result in a FrozenError exception. FrozenError#receiver indicates which object you attempted to update.</p><pre>begin<br>  &#39;&#39;.freeze &lt;&lt; 1<br>rescue FrozenError =&gt; e<br>  p e.receiver<br>  #=&gt; &quot;&quot;, indicating that you tried to change a frozen string<br>end</pre><p>You can now create FrozenError by specifying receiver as FrozenError.new (receiver: obj). But, well, there is rarely a chance to make such a thing.</p><p>When I searched Gem code published in rubygems with FrozenError.new, I found 122 lines. It’s surprising.</p><p>(ko1)</p><h4>15. GC.compact is added</h4><ul><li>Added GC.compact method for compacting the heap.This function compacts live objects in the heap so that fewer pages may be used, and the heap may be more CoW friendly. <a href="https://bugs.ruby-lang.org/issues/15626">[Feature #15626]</a></li></ul><p>One of the highlights of Ruby 2.7 is the heap compaction functionality. Although it is called with GC.compact, compaction is not performed every time GC is performed, but every time the GC.compact method is executed manually.</p><p><a href="https://bugs.ruby-lang.org/issues/15626">See the ticket for details</a> on this major contribution from <a href="https://twitter.com/tenderlove">Aaron Patterson</a>.</p><p><strong>GC.compact: What is compaction?</strong></p><p>Here is a brief introduction to “heap compaction”. First, Ruby objects are stored on the heap, which is implemented as a collection of pages. A page is a sequence of slots that store objects.</p><p>When an object is created, we look for an empty slot and that slot is used for the new object. When a GC occurs, the unused slots are reclaimed, the unused objects are collected, and the slots are reserved again as free (empty) slots. In other words, after GC, there will be sparse free slots and busy slots.</p><p>Heap compaction in MRI means moving a live object from one page to another with a free slot. As a result, it is possible to change from “pages that have some free slots and some busy ones” to “pages that have busy slots” and “pages that have free slots”. That means eliminating fragmentation. Freeing “pages that have free slots” can be more memory efficient.</p><p><strong>GC.compact: The feature introduced in 2.7</strong></p><p>Copying GC is one GC algorithm which automatically performs compaction. I knew of its existence, of course, and I wanted to introduce it to Ruby. However, for various reasons (mainly for performance reasons), I thought it would be difficult to introduce a GC that performs compaction every time, such as copy GC. GC.compact has been realized brilliantly thanks to a change in the idea: human beings explicitly instructing the timing of compaction.</p><p>Technically, there are some objects that cannot be “moved” in MRI (due to historical reasons). Therefore, the algorithm is to leave them as is and move only those that can be moved. It is called “mostly compaction algorithm”.</p><p>Many changes of MRI were introduced for GC.compact, so it may still have problems. When you use it, please be prepared for this possibility. Maybe, GC.compact is not something you call manually (the framework might call it). Please let me know if you find any problems.</p><p>(ko1)</p><h4>16. IO#set_encoding_by_bom is added</h4><ul><li>Added IO#set_encoding_by_bom to check the BOM and set the external encoding. <a href="https://bugs.ruby-lang.org/issues/15210">[Bug #15210]</a></li></ul><p>Unicode data may have a BOM at the beginning. IO#set_encoding_by_bom has been added to set the external encoding according to the BOM if it is attached to IO and discard the BOM.</p><p>IO needs to be opened in binmode.</p><pre>io = open(&quot;with_bom&quot;, &#39;rb&#39;)<br>p io.tell                #=&gt; 0<br>p io.external_encoding   #=&gt; #&lt;Encoding: ASCII-8BIT&gt;<br>p io.set_encoding_by_bom #=&gt; #&lt;Encoding: UTF-8&gt;<br>p io.tell                #=&gt; 3 (discarded)<br>p io.external_encoding   #=&gt; #&lt;Encoding: UTF-8&gt;</pre><p>(ko1)</p><h4>17. Integer#[] supports Range</h4><ul><li>Integer#[] now supports range operation. <a href="https://bugs.ruby-lang.org/issues/8842">[Feature #8842]</a></li></ul><p>There is a method called Integer#[] that extracts 0 or 1 for the <em>n</em>-th bit number.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d33bd644fecb1ca13b4aaa41ef3085b4/href">https://medium.com/media/d33bd644fecb1ca13b4aaa41ef3085b4/href</a></iframe><p>Bitwise operations are rather complicated, so isn’t it convenient when you want to do that kind of thing?</p><p>(mame)</p><h4>18. Rich result of Method#inspect</h4><ul><li>Method#inspect shows much information. <a href="https://bugs.ruby-lang.org/issues/14145">[Feature #14145]</a></li></ul><p>The notation of Method#inspect has been enhanced. In particular, these two are added:</p><ul><li>(1) Parameter information</li><li>(2) Information on defined location</li></ul><pre>def foo(a, b=1, *r, p1, k1: 1, rk:); end<br>p method(:foo)<br>#=&gt; #&lt;Method: main.foo(a, b=..., *r, p1, rk:, k1: ...) t.rb:2&gt;</pre><p>In this case, (1) is (a, b=..., *r, p1, k1: ...) and (2) is t.rb:2.</p><p>When you wonder “what is this method?”, you can inspect the method object. I hear you can get various information with $ on pry, though.</p><p>As an aside, at first, (1) and (2) were separated by @ in the meaning of “at” which represents a place. However, when copying the file name on the terminal, the space delimiter is easier because we can just double-click. Looks good.</p><p>(ko1)</p><h4>19. Module#const_source_location is added</h4><ul><li>Added Module#const_source_location to retrieve the location where a constant is defined. <a href="https://bugs.ruby-lang.org/issues/10771">[Feature #10771]</a></li></ul><p>Module#const_source_location which returns the definition location of the constant has been added. The position is returned as an array of [file_name, line_number].</p><pre>class C<br>  class D<br>  end<br>end</pre><pre>p Object.const_source_location(&#39;C&#39;)<br># =&gt; [&quot;t.rb&quot;, 1]<br>p Object.const_source_location(&#39;C::D&#39;)<br># =&gt; [&quot;t.rb&quot;, 2]</pre><p>(ko1)</p><h4>20. Module#autoload? Supports inherit option</h4><ul><li>Module#autoload? now takes an inherit optional argument, like as Module#const_defined?. <a href="https://bugs.ruby-lang.org/issues/15777">[Feature #15777]</a></li></ul><p>Module#autoload? now has an inherit option to indicate whether to see the autoload status of the class it inherits from. The default is true.</p><p>Sample from RDoc.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d4332c031b2feb17a9be51bb92747e0b/href">https://medium.com/media/d4332c031b2feb17a9be51bb92747e0b/href</a></iframe><p>(ko1)</p><h4>21. Various frozen character strings</h4><ul><li>Module#name now always returns a frozen String. The returned String is always the same for a given Module. This change is experimental. <a href="https://bugs.ruby-lang.org/issues/16150">[Feature #16150]</a></li><li>NilClass#to_s, TrueClass#to_s and FalseClass#to_s now always return a frozen String. The returned String is always the same for each of these values. This change is experimental. <a href="https://bugs.ruby-lang.org/issues/16150">[Feature #16150]</a></li></ul><p>Results such as Module#name, which returns the module name, and true.to_s, are now unique frozen strings. In the past, they allocated a new object every time for string modification.</p><p>By the way, an experiment with Symbol#to_s being frozen was also tried, but it did not work and was reverted.</p><p>(ko1)</p><h4>22. ObjectSpace::WeakMap#[]= can also hold symbols etc.</h4><ul><li>ObjectSpace::WeakMap#[]= now accepts special objects as either key or values. <a href="https://bugs.ruby-lang.org/issues/16035">[Feature #16035]</a></li></ul><p>You don’t need to read this section because it is about a class called ObjectSpace::WeakMap, which is not recommended for direct use. Don’t read.</p><p>WeakMap is an object like a hash, but when keys or values are collected by the GC, the contents disappear.</p><pre>o = ObjectSpace::WeakMap.new<br>o[&quot;key&quot;] = &quot;value&quot;<br>p o.size #=&gt; 1</pre><pre>GC.start</pre><pre>p o.size #=&gt; 0</pre><p>Because it depends on GC, the result zero is not guaranteed. Note that if it is # frozen-string-literal: true, it will not disappear.</p><p>WeakMap internally sets finalizers on keys and values, so it was not possible to have objects that cannot have a finalizer, such as symbols and numbers.</p><p>But this time it was allowed. It is because this restriction was troublesome to use WeakMap as a cache, but since it is a class that is not recommended to use, I do not know what will happen actually.b</p><p>(mame)</p><h4>23. $LOAD_PATH.resolve_feature_path is added</h4><p>Ruby 2.6 introduced a method called RubyVM.resolve_feature_path which identifies the file to be read when calling require. The method has been moved to the $LOAD_PATH singleton method.</p><p>RubyVM is a place to put something specific to the so-called MRI (Matz Ruby Implementation), but resolve_feature_path is likely to be used in other implementations, so we decided to move it outside. However, since it is not a method used by many people, it is not good to put it in the Kernel. And after discussion, it became a very delicate position as a singleton method of $LOAD_PATH.</p><p>(mame)</p><h4>24. Unicode version is updated</h4><ul><li>Update Unicode version to 12.1.0, adding support for U + 32FF SQUARE ERA NAME REIWA. <a href="https://bugs.ruby-lang.org/issues/15195">[Feature #15195]</a></li><li>Update Unicode Emoji version to 12.1 <a href="https://bugs.ruby-lang.org/issues/16272">[Feature #16272]</a></li></ul><p>The corresponding Unicode version has been increased from 11 to 12.1.0.</p><p>For example, Unicode 12 contains <a href="https://www.publickey1.jp/blog/19/unicode_120121.html">small “ゐ” (U + 1B150)</a>. It could not be displayed in my environment, though. This has a character property called insmallkanaextension, so you can match this with a regular expression.</p><pre>p /\p{insmallkanaextension}/ =~ &quot;\u{1b150}&quot; #=&gt; 0</pre><p>When the characters are widespread in future, some people may gain the benefit.</p><p>(mame)</p><h4>25. Symbol#start_with? and Symbol#end_with? are added</h4><ul><li>Added Symbol#start_with? And Symbol#end_with? Method. <a href="https://bugs.ruby-lang.org/issues/16348">[Feature #16348]</a></li></ul><p>The title says all. Two methods in String have been added to Symbol.</p><p>String and Symbol, where are the lines drawn?</p><p>Where do you draw the line between String and Symbol? This is a historical issue. Symbol fundamentalists claim to be completely different, while String extremists claim to do the same. I feel like it’s flowing to String extremists.</p><p>(ko1)</p><h4>26. Time#ceil and Time#floor methods are added</h4><ul><li>Added Time#ceil method. <a href="https://bugs.ruby-lang.org/issues/15772">[Feature #15772]</a></li><li>Added Time#floor method. <a href="https://bugs.ruby-lang.org/issues/15653">[Feature #15653]</a></li></ul><p>Time objects can actually have a time with a higher precision than seconds (i.e., nanosecond).</p><pre>p Time.now.nsec #=&gt; 532872900</pre><p>The Time#round method rounds time to seconds, but similarly, floor (round up) and ceil (round down) have been added.</p><p>(ko1)</p><h4>27. Time#inspect is different from Time#to_s, and inspect outputs up to nanoseconds</h4><ul><li>Time#inspect is separated from Time#to_s and it shows its sub second. <a href="https://bugs.ruby-lang.org/issues/15958">[Feature #15958]</a></li></ul><p>Up to Ruby 2.6, to_s and inspect truncated and displayed information smaller than seconds. So, by using p or inspect, different Time objects might be displayed like the same.</p><p>So we separated Time#to_s and Time#inspect, and Time#inspect now shows nanoseconds (if any). It seems that Time#to_s was not changed to maintain compatibility.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b8f6572cd66943dddc7cca2dfc9ab9f8/href">https://medium.com/media/b8f6572cd66943dddc7cca2dfc9ab9f8/href</a></iframe><p>(ko1)</p><h4>28. UnboundMethod#bind_call is added</h4><ul><li>Added UnboundMethod#bind_call method. <a href="https://bugs.ruby-lang.org/issues/15955">[Feature #15955]</a></li></ul><p>This is a feature for advanced Ruby users. Do not use it in normal programs.</p><p>Overriding a method by inheriting from a class causes the new method to be called.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/622c2fd7a8e874f5a1253fc97e5581d0/href">https://medium.com/media/622c2fd7a8e874f5a1253fc97e5581d0/href</a></iframe><p>It is natural. However, in the rare case like a black magic, there is a request to call a method before it is overridden. At this time, some people use evil idioms that combine UnboundMethod#bind and Method#call.</p><pre>p Foo.instance_method(:foo).bind(obj).call #=&gt; &quot;foo&quot;</pre><p>It takes out an instance method object (UnboundMethod) of Foo, binds it to the target object, and calls it, so that the method before being overridden can be called. However, since bind and call are quite heavy operations, bind_call was introduced because it would be a little faster if you put them together.</p><pre>p Foo.instance_method(:foo).bind_call(obj) #=&gt; &quot;foo&quot;</pre><p>You need this evil idiom only if you can’t predict what objects will come, like pp or some runtime monitoring libraries. Again, never use it in normal programs.</p><p>(mame)</p><h4>29. Add filters by category of warning (add Warning.[], Warning.[]=)</h4><ul><li>Added Warning.[] and Warning.[]= to manage emit / suppress of some categories of warnings. <a href="https://bugs.ruby-lang.org/issues/16345">[Feature #16345]</a></li></ul><p>By setting Warning[category] = true or false, warnings belonging to category can be enabled or disabled.</p><p>Many compatibility warnings have been introduced in Ruby 2.7, and we discussed how to eliminate the warnings at once. However, dismissing all warnings also suppresses warnings that may be of interest.</p><p>Therefore, Warning.[] and Warning.[]= have been added because we want to control only the warnings for certain categories. Currently there are only two categories: :deprecated and :experimental. I guess it will be organized in the future (But who does it?).</p><pre>Warning[:deprecated] = false</pre><pre>def foo<br>  proc # deprecated warning by default, suppresses the warning<br>end<br>foo {}</pre><p>(ko1)</p><h3>Update standard attached library</h3><p><em>The library has also been updated in various ways. There are some in NEWS, but we will introduce only those that are interesting to us.</em></p><h4>1. CGI.escapeHTML is 2-5 times faster in some case</h4><ul><li>CGI.escapeHTML becomes 2 ~ 5x faster when there’s at least one escaped character. https: <a href="https://github.com/ruby/ruby/pull/2226">//github.com/ruby/ruby/pull/2226</a></li></ul><p>CGI.escapeHTML seems to be faster.</p><p>(ko1)</p><h4>2. Renewal of IRB</h4><p>irb has been redesigned to include the following features:</p><ul><li>Multiple-line edit</li><li>Auto indentation</li><li>Method-name completion</li><li>Document (rdoc) view</li><li>Syntax highlighting</li></ul><p>It is hard to explain in this text, so please try it out yourself. Install Ruby 2.7 now.</p><p>If you can’t do it right away, you can run gem install irb because it also works on 2.6, or see the video of the <a href="http://www.ruby-lang.org/en/news/2019/12/25/ruby-2-7-0-released/">Release Announcement</a>.</p><p>The most popular Ruby interactive console is pry, but now irb has overtaken some aspects. I hear that pry is also considering multi-line editing support. I hope they will be more convenient in the competition.</p><p><strong>IRB Renewal: Amazing and Important Points</strong></p><p>A terminal emulator is a super legacy that has been improved since the days of the typewriter. It is surprisingly difficult. Coloring is not so difficult, but when it comes to multi-line editing, auto-indentation, and completion, it’s a kind of a text editor. It works on Windows consoles as well as Linux. Works on JRuby. Libraries such as ncurses are not used due to various restrictions, so it is implemented with its own code, without libraries, and is as complicated as “screen” and “tmux” commands. It is amazing.</p><p>However, it has not been widely used yet; this is the first time that the new version is widely used. As mentioned earlier, the terminal is a super legacy, and there are differences in behavior among terminals. Sakura Itoyanagi who achieved this improvement seems to have started development around 2018, so he has not been able to experience various environments and usages, and it can not be said that it is mature. So, please try it out and give us feedback if you find weird behavior. If you are having problems, you may want to start with the option irb --legacy and it will work with the previous version of IRB.</p><p>(mame)</p><h4>3. “Did you mean” is displayed if typo option in OptionParser</h4><p>Ruby has had a built-in did_you_mean gem for some time, and suggests a correction when there’s a typo in a method name or constant name. I incorporated it into OptionParser.</p><pre>$ ruby test.rb --hepl<br>Traceback (most recent call last):<br>test.rb:6:in `&lt;main&gt;&#39;: invalid option: --hepl (OptionParser::InvalidOption)<br>Did you mean?  help</pre><p>--help was mistakenly typed as --hepl, and it gives a suggestion like “Did you mean? help”.</p><p>test.rb just uses OptionParser normally.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/507f2741106479c5f494d1f4123c5e59/href">https://medium.com/media/507f2741106479c5f494d1f4123c5e59/href</a></iframe><p>(mame)</p><h3>Incompatibilities</h3><h4>1. Some libraries are no longer bundled</h4><p>The following libraries are no longer bundled gems. Install corresponding gems to use these features.</p><ul><li>CMath (cmath gem)</li><li>Scanf (scanf gem)</li><li>Shell (shell gem)</li><li>Synchronizer (sync gem)</li><li>ThreadsWait (thwait gem)</li><li>E2MM (e2mmap gem)</li></ul><p>These libraries are no longer bundled gems (that is, gems that are automatically installed when you install Ruby). If necessary, put them in Gemfile or something.</p><p>(ko1)</p><h4>2. The format of Proc#to_s has changed</h4><ul><li>Proc#to_s format was changed. <a href="https://bugs.ruby-lang.org/issues/16101">[Feature #16101]</a></li></ul><p>Proc#to_s (an alias to Proc#inspect) returns a string containing the file name and line number (where the Proc was generated). In 2.6, instead of ...@file.rb:123, @ is changed to a blank like ... file.rb:123.</p><p>In other words, it looks like this.</p><pre>p proc {}.to_s<br># =&gt;<br># Ruby 2.6<br># &quot;# &lt;Proc: 0x0000024cc385c3e0@t.rb:1&gt;&quot;<br># Ruby 2.7<br># &quot;# &lt;Proc: 0x0000024cc385c3e0 t.rb:1&gt;&quot; </pre><p>It is a change according to Method#to_s.</p><p>It’s not a big difference. But I found a test failure that uses string match with a regular expression to extract the file name from the result. So I put it in an incompatible place just in case.</p><p>(ko1)</p><h3>Library incompatibilities</h3><h4>1. Gem conversion</h4><p>Promote stdlib to default gems. The following default gems were published at <a href="http://rubygems.org">rubygems.org</a></p><ul><li>benchmark</li><li>cgi</li><li>delegate</li><li>getoptlong</li><li>net-pop</li><li>net-smtp</li><li>open3</li><li>pstore</li><li>singleton</li></ul><p>The following default gems only promoted ruby-core, not yet published at <a href="http://rubygems.org">rubygems.org</a>. (looks like coming soon)</p><ul><li>monitor</li><li>observer</li><li>timeout</li><li>tracer</li><li>uri</li><li>yaml</li></ul><p>Additionally, the did_you_mean gem has been promoted up to a default gem from a bundled gem</p><p>(ko1)</p><h4>2. Pathname()</h4><ul><li>Kernel#Pathname when called with a Pathname argument now returns the argument instead of creating a new Pathname. This is more similar to other Kernel methods, but can break code that modifies the return value and expects the argument not to be modified.</li></ul><p>Pathname(obj) returns obj itself if obj is Pathname, instead of returning a new Pathname.</p><pre>p1 = Pathname(&#39;/foo/bar&#39;)<br>p2 = Pathname(p1)<br>p p1.equal?(p2)<br>#=&gt; Ruby 2.6: false<br>#=&gt; Ruby 2.7: true</pre><p>(ko1)</p><h4>3. profile.rb, Profiler__</h4><ul><li>Removed from standard library. No one updated it from Ruby 2.0.0.</li></ul><p>Removed from standard library. Nobody is maintaining it. It will be available as a gem, but not yet.</p><p>(ko1)</p><h3>Changes to command line options</h3><h4>1. Added -W:(no-)category option</h4><ul><li>-W option has been extended with a following :, to manage categorized warnings. <a href="https://bugs.ruby-lang.org/issues/16345">[Feature #16345]</a> <a href="https://bugs.ruby-lang.org/issues/16420">[Feature #16420]</a></li></ul><p>The feature of Warning[category] = true or false can now be specified on the command line.</p><ul><li>Enable warnings: -W:category</li><li>Disable warnings: -W:no-category</li></ul><p>Like Warning[category], there are currently two categories, deprecated and experimental.</p><p>Usage example:</p><pre>    # deprecation warning<br>    $ ruby -e &#39;$; = &quot;&quot;&#39;<br>    -e:1: warning: `$;&#39; is deprecated<br><br>    # suppress the deprecation warning<br>    $ ruby -W:no-deprecated -e &#39;$; = //&#39;<br><br>    # works with RUBYOPT environment variable<br>    $ RUBYOPT=-W:no-deprecated ruby -e &#39;$; = //&#39;<br><br>    # experimental feature warning<br>    $ ruby -e &#39;0 in a&#39;<br>    -e:1: warning: Pattern matching is experimental, and the behavior may change in future versions of Ruby!<br><br>    # suppress experimental feature warning<br>    $ ruby -W:no-experimental -e &#39;0 in a&#39;<br><br>    # suppress both by using RUBYOPT<br>    $ RUBYOPT=&#39;-W:no-deprecated -W:no-experimental&#39; ruby -e &#39;($; = &quot;&quot;) in a&#39;</pre><p>(ko1)</p><h3>C API changes</h3><ul><li>Many *_kw functions have been added for setting whether the final argument being passed should be treated as keywords.You may need to switch to these functions to avoid keyword argument separation warnings, and to ensure correct behavior in Ruby 3.</li></ul><p>A function with a function name ending in _kw has been added for keyword separation in Ruby 3.</p><ul><li>The : character in rb_scan_args format string is now treated as keyword arguments.Passing a positional hash instead of keyword arguments will emit a deprecation warning.</li></ul><p>The format string : in rb_scan_args() now matches the meaning of the latest keyword argument instead of the last optional hash.</p><ul><li>C API declarations with ANYARGS are changed not to use ANYARGS <a href="https://github.com/ruby/ruby/pull/2404">https://github.com/ruby/ruby/pull/2404</a></li></ul><p>When receiving a function pointer, you can no longer use the ANYARGS feature to indicate that you don’t know its argument. Let’s write the type of the function pointer properly.</p><p>(ko1)</p><h3>Performance Improvements</h3><p><em>Performance improvements made for Ruby 2.7.</em></p><h4>1. Fiber and thread implementation improvements</h4><ul><li>Allow selecting different coroutine implementation by using --with-coroutine=, e.g.</li></ul><pre>./configure --with-coroutine=ucontext<br>./configure --with-coroutine=copy</pre><p>In configure, you can select the Fiber implementation. Well, you don’t need to worry (the default is fine).</p><ul><li>Replace previous stack cache with fiber pool cache.The fiber pool allocates many stacks in a single memory region.Stack allocation becomes O (log N) and fiber creation is amortized O (1) .Around 10x performance improvement was measured in micro-benchmarks. <a href="https://github.com/ruby/ruby/pull/2224">https://github.com/ruby/ruby/pull/2224</a></li></ul><p>Various improvements have been made to the stack strategy allocated for Fiber, makingFiber creation about 10 times faster. Hooray!</p><p>It depends on the environment, but the strategy is to reserve a large area with mmap and divide it and use it.</p><ul><li>VM stack memory allocation is now combined with native thread stack, improving thread allocation performance and reducing allocation related failures. ~ 10x performance improvement was measured in micro-benchmarks.</li></ul><p>In a similar way, getting the VM stack from the machine stack with alloca has greatly increased the VM stack allocation time. This is also about 10 times faster.</p><p>(ko1)</p><h4>2. Use of realpath(3)</h4><ul><li>File.realpath now uses realpath(3) on many platforms, which can significantly improve performance.</li></ul><p>It seems that the performance has been improved by using realpath(3) if it can be used (I don’t know well).</p><p>(ko1)</p><h4>3. Improvement of Hash data structure</h4><ul><li>Change data structure of small Hash objects. <a href="https://bugs.ruby-lang.org/issues/15602">[Feature #15602]</a></li></ul><p>A small hash (specifically 1 to 8 elements) requires 128 bytes instead of 192 bytes of memory (64-bit environment).</p><p>This is achieved by changing the storage of the hash value for each key-value pair to only one byte. I hope it is effective.</p><p>(ko1)</p><h4>4. Speed up by reimplementing Monitor in C</h4><ul><li>Monitor class is written in C-extension. <a href="https://bugs.ruby-lang.org/issues/16255">[Feature #16255]</a></li></ul><p>The Monitor class (MonitorMixin module) was written in Ruby. But due to handle_interrupt method, the overhead was not be negligible. Especially, in Ruby 2.6, some code was added for proper implementation, which made it much slower.</p><p>So, by rewriting in C, it was reasonably faster than before.</p><p>(ko1)</p><h4>5. Improved inline method cache</h4><ul><li>Per-call-site method cache, which has been there since around 1.9, was improved: cache hit rate raised from 89% to 94%. See <a href="https://github.com/ruby/ruby/pull/2583">https://github.com/ruby/ruby/pull/2583</a></li></ul><p>An inline method cache is a cache that is placed at a method call, and that saved the result of the previous method search. In 2.6, if the receiver class was different from the previous call, the cache was not used, even if it invokes the same method. Now, Ruby 2.7 uses the cache by saving the different classes as a cache key.</p><p>In an experiment based on Rails app (the discourse benchmark), the inline method cache hit rate increased from 89% to 94%. It’s very important to be faster because method calls are a lot of work in Ruby.</p><p>(ko1)</p><h4>6. JIT improvement</h4><ul><li>JIT-ed code is recompiled to less-optimized code when an optimization assumption is invalidated.</li></ul><p>There are some prerequisites for advanced optimization, but when those conditions are not satisfied, it recompiles to a version without the optimization.</p><ul><li>Method inlining is performed when a method is considered as pure. This optimization is still experimental and many methods are NOT considered as pure yet.</li></ul><p>An experimental feature to inline “pure” methods has been implemented. I don’t explain the definition of “pure” because it is troublesome, but in most cases a method is not pure.</p><ul><li>Default value of --jit-max-cache is changed from 1,000 to 100</li></ul><p>The parameter --jit-max-cache now defaults from 1,000 to 100. This is a matter of how many methods remain JIT.</p><ul><li>Default value of --jit-min-calls is changed from 5 to 10,000</li></ul><p>The default value of the parameter --jit-min-calls has been increased from 5 to 10,000. This parameter is the threshold for how many times it is called before JIT compilation.</p><p>(ko1)</p><h4>7. Reduce the size of the compiled instruction sequence</h4><ul><li>RubyVM::InstructionSequence#to_binary method generate compiled binary. The binary size is reduced. <a href="https://bugs.ruby-lang.org/issues/16163">[Feature #16163]</a></li></ul><p>With the method RubyVM::InstructionSequence#to_binary, you can convert the sequence of instructions executed by the VM, so-called bytecode, to binary and output it. These binaries are used in Bootsnap, etc., and are used to speed up the startup of Ruby applications.</p><p>This output was in a very wasteful format, so we asked Nagayama-san, who came to be an intern at Cookpad to review the specifications, make it slim, and reduce the output size. Please refer to <a href="https://techlife.cookpad.com/entry/2019/09/26/143000">Improve Binary Output of Ruby Intermediate Expression-Cookpad Developer Blog</a> for details (in Japanese).</p><p>(ko1)</p><h3>Other</h3><p><em>Other notable changes in Ruby 2.7.</em></p><h4>1. IA64 support discontinued</h4><ul><li>Support for IA64 architecture has been removed.Hardware for testing was difficult to find, native fiber code is difficult to implement, and it added non-trivial complexity to the interpreter. <a href="https://bugs.ruby-lang.org/issues/15894">[Feature #15894]</a></li></ul><p>It seems that the production of Itanium has ended. So, we decided to stop its support. The code base had quite special processing.</p><p>(ko1)</p><h4>2. Use of C99</h4><ul><li>Require compilers to support C99 <a href="https://bugs.ruby-lang.org/issues/15347">[Misc #15347]</a></li><li>Details of our dialect: <a href="https://bugs.ruby-lang.org/projects/ruby-trunk/wiki/C99">https://bugs.ruby-lang.org/projects/ruby-trunk/wiki/C99</a></li></ul><p>MRI implementations can now be written in C99 instead of C89 (with some restrictions). We can write // comments! But C99 is published 20 years ago.</p><p>(ko1)</p><h4>3. Git</h4><ul><li>Ruby’s upstream repository is changed from Subversion to Git.</li></ul><p>The source code is now managed by Git. Instead of managing everything on GitHub, there is a separate Git repository, and the GitHub repository is synced up nicely.</p><ul><li>RUBY_REVISION class is changed from Integer to String.</li></ul><p>With the conversion to Git, RUBY_REVISION, which was a Subversion revision (numerical value), is now a Git commit hash.</p><pre>p RUBY_REVISION<br># =&gt; &quot;fbe229906b6e55c2e7bb1e68452d5c225503b9ca&quot;</pre><ul><li>RUBY_DESCRIPTION includes Git revision instead of Subversion’s one.</li></ul><p>Similarly, RUBY_DESCRIPTION, which included the Subversion revision, now includes a commit hash.</p><pre>p RUBY_DESCRIPTION<br># =&gt; &quot;ruby 2.7.0dev (2019-12-17T04: 15: 38Z master fbe229906b) [x64-mswin64_140]&quot;<br># Since it is a development version, the release version is probably different.</pre><p>(ko1)</p><h4>4. Support for writing embedded classes in Ruby</h4><ul><li>Support built-in methods in Ruby with __builtin_ syntax. [<a href="https://bugs.ruby-lang.org/issues/16254">Feature #16254</a> Some methods are defined in * .rb (such as trace_point .rb) .For example, it is easy to define a method which accepts keyword arguments.</li></ul><p>This is what I (ko1) talked at RubyKaigi 2019. See in detail “<a href="https://techlife.cookpad.com/entry/2019/04/17/014142">RubyKaigi 2019: Write a Ruby interpreter in Ruby for Ruby 3-Cookpad developer blog</a>” (in Japanese) or presentation slides (<a href="https://rubykaigi.org/2019/presentations/ko1.html">Write a Ruby interpreter in Ruby for Ruby 3 — RubyKaigi 2019</a>).</p><p>To put it simply, built-in classes such as Array had been written only in C, but now we can write them by easily combining Ruby and C.</p><p>Currently, it is only used by some classes (for example, the definition of TracePoint class is written in a file called trace_point.rb), but I would like to rewrite other classes (:contribution_chance:).</p><p>I will summarize the details of this mechanism in another article.</p><p>The premise of incorporating this mechanism was the size reduction of the compiled instruction sequence described earlier.</p><p>(ko1)</p><h3>Conclusion</h3><p>Ruby 2.7 also has various changes. Please give it a try.</p><p>Next year Ruby 3 is finally scheduled to be released. We are looking forward to it (we hope it gets out properly).</p><p>Happy hacking with Ruby 2.7!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bdbaacb36d0c" width="1" height="1" alt=""><hr><p><a href="https://sourcediving.com/ruby-2-7-news-commentary-by-cookpads-full-time-ruby-comitters-bdbaacb36d0c">Ruby 2.7 NEWS: Commentary by Cookpad’s Full Time Ruby Comitters</a> was originally published in <a href="https://sourcediving.com">Source Diving</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Progressive enhancement as a valuable philosophy in the age of JavaScript]]></title>
            <link>https://sourcediving.com/progressive-enhancement-as-a-valuable-philosophy-in-the-age-of-javascript-aac2e26364d2?source=rss----217db81c7a01--ruby</link>
            <guid isPermaLink="false">https://medium.com/p/aac2e26364d2</guid>
            <category><![CDATA[ruby]]></category>
            <category><![CDATA[ruby-on-rails]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[rails]]></category>
            <category><![CDATA[programming]]></category>
            <dc:creator><![CDATA[Jens Jakob Balvig]]></dc:creator>
            <pubDate>Tue, 05 Nov 2019 09:16:40 GMT</pubDate>
            <atom:updated>2020-08-10T22:26:41.161Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gIjMubYnELyYY9JyXx63LA.jpeg" /></figure><p>Not <em>that</em> long ago, browsers disabling JavaScript or simply lacking support was an important concern when developing for the web.</p><p>Words like <a href="https://en.wikipedia.org/wiki/Progressive_enhancement">“progressive enhancement”</a> and “graceful degradation” were often heard, even spawning <a href="https://www.viget.com/articles/the-case-against-progressive-enhancements-flimsy-moral-foundation/">battles</a> and generally <a href="https://sixtwothree.org/posts/the-practical-case-for-progressive-enhancement">heated discussions</a> on the subject.</p><p>Today, it can pretty much be assumed that JavaScript will work everywhere.</p><p>It is therefore at the risk of reviving a cold case, that we in this post share a few reasons why - in spite of all that - we at Cookpad follow this approach and still believe it to be relevant.</p><h4>OK, let me turn off JavaScript in my browser real quick</h4><p>Note that this <em>isn’t</em> advocating a purist approach of “everything must work with JavaScript disabled”.</p><p>As Rails developers, we are more than happy to make use of the default toolbox of <a href="https://github.com/rails/rails/tree/966d3a7bf2c08c5c20213d29686cbb409a07734a/actionview/app/assets/javascripts">rails-ujs</a> and <a href="https://github.com/turbolinks/turbolinks">turbolinks</a> that makes the framework so powerful.</p><p>Instead, we’re concerned with practical benefits of using the philosophy when considering any JavaScript we layer <em>on top</em> of that foundation.</p><h3><strong>Benefit 1: Drives out good code design</strong></h3><p>When working from a rich design mockup complete with in-page transitions, animations, and all the client-side bells and whistles, the impetus might be to immediately reach for JavaScript.</p><p>Take something like a modal login dialog, that when activated is overlaid on top of the current page content:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QXzbF6m5inPM6-yXV1KnjA.gif" /></figure><p>One might be tempted to implement it by f.ex. having some hidden content in the header template that gets revealed using JavaScript when clicking the link:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8dce4d87217773fb591aa1a17b7de16e/href">https://medium.com/media/8dce4d87217773fb591aa1a17b7de16e/href</a></iframe><p>However, what if we instead, <em>at least as the first version</em>, simply did a full page load showing a blank page with a similar modal-like layout?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2GWOzHZSdZCiC6FfcASo5g.gif" /></figure><p>Certainly, the difference in UX is noticeable, but not <em>hugely</em> different?<br>The implementation could look like something like:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b4fd1dc8aeea84551fa638e2a597d432/href">https://medium.com/media/b4fd1dc8aeea84551fa638e2a597d432/href</a></iframe><p>This immediately gives a few design advantages:</p><ul><li>Everything now sits nicely in a dedicated place: authentication-related HTML can be found in <strong>/authentications</strong>.</li><li>We’re not cluttering our header template with markup that’s hidden most of the time.</li><li>The login page has become linkable, which could come in handy for f.ex. authentication control on other pages.</li><li>Should we need to add any extra logic such as tracking or exposing more variables to the login form, we now have a natural location for it to live:</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/dc095e3d40f4cb8c51b47d682d0e4c82/href">https://medium.com/media/dc095e3d40f4cb8c51b47d682d0e4c82/href</a></iframe><p>We might have struggled with finding a place for this when everything was client-side or been forced to add <em>more</em> JavaScript to make it work.</p><h4>Applying progressive enhancement</h4><p>We could <em>then</em> always explore sprinkling on a bit of JavaScript to get the best of both worlds by f.ex. using AJAX to load the content of our newly created endpoint:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/57fc2c16e2838a204c134788babd08b3/href">https://medium.com/media/57fc2c16e2838a204c134788babd08b3/href</a></iframe><p>This allows us to keep a nice code structure while still having good UX.</p><p>We might not have arrived at this solution had we gone instantly to a JavaScript-first implementation!</p><p>This is just one example, but in much the same way as f.ex. <a href="http://jeromedalbert.com/how-dhh-organizes-his-rails-controllers/">restricting oneself to the 7 RESTful actions of Rails</a> helps reveal missing models or controllers, applying a constraint like progressive enhancement can often tease out a better design.</p><h3>Benefit 2: Faster tests</h3><p>With the advancements of system tests and headless drivers, JavaScript-driven features have now become much easier to test.</p><p>Depending on your framework of choice, to test the full stack, including any JavaScript, in a headless browser, it’s usually a matter of adding the following to a system test:</p><pre>driven_by :selenium</pre><p>These tests however still come at a substantial cost.<br>Using our modal example above, here is what a test might look like:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/589ffa6cdac1c8f105b0be74abe1c600/href">https://medium.com/media/589ffa6cdac1c8f105b0be74abe1c600/href</a></iframe><p>In our app, running this test locally with the default <strong>Rack::Test</strong> driver results in:</p><pre>Finished in 3.55 seconds (files took 2.33 seconds to load)</pre><pre>1 example, 0 failures</pre><p>The same spec run with <strong>Selenium</strong> takes more than <em>3 times</em> as long to run:</p><pre>Finished in 11.26 seconds (files took 2.31 seconds to load)</pre><pre>1 example, 0 failures</pre><p>Now imagine also wanting to test a few edge cases, such as failed login, validations, etc. Those slow specs will quickly add up, and we haven’t even touched on the fact that tests involving JavaScript tend to be more prone to random failures.</p><p>If we have a version that also runs <em>without </em>additional<em> </em>JavaScript, we can reserve the slower Selenium test for just the happy path and take advantage of the faster fallback version for everything else.</p><h3>Benefit 3: Shorter time to release</h3><p>As seen above, adopting the progressive enhancement mindset often pushes developers to think of a simpler version of a new feature.</p><p>Additionally, we don’t have to spend time up front writing and testing JavaScript, so overall we should be able to release the first iteration of the feature more quickly.</p><p>Although the first version may not be “flashy”, designers and end users will be able to get their hands on and start using the feature sooner.</p><p>This is where at times it turns out that the basic version might just be <em>good enough.</em></p><p>At the very least, it allows the team to have a discussion as to whether it’s worth investing more time in enhancing it further.</p><h3>Benefit 4: Smaller pull requests</h3><p>At Cookpad, we care a lot about keeping pull requests small and easy to review.</p><p>One approach we use is <a href="https://sourcediving.com/a-practical-guide-to-small-and-easy-to-review-pull-requests-a7f04a01d5d5">top-down development combined with shipping in-progress code behind feature toggles</a>.</p><p>Progressive enhancement fits very well into that approach!</p><p>If a reviewer can first see an implementation using no more than the basic HTTP request &amp; response cycle, they will be a lot less overwhelmed than if they also have to take a bunch of JavaScript into consideration.</p><p>Any subsequent pull requests enhancing the UX of the existing feature will then allow reviewers to concentrate on the JavaScript and give more focused feedback.</p><p>It’s not unusual to see pull requests like this (note the small diff size):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eauF33leuWSfMRmkDWnBbQ.png" /></figure><p>Dozens of other arguments (fault tolerance, accessibility, low-bandwidth browser support, etc.) could be made for progressive enhancement.</p><p>Equally, in a time where JavaScript runs on everything (including your <a href="http://enyojs.com/">TV</a>), a corresponding number of arguments can be found <em>against.</em></p><p>However, while it may not <em>always</em> be the right choice, we believe the practical benefits achieved by simply having the <em>mindset</em> continue to make it a really appealing way to work.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=aac2e26364d2" width="1" height="1" alt=""><hr><p><a href="https://sourcediving.com/progressive-enhancement-as-a-valuable-philosophy-in-the-age-of-javascript-aac2e26364d2">Progressive enhancement as a valuable philosophy in the age of JavaScript</a> was originally published in <a href="https://sourcediving.com">Source Diving</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>