<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet  type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet  type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:georss="http://www.georss.org/georss" xmlns:gd="http://schemas.google.com/g/2005" xmlns:thr="http://purl.org/syndication/thread/1.0" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><id>tag:blogger.com,1999:blog-8474926331452026626</id><updated>2021-08-31T12:11:33.010-07:00</updated><category term="Machine Learning"/><category term="Deep Learning"/><category term="Computer Vision"/><category term="Google Brain"/><category term="open source"/><category term="Natural Language Processing"/><category term="Publications"/><category term="Research"/><category term="TensorFlow"/><category term="Education"/><category term="Machine Perception"/><category term="University Relations"/><category term="Natural Language Understanding"/><category term="conference"/><category term="Neural Networks"/><category term="conferences"/><category term="datasets"/><category term="Reinforcement Learning"/><category term="Health"/><category term="Robotics"/><category term="AI"/><category term="Research Awards"/><category term="CVPR"/><category term="Computer Science"/><category term="NLP"/><category term="MOOC"/><category term="Computational Photography"/><category term="Machine Intelligence"/><category term="Speech"/><category term="Quantum Computing"/><category term="YouTube"/><category term="Algorithms"/><category term="Machine Translation"/><category term="Visualization"/><category term="On-device Learning"/><category term="Pixel"/><category term="Image Classification"/><category term="Android"/><category term="Awards"/><category term="HCI"/><category term="ICLR"/><category term="Information Retrieval"/><category term="accessibility"/><category term="Audio"/><category term="Hardware"/><category term="Image Processing"/><category term="Structured Data"/><category term="TPU"/><category term="optimization"/><category term="ACL"/><category term="AutoML"/><category term="ICML"/><category term="ML Fairness"/><category term="Quantum AI"/><category term="Security and Privacy"/><category term="Speech Recognition"/><category term="TTS"/><category term="AI for Social Good"/><category term="Google Accelerated Science"/><category term="Physics"/><category term="Search"/><category term="ACM"/><category term="Earth Engine"/><category term="Graph Mining"/><category term="K-12"/><category term="ML"/><category term="NeurIPS"/><category term="Collaboration"/><category term="Diversity"/><category term="Google Maps"/><category term="Google Translate"/><category term="Self-Supervised Learning"/><category term="User Experience"/><category term="Voice Search"/><category term="ph.d. fellowship"/><category term="statistics"/><category term="Automatic Speech Recognition"/><category term="Chemistry"/><category term="DeepMind"/><category term="EMNLP"/><category term="NIPS"/><category term="UI"/><category term="Video Analysis"/><category term="Vision Research"/><category term="grants"/><category term="video"/><category term="Faculty Summit"/><category term="Google Cloud Platform"/><category term="Google Genomics"/><category term="Translate"/><category term="crowd-sourcing"/><category term="data science"/><category term="distributed systems"/><category term="market algorithms"/><category term="Art"/><category term="Augmented Reality"/><category term="Compression"/><category term="Course Builder"/><category term="Environment"/><category term="Google Photos"/><category term="Google+"/><category term="Interspeech"/><category term="Multimodal Learning"/><category term="PhD Fellowship"/><category term="Semi-supervised Learning"/><category term="Supervised Learning"/><category term="WWW"/><category term="Cloud Computing"/><category term="Computational Imaging"/><category term="Data Discovery"/><category term="Expander"/><category term="Fusion Tables"/><category term="Google Books"/><category term="Machine Hearing"/><category term="Moore's Law"/><category term="Ngram"/><category term="Social Networks"/><category term="Software"/><category term="Systems"/><category term="Unsupervised Learning"/><category term="renewable energy"/><category term="schema.org"/><category term="API"/><category term="Acoustic Modeling"/><category term="App Engine"/><category term="Europe"/><category term="Gmail"/><category term="Google Play Apps"/><category term="High Dynamic Range Imaging"/><category term="ICCV"/><category term="Image Annotation"/><category term="Internet of Things"/><category term="Networks"/><category term="Optical Character Recognition"/><category term="Recommender Systems"/><category term="Semantic Models"/><category term="Virtual Reality"/><category term="Year in Review"/><category term="economics"/><category term="internationalization"/><category term="publication"/><category term="search ads"/><category term="wikipedia"/><category term="Adaptive Data Analysis"/><category term="Africa"/><category term="App Inventor"/><category term="China"/><category term="DeepDream"/><category term="EMEA"/><category term="Exacycle"/><category term="Google Drive"/><category term="Google Science Fair"/><category term="Graph"/><category term="Inbox"/><category term="India"/><category term="KDD"/><category term="Kaggle"/><category term="Keyboard Input"/><category term="Labs"/><category term="Low-Light Photography"/><category term="MapReduce"/><category term="Policy"/><category term="Proposals"/><category term="Style Transfer"/><category term="TensorBoard"/><category term="VLDB"/><category term="ads"/><category term="electronics"/><category term="resource optimization"/><category term="trends"/><category term="Android Wear"/><category term="April Fools"/><category term="Australia"/><category term="BigQuery"/><category term="Cantonese"/><category term="Chrome"/><category term="Conservation"/><category term="Data Center"/><category term="Electronic Commerce and Algorithms"/><category term="Encryption"/><category term="Entity Salience"/><category term="Faculty Institute"/><category term="Flu Trends"/><category term="Gboard"/><category term="Google Docs"/><category term="Google Sheets"/><category term="Google Trips"/><category term="Google Voice Search"/><category term="Government"/><category term="ICSE"/><category term="IPython"/><category term="Journalism"/><category term="Klingon"/><category term="Korean"/><category term="Linear Optimization"/><category term="Magenta"/><category term="Market Research"/><category term="Mixed Reality"/><category term="NAACL"/><category term="Network Management"/><category term="Nexus"/><category term="Peer Review"/><category term="PhotoScan"/><category term="PiLab"/><category term="Professional Development"/><category term="Public Data Explorer"/><category term="SIGCOMM"/><category term="SIGMOD"/><category term="Site Reliability Engineering"/><category term="Sound Search"/><category term="TV"/><category term="UNIX"/><category term="Visiting Faculty"/><category term="Wiki"/><category term="adsense"/><category term="adwords"/><category term="correlate"/><category term="entities"/><category term="gamification"/><category term="jsm"/><category term="jsm2011"/><category term="localization"/><category term="operating systems"/><category term="osdi"/><category term="osdi10"/><category term="patents"/><title type="text">Google AI Blog</title><subtitle type="html">The latest news from Google AI.</subtitle><link rel="alternate" type="text/html" href="http://ai.googleblog.com/"><link rel="next" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default?alt=atom&amp;start-index=26&amp;max-results=25&amp;redirect=false"><author><name>ewood</name><uri>http://www.blogger.com/profile/12341551220176883769</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><generator version="7.00" uri="http://www.blogger.com">Blogger</generator><openSearch:totalResults>978</openSearch:totalResults><openSearch:startIndex>1</openSearch:startIndex><openSearch:itemsPerPage>25</openSearch:itemsPerPage><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/blogspot/gJZg"/><feedburner:info uri="blogspot/gjzg"/><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/"/><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-9122495542218916204</id><published>2021-08-31T12:11:00.000-07:00</published><updated>2021-08-31T12:11:00.161-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computational Photography"/><category scheme="http://www.blogger.com/atom/ns#" term="CVPR"/><category scheme="http://www.blogger.com/atom/ns#" term="Video Analysis"/><title type="text">Introducing Omnimattes: A New Approach to Matte Generation using Layered Neural Rendering</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Forrester Cole, Software Engineer and Tali Dekel, Research Scientist&lt;/span&gt; &lt;p&gt;Image and video editing operations often rely on accurate &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Matte_(filmmaking)&quot;&gt;mattes&lt;/a&gt;&lt;/em&gt; — images that define a separation between foreground and background. While recent computer vision techniques can produce high-quality mattes for natural images and videos, allowing real-world applications such as &lt;a href=&quot;https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html&quot;&gt;generating synthetic depth-of-field&lt;/a&gt;, &lt;a href=&quot;https://nvlabs.github.io/SPADE/&quot;&gt;editing and synthesising images&lt;/a&gt;, or &lt;a href=&quot;http://remove.bg&quot;&gt;removing backgrounds from images&lt;/a&gt;, one fundamental piece is missing: the various scene effects that the subject may generate, like shadows, reflections, or smoke, are typically overlooked. &lt;/p&gt;&lt;p&gt;In “&lt;a href=&quot;https://arxiv.org/pdf/2105.06993.pdf&quot;&gt;Omnimatte: Associating Objects and Their Effects in Video&lt;/a&gt;”, presented at &lt;a href=&quot;http://cvpr2021.thecvf.com/&quot;&gt;CVPR 2021&lt;/a&gt;, we describe a new approach to matte generation that leverages &lt;a href=&quot;http://retiming.github.io&quot;&gt;layered neural rendering&lt;/a&gt; to separate a video into layers called &lt;em&gt;omnimattes&lt;/em&gt; that include not only the subjects but also all of the effects related to them in the scene. Whereas a typical state-of-the-art segmentation model extracts masks for the subjects in a scene, for example, a person and a dog, the method proposed here can isolate and extract additional details associated with the subjects, such as shadows cast on the ground.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-Ls9dI4deXPg/YS5Tady9aHI/AAAAAAAAICI/kzxGDtAYNQsI9EBm7id_t3c4kk7Mg-sTwCLcBGAsYHQ/s1384/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;256&quot; data-original-width=&quot;1384&quot; src=&quot;https://1.bp.blogspot.com/-Ls9dI4deXPg/YS5Tady9aHI/AAAAAAAAICI/kzxGDtAYNQsI9EBm7id_t3c4kk7Mg-sTwCLcBGAsYHQ/s16000/image6.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A state-of-the-art segmentation network (e.g., &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;MaskRCNN&lt;/a&gt;) takes an input video (&lt;b&gt;left&lt;/b&gt;) and produces plausible masks for people and animals (&lt;b&gt;middle&lt;/b&gt;), but misses their associated effects. Our method produces mattes that include not only the subjects, but their shadows as well (&lt;b&gt;right&lt;/b&gt;; individual channels for person and dog visualized as blue and green).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Also unlike segmentation masks, omnimattes can capture partially-transparent, soft effects such as reflections, splashes, or tire smoke. Like conventional mattes, omnimattes are &lt;a href=&quot;https://en.wikipedia.org/wiki/RGBA_color_model&quot;&gt;RGBA images&lt;/a&gt; that can be manipulated using widely-available image or video editing tools, and can be used wherever conventional mattes are used, for example, to insert text into a video underneath a smoke trail. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-vN2PKtEEeKc/YS5UOQke4cI/AAAAAAAAICY/XQGVCx2z4H4M_jNz-tBgYRBcYGJEdJqiwCLcBGAsYHQ/s1920/drift.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;866&quot; data-original-width=&quot;1920&quot; src=&quot;https://1.bp.blogspot.com/-vN2PKtEEeKc/YS5UOQke4cI/AAAAAAAAICY/XQGVCx2z4H4M_jNz-tBgYRBcYGJEdJqiwCLcBGAsYHQ/s16000/drift.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Layered Decomposition of Video&lt;/b&gt;&lt;br /&gt;To generate omnimattes, we split the input video into a set of layers: one for each moving subject, and one additional layer for stationary background objects. In the example below, there is one layer for the person, one for the dog, and one for the background. When merged together using conventional &lt;a href=&quot;https://en.wikipedia.org/wiki/Alpha_compositing&quot;&gt;alpha blending&lt;/a&gt;, these layers reproduce the input video. &lt;/p&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-Qq3cN8QyjUg/YS5UYDmPN1I/AAAAAAAAICc/CFjjT0MYe681xNRMcB1wukJSPSQ6lXp4ACLcBGAsYHQ/s1892/image5.gif&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;300&quot; data-original-width=&quot;1892&quot; src=&quot;https://1.bp.blogspot.com/-Qq3cN8QyjUg/YS5UYDmPN1I/AAAAAAAAICc/CFjjT0MYe681xNRMcB1wukJSPSQ6lXp4ACLcBGAsYHQ/s16000/image5.gif&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;Besides reproducing the video, the decomposition must capture the correct effects in each layer. For example, if the person’s shadow appears in the dog’s layer, the merged layers would still reproduce the input video, but inserting an additional element between the person and dog would produce an obvious error. The challenge is to find a decomposition where each subject’s layer captures only that subject’s effects, producing a true omnimatte.  &lt;/p&gt;&lt;p&gt;Our solution is to apply our previously developed layered neural rendering approach to train a &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;&gt;convolutional neural network&lt;/a&gt; (CNN) to map the subject’s segmentation mask and a background noise image into an omnimatte. Due to their structure, CNNs are naturally inclined to learn correlations between image effects, and the stronger the correlation between the effects, the easier for the CNN to learn. In the above video, for example, the spatial relationships between the person and their shadow, and the dog and its shadow, remain similar as they walk from right to left. The relationships change more (hence, the correlations are weaker) between the &lt;em&gt;person&lt;/em&gt; and the &lt;em&gt;dog’s shadow&lt;/em&gt;, or the &lt;em&gt;dog&lt;/em&gt; and the &lt;em&gt;person’s shadow&lt;/em&gt;. The CNN learns the stronger correlations first, leading to the correct decomposition.  &lt;/p&gt;&lt;p&gt;The omnimatte system is shown in detail below. In a preprocess, the user chooses the subjects and specifies a layer for each. A segmentation mask for each subject is extracted using an off-the-shelf segmentation network, such as &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;MaskRCNN&lt;/a&gt;, and camera transformations relative to the background are found using standard camera stabilization tools. A random noise image is defined in the background reference frame and sampled using the camera transformations to produce per-frame noise images. The noise images provide image features that are random but consistently track the background over time, providing a natural input for the CNN to learn to reconstruct the background colors. &lt;/p&gt;&lt;p&gt;The rendering CNN takes as input the segmentation mask and the per-frame noise images and produces the RGB color images and alpha maps, which capture the transparency of each layer. These outputs are merged using conventional alpha-blending to produce the output frame. The CNN is trained from scratch to reconstruct the input frames by finding and associating the effects not captured in a mask (e.g., shadows, reflections or smoke) with the given foreground layer, and to ensure the subject’s alpha roughly includes the segmentation mask. To make sure the foreground layers only capture the foreground elements and none of the stationary background, a sparsity loss is also applied on the foreground alpha. &lt;/p&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-f9_xN5FA7YU/YS5UnO_1NpI/AAAAAAAAICk/IQGAVz3uoewvVyG6E_728h5lIYe9xXX3gCLcBGAsYHQ/s1999/image4.jpg&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;912&quot; data-original-width=&quot;1999&quot; src=&quot;https://1.bp.blogspot.com/-f9_xN5FA7YU/YS5UnO_1NpI/AAAAAAAAICk/IQGAVz3uoewvVyG6E_728h5lIYe9xXX3gCLcBGAsYHQ/s16000/image4.jpg&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;A new rendering network is trained for each video. Because the network is only required to reconstruct the single input video, it is able to capture fine structures and fast motion in addition to separating the effects of each subject, as seen below. In the walking example, the omnimatte includes the shadow cast on the slats of the park bench. In the tennis example, the thin shadow and even the tennis ball are captured. In the soccer example, the shadow of the player and the ball are decomposed into their proper layers (with a slight error when the player’s foot is occluded by the ball). &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-jVTCSl4Xxdw/YS5WEgGzsQI/AAAAAAAAIC4/oHDnCt02wSUj_n_M9j9e7eeKUJCPnBqtgCLcBGAsYHQ/s1958/LNR1.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;260&quot; data-original-width=&quot;1958&quot; src=&quot;https://1.bp.blogspot.com/-jVTCSl4Xxdw/YS5WEgGzsQI/AAAAAAAAIC4/oHDnCt02wSUj_n_M9j9e7eeKUJCPnBqtgCLcBGAsYHQ/s16000/LNR1.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/--uMevLo4TT0/YS5WI-skb8I/AAAAAAAAIC8/ona8JErFlekbSmL4bQfr855k7xJ8Iw-BgCLcBGAsYHQ/s1999/LNR2.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;261&quot; data-original-width=&quot;1999&quot; src=&quot;https://1.bp.blogspot.com/--uMevLo4TT0/YS5WI-skb8I/AAAAAAAAIC8/ona8JErFlekbSmL4bQfr855k7xJ8Iw-BgCLcBGAsYHQ/s16000/LNR2.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-GvZZz55oQDU/YS5WOIFGrCI/AAAAAAAAIDA/iyBI8gu_CwsNpYwZNuhi72ajZCAqu4OcQCLcBGAsYHQ/s1999/image7.gif&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;523&quot; data-original-width=&quot;1999&quot; src=&quot;https://1.bp.blogspot.com/-GvZZz55oQDU/YS5WOIFGrCI/AAAAAAAAIDA/iyBI8gu_CwsNpYwZNuhi72ajZCAqu4OcQCLcBGAsYHQ/s16000/image7.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This basic model already works well, but &lt;a href=&quot;https://omnimatte.github.io/supplementary/index.html#ablations&quot;&gt;one can improve the results&lt;/a&gt; by augmenting the input of the CNN with additional buffers such as optical flow or texture coordinates. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Applications&lt;/b&gt;&lt;br&gt;Once the omnimattes are generated, how can they be used? As shown above, we can remove objects, simply by removing their layer from the composition. We can also duplicate objects, by repeating their layer in the composition. In the example below, the video has been “unwrapped” into a panorama, and the horse duplicated several times to produce a &lt;a href=&quot;https://en.wikipedia.org/wiki/Stroboscopic_effect&quot;&gt;stroboscopic photograph&lt;/a&gt; effect. Note that the shadow that the horse casts on the ground and onto the obstacle is correctly captured. &lt;/p&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-At-5jhezNCY/YS5WZJ4R8tI/AAAAAAAAIDI/R1ss5X7zw5cuLoIloOyWkY9b8RFsSE68gCLcBGAsYHQ/s1652/image2.gif&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;296&quot; data-original-width=&quot;1652&quot; src=&quot;https://1.bp.blogspot.com/-At-5jhezNCY/YS5WZJ4R8tI/AAAAAAAAIDI/R1ss5X7zw5cuLoIloOyWkY9b8RFsSE68gCLcBGAsYHQ/s16000/image2.gif&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;A more subtle, but powerful application is to &lt;em&gt;retime&lt;/em&gt; the subjects. Manipulation of time is widely used in film, but usually requires separate shots for each subject and a controlled filming environment. A decomposition into omnimattes makes retiming effects possible for everyday videos using only post-processing, simply by independently changing the playback rate of each layer. Since the omnimattes are standard RGBA images, this retiming edit can be done using conventional video editing software.  &lt;/p&gt;&lt;p&gt;The video below is decomposed into three layers, one for each child. The children’s initial, unsynchronized jumps are aligned by simply adjusting the playback rate of their layers, producing realistic retiming for the splashes and reflections in the water.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-scdYcnEj-Zs/YS5Wgas87cI/AAAAAAAAIDQ/43nqnHxuRTcl4rSosq-6aM7OrpIgq3aXQCLcBGAsYHQ/s720/image8.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;192&quot; data-original-width=&quot;720&quot; src=&quot;https://1.bp.blogspot.com/-scdYcnEj-Zs/YS5Wgas87cI/AAAAAAAAIDQ/43nqnHxuRTcl4rSosq-6aM7OrpIgq3aXQCLcBGAsYHQ/s16000/image8.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;In the original video (&lt;b&gt;left&lt;/b&gt;), each child jumps at a different time. After editing (&lt;b&gt;right&lt;/b&gt;), everyone jumps together.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;It’s important to consider that any novel technique for manipulating images should be developed and applied responsibly, as it could be misused to produce fake or misleading information. Our technique was developed in accordance with our &lt;a href=&quot;https://ai.google/principles/&quot;&gt;AI Principles&lt;/a&gt; and only allows rearrangement of content already present in the video, but even simple rearrangement can significantly alter the effect of a video, as shown in these examples. Researchers should be aware of these risks.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Future Work&lt;/b&gt;&lt;br&gt;There are a number of exciting directions to improve the quality of the omnimattes. On a practical level, this system currently only supports backgrounds that can be modeled as panoramas, where the position of the camera is fixed. When the camera position moves, the panorama model cannot accurately capture the entire background, and some background elements may clutter the foreground layers (sometimes visible in the above figures). Handling fully general camera motion, such as walking through a room or down a street, would require a 3D background model. Reconstruction of 3D scenes in the presence of moving objects and effects is still a difficult research challenge, but one that has seen promising recent progress. &lt;/p&gt;&lt;p&gt;On a theoretical level, the ability of CNNs to learn correlations is powerful, but still somewhat mysterious, and does not always lead to the expected layer decomposition. While our system allows for manual editing when the automatic result is imperfect, a better solution would be to fully understand the capabilities and limitations of CNNs to learn image correlations. Such an understanding could lead to improved denoising, inpainting, and many other video editing applications besides layer decomposition. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br&gt;&lt;em&gt;Erika Lu, from the University of Oxford, developed the omnimatte system during two internships at Google, in collaboration with Google researchers Forrester Cole, Tali Dekel, Michael Rubinstein, William T. Freeman and David Salesin, and University of Oxford researchers Weidi Xie and Andrew Zisserman.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you to the friends and families of the authors who agreed to appear in the example videos. The “horse jump low”, “lucia”, and “tennis” videos are from the &lt;a href=&quot;https://davischallenge.org/&quot;&gt;DAVIS 2016 dataset&lt;/a&gt;. The soccer video is used by permission from &lt;a href=&quot;https://www.youtube.com/watch?v=70k5fJ4A_6g&amp;amp;t=75s&quot;&gt;Online Soccer Skills&lt;/a&gt;. The car drift video was licensed from Shutterstock. &lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=5iEJxUm_GYw:LukkNC0p9ls:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/5iEJxUm_GYw&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/9122495542218916204/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/08/introducing-omnimattes-new-approach-to.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/9122495542218916204"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/9122495542218916204"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/5iEJxUm_GYw/introducing-omnimattes-new-approach-to.html" title="Introducing Omnimattes: A New Approach to Matte Generation using Layered Neural Rendering"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-Ls9dI4deXPg/YS5Tady9aHI/AAAAAAAAICI/kzxGDtAYNQsI9EBm7id_t3c4kk7Mg-sTwCLcBGAsYHQ/s72-c/image6.gif" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/08/introducing-omnimattes-new-approach-to.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-1379685519283622634</id><published>2021-08-30T10:41:00.000-07:00</published><updated>2021-08-30T10:41:23.850-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="AI for Social Good"/><category scheme="http://www.blogger.com/atom/ns#" term="Health"/><category scheme="http://www.blogger.com/atom/ns#" term="Speech"/><category scheme="http://www.blogger.com/atom/ns#" term="TTS"/><title type="text">Recreating Natural Voices for People with Speech Impairments</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Ye Jia, Software Engineer and Julie Cattiau, Product Manager, Google Research&lt;/span&gt; &lt;p&gt;On June 2nd, 2021, Major League Baseball in the United States celebrated &lt;a href=&quot;https://www.mlb.com/news/lou-gehrig-day-june-2nd-2021-details#:~:text=It%20will%20happen%20on%20June,at%20the%20age%20of%2037.&quot;&gt;Lou Gehrig Day&lt;/a&gt;, commemorating both the day in 1925 that &lt;a href=&quot;https://en.wikipedia.org/wiki/Lou_Gehrig&quot;&gt;Lou Gehrig&lt;/a&gt; became the Yankees’ starting first baseman, and the day in 1941 that he passed away from &lt;a href=&quot;https://en.wikipedia.org/wiki/Amyotrophic_lateral_sclerosis&quot;&gt;amyotrophic lateral sclerosis&lt;/a&gt; (ALS, also known as Lou Gehrig’s disease) at the age of 37. ALS is a &lt;a href=&quot;https://www.als.org/understanding-als&quot;&gt;progressive neurodegenerative disease&lt;/a&gt; that affects motor neurons, which connect the brain with the muscles throughout the body, and govern muscle control and voluntary movements. When voluntary muscle control is affected, people may lose their ability to speak, eat, move and breathe. &lt;/p&gt;&lt;p&gt;In honor of Lou Gehrig, former &lt;a href=&quot;https://en.wikipedia.org/wiki/National_Football_League&quot;&gt;NFL&lt;/a&gt; player and ALS advocate &lt;a href=&quot;https://en.wikipedia.org/wiki/Steve_Gleason&quot;&gt;Steve Gleason&lt;/a&gt;, who lost his ability to speak due to ALS, recited Gehrig’s famous &lt;a href=&quot;https://baseballhall.org/discover-more/stories/baseball-history/lou-gehrig-luckiest-man&quot;&gt;“Luckiest Man” speech&lt;/a&gt; at the June 2nd event using a recreation of his voice generated by a machine learning (ML) model. Gleason’s voice recreation was developed in collaboration with Google’s &lt;a href=&quot;https://sites.research.google/euphonia/about/&quot;&gt;Project Euphonia&lt;/a&gt;, which aims to empower people who have impaired speaking ability due to ALS to better communicate using their own voices.  &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/-YD6LEailYo&quot; width=&quot;640&quot; youtube-src-id=&quot;-YD6LEailYo&quot;&gt;&lt;/iframe&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Steve Gleason, who lost his voice to ALS, worked with Google’s Project Euphonia to generate a speech in his own voice in honor of Lou Gehrig. A portion of Gleason’s speech was broadcast in ballparks across the country during the 4&lt;sup&gt;th&lt;/sup&gt; inning on June 2&lt;sup&gt;nd&lt;/sup&gt;, 2021.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Today we describe PnG NAT, the model adopted by Project Euphonia to recreate Steve Gleason’s voice. PnG NAT is a new &lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_synthesis&quot;&gt;text-to-speech synthesis&lt;/a&gt; (TTS) model that merges two state-of-the-art technologies, &lt;a href=&quot;https://arxiv.org/abs/2103.15060&quot;&gt;PnG BERT&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2010.04301&quot;&gt;Non-Attentive Tacotron&lt;/a&gt; (NAT), into a single model. It demonstrates significantly better quality and fluency than previous technologies, and represents a promising approach that can be extended to a wider array of users. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Recreating a Voice&lt;/b&gt;&lt;br /&gt;Non-Attentive Tacotron (NAT) is the successor to &lt;a href=&quot;https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html&quot;&gt;Tacotron 2&lt;/a&gt;, a sequence-to-sequence neural TTS model proposed in 2017. Tacotron 2 used an attention module to connect the input text sequence and the output speech spectrogram frame sequence, so that the model knows which part of the text to pay attention to when generating each time step of the synthesized speech spectrogram. Tacotron 2 was the first TTS model that was able to synthesize speech that sounds as natural as a person speaking. However, with extensive experimentation we discovered that there is a small probability that the model can suffer from robustness issues — such as babbling, repeating, or skipping part of the text — due to the inherent flexibility of the attention mechanism.  &lt;/p&gt;&lt;p&gt;NAT improves upon Tacotron 2 by replacing the attention module with a duration-based upsampler, which predicts a duration for each input &lt;a href=&quot;https://en.wikipedia.org/wiki/Phoneme&quot;&gt;phoneme&lt;/a&gt; and upsamples the encoded phoneme representation so that the output length corresponds to the length of the predicted speech spectrogram. Such a change both resolves the robustness issue, and improves the naturalness of the synthesized speech. This approach also enables precise control of the speech duration for each phoneme of the input text while still maintaining highly natural synthesis quality. Because recordings of people with ALS often exhibit disfluent speech, this ability to exert per-phoneme control is key for achieving the fluency of the recreated voice. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-XV_ji-X7EFo/YSfC6k9H9JI/AAAAAAAAICA/bUSBpTSQjScN_n8hPkvtPwL18ahmuot0QCLcBGAsYHQ/s419/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;419&quot; data-original-width=&quot;285&quot; height=&quot;640&quot; src=&quot;https://1.bp.blogspot.com/-XV_ji-X7EFo/YSfC6k9H9JI/AAAAAAAAICA/bUSBpTSQjScN_n8hPkvtPwL18ahmuot0QCLcBGAsYHQ/w436-h640/image3.jpg&quot; width=&quot;436&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Non-Attentive Tacotron (NAT) model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;While NAT addresses the robustness issue and enables precise duration control in neural TTS, we build upon it to further improve the natural language understanding of the TTS input. For this, we apply PnG BERT, which uses an approach similar to &lt;a href=&quot;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&quot;&gt;BERT&lt;/a&gt;, but is specifically designed for TTS. It is pre-trained with self-supervision on both the phoneme representation and the grapheme representation of the same content from a large text corpus, and then is used as the encoder of the TTS model. This results in a significant improvement of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Prosody_(linguistics)&quot;&gt;prosody&lt;/a&gt; and pronunciation of the synthesized speech, especially in difficult cases.  &lt;/p&gt;&lt;p&gt;Take, for example, the following audio, which was synthesized from a regular NAT model that takes only phonemes as input: &lt;/p&gt;&lt;table align=&quot;center&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/tacotron/publications/png_bert/two_vs_too/h_nat.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In comparison, the audio synthesized from PnG NAT on the same input text includes an additional pause that makes the meaning more clear.  &lt;/p&gt;&lt;table align=&quot;center&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/tacotron/publications/png_bert/two_vs_too/h_png.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The input text to both models is, “&lt;em&gt;To cancel the payment, press one; or to continue, two.&lt;/em&gt;” Notice the different pause lengths before the ending “&lt;em&gt;two&lt;/em&gt;” in the two versions. The word “&lt;em&gt;two&lt;/em&gt;” in the version output by the regular NAT model could be confused for “&lt;em&gt;too&lt;/em&gt;”. Because “&lt;em&gt;too&lt;/em&gt;” and “&lt;em&gt;two&lt;/em&gt;” have identical pronunciation (and thus the same phoneme representation), the regular NAT model does not understand which of the two is appropriate, and assumes it to be the word that more frequently follows a comma, “&lt;em&gt;too&lt;/em&gt;”. In contrast, the PnG NAT model can more easily tell the difference, because it takes graphemes in addition to phonemes as input, and thus makes more appropriate pause.   &lt;/p&gt;&lt;p&gt;The PnG NAT model integrates the pre-trained PnG BERT model as the encoder to the NAT model. The hidden representations output from the encoder are used by NAT to predict the duration of each phoneme, and are then upsampled to match the length of the audio spectrogram, as outlined above. In the final step, a non-attentive decoder converts the upsampled hidden representations into audio speech spectrograms, which are finally converted into audio waveforms by a neural vocoder. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-ulBEbqJp56o/YSe96CEBKkI/AAAAAAAAIBw/yMpTpbfu4dUREOr-cpNyirdQnkF2pwkNgCLcBGAsYHQ/s747/image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;452&quot; data-original-width=&quot;747&quot; height=&quot;389&quot; src=&quot;https://1.bp.blogspot.com/-ulBEbqJp56o/YSe96CEBKkI/AAAAAAAAIBw/yMpTpbfu4dUREOr-cpNyirdQnkF2pwkNgCLcBGAsYHQ/w640-h389/image2.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;PnG BERT and the pre-training objectives. Yellow boxes represent phonemes, and pink boxes represent graphemes.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-5ELecOClGzk/YSe-AghH9DI/AAAAAAAAIB0/nksuYHxjWasVHldd1W6denDoQ3wqrb91QCLcBGAsYHQ/s604/image1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;447&quot; data-original-width=&quot;604&quot; height=&quot;474&quot; src=&quot;https://1.bp.blogspot.com/-5ELecOClGzk/YSe-AghH9DI/AAAAAAAAIB0/nksuYHxjWasVHldd1W6denDoQ3wqrb91QCLcBGAsYHQ/w640-h474/image1.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;PnG NAT: PnG BERT replaces the original encoder in the NAT model. The random masking for the Masked Language Model (MLM) pre-training is removed.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;To recreate Steve Gleason’s voice, we first trained a PnG NAT model with recordings from 31 professional speakers, and then fine-tuned it with 30 minutes of Gleason’s recordings. Because these latter recordings were made after he was diagnosed with ALS, they exhibit signs of slurring. The fine tuned model was able to synthesize speech that sounds very similar to these recordings. However, because the symptoms of ALS were already present in Gleason’s speech, they exhibited some similar disfluencies.  &lt;/p&gt;&lt;p&gt;To mitigate this, we leveraged the phoneme duration control of NAT as well as the model trained with professional speakers. We first predicted the durations of each phoneme for both a professional speaker and for Gleason, and then used the geometric mean of the two durations for each phoneme to guide the NAT output. As a result, the model is able to speak in Gleason’s voice, but more fluently than in the original recordings. &lt;/p&gt;&lt;p&gt;Here is the full version of the synthesized Lou Gehrig speech in Gleason’s voice: &lt;/p&gt;&lt;table align=&quot;center&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/tacotron/publications/png_bert/gleason/luckiest_man.mp3&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;!-- As a comparison, following is one of Gleason’s recordings that was used to train the model: &lt;/p&gt;&lt;table align=&quot;center&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;TBD&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; --&gt;  &lt;p&gt;Besides recreating voices for people with ALS, PnG NAT is also powering voices for a variety of customers through &lt;a href=&quot;https://cloud.google.com/text-to-speech/custom-voice/docs&quot;&gt;Google Cloud Custom Voice&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Project Euphonia&lt;/b&gt;&lt;br /&gt;Of the millions of people around the world who have neurologic conditions that may impact their speech, such as ALS, &lt;a href=&quot;https://en.wikipedia.org/wiki/Cerebral_palsy&quot;&gt;cerebral palsy&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Down_syndrome&quot;&gt;Down syndrome&lt;/a&gt;, many may find it difficult to be understood, which can make face-to-face communication challenging. Using voice-activated technologies can be frustrating too, as they don’t always work reliably. &lt;a href=&quot;https://sites.research.google/euphonia/about/&quot;&gt;Project Euphonia&lt;/a&gt; is a Google Research initiative focused on helping people with impaired speech be better understood. The team is researching ways to improve speech recognition for individuals with speech impairments (see &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/project-euphonia-1000-hours-speech-recordings/&quot;&gt;recent blog post&lt;/a&gt; and &lt;a href=&quot;https://www.today.com/video/new-google-technology-is-helping-people-with-als-record-their-voices-78999109798 &quot;&gt;segment in TODAY show&lt;/a&gt;), as well as customized text-to-speech technology (see &lt;a href=&quot;https://www.youtube.com/watch?v=V5aZjsWM2wo&quot;&gt;Age of AI documentary&lt;/a&gt; featuring former NFL player Tim Shaw). &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;Many people across Google Research, Google Cloud and Consumer Apps, and Google Accessibility teams contributed to this project and the event, including Michael Brenner, Bob MacDonald, Heiga Zen, Yu Zhang, Jonathan Shen, Isaac Elias‎, Yonghui Wu, Anne Keck, Danielle Notaro, Kevin Hogan, Zack Kaplan, KR Liu, Kyndra Price, Zoe Ortiz.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=X94ceFo3RPU:VYtz9-7L5Ek:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/X94ceFo3RPU&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/1379685519283622634/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/08/recreating-natural-voices-for-people.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1379685519283622634"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1379685519283622634"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/X94ceFo3RPU/recreating-natural-voices-for-people.html" title="Recreating Natural Voices for People with Speech Impairments"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://img.youtube.com/vi/-YD6LEailYo/default.jpg" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/08/recreating-natural-voices-for-people.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-6421549516845301089</id><published>2021-08-12T10:00:00.001-07:00</published><updated>2021-08-12T10:04:36.838-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Audio"/><title type="text">SoundStream: An End-to-End Neural Audio Codec</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Neil Zeghidour, Research Scientist and Marco Tagliasacchi, Staff Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;Audio codecs are used to efficiently compress audio to reduce either storage requirements or network bandwidth. Ideally, audio codecs should be transparent to the end user, so that the decoded audio is perceptually indistinguishable from the original and the encoding/decoding process does not introduce perceivable latency.  &lt;/p&gt;&lt;p&gt;Over the past few years, different audio codecs have been successfully developed to meet these requirements, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Opus_(audio_format)&quot;&gt;Opus&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Enhanced_Voice_Services&quot;&gt;Enhanced Voice Services&lt;/a&gt; (EVS). Opus is a versatile speech and audio codec, supporting bitrates from 6 kbps (kilobits per second) to 510 kbps, which has been widely deployed across applications ranging from video conferencing platforms, like Google Meet, to streaming services, like YouTube. EVS is the latest codec developed by the &lt;a href=&quot;https://www.3gpp.org/about-3gpp&quot;&gt;3GPP standardization body&lt;/a&gt; targeting mobile &lt;a href=&quot;https://en.wikipedia.org/wiki/Telephony&quot;&gt;telephony&lt;/a&gt;. Like Opus, it is a versatile codec operating at multiple bitrates, 5.9 kbps to 128 kbps. The quality of the reconstructed audio using either of these codecs is excellent at medium-to-low bitrates (12–20 kbps), but it degrades sharply when operating at very low bitrates (⪅3 kbps). While these codecs leverage expert knowledge of human perception as well as carefully engineered signal processing pipelines to maximize the efficiency of the compression algorithms, there has been recent interest in replacing these handcrafted pipelines by machine learning approaches that learn to encode audio in a data-driven manner. &lt;/p&gt;&lt;p&gt;Earlier this year, we released &lt;a href=&quot;https://ai.googleblog.com/2021/02/lyra-new-very-low-bitrate-codec-for.html&quot;&gt;Lyra&lt;/a&gt;, a neural audio codec for low-bitrate speech. In “&lt;a href=&quot;https://arxiv.org/abs/2107.03312&quot;&gt;SoundStream: an End-to-End Neural Audio Codec&lt;/a&gt;”, we introduce a novel neural audio codec that extends those efforts by providing higher-quality audio and expanding to encode different sound types, including clean speech, noisy and reverberant speech, music, and environmental sounds. SoundStream is the first neural network codec to work on speech and music, while being able to run in real-time on a smartphone CPU. It is able to deliver state-of-the-art quality over a broad range of bitrates with a single trained model, which represents a significant advance in learnable codecs.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Learning an Audio Codec from Data&lt;/b&gt;&lt;br /&gt;The main technical ingredient of SoundStream is a neural network, consisting of an encoder, decoder and quantizer, all of which are trained end-to-end. The encoder converts the input audio stream into a coded signal, which is compressed using the quantizer and then converted back to audio using the decoder. SoundStream leverages state-of-the-art solutions in the field of neural audio synthesis to deliver audio at high perceptual quality, by training a discriminator that computes a combination of adversarial and reconstruction loss functions that induce the reconstructed audio to sound like the uncompressed original input. Once trained, the encoder and decoder can be run on separate clients to efficiently transmit high-quality audio over a network. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-QziOS4QToRI/YRRARL7JKKI/AAAAAAAAIBM/Sl6W1OPsf0gDR6mn1mDLqaOo7aGhCk8YgCLcBGAsYHQ/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;670&quot; data-original-width=&quot;1999&quot; src=&quot;https://1.bp.blogspot.com/-QziOS4QToRI/YRRARL7JKKI/AAAAAAAAIBM/Sl6W1OPsf0gDR6mn1mDLqaOo7aGhCk8YgCLcBGAsYHQ/s16000/image2.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;SoundStream training and inference. During training, the encoder, quantizer and decoder parameters are optimized using a combination of reconstruction and adversarial losses, computed by a discriminator, which is trained to distinguish between the original input audio and the reconstructed audio. During inference, the encoder and quantizer on a transmitter client send the compressed bitstream to a receiver client that can then decode the audio signal.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Learning a Scalable Codec with Residual Vector Quantization&lt;/b&gt;&lt;br /&gt;The encoder of SoundStream produces vectors that can take an indefinite number of values. In order to transmit them to the receiver using a limited number of bits, it is necessary to replace them by close vectors from a finite set (called a codebook), a process known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_quantization&quot;&gt;vector quantization&lt;/a&gt;. This approach works well at bitrates around 1 kbps or lower, but quickly reaches its limits when using higher bitrates. For example, even at a bitrate as low as 3 kbps, and assuming the encoder produces 100 vectors per second, one would need to store a codebook with more than 1 billion vectors, which is infeasible in practice.  &lt;/p&gt;&lt;p&gt;In SoundStream, we address this issue by proposing a new residual vector quantizer (RVQ), consisting of several layers (up to 80 in our experiments). The first layer quantizes the code vectors with moderate resolution, and each of the following layers processes the residual error from the previous one. By splitting the quantization process in several layers, the codebook size can be reduced drastically. As an example, with 100 vectors per second at 3 kbps, and using 5 quantizer layers, the codebook size goes from 1 billion to 320. Moreover, we can easily increase or decrease the bitrate by adding or removing quantizer layers, respectively. &lt;/p&gt;&lt;p&gt;Because network conditions can vary while transmitting audio, ideally a codec should be “scalable” so that it can change its bitrate from low to high depending on the state of the network. While most traditional codecs are scalable, previous learnable codecs need to be trained and deployed specifically for each bitrate.  &lt;/p&gt;&lt;p&gt;To circumvent this limitation, we leverage the fact that the number of quantization layers in SoundStream controls the bitrate, and propose a new method called “quantizer dropout”. During training, we randomly drop some quantization layers to simulate a varying bitrate. This pushes the decoder to perform well at any bitrate of the incoming audio stream, and thus helps SoundStream to become “scalable” so that a single trained model can operate at any bitrate, performing as well as models trained specifically for these bitrates. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-qllbfBUIQns/YRRAXUXCqKI/AAAAAAAAIBQ/Uzg880iy_PIaqM8nYVUqe1-DOvkRTPxOgCLcBGAsYHQ/s832/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;697&quot; data-original-width=&quot;832&quot; height=&quot;335&quot; src=&quot;https://1.bp.blogspot.com/-qllbfBUIQns/YRRAXUXCqKI/AAAAAAAAIBQ/Uzg880iy_PIaqM8nYVUqe1-DOvkRTPxOgCLcBGAsYHQ/w400-h335/image3.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Comparison of SoundStream models (higher is better) that are trained at 18 kbps with quantizer dropout (bitrate scalable), without quantizer dropout (not bitrate scalable) and evaluated with a variable number of quantizers, or trained and evaluated at a fixed bitrate (bitrate specific). The bitrate-scalable model (a single model for all bitrates) does not lose any quality when compared to bitrate-specific models (a different model for each bitrate), thanks to quantizer dropout.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;A State-of-the-Art Audio Codec&lt;/b&gt;&lt;br /&gt;SoundStream at 3 kbps outperforms Opus at 12 kbps and approaches the quality of EVS at 9.6 kbps, while using 3.2x–4x fewer bits. This means that encoding audio with SoundStream can provide a similar quality while using a significantly lower amount of bandwidth. Moreover, at the same bitrate, SoundStream outperforms the current version of Lyra, which is based on an autoregressive network. Unlike Lyra, which is already deployed and optimized for production usage, SoundStream is still at an experimental stage. In the future, Lyra will incorporate the components of SoundStream to provide both  higher audio quality and reduced complexity. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-tzWjqrUOrvk/YRRAc81_kJI/AAAAAAAAIBU/wVzK2cKGjakFls0JSvcE93G6hFMmhg_bgCLcBGAsYHQ/s1061/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1034&quot; data-original-width=&quot;1061&quot; height=&quot;390&quot; src=&quot;https://1.bp.blogspot.com/-tzWjqrUOrvk/YRRAc81_kJI/AAAAAAAAIBU/wVzK2cKGjakFls0JSvcE93G6hFMmhg_bgCLcBGAsYHQ/w400-h390/image1.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;SoundStream at 3kbps vs. state-of-the-art codecs. &lt;a href=&quot;https://en.wikipedia.org/wiki/MUSHRA&quot;&gt;MUSHRA&lt;/a&gt; score is an indication of subjective quality (the higher the better).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The demonstration of SoundStream’s performance compared to Opus, EVS, and the original Lyra codec is presented in these &lt;a href=&quot;https://google-research.github.io/seanet/soundstream/examples&quot;&gt;audio examples&lt;/a&gt;, a selection of which are provided below.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Speech&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td&gt;Reference&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_noisy_speech_reference.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;Lyra (3kbps)&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_noisy_speech_lyra3kbps.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;Opus (6kbps)&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_noisy_speech_opus6kbps.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;EVS (5.9kbps)&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_noisy_speech_evs5.9kbps.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;SoundStream (3kbps)&amp;nbsp;&amp;nbsp;&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_noisy_speech_ss3kbps.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;b&gt;Music&lt;/b&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td&gt;Reference&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_music_reference.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;Lyra (3kbps)&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_music_lyra3kbps_truncated.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;Opus (6kbps)&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_music_opus6kbps.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;EVS (5.9kbps)&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_music_evs5.9kbps_truncated.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;SoundStream (3kbps)&amp;nbsp;&amp;nbsp;&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_music_ss3kbps.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;b&gt;Joint Audio Compression and Enhancement&lt;/b&gt;&lt;br /&gt;In traditional audio processing pipelines, compression and enhancement (the removal of background noise) are typically performed by different modules. For example, it is possible to apply an audio enhancement algorithm at the transmitter side, before audio is compressed, or at the receiver side, after audio is decoded. In such a setup, each processing step contributes to the end-to-end latency. Conversely, we design SoundStream in such a way that compression and enhancement can be carried out jointly by the same model, without increasing the overall latency. In the following examples, we show that it is possible to combine compression with background noise suppression, by activating and deactivating denoising dynamically (no denoising for 5 seconds, denoising for 5 seconds, no denoising for 5 seconds, etc.). &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td&gt;Original noisy audio&amp;nbsp;&amp;nbsp;&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_controllable_enhancement_noisy.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;Denoised output*&lt;/td&gt; &lt;td&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://www.gstatic.com/soundstream_examples/ai_blog/soundstream_controllable_enhancement_denoised.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;* Demonstrated by turning denoising on and off every 5 seconds.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;Efficient compression is necessary whenever one needs to transmit audio, whether when streaming a video, or during a conference call. SoundStream is an important step towards improving machine learning-driven audio codecs. It outperforms state-of-the-art codecs, such as Opus and EVS, can enhance audio on demand, and requires deployment of only a single scalable model, rather than many.  &lt;/p&gt;&lt;p&gt;SoundStream will be released as a part of the next, improved version of Lyra. By integrating SoundStream with Lyra, developers can leverage the existing Lyra APIs and tools for their work, providing both flexibility and better sound quality. We will also release it as a separate TensorFlow model for experimentation. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgments&lt;/b&gt;&lt;em&gt;The work described here was authored by Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund and Marco Tagliasacchi. We are grateful for all discussions and feedback on this work that we received from our colleagues at Google. &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=5D4f5NMIiY8:ztmTLl0ynss:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/5D4f5NMIiY8&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/6421549516845301089/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6421549516845301089"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6421549516845301089"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/5D4f5NMIiY8/soundstream-end-to-end-neural-audio.html" title="SoundStream: An End-to-End Neural Audio Codec"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-QziOS4QToRI/YRRARL7JKKI/AAAAAAAAIBM/Sl6W1OPsf0gDR6mn1mDLqaOo7aGhCk8YgCLcBGAsYHQ/s72-c/image2.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-714439963092461349</id><published>2021-08-11T12:21:00.000-07:00</published><updated>2021-08-11T12:21:23.460-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Quantum AI"/><category scheme="http://www.blogger.com/atom/ns#" term="Quantum Computing"/><title type="text">Demonstrating the Fundamentals of Quantum Error Correction</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Jimmy Chen, Quantum Research Scientist and Matt McEwen, Student Researcher, Google Quantum AI&lt;/span&gt; &lt;p&gt;The &lt;a href=&quot;https://quantumai.google/&quot;&gt;Google Quantum AI&lt;/a&gt; team has been building quantum processors made of superconducting quantum bits (&lt;a href=&quot;https://en.wikipedia.org/wiki/Qubit&quot;&gt;qubits&lt;/a&gt;) that have achieved the first &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;&gt;beyond-classical computation&lt;/a&gt;, as well as the &lt;a href=&quot;https://ai.googleblog.com/2020/08/scaling-up-fundamental-quantum.html&quot;&gt;largest quantum chemical simulations&lt;/a&gt; to date. However, current generation quantum processors still have high operational error rates — in the range of 10&lt;sup&gt;-3&lt;/sup&gt; per operation, compared to the 10&lt;sup&gt;-12&lt;/sup&gt; believed to be necessary for a variety of useful algorithms. Bridging this tremendous gap in error rates will require more than just making better qubits — quantum computers of the future will have to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_error_correction&quot;&gt;quantum error correction&lt;/a&gt; (QEC). &lt;/p&gt;&lt;p&gt;The core idea of QEC is to make a &lt;em&gt;logical qubit&lt;/em&gt; by distributing its quantum state across many physical data qubits. When a physical error occurs, one can detect it by repeatedly checking certain properties of the qubits, allowing it to be corrected, preventing any error from occurring on the logical qubit state. While logical errors may still occur if a series of physical qubits experience an error together, this error rate should exponentially decrease with the addition of more physical qubits (more physical qubits need to be involved to cause a logical error). This exponential scaling behavior relies on physical qubit errors being sufficiently rare &lt;em&gt;and&lt;/em&gt; independent. In particular, it’s important to suppress &lt;em&gt;correlated errors&lt;/em&gt;, where one physical error simultaneously affects many qubits at once or persists over many cycles of error correction. Such correlated errors produce more complex patterns of error detections that are more difficult to correct and more easily cause logical errors. &lt;/p&gt;&lt;p&gt;Our team has recently implemented the ideas of QEC in our Sycamore architecture using &lt;em&gt;quantum repetition codes&lt;/em&gt;. These codes consist of one-dimensional chains of qubits that alternate between &lt;em&gt;data qubits, &lt;/em&gt;which encode the logical qubit, and&lt;em&gt; measure qubits&lt;/em&gt;, which we use to detect errors in the logical state. While these repetition codes can only correct for one kind of quantum error at a time&lt;sup id=&quot;fnref1&quot;&gt;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;1&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;, they contain all of the same ingredients as more sophisticated error correction codes and require fewer physical qubits per logical qubit, allowing us to better explore how logical errors decrease as logical qubit size grows.  &lt;/p&gt;&lt;p&gt;In “&lt;a href=&quot;https://www.nature.com/articles/s41467-021-21982-y&quot;&gt;Removing leakage-induced correlated errors in superconducting quantum error correction&lt;/a&gt;”, published in &lt;em&gt;&lt;a href=&quot;https://www.nature.com/ncomms/&quot;&gt;Nature Communications&lt;/a&gt;&lt;/em&gt;, we use these repetition codes to demonstrate a new technique for reducing the amount of correlated errors in our physical qubits. Then, in “&lt;a href=&quot;https://www.nature.com/articles/s41586-021-03588-y&quot;&gt;Exponential suppression of bit or phase flip errors with repetitive error correction&lt;/a&gt;”, published in &lt;em&gt;&lt;a href=&quot;https://www.nature.com/&quot;&gt;Nature&lt;/a&gt;&lt;/em&gt;, we show that the logical errors of these repetition codes are exponentially suppressed as we add more and more physical qubits, consistent with expectations from QEC theory. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-oVwpb0C4-k4/YRPTld47TKI/AAAAAAAAIAs/30GZI99UwdUSFTQI8J8sBu3VRUZJElu3wCLcBGAsYHQ/s456/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;408&quot; data-original-width=&quot;456&quot; height=&quot;358&quot; src=&quot;https://1.bp.blogspot.com/-oVwpb0C4-k4/YRPTld47TKI/AAAAAAAAIAs/30GZI99UwdUSFTQI8J8sBu3VRUZJElu3wCLcBGAsYHQ/w400-h358/image5.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Layout of the repetition code (21 qubits, 1D chain) and distance-2 surface code (7 qubits) on the Sycamore device.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Leaky Qubits&lt;/b&gt;&lt;br/&gt;The goal of the repetition code is to detect errors on the data qubits without measuring their states directly. It does so by &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_entanglement&quot;&gt;entangling&lt;/a&gt; each pair of data qubits with their shared measure qubit in a way that tells us whether those data qubit states are the same or different (i.e., their &lt;em&gt;parity)&lt;/em&gt; without telling us the states themselves. We repeat this process over and over in &lt;em&gt;rounds&lt;/em&gt; that last only one microsecond. When the measured parities change between rounds, we’ve detected an error. &lt;/p&gt;&lt;p&gt;However, one key challenge stems from how we make qubits out of superconducting circuits. While a qubit needs only two energy states, which are usually labeled &lt;text style=&quot;font-family:Verdana&quot;&gt;|&lt;/text&gt;0&lt;text style=&quot;font-family:Courier New&quot;&gt;&amp;#10217;&lt;/text&gt; and &lt;text style=&quot;font-family:Verdana&quot;&gt;|&lt;/text&gt;1&lt;text style=&quot;font-family:Courier New&quot;&gt;&amp;#10217;&lt;/text&gt;, our devices feature a ladder of energy states, &lt;text style=&quot;font-family:Verdana&quot;&gt;|&lt;/text&gt;0&lt;text style=&quot;font-family:Courier New&quot;&gt;&amp;#10217;&lt;/text&gt;, &lt;text style=&quot;font-family:Verdana&quot;&gt;|&lt;/text&gt;1&lt;text style=&quot;font-family:Courier New&quot;&gt;&amp;#10217;&lt;/text&gt;, &lt;text style=&quot;font-family:Verdana&quot;&gt;|&lt;/text&gt;2&lt;text style=&quot;font-family:Courier New&quot;&gt;&amp;#10217;&lt;/text&gt;, &lt;text style=&quot;font-family:Verdana&quot;&gt;|&lt;/text&gt;3&lt;text style=&quot;font-family:Courier New&quot;&gt;&amp;#10217;&lt;/text&gt;, and so on. We use the two lowest energy states to encode our qubit with information to be used for computation (we call these the &lt;em&gt;computational states)&lt;/em&gt;. We use the higher energy states (&lt;text style=&quot;font-family:Verdana&quot;&gt;|&lt;/text&gt;2&lt;text style=&quot;font-family:Courier New&quot;&gt;&amp;#10217;&lt;/text&gt;, &lt;text style=&quot;font-family:Verdana&quot;&gt;|&lt;/text&gt;3&lt;text style=&quot;font-family:Courier New&quot;&gt;&amp;#10217;&lt;/text&gt; and higher) to help achieve high-fidelity entangling operations, but these entangling operations can sometimes allow the qubit to “leak” into these higher states, earning them the name &lt;em&gt;leakage states.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Population in the leakage states builds up as operations are applied, which increases the error of subsequent operations and even causes other nearby qubits to leak as well — resulting in a particularly challenging source of correlated error. In our early &lt;a href=&quot;https://ai.googleblog.com/2015/03/a-step-closer-to-quantum-computation.html&quot;&gt;2015 experiments&lt;/a&gt; on error correction, we observed that as more rounds of error correction were applied, performance declined as leakage began to build. &lt;/p&gt;&lt;p&gt;Mitigating the impact of leakage required us to develop a new kind of qubit operation that could “empty out” leakage states, called &lt;em&gt;multi-level reset&lt;/em&gt;. We manipulate the qubit to rapidly pump energy out into the structures used for readout, where it will quickly move off the chip, leaving the qubit cooled to the &lt;text style=&quot;font-family:Verdana&quot;&gt;|&lt;/text&gt;0&lt;text style=&quot;font-family:Courier New&quot;&gt;&amp;#10217;&lt;/text&gt; state, even if it started in &lt;text style=&quot;font-family:Verdana&quot;&gt;|&lt;/text&gt;2&lt;text style=&quot;font-family:Courier New&quot;&gt;&amp;#10217;&lt;/text&gt; or &lt;text style=&quot;font-family:Verdana&quot;&gt;|&lt;/text&gt;3&lt;text style=&quot;font-family:Courier New&quot;&gt;&amp;#10217;&lt;/text&gt;. Applying this operation to the data qubits would destroy the logical state we’re trying to protect, but we can apply it to the measure qubits without disturbing the data qubits. Resetting the measure qubits at the end of every round dynamically stabilizes the device so leakage doesn’t continue to grow and spread, allowing our devices to behave more like ideal qubits.  &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-bjLIFmu_yHw/YRPT4droTfI/AAAAAAAAIA0/frCb0p1tQwwvrgV0o0V2--BrjQNJrsSdwCLcBGAsYHQ/s686/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;578&quot; data-original-width=&quot;686&quot; height=&quot;540&quot; src=&quot;https://1.bp.blogspot.com/-bjLIFmu_yHw/YRPT4droTfI/AAAAAAAAIA0/frCb0p1tQwwvrgV0o0V2--BrjQNJrsSdwCLcBGAsYHQ/w640-h540/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Applying the multi-level reset gate to the measure qubits almost totally removes leakage, while also reducing the growth of leakage on the data qubits.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Exponential Suppression&lt;/b&gt;&lt;br/&gt;Having mitigated leakage as a significant source of correlated error, we next set out to test whether the repetition codes give us the predicted exponential reduction in error when increasing the number of qubits. Every time we run our repetition code, it produces a collection of error detections. Because the detections are linked to pairs of qubits rather than individual qubits, we have to look at all of the detections to try to piece together where the errors have occurred, a procedure known as &lt;em&gt;decoding&lt;/em&gt;. Once we’ve decoded the errors, we then know which corrections we need to apply to the data qubits. However, decoding can fail if there are too many error detections for the number of data qubits used, resulting in a logical error.  &lt;/p&gt;&lt;p&gt;To test our repetition codes, we run codes with sizes ranging from 5 to 21 qubits while also varying the number of error correction rounds. We also run two different types of repetition codes — either a phase-flip code or bit-flip code — that are sensitive to different kinds of quantum errors. By finding the logical error probability as a function of the number of rounds, we can fit a logical error rate for each code size and code type. In our data, we see that the logical error rate does in fact get suppressed exponentially as the code size is increased. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-k1hbFfMrFyM/YRPUCD8Yz1I/AAAAAAAAIA4/mXTgHD1kOrksK-UnO0KG4qTWm2dPRFIrwCLcBGAsYHQ/s960/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;752&quot; data-original-width=&quot;960&quot; height=&quot;502&quot; src=&quot;https://1.bp.blogspot.com/-k1hbFfMrFyM/YRPUCD8Yz1I/AAAAAAAAIA4/mXTgHD1kOrksK-UnO0KG4qTWm2dPRFIrwCLcBGAsYHQ/w640-h502/image3.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Probability of getting a logical error after decoding versus number of rounds run, shown for various sizes of phase-flip repetition code.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;We can quantify the error suppression with the error scaling parameter Lambda (Λ), where a Lambda value of 2 means that we halve the logical error rate every time we add four data qubits to the repetition code. In our experiments, we find Lambda values of 3.18 for the phase-flip code and 2.99 for the bit-flip code. We can compare these experimental values to a numerical simulation of the expected Lambda based on a simple error model with no correlated errors, which predicts values of 3.34 and 3.78 for the bit- and phase-flip codes respectively. &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-xh1usf6Um9I/YRPUKIXdf2I/AAAAAAAAIBA/1rORJ1sxFD4EmL7TldvFA10CQpk3StZYgCLcBGAsYHQ/s688/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;506&quot; data-original-width=&quot;688&quot; height=&quot;470&quot; src=&quot;https://1.bp.blogspot.com/-xh1usf6Um9I/YRPUKIXdf2I/AAAAAAAAIBA/1rORJ1sxFD4EmL7TldvFA10CQpk3StZYgCLcBGAsYHQ/w640-h470/image4.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Logical error rate per round versus number of qubits for the phase-flip (X) and bit-flip (Z) repetition codes. The line shows an exponential decay fit, and Λ is the scale factor for the exponential decay.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;This is the first time Lambda has been measured in any platform while performing multiple rounds of error detection. We’re especially excited about how close the experimental and simulated Lambda values are, because it means that our system can be described with a fairly simple error model without many unexpected errors occurring. Nevertheless, the agreement is not perfect, indicating that there’s more research to be done in understanding the non-idealities of our QEC architecture, including additional sources of correlated errors. &lt;/p&gt;&lt;p&gt;&lt;b&gt;What’s Next&lt;/b&gt;&lt;br/&gt;This work demonstrates two important prerequisites for QEC: first, the Sycamore device can run many rounds of error correction without building up errors over time thanks to our new reset protocol, and second, we were able to validate QEC theory and error models by showing exponential suppression of error in a repetition code. These experiments were the largest stress test of a QEC system yet, using 1000 entangling gates and 500 qubit measurements in our largest test. We’re looking forward to taking what we learned from these experiments and applying it to our target QEC architecture, the 2D surface code, which will require even more qubits with even better performance.&lt;/p&gt;   &lt;!--Footnotes--&gt;&lt;hr width=&quot;80%&quot; /&gt;&lt;p&gt;  &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;&gt;&lt;sup&gt;&lt;a name=&quot;fn1&quot;&gt;&lt;b&gt;1&lt;/b&gt;&lt;/a&gt;&lt;/sup&gt;A true quantum error correcting code would require a two dimensional array of qubits in order to correct for all of the errors that could occur.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;&gt;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;       &lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=Tel7R_hqJh8:mVJRq2VljN8:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/Tel7R_hqJh8&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/714439963092461349/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/08/demonstrating-fundamentals-of-quantum.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/714439963092461349"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/714439963092461349"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/Tel7R_hqJh8/demonstrating-fundamentals-of-quantum.html" title="Demonstrating the Fundamentals of Quantum Error Correction"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-oVwpb0C4-k4/YRPTld47TKI/AAAAAAAAIAs/30GZI99UwdUSFTQI8J8sBu3VRUZJElu3wCLcBGAsYHQ/s72-w400-h358-c/image5.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/08/demonstrating-fundamentals-of-quantum.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-666974994619027383</id><published>2021-08-10T10:05:00.001-07:00</published><updated>2021-08-10T10:05:22.122-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="datasets"/><category scheme="http://www.blogger.com/atom/ns#" term="Natural Language Processing"/><title type="text">The C4_200M Synthetic Dataset for Grammatical Error Correction</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Felix Stahlberg and Shankar Kumar, Research Scientists, Google Research&lt;/span&gt; &lt;p&gt;Grammatical error correction (GEC) attempts to model grammar and other types of writing errors in order to provide &lt;a href=&quot;https://support.google.com/docs/answer/57859?hl=en&amp;amp;co=GENIE.Platform%3DDesktop&quot;&gt;grammar and spelling suggestions&lt;/a&gt;, improving the quality of written output in documents,  emails, blog posts and even informal chats.  Over the past 15 years, there has been a substantial improvement in GEC quality, which can in large part be credited to &lt;a href=&quot;https://aclanthology.org/P06-1032/&quot;&gt;recasting the problem as a &quot;translation&quot; task&lt;/a&gt;. When introduced in Google Docs, for example, this approach resulted in a &lt;a href=&quot;https://cloud.google.com/blog/products/productivity-collaboration/using-neural-machine-translation-to-correct-grammatical-in-google-docs&quot;&gt;significant increase&lt;/a&gt; in the number of accepted grammar correction suggestions. &lt;/p&gt;&lt;p&gt;One of the biggest challenges for GEC models, however, is data sparsity. Unlike other &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;&gt;natural language processing&lt;/a&gt; (NLP) tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_recognition&quot;&gt;speech recognition&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_translation&quot;&gt;machine translation&lt;/a&gt;, there is very limited training data available for GEC, even for high-resource languages like English. A common remedy for this is to generate synthetic data using a range of techniques, from heuristic-based random word- or character-level corruptions to model-based approaches. However, such methods tend to be simplistic and do not reflect the true distribution of error types from actual users. &lt;/p&gt;&lt;p&gt;In “&lt;a href=&quot;https://aclanthology.org/2021.bea-1.4/&quot;&gt;Synthetic Data Generation for Grammatical Error Correction with Tagged Corruption Models&lt;/a&gt;”, presented at the &lt;a href=&quot;https://sig-edu.org/bea/current&quot;&gt;EACL 16th Workshop on Innovative Use of NLP for Building Educational Applications&lt;/a&gt;, we introduce &lt;em&gt;tagged corruption models&lt;/em&gt;. Inspired by the popular &lt;a href=&quot;https://aclanthology.org/P16-1009/&quot;&gt;back-translation&lt;/a&gt; data synthesis technique for machine translation, this approach enables the precise control of synthetic data generation, ensuring diverse outputs that are more consistent with the distribution of errors seen in practice. We used tagged corruption models to generate a new &lt;a href=&quot;https://github.com/google-research-datasets/C4_200M-synthetic-dataset-for-grammatical-error-correction&quot;&gt;200M sentence dataset&lt;/a&gt;, which we have released in order to provide researchers with realistic pre-training data for GEC. By integrating this new dataset into our training pipeline, we were able to significantly improve on GEC baselines. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Tagged Corruption Models&lt;/b&gt;&lt;br /&gt;The idea behind applying a conventional corruption model to GEC is to begin with a grammatically correct sentence and then to “corrupt” it by adding errors. A corruption model can be easily trained by switching the source and target sentences in existing GEC datasets, a method that &lt;a href=&quot;https://aclanthology.org/D19-1119.pdf&quot;&gt;previous studies&lt;/a&gt; have shown that can be very effective for generating improved GEC datasets. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-Zx6Flw-RtQw/YRKkRss4C_I/AAAAAAAAIAU/4q9TyaAKxgoDZtwjRtbe6qe1YTRq6rRsgCLcBGAsYHQ/s898/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;145&quot; data-original-width=&quot;898&quot; height=&quot;65&quot; src=&quot;https://1.bp.blogspot.com/-Zx6Flw-RtQw/YRKkRss4C_I/AAAAAAAAIAU/4q9TyaAKxgoDZtwjRtbe6qe1YTRq6rRsgCLcBGAsYHQ/w400-h65/image1.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A conventional corruption model generates an ungrammatical sentence (red) given a clean input sentence (green).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The tagged corruption model that we propose builds on this idea by taking a clean sentence as input along with an error type tag that describes the kind of error one wishes to reproduce. It then generates an ungrammatical version of the input sentence that contains the given error type. Choosing different error types for different sentences increases the diversity of corruptions compared to a conventional corruption model. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-uCHKxsDfiMs/YRKkZaOh5wI/AAAAAAAAIAY/yFPMFMdXGbgV-6ttFnvCecQYYmAUzDuAQCLcBGAsYHQ/s905/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;395&quot; data-original-width=&quot;905&quot; height=&quot;175&quot; src=&quot;https://1.bp.blogspot.com/-uCHKxsDfiMs/YRKkZaOh5wI/AAAAAAAAIAY/yFPMFMdXGbgV-6ttFnvCecQYYmAUzDuAQCLcBGAsYHQ/w400-h175/image3.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Tagged corruption models generate corruptions (red) for the clean input sentence (green) depending on the error type tag. A determiner error may lead to dropping the “a”, whereas a noun-inflection error may produce the incorrect plural “sheeps”.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;To use this model for data generation we first randomly selected 200M clean sentences from the &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;&gt;C4 corpus&lt;/a&gt;, and assigned an error type tag to each sentence such that their relative frequencies matched the error type tag distribution of the small development set &lt;a href=&quot;https://aclanthology.org/W19-4406.pdf&quot;&gt;BEA-dev&lt;/a&gt;. Since &lt;a href=&quot;https://aclanthology.org/W19-4406.pdf&quot;&gt;BEA-dev&lt;/a&gt; is a carefully curated set that covers a wide range of different English proficiency levels, we expect its tag distribution to be representative for writing errors found in the wild. We then used a tagged corruption model to synthesize the source sentence. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-p-MlaFmtZdk/YRKkeiueWKI/AAAAAAAAIAc/G5toJpx3nHILZ6u_hGypQISNvKnOxCr3QCLcBGAsYHQ/s1213/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;545&quot; data-original-width=&quot;1213&quot; height=&quot;288&quot; src=&quot;https://1.bp.blogspot.com/-p-MlaFmtZdk/YRKkeiueWKI/AAAAAAAAIAc/G5toJpx3nHILZ6u_hGypQISNvKnOxCr3QCLcBGAsYHQ/w640-h288/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Synthetic data generation with tagged corruption models. The clean C4 sentences (green) are paired with the corrupted sentences (red) in the synthetic GEC training corpus. The corrupted sentences are generated using a tagged corruption model by following the error type frequencies in the development set (bar chart).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Results&lt;/b&gt;&lt;br /&gt;In our experiments, tagged corruption models outperformed untagged corruption models on two standard development sets (&lt;a href=&quot;https://aclanthology.org/W13-3601.pdf&quot;&gt;CoNLL-13&lt;/a&gt; and &lt;a href=&quot;https://aclanthology.org/W19-4406.pdf&quot;&gt;BEA-dev&lt;/a&gt;) by more than three F0.5-points (a &lt;a href=&quot;https://aclanthology.org/P17-1074/&quot;&gt;standard metric&lt;/a&gt; in GEC research that combines &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;&gt;precision and recall&lt;/a&gt; with more weight on precision), advancing the state-of-the-art on the two widely used academic test sets, &lt;a href=&quot;https://aclanthology.org/W14-1701.pdf&quot;&gt;CoNLL-14&lt;/a&gt; and &lt;a href=&quot;https://aclanthology.org/W19-4406.pdf&quot;&gt;BEA-test&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;In addition, the use of tagged corruption models not only yields gains on standard GEC test sets, it is also able to adapt GEC systems to the proficiency levels of users. This could be useful, for example, because the error tag distribution for native English writers often differs significantly from the distributions for non-native English speakers. For example, native speakers tend to make more punctuation and spelling mistakes, whereas determiner errors (e.g., missing or superfluous articles, like “a”, “an” or “the”) are more common in text from non-native writers. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;Neural sequence models are notoriously data-hungry, but the availability of annotated training data for grammatical error correction is rare. Our new &lt;a href=&quot;https://github.com/google-research-datasets/C4_200M-synthetic-dataset-for-grammatical-error-correction&quot;&gt;C4_200M corpus&lt;/a&gt; is a synthetic dataset containing diverse grammatical errors, which yields state-of-the-art performance when used to pre-train GEC systems. By releasing the dataset we hope to provide GEC researchers with a valuable resource to train strong baseline systems. &lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=8aK9rONXDvA:gykdO2p2itI:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/8aK9rONXDvA&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/666974994619027383/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/08/the-c4200m-synthetic-dataset-for.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/666974994619027383"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/666974994619027383"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/8aK9rONXDvA/the-c4200m-synthetic-dataset-for.html" title="The C4_200M Synthetic Dataset for Grammatical Error Correction"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-Zx6Flw-RtQw/YRKkRss4C_I/AAAAAAAAIAU/4q9TyaAKxgoDZtwjRtbe6qe1YTRq6rRsgCLcBGAsYHQ/s72-w400-h65-c/image1.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/08/the-c4200m-synthetic-dataset-for.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-4880389433895154885</id><published>2021-08-09T13:02:00.000-07:00</published><updated>2021-08-09T13:02:53.562-07:00</updated><title type="text">A Dataset Exploration Case Study with Know Your Data</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Mark Díaz and Emily Denton, Research Scientists, Google Research, Ethical AI Team&lt;/span&gt; &lt;p&gt;Data underlies much of machine learning (ML) research and development, helping to structure what a machine learning algorithm learns and how models are evaluated and benchmarked. However, data collection and labeling can be complicated by unconscious biases, data access limitations and privacy concerns, among other challenges. As a result, machine learning datasets can reflect unfair social biases along dimensions of race, gender, age, and more. &lt;/p&gt;&lt;p&gt;Methods of examining datasets that can surface information about how different social groups are represented within are a key component of ensuring development of ML models and datasets is aligned with our &lt;a href=&quot;https://ai.google/principles/&quot;&gt;AI Principles&lt;/a&gt;. Such methods can inform the responsible use of ML datasets and point toward potential mitigations of unfair outcomes. For example, prior research has demonstrated that some object recognition datasets are &lt;a href=&quot;https://research.google/pubs/pub46553/&quot;&gt;biased&lt;/a&gt; toward images sourced from North America and Western Europe, prompting Google’s &lt;a href=&quot;https://research.google/tools/datasets/open-images-extended-crowdsourced/&quot;&gt;Crowdsource&lt;/a&gt; effort to balance out image representations in other parts of the world.  &lt;/p&gt;&lt;p&gt;Today, we demonstrate some of the functionality of a dataset exploration tool, &lt;a href=&quot;https://knowyourdata.withgoogle.com/&quot;&gt;Know Your Data&lt;/a&gt; (KYD), recently introduced at Google I/O, using the &lt;a href=&quot;https://arxiv.org/pdf/1504.00325.pdf&quot;&gt;COCO Captions&lt;/a&gt; dataset as a case study. Using this tool, we find a range of gender and age biases in COCO Captions — biases that can be traced to both dataset collection and annotation practices. KYD is a dataset analysis tool that complements the growing suite of responsible AI tools being developed across Google and the broader research community. Currently, KYD only supports analysis of a small set of image datasets, but we’re working hard to make the tool accessible beyond this set. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Introducing Know Your Data&lt;/b&gt;&lt;br /&gt; Know Your Data helps ML research, product and compliance teams understand datasets, with the goal of improving data quality, and thus helping to mitigate fairness and bias issues. KYD offers a range of features that allow users to explore and examine machine learning datasets — users can filter, group, and study correlations based on annotations already present in a given dataset. KYD also presents automatically computed labels from Google’s &lt;a href=&quot;https://cloud.google.com/vision&quot;&gt;Cloud Vision API&lt;/a&gt;, providing users with a simple way to explore their data based on signals that weren’t originally present in the dataset. &lt;/p&gt;&lt;p&gt;&lt;b&gt;A KYD Case Study&lt;/b&gt;&lt;br /&gt;As a case study, we explore some of these features using the &lt;a href=&quot;https://arxiv.org/pdf/1504.00325.pdf&quot;&gt;COCO Captions dataset&lt;/a&gt;, an image dataset that contains five human-generated captions for each of over 300k images. Given the rich annotations provided by free-form text, we focus our analysis on signals already present within the dataset.&lt;/p&gt;  &lt;p&gt;&lt;b&gt;Exploring Gender Bias&lt;/b&gt;&lt;br /&gt;Previous research has demonstrated undesirable gender biases within computer vision datasets, including &lt;a href=&quot;https://arxiv.org/abs/2006.16923&quot;&gt;pornographic imagery of women&lt;/a&gt; and &lt;a href=&quot;http://markyatskar.com/publications/bias.pdf&quot;&gt;image label correlations&lt;/a&gt; that align with harmful gender stereotypes. We use KYD to explore gender biases within COCO Captions by examining gendered correlations within the image captions. We find a gender bias in the depiction of different activities across the images in the dataset, as well as biases relating to how people of different genders are described by annotators. &lt;/p&gt;&lt;p&gt;The first part of our analysis aimed to surface gender biases with respect to different activities depicted in the dataset. We examined images captioned with words describing different activities and analyzed their relation to gendered caption words, such as “man” or “woman”. The &lt;a href=&quot;https://knowyourdata.withgoogle.com/docs/#relations&quot;&gt;KYD Relations tab&lt;/a&gt; makes it easy to examine the relation between two different signals in a dataset by visualizing the extent to which two signals co-occur more (or less) than would be expected by chance. Each cell indicates either a positive (blue color) or negative (orange color) correlation between two specific signal values along with the strength of that correlation.  &lt;/p&gt;&lt;p&gt;KYD also allows users to filter rows of a relations table based on substring matching. Using this functionality, we initially probed for caption words containing “-ing”, as a simple way to filter by verbs. We immediately &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/#dataset=coco_captions&amp;amp;tab=RELATIONS&amp;amp;relations=kyd%2Fcoco_captions%2Fcaptions_words,kyd%2Fcoco_captions%2Fcaptions_words_gendered_groups&amp;amp;draw=kyd/coco_captions/objects_label,bbox,bbox&amp;amp;auto_draw=false&quot;&gt;saw strong gendered correlations&lt;/a&gt;: &lt;/p&gt;     &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-jyz_dnAXfn4/YRFcv9cWZ5I/AAAAAAAAH_U/PZsSbYZa7nEl-QYfwF4E6d-iYmxvgikMQCLcBGAsYHQ/s999/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;812&quot; data-original-width=&quot;999&quot; height=&quot;520&quot; src=&quot;https://1.bp.blogspot.com/-jyz_dnAXfn4/YRFcv9cWZ5I/AAAAAAAAH_U/PZsSbYZa7nEl-QYfwF4E6d-iYmxvgikMQCLcBGAsYHQ/w640-h520/image2.gif&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/#dataset=coco_captions&amp;amp;tab=RELATIONS&amp;amp;relations=kyd%2Fcoco_captions%2Fcaptions_words,kyd%2Fcoco_captions%2Fcaptions_words_gendered_groups&amp;amp;draw=kyd/coco_captions/objects_label,bbox,bbox&amp;amp;auto_draw=false&quot;&gt;Using KYD&lt;/a&gt; to analyze the relationship between any word and gendered words. Each cell shows if the two respective words co-occur in the same caption more (up arrow) or less often (down arrow) than pure chance.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;Digging further into these correlations, we found that several activities stereotypically associated with women, such as “shopping” and “cooking”, co-occur with images captioned with “women” or “woman” at a higher rate than with images captioned with “men” or “man”. In contrast captions describing many physically intensive activities, such as “skateboarding”, “surfing”, and “snowboarding”, co-occur with images captioned with “man” or “men” at higher rates.  &lt;/p&gt;&lt;p&gt;While individual image captions may not use stereotypical or derogatory language, such as with the example below, if certain gender groups are over (or under) represented within a particular activity across the whole dataset, models developed from the dataset risk learning stereotypical associations. KYD makes it easy to surface, quantify, and make plans to mitigate this risk.&lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-ArTBIXAoUxk/YRF33FA9o5I/AAAAAAAAIAE/bWNd4jum7GobAmPvzvTZuJpMk8zOt5SaQCLcBGAsYHQ/s623/beige%2Bkitchen.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;484&quot; data-original-width=&quot;623&quot; height=&quot;498&quot; src=&quot;https://1.bp.blogspot.com/-ArTBIXAoUxk/YRF33FA9o5I/AAAAAAAAIAE/bWNd4jum7GobAmPvzvTZuJpMk8zOt5SaQCLcBGAsYHQ/w640-h498/beige%2Bkitchen.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;An image with one of the captions: “Two women cooking in a beige and white kitchen.” Image licensed under CC-BY 2.0.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;In addition to examining biases with respect to the social groups depicted with different activities, we also explored biases in how annotators described the appearance of people they perceived as male or female. Inspired by &lt;a href=&quot;https://repeller.com/male-gaze-definition/&quot;&gt;media scholars&lt;/a&gt; who have examined the “male gaze” embedded in other forms of visual media, we examined the frequency with which individuals perceived as women in COCO are described using adjectives that position them as an object of desire. KYD allowed us to easily examine co-occurrences between words associated with binary gender (e.g. &quot;female/girl/woman&quot; vs. &quot;male/man/boy&quot;) and words associated with evaluating physical attractiveness. Importantly, these are captions written by human annotators, who are making subjective assessments about the gender of people in the image and choosing a descriptor for attractiveness. &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/#dataset=coco_captions&amp;amp;tab=RELATIONS&amp;amp;relations=kyd%2Fcoco_captions%2Fcaptions_words_attractiveness,kyd%2Fcoco_captions%2Fcaptions_words_gendered_groups&amp;amp;draw=kyd/coco_captions/objects_label,bbox,bbox&amp;amp;auto_draw=false&quot;&gt;We see that&lt;/a&gt; the words &quot;attractive&quot;, &quot;beautiful&quot;, &quot;pretty&quot;, and &quot;sexy&quot; are overrepresented in describing people perceived as women as compared to those perceived as men, confirming what prior work has said about how gender is viewed in visual media. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-AW11DtTBpkY/YRF3-NiLtnI/AAAAAAAAIAI/y7jTORPUmpYgfUXQmtUtmx3JkX0uipqkACLcBGAsYHQ/s1454/new%2Btable%2Bfor%2Bkyd.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;716&quot; data-original-width=&quot;1454&quot; height=&quot;316&quot; src=&quot;https://1.bp.blogspot.com/-AW11DtTBpkY/YRF3-NiLtnI/AAAAAAAAIAI/y7jTORPUmpYgfUXQmtUtmx3JkX0uipqkACLcBGAsYHQ/w640-h316/new%2Btable%2Bfor%2Bkyd.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A screenshot &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/#dataset=coco_captions&amp;amp;tab=RELATIONS&amp;amp;relations=kyd%2Fcoco_captions%2Fcaptions_words_attractiveness,kyd%2Fcoco_captions%2Fcaptions_words_gendered_groups&amp;amp;draw=kyd/coco_captions/objects_label,bbox,bbox&amp;amp;auto_draw=false&quot;&gt;from KYD&lt;/a&gt; showing the relationship between words that describe attractiveness and gendered words. For example, “attractive” and “male/man/boy” co-occur 12 times, but we expect ~60 times by chance (the ratio is 0.2x). On the other hand, “attractive” and “female/woman/girl” co-occur 2.62 times more than chance.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;KYD also allows us to manually inspect images for each relation by clicking on the relation in question. For example, we &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/#dataset=coco_captions&amp;tab=RELATIONS&amp;relations=kyd%2Fcoco_captions%2Fcaptions_words_attractiveness,kyd%2Fcoco_captions%2Fcaptions_words_gendered_groups&amp;relations_selected=beautiful,female%2Cwoman%2Cgirl...&amp;draw=__none__,,&amp;auto_draw=false&quot;&gt;can see images&lt;/a&gt; whose captions include female terms (e.g. “woman”) and the word “beautiful”.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Exploring Age Bias&lt;/b&gt;&lt;br /&gt;Adults older than 65 &lt;a href=&quot;https://arxiv.org/pdf/2103.09058.pdf&quot;&gt;have been shown&lt;/a&gt; to be underrepresented in datasets relative to their presence in the general population — a first step toward improving age representation is to allow developers to assess it in their datasets. By looking at caption words describing different activities and analyzing their relation to caption words describing age, KYD helped us to assess the range of example captions depicting older adults. Having example captions of adults in a range of environments and activities is important for a variety of tasks, such as image captioning or pedestrian detection. &lt;/p&gt;&lt;p&gt;The first trend that KYD made clear is how rarely annotators described people as older adults in captions detailing different activities. &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/#dataset=coco_captions&amp;amp;tab=RELATIONS&amp;amp;relations=kyd%2Fcoco_captions%2Fcaptions_words_age,kyd%2Fcoco_captions%2Fcaptions_words_movement&amp;amp;draw=kyd/coco_captions/objects_label,bbox,bbox&amp;amp;auto_draw=false&quot;&gt;The relations tab also shows&lt;/a&gt; a trend wherein “elderly”, “old”, and “older” tend not to occur with verbs that describe a variety of physical activities that might be important for a system to be able to detect. Important to note is that, relative to “young”, “old” is more often used to describe things other than people, such as belongings or clothing, so these relations are also capturing some uses that don’t describe people. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-84o11xS6RYc/YRFdYDXYB1I/AAAAAAAAH_o/9HeVEoJm7FkNsFOf5cWJpmNjrJD1dbEjACLcBGAsYHQ/s1468/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;1468&quot; height=&quot;500&quot; src=&quot;https://1.bp.blogspot.com/-84o11xS6RYc/YRFdYDXYB1I/AAAAAAAAH_o/9HeVEoJm7FkNsFOf5cWJpmNjrJD1dbEjACLcBGAsYHQ/w640-h500/image5.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The relationship between words associated with &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/#dataset=coco_captions&amp;amp;tab=RELATIONS&amp;amp;relations=kyd%2Fcoco_captions%2Fcaptions_words_age,kyd%2Fcoco_captions%2Fcaptions_words_movement&amp;amp;draw=kyd/coco_captions/objects_label,bbox,bbox&amp;amp;auto_draw=false&quot;&gt;age and movement&lt;/a&gt; from a screenshot of KYD.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;The underrepresentation of captions containing the references to older adults that we examined here could be rooted in a relative lack of images depicting older adults as well as in a tendency for annotators to omit older age-related terms when describing people in images. While manual inspection of the intersection of “old” and “running” shows a negative relation, we notice that it shows no older people and a &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/#dataset=coco_captions&amp;amp;tab=RELATIONS&amp;amp;relations=kyd%2Fcoco_captions%2Fcaptions_words_age,kyd%2Fcoco_captions%2Fcaptions_words_movement&amp;amp;relations_selected=old,running&amp;amp;draw=__none__,,&amp;amp;auto_draw=false&quot;&gt;number of locomotives&lt;/a&gt;. KYD makes it easy to quantitatively and qualitatively inspect relations to identify dataset strengths and areas for improvement. &lt;/p&gt;  &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;Understanding the contents of ML datasets is a critical first step to developing suitable strategies to mitigate the downstream impact of unfair dataset bias. The above analysis points towards several potential mitigations. For example, correlations between certain activities and social groups, which can lead trained models to reproduce social stereotypes, can be potentially mitigated by “dataset balancing” — increasing the representation of under-represented group/activity combinations. However, mitigations focused exclusively on dataset balancing are not sufficient, as our analysis of how different genders are described by annotators  demonstrated. We found annotators’ subjective judgements of people portrayed in images were reflected within the final dataset, suggesting a deeper look at methods of image annotations are needed. One solution for data practitioners who are developing image captioning datasets is to consider integrating &lt;a href=&quot;https://www.cooperhewitt.org/cooper-hewitt-guidelines-for-image-description/&quot;&gt;guidelines&lt;/a&gt; that have been developed for writing image descriptions that are sensitive to race, gender, and other identity categories. &lt;/p&gt;&lt;p&gt;The above case studies highlight only &lt;em&gt;some&lt;/em&gt; of the KYD features. For example, &lt;a href=&quot;https://cloud.google.com/vision&quot;&gt;Cloud Vision API signals&lt;/a&gt; are also integrated into KYD and can be used to infer signals that annotators haven't labeled directly. We encourage the broader ML community to perform their own KYD case studies and share their findings.  &lt;/p&gt;&lt;p&gt;KYD complements other &lt;a href=&quot;https://par.nsf.gov/servlets/purl/10208548&quot;&gt;dataset&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/pdf/2009.10795.pdf&quot;&gt;analysis&lt;/a&gt; tools being developed across the ML community, including Google's growing &lt;a href=&quot;https://www.tensorflow.org/responsible_ai&quot;&gt;Responsible AI toolkit&lt;/a&gt;. We look forward to ML practitioners using KYD to better understand their datasets and mitigate potential bias and fairness concerns. If you have feedback on KYD, please write to &lt;a href=&quot;mailto:knowyourdata-feedback@google.com&quot;&gt;knowyourdata-feedback@google.com&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;The analysis and write-up in this post were conducted with equal contribution by Emily Denton, Mark Díaz, and Alex Hanna. We thank Marie Pellat, Ludovic Peran, Daniel Smilkov, Nikhil Thorat and Tsung-Yi for their contributions to and reviews of this post.&lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=WbCPXzssu4Y:IUsMkPQuXMY:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/WbCPXzssu4Y&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/4880389433895154885/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/08/a-dataset-exploration-case-study-with.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4880389433895154885"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4880389433895154885"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/WbCPXzssu4Y/a-dataset-exploration-case-study-with.html" title="A Dataset Exploration Case Study with Know Your Data"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-jyz_dnAXfn4/YRFcv9cWZ5I/AAAAAAAAH_U/PZsSbYZa7nEl-QYfwF4E6d-iYmxvgikMQCLcBGAsYHQ/s72-w640-h520-c/image2.gif" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/08/a-dataset-exploration-case-study-with.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7428136606145436934</id><published>2021-08-05T07:57:00.011-07:00</published><updated>2021-08-05T08:44:34.256-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Health"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><title type="text">Improved Detection of Elusive Polyps via Machine Learning</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Yossi Matias, Vice President and Ehud Rivlin, Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;With the increasing ability to consistently and accurately process large amounts of data, particularly visual data, computer-aided diagnostic systems are more frequently being used to assist physicians in their work. This, in turn, can lead to meaningful improvements in health care. An example of where this could be especially useful is in the diagnosis and treatment of &lt;a href=&quot;https://en.wikipedia.org/wiki/Colorectal_cancer&quot;&gt;colorectal cancer&lt;/a&gt; (CRC), which is &lt;a href=&quot;https://www.cancer.org/research/cancer-facts-statistics/all-cancer-facts-figures/cancer-facts-figures-2019.html&quot;&gt;especially deadly&lt;/a&gt; and results in over &lt;a href=&quot;http://gco.iarc.fr/today/data/factsheets/cancers/10_8_9-Colorectum-fact-sheet.pdf&quot;&gt;900K deaths per year&lt;/a&gt;, globally.  CRC originates in small pre-cancerous lesions in the colon, called &lt;a href=&quot;https://en.wikipedia.org/wiki/Polyp_(medicine)&quot;&gt;polyps&lt;/a&gt;, the identification and removal of which is very successful in preventing CRC-related deaths.  &lt;/p&gt;&lt;p&gt;The standard procedure used by gastroenterologists (GIs) to detect and remove polyps is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Colonoscopy&quot;&gt;colonoscopy&lt;/a&gt;, and about 19 million such procedures are performed annually in the US alone. During a colonoscopy, the gastroenterologist uses a camera-containing probe to check the intestine for pre-cancerous polyps and early signs of cancer, and removes tissue that looks worrisome. However, complicating factors, such as &lt;em&gt;incomplete detection&lt;/em&gt; (in which the polyp appears within the field of view, but is missed by the GI, perhaps due to its size or shape) and &lt;em&gt;incomplete exploration&lt;/em&gt; (in which the polyp does not appear in the camera’s field of view), can lead to a high fraction of missed polyps. In fact, &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/22441756/&quot;&gt;studies suggest&lt;/a&gt; that 22%–28% of polyps are missed during colonoscopies, of which 20%–24% have the potential to become cancerous (&lt;a href=&quot;https://en.wikipedia.org/wiki/Adenoma&quot;&gt;adenomas&lt;/a&gt;). &lt;/p&gt;&lt;p&gt;Today, we are sharing progress made in using machine learning (ML) to help GIs fight colorectal cancer by making colonoscopies more effective. In “&lt;a href=&quot;https://www.giejournal.org/article/S0016-5107(21)01468-1/fulltext&quot;&gt;Detection of Elusive Polyps via a Large Scale AI System&lt;/a&gt;”, we present an ML model designed to combat the problem of incomplete detection by helping the GI detect polyps that are within the field of view. This work adds to our &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9097918&quot;&gt;previously published work&lt;/a&gt; that maximizes the coverage of the colon during the colonoscopy by flagging for GI follow-up areas that may have been missed. Using clinical studies, we show that these systems significantly improve polyp detection rates.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Incomplete Exploration&lt;/b&gt;&lt;br /&gt;To help the GI detect polyps that are outside the field of view, &lt;a href=&quot;https://ai.googleblog.com/2020/08/using-machine-learning-to-detect.html&quot;&gt;we previously developed an ML system&lt;/a&gt; that reduces the rate of incomplete exploration by estimating the fractions of covered and non-covered regions of a colon during a colonoscopy. This earlier work uses computer vision and geometry in a technique we call &lt;em&gt;colonoscopy coverage deficiency via depth&lt;/em&gt;, to compute segment-by-segment coverage for the colon. It does so in two phases: first computing depth maps for each frame of the colonoscopy video, and then using these depth maps to compute the coverage in real time.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-JA6iLpIsgYw/YQmyh8uSUmI/AAAAAAAAH-o/fNdKdUrIjwMjWvds76FbsfMcET3QvIRaQCLcBGAsYHQ/s1808/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;1808&quot; src=&quot;https://1.bp.blogspot.com/-JA6iLpIsgYw/YQmyh8uSUmI/AAAAAAAAH-o/fNdKdUrIjwMjWvds76FbsfMcET3QvIRaQCLcBGAsYHQ/s16000/image1.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The ML system computes a depth image &lt;b&gt;(middle)&lt;/b&gt; from a single RGB image &lt;b&gt;(left)&lt;/b&gt;.  Then, based on the computation of depth images for a video sequence, it calculates local coverage &lt;b&gt;(right)&lt;/b&gt;, and detects where the coverage has been deficient and a second look is required (blue color indicates observed segments where red indicates uncovered ones). You can learn more about this work in our &lt;a href=&quot;https://ai.googleblog.com/2020/08/using-machine-learning-to-detect.html&quot;&gt;previous blog post&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This segment-by-segment work yields the ability to estimate what fraction of the &lt;em&gt;current&lt;/em&gt; segment has been covered.  The helpfulness of such functionality is clear: during the procedure itself, a physician may be alerted to segments with deficient coverage, and can immediately return to review these areas, potentially reducing the rates of missed polyps due to incomplete exploration. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Incomplete Detection&lt;/b&gt;&lt;br /&gt;In &lt;a href=&quot;https://www.giejournal.org/article/S0016-5107(21)01468-1/fulltext&quot;&gt;our most recent paper&lt;/a&gt;, we look into the problem of incomplete detection. We describe an ML model that aids a GI in detecting polyps that are &lt;em&gt;within&lt;/em&gt; the field of view, so as to reduce the rate of incomplete detection. We developed a system that is based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;&gt;convolutional neural networks&lt;/a&gt; (CNN) with an architecture that combines temporal logic with a single frame detector, resulting in more accurate detection. &lt;/p&gt;&lt;p&gt;This new system has two principal advantages. The first is that the system improves detection performance by reducing the number of false negatives detections of elusive polyps, those polyps that are particularly difficult for GIs to detect. The second advantage is the very low false positive rate of the system. This low false positive rate makes these systems more likely to be adopted in the clinic.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-j6eCF6SUf04/YQmyn8ZW3gI/AAAAAAAAH-s/24WIt7EddWMUGYZ6YobDN0c1i-HJAdizQCLcBGAsYHQ/s1182/image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;367&quot; data-original-width=&quot;1182&quot; src=&quot;https://1.bp.blogspot.com/-j6eCF6SUf04/YQmyn8ZW3gI/AAAAAAAAH-s/24WIt7EddWMUGYZ6YobDN0c1i-HJAdizQCLcBGAsYHQ/s16000/image2.jpg&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Examples of the variety of polyps detected by the ML system.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We trained the system on 3600 procedures (86M video frames) and tested it on 1400 procedures (33M frames). All the videos and metadata were de-identified. The system detected 97% of the polyps (i.e., it yielded 97% sensitivity) at 4.6 false alarms per procedure, which is a substantial improvement over previously &lt;a href=&quot;https://gut.bmj.com/content/68/10/1813&quot;&gt;published results&lt;/a&gt;. Of the false alarms, follow-up review showed that some were, in fact, valid polyp detections, indicating that the system was able to detect polyps that were missed by the performing endoscopist and by those who annotated the data. The performance of the system on these &lt;em&gt;elusive&lt;/em&gt; polyps suggests its generalizability in that the system has learned to detect examples that were initially missed by all who viewed the procedure. &lt;/p&gt;&lt;p&gt;We evaluated the system performance on polyps that are in the field of view for less than five seconds, which makes them more difficult for the GI to detect, and for which models typically have much lower sensitivity. In this case the system attained a sensitivity that is about three times that of  the sensitivity that the original procedure achieved. When the polyps were present in the field of view for less than 2 seconds, the difference was even more stark — the system exhibited a 4x improvement in sensitivity.  &lt;/p&gt;&lt;p&gt;It is also interesting to note that the system is fairly insensitive to the choice of neural network architecture. We used two architectures: &lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;&gt;RetinaNet&lt;/a&gt; and&amp;nbsp; &lt;a href=&quot;https://arxiv.org/pdf/1711.06368.pdf&quot;&gt;LSTM-SSD&lt;/a&gt;. RetinaNet is a leading technique for object detection on static images (used for video by applying it to frames in a consecutive fashion). It is one of the top performers on a variety of benchmarks, given a fixed computational budget, and is known for balancing speed of computation with accuracy. LSTM-SSD is a true video object detection architecture, which can explicitly account for the temporal character of the video (e.g., temporal consistency of detections, ability to deal with blur and fast motion, etc.). It is known for being robust and very computationally lightweight and can therefore run on less expensive processors. Comparable results were also obtained on the much heavier &lt;a href=&quot;https://arxiv.org/abs/1506.01497&quot;&gt;Faster R-CNN&lt;/a&gt; architecture. The fact that results are similar across different architectures implies that one can choose the network meeting the available hardware specifications.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Prospective Clinical Research Study&lt;/b&gt;&lt;br /&gt;As part of the research reported in our &lt;a href=&quot;https://www.giejournal.org/article/S0016-5107(21)01468-1/fulltext&quot;&gt;detection paper&lt;/a&gt; we ran a clinical validation on 100 procedures in collaboration with &lt;a href=&quot;https://www.szmc.org.il/eng/home/&quot;&gt;Shaare Zedek Medical Center&lt;/a&gt; in Jerusalem, where our system was used in real time to help GIs. The system helped detect an average of one polyp per procedure that would have otherwise been missed by the GI performing the procedure, while not missing any of the polyps detected by the GIs, and with 3.8 false alarms per procedure. The feedback from the GIs was consistently positive. &lt;/p&gt;&lt;p&gt;We are encouraged by the potential helpfulness of this system for improving polyp detection, and we look forward to working together with the doctors in the procedure room to further validate this research.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;The research was conducted by teams from Google Health and Google Research, Israel with support from &lt;a href=&quot;https://verily.com/&quot;&gt;Verily Life Sciences&lt;/a&gt;, and in collaboration with Shaare Zedek Medical Center. Verily is advancing this research via a newly established center in Israel, led by Ehud Rivlin. This research was conducted by Danny Veikherman, Tomer Golany, Dan M. Livovsky, Amit Aides, Valentin Dashinsky, Nadav Rabani, David Ben Shimol, Yochai Blau, Liran Katzir, Ilan Shimshoni, Yun Liu, Ori Segol, Eran Goldin, Greg Corrado, Jesse Lachter, Yossi Matias, Ehud Rivlin, and Daniel Freedman. Our appreciation also goes to several institutions and GIs who provided advice along the way and tested our system prototype&lt;/em&gt;. &lt;em&gt; We would like to thank all of our team members and collaborators who worked on this project with us, including: Chen Barshai, Nia Stoykova, and many others.&lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=yKck6ILTw_U:kEVWUWtHaWg:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/yKck6ILTw_U&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/7428136606145436934/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/08/improved-detection-of-elusive-polyps.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7428136606145436934"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7428136606145436934"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/yKck6ILTw_U/improved-detection-of-elusive-polyps.html" title="Improved Detection of Elusive Polyps via Machine Learning"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-JA6iLpIsgYw/YQmyh8uSUmI/AAAAAAAAH-o/fNdKdUrIjwMjWvds76FbsfMcET3QvIRaQCLcBGAsYHQ/s72-c/image1.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/08/improved-detection-of-elusive-polyps.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-2018640131411247692</id><published>2021-08-04T12:12:00.004-07:00</published><updated>2021-08-04T12:12:59.598-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="ACL"/><category scheme="http://www.blogger.com/atom/ns#" term="NLP"/><category scheme="http://www.blogger.com/atom/ns#" term="Speech"/><title type="text">Two New Datasets for Conversational NLP: TimeDial and Disfl-QA</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Aditya Gupta, Software Engineer and Shyam Upadhyay, Research Scientist, Google Assistant&lt;/span&gt; &lt;p&gt;A key challenge in &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;&gt;natural language processing&lt;/a&gt; (NLP) is building &lt;a href=&quot;https://en.wikipedia.org/wiki/Dialogue_system&quot;&gt;conversational agents&lt;/a&gt; that can understand and reason about different language phenomena that are unique to realistic speech. For example, because people do not always premeditate exactly what they are going to say, a natural conversation often includes interruptions to speech, called &lt;a href=&quot;https://www.semanticscholar.org/paper/Preliminaries-to-a-Theory-of-Speech-Disfluencies-Schriberg/c0ca94051f549f08e0bb4be7694540460fd47f1b&quot;&gt;disfluencies&lt;/a&gt;. Such disfluencies can be simple (like interjections, repetitions, restarts, or corrections), which simply break the continuity of a sentence, or more complex semantic disfluencies, in which the underlying meaning of a phrase changes.  In addition, understanding a conversation also often requires knowledge of temporal relationships, like whether an event precedes or follows another. However, conversational agents built on today’s NLP models often struggle when confronted with temporal relationships or with disfluencies, and progress on improving their performance has been slow. This is due, in part, to a lack of datasets that involve such interesting conversational and speech phenomena.  &lt;/p&gt;&lt;p&gt;To stir interest in this direction within the research community, we are excited to introduce &lt;a href=&quot;https://arxiv.org/abs/2106.04571&quot;&gt;TimeDial&lt;/a&gt;, for temporal commonsense reasoning in dialog, and &lt;a href=&quot;https://arxiv.org/abs/2106.04016&quot;&gt;Disfl-QA&lt;/a&gt;, which focuses on contextual disfluencies. TimeDial presents a new multiple choice span filling task targeted for temporal understanding, with an annotated test set of over ~1.1k dialogs.  Disfl-QA is the first dataset containing contextual disfluencies in an information seeking setting, namely question answering over Wikipedia passages, with ~12k human annotated disfluent questions. These benchmark datasets are the first of their kind and show a significant gap between human performance and current state of the art NLP models. &lt;/p&gt;&lt;p&gt;&lt;b&gt;TimeDial&lt;/b&gt;&lt;br /&gt;While people can effortlessly reason about everyday temporal concepts, such as duration, frequency, or relative ordering of events in a dialog, such tasks can be challenging for conversational agents. For example, current NLP models often make a poor selection when tasked with filling in a blank (as shown below) that assumes a basic level of world knowledge for reasoning, or that requires understanding explicit and implicit inter-dependencies between temporal concepts across conversational turns. &lt;/p&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-OrwOHtczNVE/YQrmg0sW9UI/AAAAAAAAH_M/15lypkBJvt0nGRP_1C6OVjHvAxhzduF3ACLcBGAsYHQ/s688/Google%2BAI%2BBlog%2BPost%2Bfor%2BDisfl-QA%2Band%2BTimeDial%2B%25281%2529.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;688&quot; data-original-width=&quot;557&quot; src=&quot;https://1.bp.blogspot.com/-OrwOHtczNVE/YQrmg0sW9UI/AAAAAAAAH_M/15lypkBJvt0nGRP_1C6OVjHvAxhzduF3ACLcBGAsYHQ/s16000/Google%2BAI%2BBlog%2BPost%2Bfor%2BDisfl-QA%2Band%2BTimeDial%2B%25281%2529.jpg&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;It is easy for a person to judge that “half past one” and “quarter to two” are more plausible options to fill in the blank than “half past three” and “half past nine”. However, performing such temporal reasoning in the context of a dialog is not trivial for NLP models, as it requires appealing to world knowledge (i.e., knowing that the participants are not yet late for the meeting) and understanding the temporal relationship between events (“half past one” is before “three o’clock”, while “half past three” is after it). Indeed, current state-of-the-art models like &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;&gt;T5&lt;/a&gt; and &lt;a href=&quot;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&quot;&gt;BERT&lt;/a&gt; end up picking the wrong answers — “half past three” (T5) and “half past nine” (BERT).&lt;/p&gt;&lt;p&gt;The TimeDial benchmark dataset (derived from the &lt;a href=&quot;https://arxiv.org/abs/1710.03957&quot;&gt;DailyDialog&lt;/a&gt; multi-turn dialog corpus) measures models’ temporal commonsense reasoning abilities within a dialog context. Each of the ~1.5k dialogs in the dataset  is presented in a multiple choice setup, in which one temporal span is masked out and the model is asked to find all  correct answers from a list of four options to fill in the blank. &lt;/p&gt;&lt;p&gt;In our experiments we found that while people can easily answer these multiple choice questions (at 97.8% accuracy), state-of-the-art pre-trained language models still struggle on this challenge set.  We experiment across three different modeling paradigms: (i) classification over the provided 4 options using BERT, (ii) mask filling for the masked span in the dialog using BERT-MLM, (iii) generative methods using T5. We observe that all the models struggle on this challenge set, with the best variant only scoring 73%.  &lt;/p&gt;&lt;table cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: 23%; margin-right: 23%;&quot;&gt;  &lt;colgroup&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 25%;&quot;&gt;&lt;/col&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 4%;&quot;&gt;&lt;/col&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 25%;&quot;&gt;&lt;/col&gt; &lt;/colgroup&gt;  &lt;tbody&gt;&lt;tr&gt;    &lt;td&gt;&lt;b&gt;Model&lt;/b&gt;   &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;2-best Accuracy&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Human    &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;97.8%    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;BERT - Classification    &lt;/td&gt;   &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;50.0%    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;BERT - Mask Filling    &lt;/td&gt;   &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;68.5%    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;T5 - Generation    &lt;/td&gt;   &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;73.0%    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.04571.pdf&quot;&gt;Qualitative error analyses show&lt;/a&gt; that the pre-trained language models often rely on shallow, spurious features (particularly text matching), instead of truly doing reasoning over the context. It is likely that building NLP models capable of performing the kind of temporal commonsense reasoning needed for TimeDial requires rethinking how temporal objects are represented within general text representations. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Disfl-QA&lt;/b&gt;&lt;br /&gt;As disfluency is inherently a speech phenomenon, it is most commonly found in text output from speech recognition systems. Understanding such disfluent text is key to building conversational agents that understand human speech. Unfortunately, research  in the NLP  and  speech community has been impeded by the lack of curated datasets containing  such  disfluencies, and the datasets that are available, like &lt;a href=&quot;https://ieeexplore.ieee.org/document/225858&quot;&gt;Switchboard&lt;/a&gt;, are limited in scale and complexity. As a result, it’s difficult to stress test NLP models in the presence of disfluencies.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: 15%; margin-right: 15%;&quot;&gt;    &lt;colgroup&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 15%;&quot;&gt;&lt;/col&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 5%;&quot;&gt;&lt;/col&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 50%;&quot;&gt;&lt;/col&gt; &lt;/colgroup&gt;  &lt;tbody&gt;&lt;tr&gt;   &lt;td&gt;&lt;b&gt;Disfluency&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;Example&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Interjection    &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;“&lt;i&gt;When is, &lt;span style=&quot;color: #666666;&quot;&gt;uh&lt;/span&gt;, Easter this year?&lt;/i&gt;”    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Repetition    &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;“&lt;i&gt;When is &lt;span style=&quot;color: red;&quot;&gt;Eas&lt;/span&gt; … &lt;span style=&quot;color: #2b00fe;&quot;&gt;Easter&lt;/span&gt; this year?&lt;/i&gt;”    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Correction    &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;“&lt;i&gt;When is &lt;span style=&quot;color: red;&quot;&gt;Lent&lt;/span&gt;, &lt;span style=&quot;color: #666666;&quot;&gt;I mean&lt;/span&gt; &lt;span style=&quot;color: #2b00fe;&quot;&gt;Easter&lt;/span&gt;, this year?&lt;/i&gt;”&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Restart    &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;“&lt;i&gt;&lt;span style=&quot;color: red;&quot;&gt;How much&lt;/span&gt;, &lt;span style=&quot;color: #666666;&quot;&gt;no wait&lt;/span&gt;, &lt;span style=&quot;color: #2b00fe;&quot;&gt;when is&lt;/span&gt; Easter this year?&lt;/i&gt;”    &lt;/td&gt;  &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;  &lt;tbody&gt;    &lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Different kinds of disfluencies. The reparandum (words intended to be corrected or ignored; in &lt;span style=&quot;color: red;&quot;&gt;red&lt;/span&gt;), interregnum (optional discourse cues; in &lt;span style=&quot;color: #666666;&quot;&gt;grey&lt;/span&gt;) and repair (the corrected words; in &lt;span style=&quot;color: #2b00fe;&quot;&gt;blue&lt;/span&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Disfl-QA is the first dataset containing contextual disfluencies in an information  seeking  setting,  namely  &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;&gt;question  answering&lt;/a&gt; over Wikipedia passages from &lt;a href=&quot;https://arxiv.org/abs/1806.03822&quot;&gt;SQuAD&lt;/a&gt;. Disfl-QA is a &lt;em&gt;targeted&lt;/em&gt; dataset for disfluencies, in which all questions (~12k) contain disfluencies, making for a much larger disfluent test set than prior datasets. Over 90% of the disfluencies in Disfl-QA are corrections or restarts, making it a much more difficult test set for disfluency correction. In addition, compared to earlier disfluency datasets, it contains a wider variety of semantic distractors, i.e., distractors that carry semantic meaning as opposed to simpler speech disfluencies.&amp;nbsp;&lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: left;&quot;&gt;&lt;b&gt;Passage:&lt;/b&gt; &lt;em&gt;…The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (&quot;Norman&quot; comes from &quot;Norseman&quot;) raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, … &lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;    &lt;colgroup&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 10%;&quot;&gt;&lt;/col&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 50%;&quot;&gt;&lt;/col&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 5%;&quot;&gt;&lt;/col&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 35%;&quot;&gt;&lt;/col&gt; &lt;/colgroup&gt;  &lt;tbody&gt;&lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;b&gt;Q&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt; &lt;td style=&quot;text-align: left;&quot;&gt;In what country is Normandy located?&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;span style=&quot;color: #93c47d;&quot;&gt;&lt;b&gt;France ✓&lt;/b&gt;&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;b&gt;DQ&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt; &lt;td style=&quot;text-align: left;&quot;&gt;In what country is &lt;span style=&quot;color: red;&quot;&gt;Norse&lt;/span&gt; found &lt;span style=&quot;color: #666666;&quot;&gt;no wait&lt;/span&gt; &lt;span style=&quot;color: #2b00fe;&quot;&gt;Normandy not Norse&lt;/span&gt;?&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;span style=&quot;color: red;&quot;&gt;&lt;b&gt;Denmark X&lt;/b&gt;&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;b&gt;Q&lt;sub&gt;2&lt;/sub&gt;:&lt;/b&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt; &lt;td style=&quot;text-align: left;&quot;&gt;When were the Normans in Normandy?&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;span style=&quot;color: #93c47d;&quot;&gt;&lt;b&gt;10th and 11th centuries ✓&lt;/b&gt;&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;b&gt;DQ&lt;sub&gt;2&lt;/sub&gt;:&lt;/b&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt; &lt;td style=&quot;text-align: left;&quot;&gt;&lt;span style=&quot;color: red;&quot;&gt;From which countries&lt;/span&gt; &lt;span style=&quot;color: #666666;&quot;&gt;no tell me&lt;/span&gt; &lt;span style=&quot;color: #2b00fe;&quot;&gt;when were the Normans in Normandy?&lt;/span&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;b&gt;&lt;span style=&quot;color: red;&quot;&gt;Denmark, Iceland and Norway X&lt;/span&gt;&lt;/b&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A passage and questions (Q&lt;sub&gt;i&lt;/sub&gt;) from &lt;a href=&quot;https://arxiv.org/abs/1806.03822&quot;&gt;SQuAD&lt;/a&gt; dataset, along with their disfluent versions (DQ&lt;sub&gt;i&lt;/sub&gt;), consisting of semantic distractors (like “Norse” and “from which countries”) and predictions from a T5 model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Here, the first question (Q&lt;sub&gt;1&lt;/sub&gt;) is seeking an answer about the location of &lt;em&gt;Normandy&lt;/em&gt;. In the disfluent version (DQ&lt;sub&gt;1&lt;/sub&gt;) &lt;em&gt;Norse&lt;/em&gt; is mentioned before the question is corrected. The presence of this correctional disfluency confuses the QA model, which tends to rely on shallow textual cues from the question for making predictions. &lt;/p&gt;&lt;p&gt;Disfl-QA also includes newer phenomena, such as&amp;nbsp;&lt;a href=&quot;https://nlp.stanford.edu/projects/coref.shtml#:~:text=Coreference%20resolution%20is%20the%20task,question%20answering%2C%20and%20information%20extraction.&quot;&gt;coreference&lt;/a&gt;&amp;nbsp;(expression referring to the same entity) between the&amp;nbsp;&lt;em&gt;reparandum&lt;/em&gt;&amp;nbsp;and the&amp;nbsp;&lt;em&gt;repair&lt;/em&gt;.&lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: 6%; margin-right: 6%;&quot;&gt;    &lt;colgroup&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 42%%;&quot;&gt;&lt;/col&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 4%;&quot;&gt;&lt;/col&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 42%;&quot;&gt;&lt;/col&gt; &lt;/colgroup&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td&gt;&lt;b&gt;SQuAD&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;b&gt;Disfl-QA&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;Who does BSkyB have an operating license from?&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;span style=&quot;color: red;&quot;&gt;Who removed [BSkyB’s] operating license&lt;/span&gt;, &lt;span style=&quot;color: #666666;&quot;&gt;no scratch that&lt;/span&gt;, &lt;span style=&quot;color: #2b00fe;&quot;&gt;who do [they] have [their] operating license from?&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Experiments show that the performance of existing state-of-the-art language model–based question answering systems degrades significantly when tested on Disfl-QA and heuristic disfluencies (presented in the paper) in a zero-shot setting.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: 20%; margin-right: 20%;&quot;&gt;    &lt;colgroup&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 20%%;&quot;&gt;&lt;/col&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 10%;&quot;&gt;&lt;/col&gt;   &lt;col span=&quot;1&quot; style=&quot;width: 30%;&quot;&gt;&lt;/col&gt; &lt;/colgroup&gt;  &lt;tbody&gt;&lt;tr&gt;   &lt;td&gt;&lt;b&gt;Dataset&lt;/b&gt;   &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;F1&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1806.03822&quot;&gt;SQuAD&lt;/a&gt;   &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;89.59    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Heuristics    &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;65.27 (-24.32)    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Disfl-QA     &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;61.64 (-27.95)    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We &lt;a href=&quot;https://arxiv.org/abs/2106.04016&quot;&gt;show&lt;/a&gt; that data augmentation methods partially recover the loss in performance and also demonstrate the efficacy of using human-annotated training data for fine-tuning. We argue that researchers need large-scale disfluency datasets in order for NLP models to be robust to disfluencies. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;Understanding  language phenomena that are unique to human speech,  like disfluencies and temporal reasoning, among others, is a key ingredient for enabling more natural human–machine communication in the near future. With TimeDial and Disfl-QA, we aim to fill a major research gap by providing these datasets as testbeds for NLP models, in order to evaluate their robustness to ubiquitous phenomena across different tasks. It is our hope that the broader NLP community will devise generalized few-shot or zero-shot approaches to effectively handle these phenomena, without requiring task-specific human-annotated training datasets, constructed specifically for these challenges. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgments&lt;/b&gt;&lt;br /&gt;&lt;em&gt;The TimeDial work has been a team effort involving Lianhui Qi, Luheng He, Yenjin Choi, Manaal Faruqui and the authors. The Disfl-QA work has been a collaboration involving Jiacheng Xu, Diyi Yang, Manaal Faruqui.&lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=QizNs26Pduw:TDKjXKDo2kw:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/QizNs26Pduw&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/2018640131411247692/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/08/two-new-datasets-for-conversational-nlp.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2018640131411247692"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2018640131411247692"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/QizNs26Pduw/two-new-datasets-for-conversational-nlp.html" title="Two New Datasets for Conversational NLP: TimeDial and Disfl-QA"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-OrwOHtczNVE/YQrmg0sW9UI/AAAAAAAAH_M/15lypkBJvt0nGRP_1C6OVjHvAxhzduF3ACLcBGAsYHQ/s72-c/Google%2BAI%2BBlog%2BPost%2Bfor%2BDisfl-QA%2Band%2BTimeDial%2B%25281%2529.jpg" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/08/two-new-datasets-for-conversational-nlp.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-2120184673555437196</id><published>2021-08-02T10:12:00.001-07:00</published><updated>2021-08-13T11:17:28.498-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="conference"/><category scheme="http://www.blogger.com/atom/ns#" term="conferences"/><title type="text">Google at ACL 2021</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Catherine Armato, Program Manager&lt;/span&gt;&lt;p&gt;This week, the &lt;a href=&quot;https://2021.aclweb.org/&quot;&gt;59th annual meeting&lt;/a&gt; of the &lt;a href=&quot;https://www.aclweb.org/portal/&quot;&gt;Association for Computational Linguistics&lt;/a&gt; (ACL), a premier conference covering a broad spectrum of research areas that are concerned with computational approaches to natural language, is taking place online. &lt;/p&gt;&lt;p&gt;As a leader in natural language processing and understanding, and a &lt;a href=&quot;https://2021.aclweb.org/sponsors/list/&quot;&gt;Diamond Level sponsor of ACL 2021&lt;/a&gt;, Google will showcase the latest research in the field with over 35 publications, and the organization of and participation in a variety of workshops and tutorials. &lt;/p&gt;&lt;p&gt;If you’re registered for ACL 2021, we hope that you’ll visit the Google virtual booth in &lt;a href=&quot;https://gather.town/app/T5x469zIFF3Nkfpe/ACL-IJCNLP&quot;&gt;Gather Town&lt;/a&gt; to learn more about the projects and opportunities at Google that go into solving interesting problems for billions of people. You can also learn more about Google's participation on the &lt;a href=&quot;https://underline.io/events/167/expo/1199-google-research&quot;&gt;ACL 2021 Expo page&lt;/a&gt;, and see&amp;nbsp;a full list of Google publications below (Google affiliations in &lt;b&gt;bold&lt;/b&gt;). &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Organizing Committee&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;em&gt;Senior Area Chairs include:&lt;/em&gt; &lt;em&gt;&lt;b&gt;Dan Roth&lt;/b&gt;, &lt;b&gt;Emily Pitler&lt;/b&gt;, &lt;b&gt;Jimmy Lin&lt;/b&gt;, &lt;b&gt;Ming-Wei Chang&lt;/b&gt;, &lt;b&gt;Sebastian Ruder&lt;/b&gt;, &lt;b&gt;Slav Petrov&lt;/b&gt;&lt;/em&gt;&lt;br /&gt;&lt;em&gt;Area Chairs include:&lt;/em&gt; &lt;em&gt;&lt;b&gt;Ankur P. Parikh&lt;/b&gt;, &lt;b&gt;Artem Sokolov&lt;/b&gt;, &lt;b&gt;Bhuwan Dhingra&lt;/b&gt;, &lt;b&gt;Cicero Nogueira dos Santos&lt;/b&gt;, &lt;b&gt;Colin Cherry&lt;/b&gt;, &lt;b&gt;Dani Yogatama&lt;/b&gt;, &lt;b&gt;David Mimno&lt;/b&gt;, &lt;b&gt;Hideto Kazawa&lt;/b&gt;, &lt;b&gt;Ian Tenney&lt;/b&gt;, &lt;b&gt;Jasmijn Bastings&lt;/b&gt;, &lt;b&gt;Jun Suzuki&lt;/b&gt;, &lt;b&gt;Katja Filippova&lt;/b&gt;, &lt;b&gt;Kyle Gorma&lt;/b&gt;, &lt;b&gt;Lu Wang&lt;/b&gt;, &lt;b&gt;Manaal Faruqui&lt;/b&gt;, &lt;b&gt;Natalie Schluter&lt;/b&gt;, &lt;b&gt;Peter Liu&lt;/b&gt;, &lt;b&gt;Radu Soricut&lt;/b&gt;, &lt;b&gt;Sebastian Gehrmann&lt;/b&gt;, &lt;b&gt;Shashi Narayan&lt;/b&gt;, &lt;b&gt;Tal Linzen&lt;/b&gt;, &lt;b&gt;Vinodkumar Prabhakaran&lt;/b&gt;, &lt;b&gt;Waleed Ammar&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Publications&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.47.pdf&quot;&gt;Parameter-Efficient Multi-task Fine-Tuning for Transformers via Shared Hypernetwork&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Rabeeh Karimi Mahabadi*, Sebastian Ruder, &lt;b&gt;Mostafa Dehghani&lt;/b&gt;, James Henderson&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.55.pdf&quot;&gt;TicketTalk: Toward Human-Level Performance with End-to-End, Transaction-Based Dialog Systems&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Bill Byrne&lt;/b&gt;, &lt;b&gt;Karthik Krishnamoorthi&lt;/b&gt;, &lt;b&gt;Saravanan Ganesh&lt;/b&gt;, &lt;b&gt;Mihir Sanjay Kale&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.58.pdf&quot;&gt;Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Feature&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Hannah Rashkin&lt;/b&gt;, &lt;b&gt;David Reitter&lt;/b&gt;, &lt;b&gt;Gaurav Singh Tomar&lt;/b&gt;, &lt;b&gt;Dipanjan Das&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.75.pdf&quot;&gt;Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Peter Shaw&lt;/b&gt;, &lt;b&gt;Ming-Wei Chang&lt;/b&gt;, &lt;b&gt;Panupong Pasupat&lt;/b&gt;, &lt;b&gt;Kristina Toutanova&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.105.pdf&quot;&gt;Exploiting Language Relatedness for Low Web-Resource Language Model Adaptation: An Indic Languages Study&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Yash Khemchandani, Sarvesh Mehtani, Vaidehi Patil, Abhijeet Awasthi, &lt;b&gt;Partha Talukdar&lt;/b&gt;, Sunita Sarawagi&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.144.pdf&quot;&gt;Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Model&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Matthew Finlayson, Aaron Mueller, &lt;b&gt;Sebastian Gehrmann&lt;/b&gt;, Stuart Shieber, Tal Linzen*, Yonatan Belinkov&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.160.pdf&quot;&gt;Modeling Fine-Grained Entity Types with Box Embeddings&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Yasumasa Onoe&lt;/b&gt;, &lt;b&gt;Michael Boratko&lt;/b&gt;, &lt;b&gt;Andrew McCallum&lt;/b&gt;, &lt;b&gt;Greg Durrett&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.293.pdf&quot;&gt;TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Parker Riley*, &lt;b&gt;Noah Constant&lt;/b&gt;, &lt;b&gt;Mandy Guo&lt;/b&gt;, Girish Kumar*, &lt;b&gt;David Uthus&lt;/b&gt;, &lt;b&gt;Zarana Parekh&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.304.pdf&quot;&gt;Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Najoung Kim*, &lt;b&gt;Ellie Pavlick&lt;/b&gt;, &lt;b&gt;Burcu Karagol Ayan&lt;/b&gt;, &lt;b&gt;Deepak Ramachandran&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.294.pdf&quot;&gt;H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Zhenhai Zhu&lt;/b&gt;, &lt;b&gt;Radu Soricut&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.335.pdf&quot;&gt;Are Pretrained Convolutions Better than Pretrained Transformers?&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Yi Tay&lt;/b&gt;, &lt;b&gt;Mostafa Dehghani&lt;/b&gt;, &lt;b&gt;Jai Gupta&lt;/b&gt;, &lt;b&gt;Dara Bahri&lt;/b&gt;, &lt;b&gt;Vamsi Aribandi&lt;/b&gt;, &lt;b&gt;Zhen Qin&lt;/b&gt;, &lt;b&gt;Donald Metzler&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.364.pdf&quot;&gt;Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Robert L Logan IV, &lt;b&gt;Andrew McCallum&lt;/b&gt;, Sameer Singh, &lt;b&gt;Dan Bikel&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.479.pdf&quot;&gt;PhotoChat: A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Xiaoxue Zang&lt;/b&gt;, &lt;b&gt;Lijuan Liu&lt;/b&gt;, &lt;b&gt;Maria Wang&lt;/b&gt;, &lt;b&gt;Yang Song*&lt;/b&gt;, &lt;b&gt;Hao Zhang&lt;/b&gt;, &lt;b&gt;Jindong Chen&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.474.pdf&quot;&gt;Focus Attention: Promoting Faithfulness and Diversity in Summarization&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Rahul Aralikatte*,&lt;b&gt; Shashi Narayan&lt;/b&gt;, &lt;b&gt;Joshua Maynez&lt;/b&gt;, &lt;b&gt;Sascha Rothe&lt;/b&gt;, Ryan McDonald*&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.404.pdf&quot;&gt;A Cognitive Regularizer for Language Modeling&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Jason Wei&lt;/b&gt;, &lt;b&gt;Clara Meister&lt;/b&gt;, &lt;b&gt;Ryan Cotterell&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.521.pdf&quot;&gt;Language Model Augmented Relevance Score&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Ruibo Liu, &lt;b&gt;Jason Wei&lt;/b&gt;, Soroush Vosoughi&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.548.pdf&quot;&gt;Cross-Replication Reliability - An Empirical Approach to Interpreting Inter-rater Reliability&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Ka Wong&lt;/b&gt;, &lt;b&gt;Praveen Paritosh&lt;/b&gt;, &lt;b&gt;Lora Aroyo&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.549.pdf&quot;&gt;TIMEDIAL: Temporal Commonsense Reasoning in Dialog&lt;/a&gt; &lt;br /&gt;&lt;em&gt;Lianhui Qin*,&lt;b&gt; Aditya Gupta&lt;/b&gt;, &lt;b&gt;Shyam Upadhyay&lt;/b&gt;, &lt;b&gt;Luheng He&lt;/b&gt;, &lt;b&gt;Yejin Choi&lt;/b&gt;, &lt;b&gt;Manaal Faruqui&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.559.pdf&quot;&gt;StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Yikang Shen*, &lt;b&gt;Yi Tay&lt;/b&gt;, &lt;b&gt;Che Zheng&lt;/b&gt;, &lt;b&gt;Dara Bahri&lt;/b&gt;, &lt;b&gt;Donald Metzler&lt;/b&gt;, Aaron Courville&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-long.559.pdf&quot;&gt;MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Nicholas FitzGerald&lt;/b&gt;, &lt;b&gt;Jan A. Botha&lt;/b&gt;, &lt;b&gt;Daniel Gillick&lt;/b&gt;, &lt;b&gt;Daniel M. Bikel&lt;/b&gt;, &lt;b&gt;Tom Kwiatkowski&lt;/b&gt;, &lt;b&gt;Andrew McCallum&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-short.35.pdf&quot;&gt;Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Yinfei Yanga&lt;/b&gt;, &lt;b&gt;Ning Jinb&lt;/b&gt;, &lt;b&gt;Kuo Linb&lt;/b&gt;, &lt;b&gt;Mandy Guoa&lt;/b&gt;, &lt;b&gt;Daniel Cera&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-short.41.pdf&quot;&gt;ROPE: Reading Order Equivariant Positional Encoding for Graph-Based Document Information Extraction&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Chen-Yu Lee&lt;/b&gt;, &lt;b&gt;Chun-Liang Li&lt;/b&gt;, &lt;b&gt;Chu Wang∗&lt;/b&gt;, &lt;b&gt;Renshen Wang&lt;/b&gt;, &lt;b&gt;Yasuhisa Fujii&lt;/b&gt;, &lt;b&gt;Siyang Qin&lt;/b&gt;, &lt;b&gt;Ashok Popat&lt;/b&gt;, &lt;b&gt;Tomas Pfister&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-short.49.pdf&quot;&gt;Measuring and Improving BERT’s Mathematical Abilities by Predicting the Order of Reasoning&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Piotr Piekos, &lt;b&gt;Henryk Michalewski&lt;/b&gt;, Mateusz Malinowsk&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-short.81.pdf&quot;&gt;Improving Compositional Generalization in Classification Tasks via Structure Annotations&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Juyong Kim, Pradeep Ravikumar, &lt;b&gt;Joshua Ainslie&lt;/b&gt;, &lt;b&gt;Santiago Ontañón&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-short.89.pdf&quot;&gt;A Simple Recipe for Multilingual Grammatical Error Correction&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Sascha Rothe&lt;/b&gt;, &lt;b&gt;Jonathan Mallinson&lt;/b&gt;, &lt;b&gt;Eric Malmi&lt;/b&gt;, &lt;b&gt;Sebastian Krause&lt;/b&gt;, &lt;b&gt;Aliaksei Severyn&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-short.87.pdf&quot;&gt;nmT5 - Is Parallel Data Still Relevant for Pre-training Massively Multilingual Language Models?&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Mihir Kale&lt;/b&gt;, &lt;b&gt;Aditya Siddhant&lt;/b&gt;, &lt;b&gt;Noah Constant&lt;/b&gt;, &lt;b&gt;Melvin Johnson&lt;/b&gt;, &lt;b&gt;Rami Al-Rfou&lt;/b&gt;, &lt;b&gt;Linting Xue&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.acl-short.83.pdf&quot;&gt;QA-Driven Zero-Shot Slot Filling with Weak Supervision Pretraining&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Xinya Du*, &lt;b&gt;Luheng He&lt;/b&gt;, &lt;b&gt;Qi Li&lt;/b&gt;, Dian Yu*, &lt;b&gt;Panupong Pasupat&lt;/b&gt;, &lt;b&gt;Yuan Zhang&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.findings-acl.299.pdf&quot;&gt;AgreeSum: Agreement-Oriented Multi-Document Summarization&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Richard Yuanzhe Pang*, &lt;b&gt;Adam D. Lelkes&lt;/b&gt;, &lt;b&gt;Vinh Q. Tran&lt;/b&gt;, &lt;b&gt;Cong Yu&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.findings-acl.293.pdf&quot;&gt;Disfl-QA: A Benchmark Dataset for Understanding Disfluencies in Question Answering&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Aditya Gupta&lt;/b&gt;, &lt;b&gt;Jiacheng Xu*&lt;/b&gt;, &lt;b&gt;Shyam Upadhyay&lt;/b&gt;, &lt;b&gt;Diyi Yang&lt;/b&gt;, &lt;b&gt;Manaal Faruqui&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.findings-acl.219.pdf&quot;&gt;Training ELECTRA Augmented with Multi-word Selection&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Jiaming Shen*, &lt;b&gt;Jialu Liu&lt;/b&gt;, &lt;b&gt;Tianqi Liu&lt;/b&gt;, &lt;b&gt;Cong Yu&lt;/b&gt;, Jiawei Han&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.findings-acl.84.pdf&quot;&gt;A Survey of Data Augmentation Approaches for NLP&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Steven Y. Feng, Varun Gangal, &lt;b&gt;Jason Wei&lt;/b&gt;, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, Eduard Hovy&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.findings-acl.81.pdf&quot;&gt;RealFormer: Transformer Likes Residual Attention&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Ruining He&lt;/b&gt;, &lt;b&gt;Anirudh Ravula&lt;/b&gt;, &lt;b&gt;Bhargav Kanagal&lt;/b&gt;, &lt;b&gt;Joshua Ainslie&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.findings-acl.343.pdf&quot;&gt;Scaling Within Document Coreference to Long Texts&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Raghuveer Thirukovalluru, Nicholas Monath, Kumar Shridhar, &lt;b&gt;Manzil Zaheer&lt;/b&gt;, Mrinmaya Sachan, Andrew McCallum&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.findings-acl.254.pdf&quot;&gt;MergeDistill: Merging Language Models using Pre-trained Distillation&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Simran Khanuja&lt;/b&gt;, &lt;b&gt;Melvin Johnson&lt;/b&gt;, &lt;b&gt;Partha Talukdar&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.findings-acl.289.pdf&quot;&gt;DoT: An Efficient Double Transformer for NLP tasks with Tables&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Syrine Krichene&lt;/b&gt;, &lt;b&gt;Thomas Müller*&lt;/b&gt;, &lt;b&gt;Julian Martin Eisenschlos&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.findings-acl.155.pdf&quot;&gt;How Reliable are Model Diagnostics?&lt;/a&gt;&lt;br /&gt;&lt;em&gt;&lt;b&gt;Vamsi Aribandi&lt;/b&gt;, &lt;b&gt;Yi Tay&lt;/b&gt;, &lt;b&gt;Donald Metzler&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Workshops&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;a href=&quot;https://sites.google.com/corp/view/internlp2021/home?authuser=0&quot;&gt;Interactive Learning for Natural Language Processing&lt;/a&gt;&lt;br /&gt;Organizers include:&lt;b&gt; &lt;em&gt;Filip Radlinski&lt;/em&gt;&lt;/b&gt;&lt;br /&gt;Invited Panelist:&lt;b&gt; &lt;em&gt;Julia Kreutzer&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/repl4nlp-2021/home?authuser=0&quot;&gt;6th Workshop on Representation Learning for NLP (RepL4NLP-2021)&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;em&gt;&lt;b&gt;Chris Dyer&lt;/b&gt;, &lt;b&gt;Laura Rimell&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://genderbiasnlp.talp.cat/&quot;&gt;Third Workshop on Gender Bias for Natural Language Processing&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;em&gt;Kellie Webster&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/kwchurch/Benchmarking_past_present_future/blob/master/README.md&quot;&gt;Benchmarking: Past, Present and Future&lt;/a&gt;&lt;br /&gt;Invited Speaker: &lt;b&gt;&lt;em&gt;Eunsol Choi&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://semeval.github.io/SemEval2021/&quot;&gt;SemEval-2021, 15th International Workshop on Semantic Evaluation&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;em&gt;Natalie Schluter&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.workshopononlineabuse.com/home&quot;&gt;Workshop on Online Abuse and Harms&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;em&gt;Vinodkumar Prabhakaran&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://gem-benchmark.com/workshop&quot;&gt;GEM: Natural Language Generation, Evaluation, and Metrics&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;em&gt;Sebastian Gehrmann&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://nlp4prog.github.io/2021/&quot;&gt;Workshop on Natural Language Processing for Programming&lt;/a&gt;&lt;br /&gt;Invited Speaker: &lt;b&gt;&lt;em&gt;Charles Sutton&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://iwpt21.sigparse.org/&quot;&gt;WPT 2021: The 17th International Conference on Parsing Technologies&lt;/a&gt;&lt;br /&gt;Organizers include:&lt;b&gt; &lt;em&gt;Weiwei Sun&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Tutorial&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;a href=&quot;https://multimodal-entailment.github.io/&quot;&gt;Recognizing Multimodal Entailment&lt;/a&gt;&lt;br /&gt;Instructors include: &lt;em&gt;&lt;b&gt;Cesar Ilharco&lt;/b&gt;, &lt;b&gt;Vaiva Imbrasaite&lt;/b&gt;, &lt;b&gt;Ricardo Marino&lt;/b&gt;, &lt;b&gt;Jannis Bulian&lt;/b&gt;, &lt;b&gt;Chen Sun&lt;/b&gt;, &lt;b&gt;Afsaneh Shirazi&lt;/b&gt;, &lt;b&gt;Lucas Smaira&lt;/b&gt;, &lt;b&gt;Cordelia Schmid&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;&lt;!--Footnotes--&gt;&lt;hr width=&quot;80%&quot; /&gt;&lt;p&gt;  &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;&lt;b&gt;*&lt;/b&gt;&lt;/sup&gt;&lt;/span&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&amp;nbsp;&amp;nbsp;Work conducted while at Google.&amp;nbsp;&lt;/span&gt;&lt;/p&gt; &lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=1QIT_sJJbqI:k8nMiQsjsHY:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/1QIT_sJJbqI&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/2120184673555437196/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/08/google-at-acl-2021.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2120184673555437196"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2120184673555437196"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/1QIT_sJJbqI/google-at-acl-2021.html" title="Google at ACL 2021"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/08/google-at-acl-2021.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-537064785672594983</id><published>2021-07-28T08:27:00.004-07:00</published><updated>2021-07-29T13:05:10.956-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Africa"/><category scheme="http://www.blogger.com/atom/ns#" term="AI for Social Good"/><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="datasets"/><title type="text">Mapping Africa’s Buildings with Satellite Imagery</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by John Quinn, Software Engineer, Google Research, Ghana&lt;/span&gt; &lt;p&gt;An accurate record of building footprints is important for a range of applications, from population estimation and urban planning to humanitarian response and environmental science. After a disaster, such as a flood or an earthquake, authorities need to estimate how many households have been affected. Ideally there would be up-to-date census information for this, but in practice such records may be out of date or unavailable. Instead, data on the locations and density of buildings can be a valuable alternative source of information. &lt;/p&gt;&lt;p&gt;A good way to collect such data is through satellite imagery, which can map the distribution of buildings across the world, particularly in areas that are isolated or difficult to access. However, detecting buildings with computer vision methods in some environments can be a challenging task. Because satellite imaging involves photographing the earth from several hundred kilometres above the ground, even at high resolution (30–50 cm per pixel), a small building or tent shelter occupies only a few pixels. The task is even more difficult for informal settlements, or rural areas where buildings constructed with natural materials can visually blend into the surroundings. There are also many types of natural and artificial features that can be easily confused with buildings in overhead imagery. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-8FOuLSKgG-s/YQFy5MkCCGI/AAAAAAAAH8s/4c2uBwe_cWYpSe6_x7-rkIQD1k1i8yxJgCLcBGAsYHQ/s901/ConfoundingObjects.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;890&quot; height=&quot;400&quot; src=&quot;https://1.bp.blogspot.com/-8FOuLSKgG-s/YQFy5MkCCGI/AAAAAAAAH8s/4c2uBwe_cWYpSe6_x7-rkIQD1k1i8yxJgCLcBGAsYHQ/w395-h400/ConfoundingObjects.png&quot; width=&quot;395&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Objects that can confuse computer vision models for building identification (clockwise from top left) pools, rocks, enclosure walls and shipping containers.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2107.12283&quot;&gt;Continental-Scale Building Detection from High-Resolution Satellite Imagery&lt;/a&gt;”, we address these challenges, using new methods for detecting buildings that work in rural and urban settings across different terrains, such as savannah, desert, and forest, as well as informal settlements and refugee facilities. We use this building detection model to create the &lt;a href=&quot;https://sites.research.google/open-buildings&quot;&gt;Open Buildings dataset&lt;/a&gt;, a new open-access data resource containing the locations and footprints of 516 million buildings with coverage across most of the African continent. The dataset will support several practical, scientific and humanitarian applications, ranging from disaster response or population mapping to planning services such as new medical facilities or studying human impact on the natural environment.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Model Development&lt;/b&gt;&lt;br /&gt;We built a training dataset for the building detection model by manually labelling 1.75 million buildings in 100k images. The figure below shows some examples of how we labelled images in the training data, taking into account confounding characteristics of different areas across the African continent. In rural areas, for example, it was necessary to identify different types of dwelling places and to disambiguate them from natural features, while in urban areas we needed to develop labelling policies for dense and contiguous structures. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-N5eiVvwK_Gk/YQFzITyXhtI/AAAAAAAAH8w/F2djNAFXCTUDj7x-SdmeC4pDfCqkoLy8ACLcBGAsYHQ/s745/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;240&quot; data-original-width=&quot;745&quot; height=&quot;206&quot; src=&quot;https://1.bp.blogspot.com/-N5eiVvwK_Gk/YQFzITyXhtI/AAAAAAAAH8w/F2djNAFXCTUDj7x-SdmeC4pDfCqkoLy8ACLcBGAsYHQ/w640-h206/image1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;(1)&lt;/b&gt; Example of a compound containing both dwelling places as well as smaller outbuildings such as grain stores. &lt;b&gt;(2)&lt;/b&gt; Example of a round, thatched-roof structure that can be difficult for a model to distinguish from trees, and where it is necessary to use cues from pathways, clearings and shadows to disambiguate. &lt;b&gt;(3)&lt;/b&gt; Example of several contiguous buildings for which the boundaries cannot be easily distinguished.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We trained the model to detect buildings in a bottom-up way, first by classifying each pixel as building or non-building, and then grouping these pixels together into individual instances. The detection pipeline was based on the &lt;a href=&quot;https://arxiv.org/abs/1505.04597&quot;&gt;U-Net&lt;/a&gt; model, which is commonly used in satellite image analysis. One advantage of U-Net is that it is a relatively compact architecture, and so can be applied to large quantities of imaging data without a heavy compute burden. This is critical, because the final task of applying this to continental-scale satellite imagery means running the model on many billions of image tiles.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-GOkhVuEYtbM/YQFzZ_CqulI/AAAAAAAAH88/TH1V6nURowY9fimNmYqXQpGYG27GpjJUgCLcBGAsYHQ/s1685/image5.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;498&quot; data-original-width=&quot;1685&quot; height=&quot;190&quot; src=&quot;https://1.bp.blogspot.com/-GOkhVuEYtbM/YQFzZ_CqulI/AAAAAAAAH88/TH1V6nURowY9fimNmYqXQpGYG27GpjJUgCLcBGAsYHQ/w640-h190/image5.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Example of segmenting buildings in satellite imagery.&amp;nbsp;&lt;b&gt;Left:&lt;/b&gt;&amp;nbsp;Source image;&amp;nbsp;&lt;b&gt;Center:&lt;/b&gt;&amp;nbsp;Semantic segmentation, with each pixel assigned a confidence score that it is a building vs. non-building;&amp;nbsp;&lt;b&gt;Right:&lt;/b&gt;&amp;nbsp;Instance segmentation, obtained by thresholding and grouping together connected components.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Initial experiments with the basic model had low &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;&gt;precision and recall&lt;/a&gt;, for example due to the variety of natural and artificial features with building-like appearance. We found a number of methods that improved performance. One was the use of &lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;mixup&lt;/a&gt; as a regularisation method, where random training images are blended together by taking a weighted average. Though mixup was originally proposed for image classification, we modified it to be used for semantic segmentation. Regularisation is important in general for this building segmentation task, because even with 100k training images, the training data do not capture the full variation of terrain, atmospheric and lighting conditions that the model is presented with at test time, and hence, there is a tendency to overfit. This is mitigated by mixup as well as random augmentation of training images. &lt;/p&gt;&lt;p&gt;Another method that we found to be effective was the use of unsupervised self-training. We prepared a set of 100 million satellite images from across Africa, and filtered these to a subset of 8.7 million images that mostly contained buildings. This dataset was used for self-training using the &lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;&gt;Noisy Student&lt;/a&gt; method, in which the output of the best building detection model from the previous stage is used as a ‘teacher’ to then train a ‘student’ model that makes similar predictions from augmented images. In practice, we found that this reduced false positives and sharpened the detection output. The student model gave higher confidence to buildings and lower confidence to background. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-kFv8TDjiGOg/YQFziW7dZWI/AAAAAAAAH9A/RuuF9oumsDgC8BZy-K7vRLXJUEqCE2_VQCLcBGAsYHQ/s1051/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;302&quot; data-original-width=&quot;1051&quot; height=&quot;184&quot; src=&quot;https://1.bp.blogspot.com/-kFv8TDjiGOg/YQFziW7dZWI/AAAAAAAAH9A/RuuF9oumsDgC8BZy-K7vRLXJUEqCE2_VQCLcBGAsYHQ/w640-h184/image4.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Difference in model output between the student and teacher models for a typical image. In panel (d), red areas are those that the student model finds more likely to be buildings than the teacher model, and blue areas more likely to be background.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;One problem that we faced initially was that our model had a tendency to create “blobby” detections, without clearly delineated edges and with a tendency for neighbouring buildings to be merged together. To address this, we applied another idea from the original &lt;a href=&quot;https://arxiv.org/abs/1505.04597&quot;&gt;U-Net paper&lt;/a&gt;, which is to use distance weighting to adapt the loss function to emphasise the importance of making correct predictions near boundaries. During training, distance weighting places greater emphasis at the edges by adding weight to the loss — particularly where there are instances that nearly touch. For building detection, this encourages the model to correctly identify the gaps in between buildings, which is important so that many close structures are not merged together. We found that the original U-Net distance weighting formulation was helpful but slow to compute. So, we developed an alternative based on Gaussian convolution of edges, which was both faster and more effective. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-eJYbpIYg0ms/YQFz9s46igI/AAAAAAAAH9I/DUt9_kPc3domDQDFAE9gUE0lmWxHlZntwCLcBGAsYHQ/s1013/Edges.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;508&quot; data-original-width=&quot;1013&quot; height=&quot;200&quot; src=&quot;https://1.bp.blogspot.com/-eJYbpIYg0ms/YQFz9s46igI/AAAAAAAAH9I/DUt9_kPc3domDQDFAE9gUE0lmWxHlZntwCLcBGAsYHQ/w400-h200/Edges.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Distance weighting schemes to emphasise nearby edges: U-Net&amp;nbsp;&lt;b&gt;(left)&lt;/b&gt;&amp;nbsp;and Gaussian convolution of edges&amp;nbsp;&lt;b&gt;(right)&lt;/b&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Our &lt;a href=&quot;https://arxiv.org/abs/2107.12283&quot;&gt;technical report&lt;/a&gt; has more details on each of these methods. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Results&lt;/b&gt;&lt;br /&gt;We evaluated the performance of the model on several different regions across the continent, in different categories: urban, rural, and medium-density. In addition, with the goal of preparing for potential humanitarian applications, we tested the model on regions with displaced persons and refugee settlements. Precision and recall did vary between regions, so achieving consistent performance across the continent is an ongoing challenge. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-Q329VaudvO4/YQF0f1AXO8I/AAAAAAAAH9Q/dfml_Xjue7Y_h1f5NhYylPr6_5HB1mCfACLcBGAsYHQ/s604/PrecisionRecall.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;604&quot; data-original-width=&quot;553&quot; height=&quot;640&quot; src=&quot;https://1.bp.blogspot.com/-Q329VaudvO4/YQF0f1AXO8I/AAAAAAAAH9Q/dfml_Xjue7Y_h1f5NhYylPr6_5HB1mCfACLcBGAsYHQ/w586-h640/PrecisionRecall.jpg&quot; width=&quot;586&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Precision-recall curves, measured at 0.5&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;intersection-over-union&lt;/a&gt;&amp;nbsp;threshold.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;When visually inspecting the detections for low-scoring regions, we noted various causes. In rural areas, label errors were problematic. For example, single buildings within a mostly-empty area can be difficult for labellers to spot. In urban areas, the model had a tendency to split large buildings into separate instances. The model also underperformed in desert terrain, where buildings were hard to distinguish against the background. &lt;/p&gt;&lt;p&gt;We carried out an ablation study to understand which methods contributed most to the final performance, measured in &lt;a href=&quot;https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision&quot;&gt;mean average precision&lt;/a&gt; (mAP). Distance weighting, mixup and the use of &lt;a href=&quot;https://image-net.org/&quot;&gt;ImageNet&lt;/a&gt; pre-training were the biggest factors for the performance of the supervised learning baseline. The ablated models that did not use these methods had a mAP difference of -0.33, -0.12 and -0.07 respectively. Unsupervised self-training gave a further significant boost of +0.06 mAP. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-MES2HEmsCKc/YQGXrO09syI/AAAAAAAAH-I/Gfx8zbXdipcEf5R_YVTGR7WBr2nhe9cYgCLcBGAsYHQ/s1350/ablations.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;545&quot; data-original-width=&quot;1350&quot; height=&quot;258&quot; src=&quot;https://1.bp.blogspot.com/-MES2HEmsCKc/YQGXrO09syI/AAAAAAAAH-I/Gfx8zbXdipcEf5R_YVTGR7WBr2nhe9cYgCLcBGAsYHQ/w640-h258/ablations.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Ablation study of training methods. The first row shows the mAP performance of the best model combined with self-training, and the second row shows the best model with supervised learning only (the baseline). By disabling each training optimization from the baseline in turn, we observe the impact on mAP test performance. Distance weighting has the most significant effect.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-u-WegC4oZ4E/YQF0z95UlnI/AAAAAAAAH9c/_ZHIfHJjJXUJU5ZOWl7LyVZoww7fakszQCLcBGAsYHQ/s648/Ablation.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;262&quot; data-original-width=&quot;648&quot; height=&quot;258&quot; src=&quot;https://1.bp.blogspot.com/-u-WegC4oZ4E/YQF0z95UlnI/AAAAAAAAH9c/_ZHIfHJjJXUJU5ZOWl7LyVZoww7fakszQCLcBGAsYHQ/w640-h258/Ablation.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Ablation study of training methods. The first row shows the mAP performance of the best model combined with self-training, and the second row shows the best model with supervised learning only (the baseline). By disabling each training optimization from the baseline in turn, we observe the impact on mAP test performance. Distance weighting has the most significant effect.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;--&gt;&lt;p&gt;&lt;b&gt;Generating the Open Buildings Dataset&lt;/b&gt;&lt;br /&gt;To create the final dataset, we applied our best building detection model to satellite imagery across the African continent (8.6 billion image tiles covering 19.4 million km&lt;sup&gt;2&lt;/sup&gt;, 64% of the continent), which resulted in the detection of 516M distinct structures. &lt;/p&gt;&lt;p&gt;Each building’s outline was simplified as a polygon and associated with a &lt;a href=&quot;https://maps.google.com/pluscodes/&quot;&gt;Plus Code&lt;/a&gt;, which is a geographic identifier made up of numbers and letters, akin to a street address, and useful for identifying buildings in areas that don’t have formal addressing systems. We also include confidence scores and guidance on suggested thresholds to achieve particular precision levels. &lt;/p&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-nM7u9-jNRB0/YQF083zsIFI/AAAAAAAAH9k/3i33jLnVbxY7c1PJE1kTT_WXX4a5fnERwCLcBGAsYHQ/s801/image2.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;585&quot; data-original-width=&quot;801&quot; height=&quot;293&quot; src=&quot;https://1.bp.blogspot.com/-nM7u9-jNRB0/YQF083zsIFI/AAAAAAAAH9k/3i33jLnVbxY7c1PJE1kTT_WXX4a5fnERwCLcBGAsYHQ/w400-h293/image2.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;  The sizes of the structures vary as shown below, tending towards small footprints. The inclusion of small structures is important, for example, to support analyses of informal settlements or refugee facilities. &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-jVxTGydW6yI/YQF1NpCMBrI/AAAAAAAAH90/622jzhqrTzIN2g5PNTw39Kr523JIXK8vwCLcBGAsYHQ/s538/FootprintDistribution.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;498&quot; data-original-width=&quot;538&quot; height=&quot;370&quot; src=&quot;https://1.bp.blogspot.com/-jVxTGydW6yI/YQF1NpCMBrI/AAAAAAAAH90/622jzhqrTzIN2g5PNTw39Kr523JIXK8vwCLcBGAsYHQ/w400-h370/FootprintDistribution.jpg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Distribution of building footprint sizes.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The data is freely available and we look forward to hearing how it is used. In the future, we may add new features and regions, depending on usage and feedback. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;This work is part of our &lt;a href=&quot;https://ai.google/social-good/&quot;&gt;AI for Social Good&lt;/a&gt; efforts and was led by Google Research, Ghana.  Thanks to the co-authors of this work: Wojciech Sirko, Sergii Kashubin, Marvin Ritter, Abigail Annkah, Yasser Salah Eddine Bouchareb, Yann Dauphin, Daniel Keysers, Maxim Neumann and Moustapha Cisse. We are grateful to Abdoulaye Diack, Sean Askay, Ruth Alcantara and Francisco Moneo for help with coordination. Rob Litzke, Brian Shucker, Yan Mayster and Michelina Pallone provided valuable assistance with geo infrastructure.  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=bcEqeVSMnBQ:_yCO0jy9YNI:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/bcEqeVSMnBQ&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/537064785672594983/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/mapping-africas-buildings-with.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/537064785672594983"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/537064785672594983"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/bcEqeVSMnBQ/mapping-africas-buildings-with.html" title="Mapping Africa’s Buildings with Satellite Imagery"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-8FOuLSKgG-s/YQFy5MkCCGI/AAAAAAAAH8s/4c2uBwe_cWYpSe6_x7-rkIQD1k1i8yxJgCLcBGAsYHQ/s72-w395-h400-c/ConfoundingObjects.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/07/mapping-africas-buildings-with.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-3366307881061046058</id><published>2021-07-27T09:49:00.001-07:00</published><updated>2021-07-28T12:56:14.486-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="AI"/><category scheme="http://www.blogger.com/atom/ns#" term="Information Retrieval"/><category scheme="http://www.blogger.com/atom/ns#" term="open source"/><category scheme="http://www.blogger.com/atom/ns#" term="TensorFlow"/><title type="text">Advances in TF-Ranking</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Michael Bendersky and Xuanhui Wang, Software Engineers, Google Research&lt;/span&gt; &lt;p&gt;In December 2018, &lt;a href=&quot;https://ai.googleblog.com/2018/12/tf-ranking-scalable-tensorflow-library.html&quot;&gt;we&lt;/a&gt; &lt;a href=&quot;https://youtu.be/Un0JDL3i5Hg&quot;&gt;introduced&lt;/a&gt; &lt;a href=&quot;https://github.com/tensorflow/ranking&quot;&gt;TF-Ranking&lt;/a&gt;,&amp;nbsp;an open-source TensorFlow-based library for developing scalable neural &lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_to_rank&quot;&gt;learning-to-rank&lt;/a&gt; (LTR) models, which are useful in settings where users expect to receive an ordered list of items in response to their query. LTR models — unlike standard classification models that classify one item at a time — receive an entire list of items as an input, and learn an ordering that maximizes the utility of the entire list. While search and recommendation systems are the most common applications of LTR models, since its release, we have seen TF-Ranking being applied in diverse domains beyond search, including &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3308560.3316603&quot;&gt;e-commerce&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1904.12084&quot;&gt;SAT solvers&lt;/a&gt;, and &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3450267.3450538&quot;&gt;smart city planning&lt;/a&gt;.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-IQTHStDjBVk/YQG1FVsdgNI/AAAAAAAAH-Q/WpWEWkgot483yRy6RhUsDvhaiRmPo2MzwCLcBGAsYHQ/s769/image1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;350&quot; data-original-width=&quot;769&quot; height=&quot;183&quot; src=&quot;https://1.bp.blogspot.com/-IQTHStDjBVk/YQG1FVsdgNI/AAAAAAAAH-Q/WpWEWkgot483yRy6RhUsDvhaiRmPo2MzwCLcBGAsYHQ/w400-h183/image1.jpg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The goal of learning-to-rank (LTR) is to learn a function f() that takes as an input a list of items (documents, products, movies, etc.) and outputs the list of items in the optimal order (descending order of relevance). Here, green shade indicates item relevance level, and the red item marked with 'x' is non-relevant.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;In May 2021, we published a &lt;a href=&quot;https://github.com/tensorflow/ranking/releases/tag/v0.4.0&quot;&gt;major release&lt;/a&gt; of TF-Ranking that enables full support for natively building LTR models using &lt;a href=&quot;https://keras.io/about/&quot;&gt;Keras&lt;/a&gt;, a high-level API of &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow 2&lt;/a&gt;. Our native Keras ranking model has a brand-new workflow design, including a flexible &lt;em&gt;ModelBuilder&lt;/em&gt;, a &lt;em&gt;DatasetBuilder&lt;/em&gt; to set up training data, and a &lt;em&gt;Pipeline&lt;/em&gt; to train the model with the provided dataset. These components make building a customized LTR model easier than ever, and facilitate rapid exploration of new model structures for production and research. If &lt;a href=&quot;https://blog.tensorflow.org/2018/12/introducing-ragged-tensors.html&quot;&gt;RaggedTensors&lt;/a&gt; are your tool of choice, TF-Ranking is now working with them &lt;a href=&quot;https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/keras/antique_ragged.py&quot;&gt;as well&lt;/a&gt;. In addition, our &lt;a href=&quot;https://github.com/tensorflow/ranking/releases/tag/v0.4.2&quot;&gt;most recent release&lt;/a&gt;, which incorporates the &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/orbit&quot;&gt;Orbit&lt;/a&gt; training library, contains a long list of advances — the culmination of two and half years of neural LTR research. Below we share a few of the key improvements available in the latest TF-Ranking version. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-bL294WB0B6c/YQG1MepjFMI/AAAAAAAAH-U/oh9jrFOSzKAEuAL-r52v87Okb901AT3VgCLcBGAsYHQ/s339/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;248&quot; data-original-width=&quot;339&quot; height=&quot;293&quot; src=&quot;https://1.bp.blogspot.com/-bL294WB0B6c/YQG1MepjFMI/AAAAAAAAH-U/oh9jrFOSzKAEuAL-r52v87Okb901AT3VgCLcBGAsYHQ/w400-h293/image3.jpg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Workflow to build and train a native Keras ranking model. Blue modules are provided by TF-Ranking, and green modules are customizable.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;      &lt;p&gt;&lt;b&gt;Learning-to-Rank with TFR-BERT&lt;/b&gt;&lt;br /&gt;Recently, pretrained language models like &lt;a href=&quot;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&quot;&gt;BERT&lt;/a&gt; have achieved state-of-the-art performance on various language understanding tasks. To capture the expressiveness of these models, TF-Ranking implements a novel TFR-BERT architecture that couples BERT with the power of LTR to optimize the ordering of list inputs. As an example, consider a query and a list of &lt;em&gt;n&lt;/em&gt; documents that one might like to rank in response to this query. Instead of learning an independent BERT representation for each &lt;em&gt;&amp;lt;query, document&amp;gt;&lt;/em&gt; pair, LTR models apply a &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf&quot;&gt;ranking loss&lt;/a&gt; to jointly learn a BERT representation that maximizes the utility of the entire ranked list with respect to the ground-truth labels. &lt;/p&gt;&lt;p&gt;The figure below illustrates this process. First, we flatten a list of &lt;em&gt;n&lt;/em&gt; documents to rank in response to a query into a list &lt;em&gt;&amp;lt;query, document&amp;gt;&lt;/em&gt; tuples. These tuples are fed into a pre-trained language model (e.g., BERT). The pooled BERT outputs for the entire document list are then jointly fine-tuned with one of the specialized &lt;a href=&quot;https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/python/losses.py&quot;&gt;ranking losses&lt;/a&gt; available in TF-Ranking. Our experience shows that this TFR-BERT architecture delivers significant improvements in pretrained language model performance, leading to state-of-the-art performance for &lt;a href=&quot;https://arxiv.org/abs/2004.08476&quot;&gt;several&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2010.00200&quot;&gt;popular&lt;/a&gt; ranking tasks, especially when multiple pretrained language models are ensembled. Our users can now get started with TFR-BERT using this &lt;a href=&quot;https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/keras/tfrbert_antique_train.py&quot;&gt;simple example&lt;/a&gt;. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-3Y-7M__j9XM/YQG1RMl5hTI/AAAAAAAAH-Y/i-tZ7G-FTKoSoGnDsCLOYsU1OrWQvEwYgCLcBGAsYHQ/s907/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;285&quot; data-original-width=&quot;907&quot; height=&quot;202&quot; src=&quot;https://1.bp.blogspot.com/-3Y-7M__j9XM/YQG1RMl5hTI/AAAAAAAAH-Y/i-tZ7G-FTKoSoGnDsCLOYsU1OrWQvEwYgCLcBGAsYHQ/s16000/image4.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;An illustration of the TFR-BERT architecture, in which a joint LTR model over a list of n documents is constructed using BERT representations of individual &amp;lt;query, document&amp;gt; pairs.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Interpretable Learning-to-Rank&lt;/b&gt;&lt;br /&gt;Transparency and interpretability are important factors in deploying LTR models in ranking systems that can be involved in determining the outcomes of processes such as loan eligibility assessment, advertisement targeting, or guiding medical treatment decisions. In such cases, the contribution of each individual feature to the final ranking should be examinable and understandable to ensure transparency, accountability and fairness of the outcomes.  &lt;/p&gt;&lt;p&gt;One possible way to achieve this is using &lt;a href=&quot;https://en.wikipedia.org/wiki/Generalized_additive_model&quot;&gt;generalized additive models&lt;/a&gt; (GAMs) — intrinsically interpretable machine learning models that are linearly composed of smooth functions of individual features. However, while GAMs have been extensively studied on &lt;a href=&quot;https://en.wikipedia.org/wiki/Regression_analysis&quot;&gt;regression&lt;/a&gt; and classification tasks, it is less clear how to apply them in a ranking setting. For instance, while GAMs can be straightforwardly applied to model each individual item in the list, modeling both item interactions and the context in which these items are ranked is a more challenging research problem. To this end, we have developed a &lt;a href=&quot;https://arxiv.org/abs/2005.02553&quot;&gt;neural ranking GAM&lt;/a&gt; — an extension of generalized additive models to ranking problems. &lt;/p&gt;&lt;p&gt;Unlike standard GAMs, a neural ranking GAM can take into account both the features of the ranked items and the context features (e.g., query or user profile) to derive an interpretable, compact model. This ensures that not only the contribution of each item-level feature is interpretable, but also the contribution of the context features. For example, in the figure below, using a neural ranking GAM makes visible how distance, price, and relevance, in the context of a given user device, contribute to the final ranking of the hotel. Neural ranking GAMs are now &lt;a href=&quot;https://github.com/tensorflow/ranking/issues/202&quot;&gt;available&lt;/a&gt; as a part of TF-Ranking, &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-wLnimKYmhbA/YPrnHWh4jiI/AAAAAAAAH8c/AndggKFG4_YTKo7MeZ6m5E67ZA-Z4wQfgCLcBGAsYHQ/s1270/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;940&quot; data-original-width=&quot;1270&quot; src=&quot;https://1.bp.blogspot.com/-wLnimKYmhbA/YPrnHWh4jiI/AAAAAAAAH8c/AndggKFG4_YTKo7MeZ6m5E67ZA-Z4wQfgCLcBGAsYHQ/s16000/image2.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;An example of applying neural ranking GAM for local search. For each input feature (e.g., price, distance), a sub-model produces a sub-score that can be examined, providing transparency. Context features (e.g., user device type) can be utilized to derive importance weights of submodels.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Neural Ranking or Gradient Boosting?&lt;/b&gt;&lt;br /&gt;While neural models have achieved state of the art performance in multiple domains, specialized &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting&quot;&gt;gradient boosted decision trees&lt;/a&gt; (GBDTs) like &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2016/02/MSR-TR-2010-82.pdf&quot;&gt;LambdaMART&lt;/a&gt; remained the baseline to beat in a variety of open LTR datasets. The success of GBDTs in open datasets is due to several reasons. First, due to their relatively small size, neural models are prone to &lt;a href=&quot;https://en.wikipedia.org/wiki/Overfitting&quot;&gt;overfitting&lt;/a&gt; on these datasets. Second, since GBDTs partition their input feature space using decision trees, they are naturally more resilient to variations in numerical scales in ranking data, which often contain features with &lt;a href=&quot;https://en.wikipedia.org/wiki/Zipf%27s_law&quot;&gt;Zipfian&lt;/a&gt; or otherwise skewed distributions. However, GBDTs do have their limitations in &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3292500.3330676&quot;&gt;more realistic&lt;/a&gt; ranking scenarios, which often combine both textual and numerical features. For instance, GBDTs cannot be directly applied to large discrete feature spaces, such as raw document text. They are also, in general, less scalable than neural ranking models. &lt;/p&gt;&lt;p&gt;Therefore, since the TF-Ranking release, our team has significantly deepened the understanding of how best to leverage neural models in ranking with numerical features. This culminated in a Data Augmented Self-Attentive Latent Cross (DASALC) model, described in an &lt;a href=&quot;https://research.google/pubs/pub50030/&quot;&gt;ICLR 2021 paper&lt;/a&gt;, which is the first to establish parity, and in some cases statistically significant improvements, of neural ranking models over strong LambdaMART baselines on open LTR datasets. This achievement is made possible through a combination of techniques, which include data augmentation, &lt;a href=&quot;https://research.google/pubs/pub49171/&quot;&gt;neural feature transformation&lt;/a&gt;, self-attention for &lt;a href=&quot;https://research.google/pubs/pub49364/&quot;&gt;modeling&lt;/a&gt; document interactions, &lt;a href=&quot;https://research.google/pubs/pub48321/&quot;&gt;listwise ranking loss&lt;/a&gt;, and model ensembling similar to boosting in GBDTs. The architecture of the DASALC model was entirely implemented using the TF-Ranking library. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;All in all, we believe that the new Keras-based TF-Ranking version will make it easier to conduct neural LTR research and deploy production-grade ranking systems. We encourage everyone to try out the &lt;a href=&quot;https://github.com/tensorflow/ranking/releases/tag/v0.4.0&quot;&gt;latest version&lt;/a&gt; and follow &lt;a href=&quot;https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/keras/keras_dnn_tfrecord.py&quot;&gt;this introductory example&lt;/a&gt; for a hands-on experience. While we are very excited about this new release, our research and development journey is far from over, so we will continue to advance our understanding of learning-to-rank problems and share these advances with our users. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;This project was only possible thanks to the current and past members of the TF-Ranking team: Honglei Zhuang, ‎Le Yan, Rama Pasumarthi, Rolf Jagerman, Zhen Qin, Shuguang Han, Sebastian Bruch, Nathan Cordeiro, Marc Najork and Patrick McGregor. We also extend special thanks to our collaborators from the Tensorflow team: Zhenyu Tan, Goldie Gadde, Rick Chao, Yuefeng Zhou‎, Hongkun Yu, and Jing Li.&lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=bzF_yMoyjxE:hL_Y6nR_WqQ:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/bzF_yMoyjxE&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/3366307881061046058/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/advances-in-tf-ranking.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3366307881061046058"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3366307881061046058"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/bzF_yMoyjxE/advances-in-tf-ranking.html" title="Advances in TF-Ranking"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-IQTHStDjBVk/YQG1FVsdgNI/AAAAAAAAH-Q/WpWEWkgot483yRy6RhUsDvhaiRmPo2MzwCLcBGAsYHQ/s72-w400-h183-c/image1.jpg" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/07/advances-in-tf-ranking.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-791697466172075100</id><published>2021-07-23T12:12:00.000-07:00</published><updated>2021-07-23T12:12:34.579-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="accessibility"/><category scheme="http://www.blogger.com/atom/ns#" term="Audio"/><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Health"/><category scheme="http://www.blogger.com/atom/ns#" term="Research"/><title type="text">Applying Advanced Speech Enhancement in Cochlear Implants</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Samuel J. Yang, Research Scientist and Dick Lyon, Principal Scientist, Google Research&lt;/span&gt; &lt;p&gt;For the &lt;a href=&quot;https://www.who.int/news-room/fact-sheets/detail/deafness-and-hearing-loss&quot;&gt;~466 million people in the world&lt;/a&gt; who are deaf or hard of hearing, the lack of easy access to accessibility services can be a barrier to participating in spoken conversations encountered daily. While hearing aids can help alleviate this, simply amplifying sound is insufficient for many. One additional option that may be available is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cochlear_implant&quot;&gt;cochlear implant&lt;/a&gt; (CI), which is an electronic device that is surgically inserted into a part of the inner ear, called the &lt;a href=&quot;https://simple.wikipedia.org/wiki/Cochlea&quot;&gt;cochlea&lt;/a&gt;, and stimulates the auditory nerve electrically via external sound processors. While many individuals with these cochlear implants can learn to interpret these electrical stimulations as audible speech, the listening experience can be quite varied and particularly challenging in noisy environments.  &lt;/p&gt;&lt;p&gt;Modern cochlear implants drive electrodes with pulsatile signals (i.e., discrete stimulation pulses) that are computed by external sound processors. The main challenge still facing the CI field is how to best process sounds — to convert sounds to pulses on electrodes — in a way that makes them more intelligible to users. Recently, to stimulate progress on this problem, scientists in industry and academia organized a &lt;a href=&quot;https://cihackathon.com/&quot;&gt;CI Hackathon&lt;/a&gt; to open the problem up to a wider range of ideas. &lt;/p&gt;&lt;p&gt;In this post, we share exploratory research demonstrating that a speech enhancement preprocessor — specifically, a noise suppressor — can be used at the input of a CI’s processor to enhance users’ understanding of speech in noisy environments. We also discuss how we built on this work in &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/cochlear_implant&quot;&gt;our entry&lt;/a&gt; for the CI Hackathon and how we will continue developing this work. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Improving CIs with Noise Suppression&lt;/b&gt;&lt;br/&gt;In 2019, a small internal project demonstrated the benefits of noise suppression at the input of a CI’s processor. In this project, participants listened to 60 pre-recorded and pre-processed audio samples and ranked them by their listening comfort. CI users listened to the audio using their devices' existing strategy for generating electrical pulses. &lt;/p&gt;  &lt;table align=&quot;center&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center; width: 60%;&quot;&gt;&lt;tbody&gt;       &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Audio without background noise&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://github.com/google-research/google-research/raw/77439a33a5c2234a1f4aec9e4e986fed4af5bd94/cochlear_implant/sample_audio/clean.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;        &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;        &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Audio with background noise&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://github.com/google-research/google-research/raw/master/cochlear_implant/sample_audio/noisy.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;      &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;     &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Audio with background noise + noise suppression&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://github.com/google-research/google-research/raw/master/cochlear_implant/sample_audio/enhanced.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;     &lt;/tbody&gt;&lt;/table&gt;&lt;br/&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Background audio clip from &lt;a href=&quot;https://www.flickr.com/photos/39881443@N06/4160025701/&quot;&gt;“IMG_0991.MOV” by Kenny MacCarthy&lt;/a&gt;, license: &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;&gt;CC-BY 2.0&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;     &lt;p&gt;As shown below, both listening comfort and intelligibility usually increased, sometimes dramatically, when speech with noise (the lightest bar) was processed with noise suppression. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/--_N9quZ9cBU/YPrSHh7uklI/AAAAAAAAH74/IGCBSmDqzT0Q_rIFfTFkRmxRx7EQtUnNwCLcBGAsYHQ/s1358/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1358&quot; data-original-width=&quot;1258&quot; height=&quot;400&quot; src=&quot;https://1.bp.blogspot.com/--_N9quZ9cBU/YPrSHh7uklI/AAAAAAAAH74/IGCBSmDqzT0Q_rIFfTFkRmxRx7EQtUnNwCLcBGAsYHQ/w370-h400/image1.png&quot; width=&quot;370&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;CI users in an early research study have improved listening comfort — qualitatively scored from &quot;very poor&quot; (0.0) to &quot;OK&quot; (0.5) to &quot;very good&quot; (1.0) — and speech intelligibility (i.e., the fraction of words in a sentence correctly transcribed) when trying to listen to noisy audio samples of speech with noise suppression applied.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;For the CI Hackathon, we &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/cochlear_implant&quot;&gt;built on the project&lt;/a&gt; above, continuing to leverage our use of a noise suppressor while additionally exploring an approach to compute the pulses too &lt;/p&gt;&lt;p&gt;&lt;b&gt;Overview of the Processing Approach&lt;/b&gt;&lt;br/&gt;The hackathon considered a CI with 16 electrodes. Our approach decomposes the audio into 16 overlapping frequency bands, corresponding to the positions of the electrodes in the cochlea.  Next, because the dynamic range of sound easily spans multiple orders of magnitude more than what we expect the electrodes to represent, we aggressively compress the dynamic range of the signal by applying &lt;a href=&quot;https://doi.org/10.1109/ICASSP.2017.7953242&quot;&gt;&quot;per-channel energy normalization&quot;&lt;/a&gt; (PCEN). Finally, the range-compressed signals are used to create the electrodogram (i.e., what the CI displays on the electrodes). &lt;/p&gt;&lt;p&gt;In addition, the hackathon required a submission be evaluated in multiple audio categories, including music, which is an important but notoriously difficult category of sounds for CI users to enjoy. However, the speech enhancement network was trained to suppress non-speech sounds, including both noise and music, so we needed to take extra measures to avoid suppressing instrumental music (note that in general, music suppression might be preferred by some users in certain contexts). To do this, we created a “mix” of the original audio with the noise-suppressed audio so that enough of the music would pass through to remain audible. We varied in real-time the fraction of original audio mixed from 0% to 40% (0% if all of the input is estimated as speech, up to 40% as more of the input is estimated as non-speech) based on the estimate from the open-source &lt;a href=&quot;https://www.tensorflow.org/hub/tutorials/yamnet&quot;&gt;YAMNet&lt;/a&gt; classifier on every ~1 second window of audio of whether the input is speech or non-speech. &lt;/p&gt;&lt;p&gt;&lt;b&gt;The Conv-TasNet Speech Enhancement Model&lt;/b&gt;&lt;br/&gt;To implement a speech enhancement module that suppresses non-speech sounds, such as noise and music, we use the &lt;a href=&quot;https://doi.org/10.1109/TASLP.2019.2915167&quot;&gt;Conv-TasNet&lt;/a&gt; model, which can separate different kinds of sounds. To start, the raw audio waveforms are transformed and processed into a form that can be used by a neural network. The model transforms short, 2.5 millisecond frames of input audio with a learnable analysis transform to generate features optimized for sound separation. The network then produces two “masks” from those features: one mask for speech and one mask for noise. These masks indicate the degree to which each feature corresponds to either speech or noise. Separated speech and noise are reconstructed back to the audio domain by multiplying the masks with the analysis features, applying a synthesis transform back to audio-domain frames, and stitching the resulting short frames together. As a final step, the speech and noise estimates are processed by a &lt;a href=&quot;https://research.google/pubs/pub47816/&quot;&gt;mixture consistency layer&lt;/a&gt;, which improves the quality of the estimated waveforms by ensuring that they sum up to the original input mixture waveform.  &lt;/p&gt;&lt;p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-ZV96R5C1OVk/YPrSTU1EWDI/AAAAAAAAH78/CDaBE7sdn2ckUZgrUg32Ohv-VXnuog2SQCLcBGAsYHQ/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;427&quot; data-original-width=&quot;1999&quot; height=&quot;136&quot; src=&quot;https://1.bp.blogspot.com/-ZV96R5C1OVk/YPrSTU1EWDI/AAAAAAAAH78/CDaBE7sdn2ckUZgrUg32Ohv-VXnuog2SQCLcBGAsYHQ/w640-h136/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Block diagram of the speech enhancement system, which is based on Conv-TasNet.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;The model is both &lt;a href=&quot;https://en.wikipedia.org/wiki/Causal_system&quot;&gt;causal&lt;/a&gt; and low latency: for each 2.5 milliseconds of input audio, the model produces estimates of separated speech and noise, and thus could be used in real-time. For the hackathon, to demonstrate what could be possible with increased compute power in future hardware, we chose to use a model variant with 2.9 million parameters. This model size is too large to be practically implemented in a CI today, but demonstrates what kind of performance would be possible with more capable hardware in the future. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Listening to the Results&lt;/b&gt;&lt;br/&gt;As we optimized our models and overall solution, we used the hackathon-provided &lt;a href=&quot;https://en.wikipedia.org/wiki/Vocoder&quot;&gt;vocoder&lt;/a&gt; (which required a fixed temporal spacing of electrical pulses) to produce audio simulating what CI users might perceive. We then conducted blind A-B listening tests as typical hearing users.  &lt;/p&gt;&lt;p&gt;Listening to the vocoder simulations below, the speech in the reconstructed sounds — from the vocoder processing the electrodograms — is reasonably intelligible when the input sound doesn't contain too much background noise, however there is still room to improve the clarity of the speech. Our submission performed well in the speech-in-noise category and achieved second place overall. &lt;/p&gt;   &lt;table align=&quot;center&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center; width: 60%;&quot;&gt;&lt;tbody&gt;       &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Simulated audio with fixed temporal spacing&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://github.com/google-research/google-research/raw/7b9d963a56f58ae7e036243e6a5b75d3294175ea/cochlear_implant/sample_audio/enhanced_vocoded.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;            &lt;/tbody&gt;&lt;/table&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Vocoder simulation of what CI users might perceive from audio from an electrodogram with fixed temporal spacing, with background noise and noise suppression applied.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;A bottleneck on quality is that the fixed temporal spacing of stimulation pulses sacrifices fine-time structure in the audio. A change to the processing to produce pulses timed to peaks in the filtered sound waveforms captures more information about the pitch and structure of sound than is conventionally represented in implant stimulation patterns. &lt;/p&gt;   &lt;table align=&quot;center&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center; width: 60%;&quot;&gt;&lt;tbody&gt;       &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Simulated audio with adaptive spacing and fine time structure&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&lt;audio controls=&quot;controls&quot; src=&quot;https://github.com/google-research/google-research/raw/7b9d963a56f58ae7e036243e6a5b75d3294175ea/cochlear_implant/sample_audio/enhanced_vocoded_fine_time.wav&quot;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;            &lt;/tbody&gt;&lt;/table&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Vocoder simulation, using the same vocoder as above, but on an electrodogram from the modified processing that synchronizes stimulation pulses to peaks of the sound waveform.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;It's important to note that this second vocoder output is overly optimistic about how well it might sound to a real CI user. For instance, the simple vocoder used here does not model how current spread in the cochlea blurs the stimulus, making it harder to resolve different frequencies. But this at least suggests that preserving fine-time structure is valuable and that the electrodogram itself is not the bottleneck. &lt;/p&gt;&lt;p&gt;Ideally, all processing approaches would be evaluated by a broad range of CI users, with the electrodograms implemented directly on their CIs rather than relying upon vocoder simulations.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusion and a Call to Collaborate&lt;/b&gt;&lt;br/&gt;We are planning to follow up on this experience in two main directions. First, we plan to explore the application of noise suppression to other hearing-accessibility modalities, including hearing aids, transcription, and vibrotactile sensory substitution. Second, we'll take a deeper dive into the creation of electrodogram patterns for cochlear implants, exploiting fine temporal structure that is not accommodated in the usual CIS (continous interleaved sampling) patterns that are standard in the industry. According to &lt;a href=&quot;https://doi.org/10.1109/79.708543&quot;&gt;Louizou&lt;/a&gt;: “It remains a puzzle how some single-channel patients can perform so well given the limited spectral information they receive''. Therefore, using &lt;a href=&quot;https://doi.org/10.1109/TBCAS.2012.2219530&quot;&gt;fine temporal structure&lt;/a&gt; might be a &lt;a href=&quot;http://www.machinehearing.org/&quot;&gt;critical step&lt;/a&gt; towards achieving an improved CI experience. &lt;/p&gt;&lt;p&gt;Google is committed to building technology &lt;a href=&quot;http://g.co/pwd&quot;&gt;with and for people with disabilities&lt;/a&gt;. If you are interested in collaborating to improve the state of the art in cochlear implants (or hearing aids), please reach out to &lt;a href=&quot;mailto:ci-collaborators@googlegroups.com&quot;&gt;ci-collaborators@googlegroups.com&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br/&gt;&lt;em&gt;We would like to thank the Cochlear Impact hackathon organizers for giving us this opportunity and partnering with us. The participating team within Google is Samuel J. Yang, Scott Wisdom, Pascal Getreuer, Chet Gnegy, Mihajlo Velimirović, Sagar Savla, and Richard F. Lyon with guidance from Dan Ellis and Manoj Plakal.&lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=mxT65-EVcFU:53DYr2CCzJo:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/mxT65-EVcFU&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/791697466172075100/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/applying-advanced-speech-enhancement-in.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/791697466172075100"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/791697466172075100"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/mxT65-EVcFU/applying-advanced-speech-enhancement-in.html" title="Applying Advanced Speech Enhancement in Cochlear Implants"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/--_N9quZ9cBU/YPrSHh7uklI/AAAAAAAAH74/IGCBSmDqzT0Q_rIFfTFkRmxRx7EQtUnNwCLcBGAsYHQ/s72-w370-h400-c/image1.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/07/applying-advanced-speech-enhancement-in.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-8328077619605477836</id><published>2021-07-22T09:58:00.000-07:00</published><updated>2021-07-22T09:58:47.986-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Health"/><title type="text">Multi-task Prediction of Organ Dysfunction in ICUs</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Subhrajit Roy, Research Scientist and Diana Mincu, Research Software Engineer, Google Research&lt;/span&gt; &lt;p&gt;The intensive care unit (ICU) of a hospital looks after the most medically vulnerable patients, many of whom require organ support, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Mechanical_ventilation&quot;&gt;mechanical ventilation&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Dialysis&quot;&gt;dialysis&lt;/a&gt;. While always critical, the demand on ICU services during the COVID-19 pandemic has further underscored the importance of data-driven decision-making in healthcare. Furthermore, the ability to accurately predict the clinical outcomes of ICU patients has the potential  to guide therapy and may inform decisions about most effective care, including staffing and triage support.  &lt;/p&gt;&lt;p&gt;Applying machine learning (ML) to electronic health records (EHRs) has shown promise in predicting clinical outcomes. However, many of these ML models are based on &lt;em&gt;single-task&lt;/em&gt; learning (ST), where the models are trained only to predict a specific adverse event, such as an organ dysfunction or the need for a life-support intervention. Of greater benefit would be to train &lt;em&gt;multi-task&lt;/em&gt; models, which take into account a variety of competing risks along with the interdependencies between organ systems that factor into patient outcomes in a realistic setting. &lt;/p&gt;&lt;p&gt;In “&lt;a href=&quot;https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocab101/6307184&quot;&gt;Multi-task prediction of organ dysfunction in the ICU using sequential sub-network routing&lt;/a&gt;”, we propose a multi-task learning (MTL) architecture, called Sequential Sub-Network Routing (SeqSNR), that better captures the complexity of a realistic setting. Inspired by a clinician's holistic approach to diagnosing problems, SeqSNR is designed to use flexible parameter sharing and routing to find related tasks and encourage cross-learning between them. We successfully applied SeqSNR to the task of continuous adverse event prediction in an ICU setting and showed advantages over single-task and naïve multi-tasking, especially in low training data scenarios.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Data and Labels&lt;/b&gt;&lt;br /&gt;In this study, we used the freely available, open access, de-identified &lt;a href=&quot;https://mimic.mit.edu/&quot;&gt;MIMIC-III&lt;/a&gt; EHR dataset, which includes a patient cohort consisting of 36,498 adults across 52,038 critical care admissions at the Beth Israel Deaconess Medical Center between 2001 and 2012. Similar to our &lt;a href=&quot;https://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html&quot;&gt;previous&lt;/a&gt; &lt;a href=&quot;https://ai.googleblog.com/2020/04/a-step-towards-protecting-patients-from.html&quot;&gt;studies&lt;/a&gt;, we employed a &lt;a href=&quot;https://www.nature.com/articles/s41746-018-0029-1&quot;&gt;version&lt;/a&gt; of the MIMIC-III dataset that was mapped to the &lt;a href=&quot;https://www.hl7.org/fhir/overview.html&quot;&gt;Fast Healthcare Interoperability Resource&lt;/a&gt; (FHIR) standard and used a comprehensive set of features, including a sequence of vital signs, laboratory results, past medications, procedures, diagnoses, and more.  &lt;/p&gt;&lt;p&gt;The MIMIC-III database contains multi-modal recordings from ICU patients. Unlike most datasets in ML, the input and targets are often not explicitly defined and must be inferred from the data. So, using a combination of automated rule-based methods and clinical review, we defined a suite of diverse endpoints, including critical care interventions, specific organ dysfunctions, and overall patient outcomes.  &lt;/p&gt;&lt;p&gt;The task given to the model was to predict the onset of a selection of adverse events within 24–48 hours for every hour after a patient’s admission into the ICU. The defined adverse events included &lt;a href=&quot;https://en.wikipedia.org/wiki/Acute_kidney_injury&quot;&gt;acute kidney injury&lt;/a&gt; (AKI), &lt;a href=&quot;https://en.wikipedia.org/wiki/Renal_replacement_therapy#Continuous_Renal_Replacement_Therapy_(CRRT)&quot;&gt;continuous renal replacement therapy&lt;/a&gt; (CRRT) dialysis, administration of &lt;a href=&quot;https://en.wikipedia.org/wiki/Antihypotensive_agent&quot;&gt;vasopressors&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Inotrope&quot;&gt;inotropes&lt;/a&gt;, mechanical ventilation (MV), mortality, and remaining length of stay (LoS).  &lt;/p&gt;&lt;p&gt;&lt;b&gt;The SeqSNR Algorithm&lt;/b&gt;&lt;br /&gt;While multi-task learning captures the interdependencies between organ systems and balances competing risks, it can be challenging to implement successfully. In practice, jointly-trained tasks often impair one another, an effect called “&lt;a href=&quot;https://arxiv.org/abs/2007.10185&quot;&gt;negative transfer&lt;/a&gt;”. The intuition behind SeqSNR was that modular ‘sub-networks’ would mitigate this issue by automatically optimizing how information is shared across multiple tasks.  &lt;/p&gt;&lt;p&gt;SeqSNR is a time series adaptation of the &lt;a href=&quot;https://ojs.aaai.org//index.php/AAAI/article/view/3788&quot;&gt;SNR architecture&lt;/a&gt; and is a combination of a deep embedding layer followed by stacked &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;&gt;recurrent neural network&lt;/a&gt; (RNN) layers. Modularisation is achieved by splitting  both the embedding layer and the RNN stack into multiple modules connected by routing variables that are learned during the training phase. The routing connections are always created between blocks in one layer and the next. This approach minimizes negative transfer by ensuring that data of low relevance to a particular task layer is filtered out. In essence, this means that each task utilizes a different path through the model.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-dSlDDAcwWy4/YPhGmmI7rYI/AAAAAAAAH7Y/wmhlt332d5U0p-shL8a1I-PGQrU_mQkiACLcBGAsYHQ/s1054/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;520&quot; data-original-width=&quot;1054&quot; height=&quot;316&quot; src=&quot;https://1.bp.blogspot.com/-dSlDDAcwWy4/YPhGmmI7rYI/AAAAAAAAH7Y/wmhlt332d5U0p-shL8a1I-PGQrU_mQkiACLcBGAsYHQ/w640-h316/image5.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A high-level overview of the SeqSNR architecture.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Findings&lt;/b&gt;&lt;br /&gt;SeqSNR shows a modest improvement in discriminative performance overall relative to single-task and naïve multitasking. However, it's performance improvement is more significant in scenarios with few training labels. &lt;/p&gt;&lt;p&gt;Because the prevalence of different outcomes varied widely in the dataset (e.g. ~38% of patients had MV, but CRRT dialysis is present for only ~3%), many accuracy metrics are not suitable. Instead, we report the area under the &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;&gt;precision recall&lt;/a&gt; curve (AU PRC), which is more reliable given imbalanced data. Moreover, we performed the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test&quot;&gt;Wilcoxon Signed Rank Tests&lt;/a&gt; to draw statistically significant conclusions for pairwise comparisons of ST learning, &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_5&quot;&gt;shared-bottom&lt;/a&gt; (SB) multi-task learning (i.e., naïve multi-task learning), and SeqSNR across &lt;a href=&quot;https://www.taylorfrancis.com/books/mono/10.1201/9780429246593/introduction-bootstrap-bradley-efron-tibshirani&quot;&gt;bootstrapped samples&lt;/a&gt; from the held-out test set. The performance differences between the three architectures were modest, but SeqSNR outperformed both ST and SB in four out of six tasks (p-values are reported in the &lt;a href=&quot;https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocab101/6307184&quot;&gt;paper&lt;/a&gt;). &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-5IBcZIbKSvM/YPhGypVHCmI/AAAAAAAAH7c/hNpBFUQDx0Mg7FRzWjxo8VH_RMX4bBuxACLcBGAsYHQ/s1532/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;992&quot; data-original-width=&quot;1532&quot; height=&quot;414&quot; src=&quot;https://1.bp.blogspot.com/-5IBcZIbKSvM/YPhGypVHCmI/AAAAAAAAH7c/hNpBFUQDx0Mg7FRzWjxo8VH_RMX4bBuxACLcBGAsYHQ/w640-h414/image4.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Comparison of single task (ST), shared bottom (SB) and SeqSNR performance on the MIMIC-III dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Label Efficiency&lt;/b&gt;&lt;br /&gt;We hypothesized that multi-task learning could assist in low-data scenarios by using easy-to-label auxiliary tasks to boost the performance of the main tasks. We formulated prediction tasks with only a portion of the training labels available for the primary prediction task, but kept the entire dataset for the “helper tasks”. The latter are chosen because they are reliably encoded in the EHR and are straightforward to timestamp. An example of such a helper task is length of stay,  since the start and end of admissions are accurately timestamped in MIMIC-III. On the other hand, the start and end of mechanical ventilation events are not reliably timestamped. So, we defined a set of rules based on expert-defined heuristics to determine the ventilation times using multiple sources of mechanical ventilator–related settings along with physiological measurements in the EHR dataset that are indicative of MV. &lt;/p&gt;&lt;p&gt;The development of these rules for a new clinical endpoint was time-consuming and involved manual review of the dataset by experts. The difficulty in exhaustively labeling the dataset led us to test the model performance with only 1–10% of the data labeled, which resulted in a decline in model performance. The “helper tasks” are useful in this scenario since they are 100% labeled and can be used with the primary tasks (1–10% labeled) to jointly train the multi-task model for improved overall performance.  &lt;/p&gt;&lt;p&gt;We chose AKI, mechanical ventilation, CRRT Dialysis, and vasoactive medications as primary endpoints using 1%, 5%, and 10% of the training labels, along with 100% of labels for the helper tasks — labs and vitals, mortality, and LoS. Performance of both ST and SeqSNR decreased as the percentage of labels for the primary endpoint was reduced, but SeqSNR outperformed ST across all tasks and all training data reduction percentages, with a statistically significant boost in performance for all cases. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-u60t3oAhlzg/YPhHQLNGopI/AAAAAAAAH7o/BeR1I4a3ITkrXj7W4-mpOTACPab3cQDaACLcBGAsYHQ/s1240/LabelEfficiency.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;986&quot; data-original-width=&quot;1240&quot; height=&quot;508&quot; src=&quot;https://1.bp.blogspot.com/-u60t3oAhlzg/YPhHQLNGopI/AAAAAAAAH7o/BeR1I4a3ITkrXj7W4-mpOTACPab3cQDaACLcBGAsYHQ/w640-h508/LabelEfficiency.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Label efficiency results showing the discriminative performance when the training dataset for the primary endpoint is reduced to 1%, 5% and 10% while the helper tasks have access to all training labels.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This is a useful finding, given the difficulties of annotating endpoint labels in EHR datasets, which frequently necessitates human evaluation by doctors. The ability to use numerous endpoints, some of which may be easier to label (like duration of stay or mortality), could lessen the need for manual curation on more difficult endpoints that are annotated differently (like  mechanical ventilation).  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Subgroup Performance&lt;/b&gt;&lt;br /&gt;While the version of the MIMIC-III dataset used contained labels for gender and age, it did not contain information on race and the information on ethnicity was limited. We computed the performance of all selected models across age and gender subgroups. We observed that in the scenarios with few instances in the dataset, the MTL models (both SB models and SeqSNR) often outperform ST. Even though there are exceptions, on average all models seem to be relatively balanced across age and gender subgroups. We invite the reader to refer to the &lt;a href=&quot;https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocab101/6307184#265336640&quot;&gt;supplemental section&lt;/a&gt; of our paper for a detailed performance breakdown. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Next Steps&lt;/b&gt;&lt;br /&gt;This work is a proof of concept for SeqSNR on a set of canonical EHR prediction tasks. The code for this architecture is publicly available &lt;a href=&quot;https://github.com/google/ehr-predictions&quot;&gt;here&lt;/a&gt;. And will hopefully stimulate further research in EHR multi-tasking and other deep learning architectures inspired by clinical reasoning.   &lt;/p&gt;&lt;p&gt;In future, it will be important to evaluate the  performance of SeqSNR on different combinations of tasks, different time horizons and different datasets. One other area of potential growth in this project is to expand subgroup analysis by including datasets with additional population information, race, ethnicity, etc.&amp;nbsp;Another area we are exploring is expanding subgroup analysis by including datasets with additional population information, such as race, ethnicity, etc. We also emphasize that these are prototype models designed to showcase methodologies, and more rigorous evaluation would be needed to bring these tools into deployment.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;This work involved collaborative efforts from a multidisciplinary team of researchers, software engineers, clinicians, and cross-functional contributors. We thank our co-authors:  Eric Loreaux, Anne Mottram, Ivan Protsyuk, Natalie Harris, Sebastien Baur, Yuan Xue, Jessica Schrouff, Ali Connell, Alan Karthikesalingam, Martin Seneviratne from Google, Nenad Tomasev from Deepmind, and Hugh Montgomery from University College London. We also thank Zhe Zhao from Google Research and Kathryn Rough, Cian Hughes, Megumi Morigami and Doris Wong from Google Health for their input and review, and the MIMIC team for curating this open access dataset for the research community. &lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=4G4KoZEbE9Y:PB9MFTFbdts:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/4G4KoZEbE9Y&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/8328077619605477836/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/multi-task-prediction-of-organ.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8328077619605477836"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8328077619605477836"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/4G4KoZEbE9Y/multi-task-prediction-of-organ.html" title="Multi-task Prediction of Organ Dysfunction in ICUs"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-dSlDDAcwWy4/YPhGmmI7rYI/AAAAAAAAH7Y/wmhlt332d5U0p-shL8a1I-PGQrU_mQkiACLcBGAsYHQ/s72-w640-h316-c/image5.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/07/multi-task-prediction-of-organ.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-4133940225737160804</id><published>2021-07-19T11:40:00.005-07:00</published><updated>2021-07-20T11:39:32.632-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="conferences"/><category scheme="http://www.blogger.com/atom/ns#" term="ICML"/><title type="text">Google at ICML 2021</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Cat Armato and Jaqui Herman, Program Managers&lt;/span&gt; &lt;p&gt;Groups across Google are actively pursuing research across the field of machine learning, ranging from theory to application. With scalable tools and architectures, we build machine learning systems to solve deep scientific and engineering challenges in areas of language, music, visual processing, and more. &lt;/p&gt;&lt;p&gt;Google is proud to be a &lt;a href=&quot;https://icml.cc/Conferences/2021/Sponsors&quot;&gt;Platinum Sponsor&lt;/a&gt; of the thirty-eighth &lt;a href=&quot;https://icml.cc/Conferences/2021&quot;&gt;International Conference on Machine Learning&lt;/a&gt; (ICML 2021), a premier annual event happening this week. As a leader in machine learning research — with over 100 accepted publications and Googlers participating in workshops — we look forward to our continued partnership with the broader machine learning research community. &lt;/p&gt;&lt;p&gt;Registered for ICML 2021? We hope you’ll visit the Google virtual booth to learn more about the exciting work, creativity, and fun that goes into solving a portion of the field’s most interesting challenges. Take a look below to learn more about the Google research being presented at ICML 2021 &lt;b&gt;(Google affiliations in bold).&lt;/b&gt;&lt;/p&gt;&lt;p&gt;     &lt;b&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;Organizing Committee&lt;/span&gt;&lt;/b&gt;&lt;br/&gt;  ICML Board Members include: &lt;b&gt;&lt;em&gt;Corinna Cortes, Hugo Larochelle, Shakir Mohamed&lt;/em&gt;&lt;/b&gt;&lt;br/&gt;    ICML Emeritus Board includes: &lt;b&gt;&lt;em&gt;William Cohen, Andrew McCallum&lt;/em&gt;&lt;/b&gt;&lt;br/&gt;    Tutorial Co-Chair member: &lt;b&gt;&lt;em&gt;Quoc Lee&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;b&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;Publications&lt;/span&gt;&lt;/b&gt;&lt;br/&gt;  &lt;a href=&quot;http://proceedings.mlr.press/v139/dong21a/dong21a.pdf&quot;&gt;Attention Is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Yihe Dong, Jean-Baptiste Cordonnier, Andreas Loukas&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/leibo21a/leibo21a.pdf&quot;&gt;Scalable Evaluation of Multi-agent Reinforcement Learning with Melting Pot&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Joel Z. Leibo, Edgar Duéñez-Guzmán, Alexander Sasha Vezhnevets, John P. Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charles Beattie,&lt;b&gt; Igor Mordatch&lt;/b&gt;, Thore Graepel       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/xiao21b/xiao21b.pdf&quot;&gt;On the Optimality of Batch Policy Optimization Algorithms&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Chenjun Xiao, &lt;/b&gt;Yifan Wu, Tor Lattimore,&lt;b&gt; Bo Dai, Jincheng Mei&lt;/b&gt;, Lihong Li*, Csaba Szepesvari, &lt;b&gt;Dale Schuurmans&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/scetbon21a/scetbon21a.pdf&quot;&gt;Low-Rank Sinkhorn Factorization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Meyer Scetbon, &lt;b&gt;Marco Cuturi&lt;/b&gt;, Gabriel Peyré   &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/grathwohl21a/grathwohl21a.pdf&quot;&gt;Oops I Took A Gradient: Scalable Sampling for Discrete Distributions&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Will Grathwohl, Kevin Swersky, Milad Hashemi, David Duvenaud&lt;/b&gt;, Chris J. Maddison   &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/farahmand21a/farahmand21a.pdf&quot;&gt;PID Accelerated Value Iteration Algorithm&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Amir-Massoud Farahmand&lt;b&gt;, Mohammad Ghavamzadeh&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/saha21b/saha21b.pdf&quot;&gt;Dueling Convex Optimization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Aadirupa Saha, &lt;b&gt;Tomer Koren, Yishay Mansour&lt;/b&gt;  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/izmailov21a/izmailov21a.pdf&quot;&gt;What Are Bayesian Neural Network Posteriors Really Like?&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Pavel Izmailov, &lt;b&gt;Sharad Vikram, Matthew D. Hoffman&lt;/b&gt;, Andrew Gordon Wilson   &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/dadashi21a/dadashi21a.pdf&quot;&gt;Offline Reinforcement Learning with Pseudometric Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Robert Dadashi&lt;/b&gt;, Shideh Rezaeifar,&lt;b&gt; Nino Vieillard, Léonard Hussenot, Olivier Pietquin, Matthieu Geist&lt;/b&gt;  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/ceron21a/ceron21a.pdf&quot;&gt;Revisiting Rainbow: Promoting More Insightful and Inclusive Deep Reinforcement Learning Research&lt;/a&gt; &lt;em&gt;(see &lt;a href=&quot;https://ai.googleblog.com/2021/07/reducing-computational-cost-of-deep.html&quot;&gt;blog post&lt;/a&gt;)&lt;/em&gt;&lt;br/&gt;&lt;em&gt;  Johan S. Obando-Ceron, &lt;b&gt;Pablo Samuel Castro&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/ghasemipour21a/ghasemipour21a.pdf&quot;&gt;EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Seyed Kamyar Seyed Ghasemipour*, &lt;b&gt;Dale Schuurmans, Shixiang Shane Gu&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/frerix21a/frerix21a.pdf&quot;&gt;Variational Data Assimilation with a Learned Inverse Observation Operator&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Thomas Frerix, Dmitrii Kochkov, Jamie A. Smith&lt;/b&gt;, Daniel Cremers, &lt;b&gt;Michael P. Brenner, Stephan Hoyer&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/ruiz-garcia21a/ruiz-garcia21a.pdf&quot;&gt;Tilting the Playing Field: Dynamical Loss Functions for Machine Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Miguel Ruiz-Garcia, Ge Zhang, &lt;b&gt;Samuel S. Schoenholz&lt;/b&gt;, Andrea J. Liu       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/rybkin21b/rybkin21b.pdf&quot;&gt;Model-Based Reinforcement Learning via Latent-Space Collocation&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Oleh Rybkin, Chuning Zhu, Anusha Nagabandi, Kostas Daniilidis, &lt;b&gt;Igor Mordatch&lt;/b&gt;, Sergey Levine       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/sander21a/sander21a.pdf&quot;&gt;Momentum Residual Neural Networks&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Michael E. Sander, Pierre Ablin, &lt;b&gt;Mathieu Blondel&lt;/b&gt;, Gabriel Peyré       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/tay21b/tay21b.pdf&quot;&gt;OmniNet: Omnidirectional Representations from Transformers&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Yi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta, Philip Pham, Zhen Qin, Dara Bahri, Da-Cheng Juan, Donald Metzler&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/tay21a/tay21a.pdf&quot;&gt;Synthesizer: Rethinking Self-Attention for Transformer Models&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/verma21a/verma21a.pdf&quot;&gt;Towards Domain-Agnostic Contrastive Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Vikas Verma, Minh-Thang Luong&lt;/b&gt;, Kenji Kawaguchi,&lt;b&gt; Hieu Pham, Quoc V. Le&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2006.04222.pdf&quot;&gt;Randomized Entity-wise Factorization for Multi-agent Reinforcement Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Shariq Iqbal, Christian A. Schroeder de Witt, Bei Peng, Wendelin Böhmer, Shimon Whiteson, &lt;b&gt;Fei Sha&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/wu21c/wu21c.pdf&quot;&gt;LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Yuhuai Wu, &lt;b&gt;Markus Rabe,&lt;/b&gt; Wenda Li, Jimmy Ba, Roger Grosse, &lt;b&gt;Christian Szegedy&lt;/b&gt;   &lt;span style=&quot;text-decoration:underline;&quot;&gt; &lt;/span&gt;  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/ndousse21a/ndousse21a.pdf&quot;&gt;Emergent Social Learning via Multi-agent Reinforcement Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Kamal Ndousse, &lt;b&gt;Douglas Eck, Sergey Levine, Natasha Jaques&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/du21b/du21b.pdf&quot;&gt;Improved Contrastive Divergence Training of Energy-Based Models&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Yilun Du, Shuang Li, Joshua Tenenbaum, &lt;b&gt;Igor Mordatch&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/jiang21k/jiang21k.pdf&quot;&gt;Characterizing Structural Regularities of Labeled Data in Overparameterized Models&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Ziheng Jiang*, &lt;b&gt;Chiyuan Zhang, Kunal Talwar, Michael Mozer&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/chebotar21a/chebotar21a.pdf&quot;&gt;Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, Sergey Levine&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/filos21a/filos21a.pdf&quot;&gt;PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Angelos Filos, Clare Lyle, Yarin Gal, Sergey Levine, &lt;b&gt;Natasha Jaques&lt;/b&gt;, Gregory Farquhar       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/tan21a/tan21a.pdf&quot;&gt;EfficientNetV2: Smaller Models and Faster Training&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Mingxing Tan, Quoc V. Le&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/vicol21a/vicol21a.pdf&quot;&gt;Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Paul Vicol, Luke Metz, Jascha Sohl-Dickstein&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/yuan21d/yuan21d.pdf&quot;&gt;Federated Composite Optimization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Honglin Yuan*, &lt;b&gt;Manzil Zaheer, Sashank Reddi&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/chierichetti21a/chierichetti21a.pdf&quot;&gt;Light RUMs&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Flavio Chierichetti, &lt;b&gt;Ravi Kumar, Andrew Tomkins&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/davis21a/davis21a.pdf&quot;&gt;Catformer: Designing Stable Transformers via Sensitivity Analysis&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Jared Quincy Davis, Albert Gu,&lt;b&gt; Krzysztof Choromanski&lt;/b&gt;, Tri Dao, Christopher Re, &lt;b&gt;Chelsea Finn&lt;/b&gt;, Percy Liang       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/yang21h/yang21h.pdf&quot;&gt;Representation Matters: Offline Pretraining for Sequential Decision Making&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Mengjiao Yang, Ofir Nachum&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/choi21b/choi21b.pdf&quot;&gt;Variational Empowerment as Representation Learning for Goal-Conditioned Reinforcement Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Jongwook Choi*, Archit Sharma*, Honglak Lee, &lt;b&gt;Sergey Levine, Shixiang Shane Gu&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/chung21a/chung21a.pdf&quot;&gt;Beyond Variance Reduction: Understanding the True Impact of Baselines on Policy Optimization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Wesley Chung,&lt;b&gt; &lt;/b&gt;Valentin Thomas, &lt;b&gt;Marlos C. Machado, Nicolas Le Roux&lt;/b&gt;          &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/wadia21a/wadia21a.pdf&quot;&gt;Whitening and Second Order Optimization Both Make Information in the Dataset Unusable During Training, and Can Reduce or Prevent Generalization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Neha S. Wadia,&lt;b&gt; Daniel Duckworth, Samuel S. Schoenholz, Ethan Dyer, Jascha Sohl-Dickstein&lt;/b&gt;  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/teterwak21a/teterwak21a.pdf&quot;&gt;Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Piotr Teterwak*, &lt;b&gt;Chiyuan Zhang, Dilip Krishnan, Michael C. Mozer &lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/furuta21a/furuta21a.pdf&quot;&gt;Policy Information Capacity: Information-Theoretic Measure for Task Complexity in Deep Reinforcement Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Hiroki Furuta, Tatsuya Matsushima, Tadashi Kozuno, Yutaka Matsuo, &lt;b&gt;Sergey Levine, Ofir Nachum, Shixiang Shane Gu&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/hussenot21a/hussenot21a.pdf&quot;&gt;Hyperparameter Selection for Imitation Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Leonard Hussenot, Marcin Andrychowicz, Damien Vincent, Robert Dadashi, Anton Raichuk, Lukasz Stafiniak, Sertan Girgin, Raphael Marinier, Nikola Momchev, Sabela Ramos, Manu Orsini, Olivier Bachem, Matthieu Geist, Olivier Pietquin&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/rawat21a/rawat21a.pdf&quot;&gt;Disentangling Sampling and Labeling Bias for Learning in Large-Output Spaces&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Ankit Singh Rawat, Aditya Krishna Menon, Wittawat Jitkrittum, Sadeep Jayasumana, Felix X. Yu,&lt;/b&gt;  &lt;b&gt;Sashank J. Reddi, Sanjiv Kumar &lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/deng21c/deng21c.pdf&quot;&gt;Revenue-Incentive Tradeoffs in Dynamic Reserve Pricing&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Yuan Deng, Sebastien Lahaie, Vahab Mirrokni, Song Zuo&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/likhosherstov21a/likhosherstov21a.pdf&quot;&gt;Debiasing a First-Order Heuristic for Approximate Bi-Level Optimization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Valerii Likhosherstov, &lt;b&gt;Xingyou Song, Krzysztof Choromanski&lt;/b&gt;, Jared Davis, Adrian Weller   &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/wen21b/wen21b.pdf&quot;&gt;Characterizing the Gap Between Actor-Critic and Policy Gradient&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Junfeng Wen, Saurabh Kumar, &lt;b&gt;Ramki Gummadi, Dale Schuurmans&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/whang21b/whang21b.pdf&quot;&gt;Composing Normalizing Flows for Inverse Problems&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Jay Whang, &lt;b&gt;Erik Lindgren&lt;/b&gt;, Alexandros Dimakis        &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/cassel21a/cassel21a.pdf&quot;&gt;Online Policy Gradient for Model Free Learning of Linear Quadratic Regulators with √T Regret&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Asaf Cassel, &lt;b&gt;Tomer Koren &lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/leme21a/leme21a.pdf&quot;&gt;Learning to Price Against a Moving Target&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Renato Paes Leme, Balasubramanian Sivan&lt;/b&gt;, Yifeng Teng,&lt;b&gt; Pratik Worah&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/correa21a/correa21a.pdf&quot;&gt;Fairness and Bias in Online Selection&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Jose Correa, Andres Cristi, &lt;b&gt;Paul Duetting, Ashkan Norouzi-Fard&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/nock21a/nock21a.pdf&quot;&gt;The Impact of Record Linkage on Learning from Feature Partitioned Data&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Richard Nock&lt;/b&gt;, Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Jakub Nabaglo, Giorgio Patrini,   Guillaume Smith, Brian Thorne       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/feng21b/feng21b.pdf&quot;&gt;Reserve Price Optimization for First Price Auctions in Display Advertising&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Zhe Feng*,&lt;b&gt; Sébastien Lahaie, Jon Schneider, Jinchao Ye&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/agarwal21b/agarwal21b.pdf&quot;&gt;A Regret Minimization Approach to Iterative Learning Control&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Naman Agarwal, Elad Hazan, Anirudha Majumdar&lt;/b&gt;, Karan Singh       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/menon21a/menon21a.pdf&quot;&gt;A Statistical Perspective on Distillation&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Aditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, Seungyeon Kim, Sanjiv Kumar&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/cella21a/cella21a.pdf&quot;&gt;Best Model Identification: A Rested Bandit Formulation&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Leonardo Cella, Massimiliano Pontil, &lt;b&gt;Claudio Gentile&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/cranko21a/cranko21a.pdf&quot;&gt;Generalised Lipschitz Regularisation Equals Distributional Robustness&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Zac Cranko, Zhan Shi, Xinhua Zhang, &lt;b&gt;Richard Nock&lt;/b&gt;, &lt;b&gt;Simon Kornblith&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/lancewicki21a/lancewicki21a.pdf&quot;&gt;Stochastic Multi-armed Bandits with Unrestricted Delay Distributions&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Tal Lancewicki, Shahar Segal,&lt;b&gt; Tomer Koren, Yishay Mansour&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/balseiro21a/balseiro21a.pdf&quot;&gt;Regularized Online Allocation Problems: Fairness and Beyond&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Santiago Balseiro, Haihao Lu, Vahab Mirrokni&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/kumar21b/kumar21b.pdf&quot;&gt;Implicit Rate-Constrained Optimization of Non-decomposable Objectives&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Abhishek Kumar, Harikrishna Narasimhan, Andrew Cotter&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/mei21a/mei21a.pdf&quot;&gt;Leveraging Non-uniformity in First-Order Non-Convex Optimization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Jincheng Mei&lt;/b&gt;, Yue Gao, &lt;b&gt;Bo Dai&lt;/b&gt;, Csaba Szepesvari, &lt;b&gt;Dale Schuurmans &lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/cutkosky21a/cutkosky21a.pdf&quot;&gt;Dynamic Balancing for Model Selection in Bandits and RL&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Ashok Cutkosky&lt;b&gt;, Christoph Dann, Abhimanyu Das, Claudio Gentile, &lt;/b&gt;Aldo Pacchiano, &lt;b&gt;Manish Purohit&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/saha21a/saha21a.pdf&quot;&gt;Adversarial Dueling Bandits&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Aadirupa Saha, &lt;b&gt;Tomer Koren, Yishay Mansour&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/hiranandani21a/hiranandani21a.pdf&quot;&gt;Optimizing Black-Box Metrics with Iterative Example Weighting&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Gaurush Hiranandani*, Jatin Mathur,&lt;b&gt; Harikrishna Narasimhan, Mahdi Milani Fard, Oluwasanmi Koyejo&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/cortes21a/cortes21a.pdf&quot;&gt;Relative Deviation Margin Bounds&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Corinna Cortes, Mehryar Mohri, Ananda Theertha Suresh&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/hoedt21a/hoedt21a.pdf&quot;&gt;MC-LSTM: Mass-Conserving LSTM&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Pieter-Jan Hoedt, Frederik Kratzert, Daniel Klotz, Christina Halmich, Markus Holzleitner, &lt;b&gt;Grey Nearing&lt;/b&gt;, Sepp Hochreiter, Günter Klambauer       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/golany21a/golany21a.pdf&quot;&gt;12-Lead ECG Reconstruction via Koopman Operators&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Authors:Tomer Golany, Kira Radinsky, &lt;b&gt;Daniel Freedman&lt;/b&gt;, Saar Minha       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/heidari21a/heidari21a.pdf&quot;&gt;Finding Relevant Information via a Discrete Fourier Expansion&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Mohsen Heidari, Jithin Sreedharan&lt;b&gt;, Gil Shamir, &lt;/b&gt;Wojciech Szpankowski       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/ren21a/ren21a.pdf&quot;&gt;LEGO: Latent Execution-Guided Reasoning for Multi-hop Question Answering on Knowledge Graphs&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Hongyu Ren,&lt;b&gt; Hanjun Dai, Bo Dai&lt;/b&gt;, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, &lt;b&gt;Dale Schuurmans&lt;/b&gt;, Jure Leskovec,&lt;b&gt; Denny Zhou&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/chen21m/chen21m.pdf&quot;&gt;SpreadsheetCoder: Formula Prediction from Semi-structured Context&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Xinyun Chen,&lt;b&gt; Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, Denny Zhou&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/atsidakou21a/atsidakou21a.pdf&quot;&gt;Combinatorial Blocking Bandits with Stochastic Delays&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Alexia Atsidakou, Orestis Papadigenopoulos, &lt;b&gt;Soumya Basu&lt;/b&gt;, Constantine Caramani, Sanjay Shakkottai       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/basu21a/basu21a.pdf&quot;&gt;Beyond log2(T) Regret for Decentralized Bandits in Matching Markets&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Soumya Basu&lt;/b&gt;, Karthik Abinav Sankararaman, Abishek Sankararaman       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/alieva21a/alieva21a.pdf&quot;&gt;Robust Pure Exploration in Linear Bandits with Limited Budget&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Ayya Alieva, Ashok Cutkosky,&lt;b&gt; Abhimanyu Das&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/hong21a/hong21a.pdf&quot;&gt;Latent Programmer: Discrete Latent Codes for Program Synthesis&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Joey Hong, David Dohan, Rishabh Singh, Charles Sutton, Manzil Zaheer&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/jia21b/jia21b.pdf&quot;&gt;Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision&lt;/a&gt; &lt;em&gt;(see &lt;a href=&quot;https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html&quot;&gt;blog post&lt;/a&gt;)&lt;/em&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/roeder21a/roeder21a.pdf&quot;&gt;On Linear Identifiability of Learned Representations&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Geoffrey Roeder, &lt;b&gt;Luke Metz, Diederik P. Kingma&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/rajagopalan21a/rajagopalan21a.pdf&quot;&gt;Hierarchical Clustering of Data Streams: Scalable Algorithms and Approximation Guarantees&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Anand Rajagopalan&lt;/b&gt;, Fabio Vitale, Danny Vainstein,&lt;b&gt; Gui Citovsky, Cecilia M Procopiuc, Claudio Gentile&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/gillenwater21a/gillenwater21a.pdf&quot;&gt;Differentially Private Quantiles&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Jennifer Gillenwater, Matthew Joseph, Alex Kulesza&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/jiang21i/jiang21i.pdf&quot;&gt;Active Covering&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Heinrich Jiang, Afshin Rostamizadeh&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/rematas21a/rematas21a.pdf&quot;&gt;Sharf: Shape-Conditioned Radiance Fields from a Single View&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Konstantinos Rematas, Ricardo Martin-Brualla, Vittorio Ferrari&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/triantafillou21a/triantafillou21a.pdf&quot;&gt;Learning a Universal Template for Few-Shot Dataset Generalization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Eleni Triantafillou*, &lt;b&gt;Hugo Larochelle&lt;/b&gt;, Richard Zemel, &lt;b&gt;Vincent Dumoulin&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/chien21a/chien21a.pdf&quot;&gt;Private Alternating Least Squares: Practical Private Matrix Completion with Tighter Rates&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Steve Chien, Prateek Jain, Walid Krichene, Steffen Rendle, Shuang Song, Abhradeep Thakurta, Li Zhang&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/cohen21c/cohen21c.pdf&quot;&gt;Differentially-Private Clustering of Easy Instances&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Edith Cohen, Haim Kaplan, Yishay Mansour,  Uri Stemmer, Eliad Tsfadia&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2007.14321.pdf&quot;&gt;Label-Only Membership Inference Attacks&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Christopher A. Choquette-Choo, Florian Tramèr, &lt;b&gt;Nicholas Carlini&lt;/b&gt;, Nicolas Papernot       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/chen21f/chen21f.pdf&quot;&gt;Neural Feature Matching in Implicit 3D Representations&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Yunlu Chen, Basura Fernando, Hakan Bilen&lt;b&gt;, Thomas Mensink,&lt;/b&gt; Efstratios Gavves       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/chang21a/chang21a.pdf&quot;&gt;Locally Private k-Means in One Round&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Alisa Chang, Badih Ghazi, Ravi Kumar, Pasin Manurangsi&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/shin21a/shin21a.pdf&quot;&gt;Large-Scale Meta-learning with Continual Trajectory Shifting&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Jaewoong Shin, Hae Beom Lee,&lt;b&gt; Boqing Gong&lt;/b&gt;, Sung Ju Hwang       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/kandiros21a/kandiros21a.pdf&quot;&gt;Statistical Estimation from Dependent Data&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Vardis Kandiros, Yuval Dagan,&lt;b&gt; Nishanth Dikkala&lt;/b&gt;, Surbhi Goel, Constantinos Daskalakis       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/qiao21b/qiao21b.pdf&quot;&gt;Oneshot Differentially Private Top-k Selection&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Gang Qiao, Weijie J. Su, &lt;b&gt;Li Zhang&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/sabour21a/sabour21a.pdf&quot;&gt;Unsupervised Part Representation by Flow Capsules&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Sara Sabour, Andrea Tagliasacchi, Soroosh Yazdani, Geoffrey E. Hinton, David J. Fleet&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/asi21b/asi21b.pdf&quot;&gt;Private Stochastic Convex Optimization: Optimal Rates in L1 Geometry&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Hilal Asi, Vitaly Feldman,&lt;b&gt; Tomer Koren,&lt;/b&gt; Kunal Talwar       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/kairouz21b/kairouz21b.pdf&quot;&gt;Practical and Private (Deep) Learning Without Sampling or Shuffling&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, Zheng Xu&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/ghazi21a/ghazi21a.pdf&quot;&gt;Differentially Private Aggregation in the Shuffle Model: Almost Central Accuracy in Almost a Single Message&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Badih Ghazi, Ravi Kumar, Pasin Manurangsi, Rasmus Pagh, Amer Sinha &lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/liu21w/liu21w.pdf&quot;&gt;Leveraging Public Data for Practical Private Query Release&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Terrance Liu, Giuseppe Vietri, &lt;b&gt;Thomas Steinke&lt;/b&gt;, Jonathan Ullman, Zhiwei Steven Wu       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/kveton21a/kveton21a.pdf&quot;&gt;Meta-Thompson Sampling&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Branislav Kveton, &lt;/b&gt;Mikhail Konobeev,&lt;b&gt; Manzil Zaheer, Chih-wei Hsu, Martin Mladenov, Craig Boutilier, &lt;/b&gt;Csaba Szepesvári       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/murphy21a/murphy21a.pdf&quot;&gt;Implicit-PDF: Non-parametric Representation of Probability Distributions on the Rotation Manifold&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Kieran A Murphy, Carlos Esteves, Varun Jampani, Srikumar Ramalingam, Ameesh Makadia&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/cohen-addad21a/cohen-addad21a.pdf&quot;&gt;Improving Ultrametrics Embeddings Through Coresets&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Vincent Cohen-Addad&lt;/b&gt;, Rémi de Joannis de Verclos, Guillaume Lagarde       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/cortes21b/cortes21b.pdf&quot;&gt;A Discriminative Technique for Multiple-Source Adaptation&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Corinna Cortes, Mehryar Mohri, Ananda Theertha Suresh&lt;/b&gt;, Ningshan Zhang       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/cheng21b/cheng21b.pdf&quot;&gt;Self-Supervised and Supervised Joint Training for Resource-Rich Machine Translation&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Yong Cheng&lt;/b&gt;, Wei Wang*,&lt;b&gt; Lu Jiang, Wolfgang Macherey&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.08448.pdf&quot;&gt;Correlation Clustering in Constant Many Parallel Rounds&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Vincent Cohen-Addad, Silvio Lattanzi, &lt;/b&gt;Slobodan Mitrović&lt;b&gt;, Ashkan Norouzi-Fard, Nikos Parotsidis,&lt;/b&gt;  Jakub Tarnawski       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/dhulipala21a/dhulipala21a.pdf&quot;&gt;Hierarchical Agglomerative Graph Clustering in Nearly-Linear Time&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Laxman Dhulipala, &lt;b&gt;David Eisenstat, Jakub Łącki, Vahab Mirrokni&lt;/b&gt;, Jessica Shi    &lt;span style=&quot;text-decoration:underline;&quot;&gt; &lt;/span&gt;  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/sandler21a/sandler21a.pdf&quot;&gt;Meta-learning Bidirectional Update Rules&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Mark Sandler, Max Vladymyrov, Andrey Zhmoginov, Nolan Miller, Andrew Jackson, Tom Madams, Blaise Aguera y Arcas&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/rosca21a/rosca21a.pdf&quot;&gt;Discretization Drift in Two-Player Games&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Mihaela Rosca, Yan Wu, &lt;b&gt;Benoit Dherin&lt;/b&gt;, David G.T. Barrett       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/sun21e/sun21e.pdf&quot;&gt;Reasoning Over Virtual Knowledge Bases With Open Predicate Relations&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Haitian Sun*&lt;b&gt;, Pat Verga, Bhuwan Dhingra, &lt;/b&gt;Ruslan Salakhutdinov,&lt;b&gt; William W. Cohen&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/merchant21a/merchant21a.pdf&quot;&gt;Learn2Hop: Learned Optimization on Rough Landscapes&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Amil Merchant, Luke Metz, Samuel Schoenholz, Ekin Cubuk&lt;/b&gt;       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/bahri21a/bahri21a.pdf&quot;&gt;Locally Adaptive Label Smoothing Improves Predictive Churn&lt;/a&gt;&lt;br/&gt;    &lt;em&gt;&lt;b&gt;Dara Bahri, Heinrich Jiang&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v139/chen21v/chen21v.pdf&quot;&gt;Overcoming Catastrophic Forgetting by Bayesian Generative Regularization&lt;/a&gt;&lt;br/&gt;    &lt;em&gt;Patrick H. Chen,&lt;b&gt; Wei Wei&lt;/b&gt;, Cho-jui Hsieh, &lt;b&gt;Bo Dai&lt;/b&gt;&lt;/em&gt;      &lt;p&gt;&lt;b&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;Workshops&lt;/span&gt;&lt;/b&gt; (&lt;em&gt;only Google affiliations are noted&lt;/em&gt;)&lt;br/&gt;  &lt;a href=&quot;https://www.latinxinai.org/icml-2021-about&quot;&gt;LatinX in AI (LXAI) Research at ICML 2021&lt;/a&gt;&lt;br/&gt;  Hosts: &lt;em&gt;&lt;b&gt;Been Kim, Natasha Jaques&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/udlworkshop2021/home&quot;&gt;Uncertainty and Robustness in Deep Learning&lt;/a&gt;&lt;br/&gt;    Organizers:&lt;em&gt;&lt;b&gt; Balaji Lakshminarayanan, Jasper Snoek&lt;/b&gt;&lt;/em&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Dustin Tran&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/view/RL4RealLife&quot;&gt;Reinforcement Learning for Real Life&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;em&gt;&lt;b&gt; Minmin Chen, Lihong Li&lt;/b&gt;&lt;/em&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Ed Chi&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/view/imlh2021/&quot;&gt;Interpretable Machine Learning in Healthcare&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;em&gt;&lt;b&gt; Alan Karthikesalingam&lt;/b&gt;&lt;/em&gt;  Invited Speakers: &lt;em&gt;&lt;b&gt;Abhijit Guha Roy, Jim Winkens&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/view/naci2021&quot;&gt;The Neglected Assumptions in Causal Inference&lt;/a&gt;&lt;br/&gt;  Organizer:&lt;em&gt;&lt;b&gt; Alexander D'Amour&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/view/recourse21&quot;&gt;ICML Workshop on Algorithmic Recourse&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;em&gt;&lt;b&gt;Been Kim, Berk Ustun&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://advml-workshop.github.io/icml2021/&quot;&gt;A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Nicholas Carlini&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/view/icml2021oppo&quot;&gt;Overparameterization: Pitfalls and Opportunities&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;em&gt;&lt;b&gt;Yasaman Bahri,&lt;/b&gt; &lt;b&gt;Hanie Sedghi&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/itr3&quot;&gt;Information-Theoretic Methods for Rigorous, Responsible, and Reliable Machine Learning (ITR3)&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Thomas Steinke&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/view/optml-icml2021&quot;&gt;Beyond First-Order Methods in Machine Learning Systems&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Courtney Paquette&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://icml21ssl.github.io/&quot;&gt;ICML 2021 Workshop: Self-Supervised Learning for Reasoning and Perception&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Chelsea Finn&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://lyang36.github.io/icml2021_rltheory/&quot;&gt;Workshop on Reinforcement Learning Theory&lt;/a&gt;&lt;br/&gt;    Invited Speaker: &lt;em&gt;&lt;b&gt;Bo Dai&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;b&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;Tutorials&lt;/span&gt;&lt;/b&gt; (&lt;em&gt;only Google affiliations are noted&lt;/em&gt;)&lt;br/&gt;  &lt;a href=&quot;https://sites.google.com/view/ResponsibleAITutorial&quot;&gt;Responsible AI in Industry: Practical Challenges and Lessons Learned&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;em&gt;&lt;b&gt;Ben Packer&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/nsc-tutorial/home&quot;&gt;Online and Non-stochastic Control&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;em&gt;&lt;b&gt;Elad Hazan&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://random-matrix-learning.github.io/&quot;&gt;Random Matrix Theory and ML (RMT +ML)&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;em&gt; &lt;b&gt;Fabian Pedregosa, Jeffrey Pennington&lt;/b&gt;, &lt;b&gt;Courntey Paquette&lt;/b&gt;&lt;/em&gt;      Self-Attention for Computer Vision   Organizers: &lt;em&gt;&lt;b&gt;Prajit Ramachandran, Ashish Vaswani&lt;/b&gt;&lt;/em&gt;  &lt;p&gt;&lt;em&gt;* Indicates work done while at Google&lt;/em&gt;&lt;/em&gt;&lt;/p&gt; &lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=8690zr_BdnY:1RVAYLqxM_U:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/8690zr_BdnY&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/4133940225737160804/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/google-at-icml-2021.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4133940225737160804"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4133940225737160804"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/8690zr_BdnY/google-at-icml-2021.html" title="Google at ICML 2021"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/07/google-at-icml-2021.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-8564855857063590663</id><published>2021-07-16T10:22:00.020-07:00</published><updated>2021-07-16T12:24:07.211-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Research"/><title type="text">High Fidelity Image Generation Using Diffusion Models</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Jonathan Ho, Research Scientist and Chitwan Saharia, Software Engineer, Google Research, Brain Team&lt;/span&gt; &lt;p&gt;Natural image synthesis is a broad class of machine learning (ML) tasks with wide-ranging applications that pose a number of design challenges. One example is image super-resolution, in which a model is trained to transform a low resolution image into a detailed high resolution image (e.g., &lt;a href=&quot;https://ai.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html&quot;&gt;RAISR&lt;/a&gt;). Super-resolution has many applications that can range from restoring old family portraits to &lt;a href=&quot;https://en.wikipedia.org/wiki/Super-resolution_imaging&quot;&gt;improving medical imaging systems&lt;/a&gt;. Another such image synthesis task is class-conditional image generation, in which a model is trained to generate a sample image from an input class label. The resulting generated sample images can be used to improve performance of downstream models for image classification, segmentation, and more.&lt;/p&gt;&lt;p&gt;Generally, these image synthesis tasks are performed by deep generative models, such as &lt;a href=&quot;https://arxiv.org/abs/1406.2661&quot;&gt;GANs&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;VAEs&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/1601.06759&quot;&gt;autoregressive models&lt;/a&gt;. Yet each of these generative models has its downsides when trained to synthesize high quality samples on difficult, high resolution datasets. For example, GANs often suffer from &lt;a href=&quot;https://developers.google.com/machine-learning/gan/problems&quot;&gt;unstable training and mode collapse&lt;/a&gt;, and autoregressive models typically suffer from slow synthesis speed. &lt;/p&gt;&lt;p&gt;Alternatively, &lt;a href=&quot;https://arxiv.org/abs/1503.03585&quot;&gt;diffusion models&lt;/a&gt;, originally proposed in 2015, have seen a recent revival in interest due to their training stability and their promising sample quality results on &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;&gt;image&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1907.05600&quot;&gt;and&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2009.00713&quot;&gt;audio&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2009.09761&quot;&gt;generation&lt;/a&gt;. Thus, they offer potentially favorable trade-offs compared to other types of deep generative models. Diffusion models work by corrupting the training data by progressively adding Gaussian noise, slowly wiping out details in the data until it becomes pure noise, and then training a neural network to reverse this corruption process. Running this reversed corruption process synthesizes data from pure noise by gradually denoising it until a clean sample is produced. This synthesis procedure &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;&gt;can be interpreted&lt;/a&gt; as an optimization algorithm that &lt;a href=&quot;https://arxiv.org/abs/1907.05600&quot;&gt;follows the gradient of the data density&lt;/a&gt; to produce likely samples. &lt;/p&gt;&lt;p&gt;Today we present two connected approaches that push the boundaries of the image synthesis quality for diffusion models — &lt;a href=&quot;https://iterative-refinement.github.io/&quot;&gt;Super-Resolution via Repeated Refinements&lt;/a&gt; (SR3) and a model for class-conditioned synthesis, called &lt;a href=&quot;https://cascaded-diffusion.github.io/&quot;&gt;Cascaded Diffusion Models&lt;/a&gt; (CDM). We show that by scaling up diffusion models and with carefully selected data augmentation techniques, we can outperform existing approaches. Specifically, SR3 attains strong image super-resolution results that surpass GANs in human evaluations. CDM generates high fidelity ImageNet samples that surpass &lt;a href=&quot;https://arxiv.org/abs/1809.11096&quot;&gt;BigGAN-deep&lt;/a&gt; and&lt;a href=&quot;https://arxiv.org/abs/1906.00446&quot;&gt; VQ-VAE2&lt;/a&gt; on both &lt;a href=&quot;https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance&quot;&gt;FID score&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1905.10887&quot;&gt;Classification Accuracy Score&lt;/a&gt; by a large margin.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;SR3: Image Super-Resolution&lt;/b&gt;&lt;br /&gt;&lt;a href=&quot;http://iterative-refinement.github.io&quot;&gt;SR3&lt;/a&gt; is a super-resolution diffusion model that takes as input a low-resolution image, and builds a corresponding high resolution image from pure noise. The model is trained on an image corruption process in which noise is progressively added to a high-resolution image until only pure noise remains. It then learns to reverse this process, beginning from pure noise and progressively removing noise to reach a target distribution through the guidance of the input low-resolution image..  &lt;/p&gt;&lt;video controls=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; width=&quot;95%&quot;&gt;&lt;source src=&quot;https://iterative-refinement.github.io/assets/cascade_movie2_mp4.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;&lt;/video&gt;&lt;p&gt;With large scale training, SR3 achieves strong benchmark results on the super-resolution task for face and natural images when scaling to resolutions 4x–8x that of the input low-resolution  image. These super-resolution models can further be cascaded together to increase the effective super-resolution scale factor, e.g., stacking a 64x64 → 256x256 and a 256x256 → 1024x1024 face super-resolution model together in order to perform a 64x64 → 1024x1024 super-resolution task.  &lt;/p&gt;&lt;p&gt;We compare SR3 with existing methods using human evaluation study. We conduct a &lt;a href=&quot;https://en.wikipedia.org/wiki/Two-alternative_forced_choice&quot;&gt;Two-Alternative Forced Choice Experiment&lt;/a&gt; where subjects are asked to choose between the reference high resolution image, and the model output when asked the question, “&lt;em&gt;Which image would you guess is from a camera?&lt;/em&gt;” We measure the performance of the model through confusion rates (% of time raters choose the model outputs over reference images, where a perfect algorithm would achieve a 50% confusion rate). The results of this study are shown in the figure below. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-MRJn88KECNA/YPG5dmVIpYI/AAAAAAAAH6s/cNP2QlhlZz0xJ8yWNbpgr1JJVIXwXJ-3QCLcBGAsYHQ/s1462/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;744&quot; data-original-width=&quot;1462&quot; height=&quot;326&quot; src=&quot;https://1.bp.blogspot.com/-MRJn88KECNA/YPG5dmVIpYI/AAAAAAAAH6s/cNP2QlhlZz0xJ8yWNbpgr1JJVIXwXJ-3QCLcBGAsYHQ/w640-h326/image4.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Above:&lt;/b&gt; We achieve close to 50% confusion rate on the task of 16x16 → 128x128 faces, outperforming state-of-the-art face super-resolution methods&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2003.03808&quot;&gt;PULSE&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/1711.10703&quot;&gt;FSRGAN&lt;/a&gt;. &lt;b&gt;Below:&lt;/b&gt; We also achieve a 40% confusion rate on the much more difficult task of 64x64 → 256x256 natural images, outperforming the regression baseline by a large margin.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;CDM: Class-Conditional ImageNet Generation&lt;/b&gt;&lt;br /&gt;Having shown the effectiveness of SR3 in performing natural image super-resolution, we go a step further and use these SR3 models for class-conditional image generation. &lt;a href=&quot;https://cascaded-diffusion.github.io/&quot;&gt;CDM&lt;/a&gt; is a class-conditional diffusion model trained on ImageNet data to generate high-resolution natural images. Since ImageNet is a difficult, high-entropy dataset, we built CDM as a cascade of multiple diffusion models. This cascade approach involves chaining together multiple generative models over several spatial resolutions: one diffusion model that generates data at a low resolution, followed by a sequence of SR3 super-resolution diffusion models that gradually increase the resolution of the generated image to the highest resolution. It is well known that cascading improves quality and training speed for high resolution data, as shown by previous studies (for example in &lt;a href=&quot;https://arxiv.org/abs/1812.01608&quot;&gt;autoregressive models&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1906.00446&quot;&gt;VQ-VAE-2&lt;/a&gt;) and in &lt;a href=&quot;https://arxiv.org/abs/2102.09672&quot;&gt;concurrent&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2105.05233&quot;&gt;work&lt;/a&gt; for diffusion models. As demonstrated by our quantitative results below, CDM further highlights the effectiveness of cascading in diffusion models for sample quality and usefulness in downstream tasks, such as image classification. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-fZUmX3WWlJ8/YPG5qDDvVmI/AAAAAAAAH6w/nK7NMZzGZno-t_rBeuhkPHBq4k8Z32kYQCLcBGAsYHQ/s800/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;295&quot; data-original-width=&quot;800&quot; height=&quot;236&quot; src=&quot;https://1.bp.blogspot.com/-fZUmX3WWlJ8/YPG5qDDvVmI/AAAAAAAAH6w/nK7NMZzGZno-t_rBeuhkPHBq4k8Z32kYQCLcBGAsYHQ/w640-h236/image3.gif&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Example of the cascading pipeline that includes a sequence of diffusion models: the first generates a low resolution image, and the rest perform upsampling to the final high resolution image. Here the pipeline is for class-conditional ImageNet generation, which begins with a class-conditional diffusion model at 32x32 resolution, followed by 2x and 4x class-conditional super-resolution using SR3.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-sevsNGJ66P4/YPG53LmbXAI/AAAAAAAAH64/MPy1jbpdbbY1CQ7fIWjl_PZXzKJ38rCXwCLcBGAsYHQ/s1521/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1141&quot; data-original-width=&quot;1521&quot; height=&quot;480&quot; src=&quot;https://1.bp.blogspot.com/-sevsNGJ66P4/YPG53LmbXAI/AAAAAAAAH64/MPy1jbpdbbY1CQ7fIWjl_PZXzKJ38rCXwCLcBGAsYHQ/w640-h480/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Selected generated images from our 256x256 cascaded class-conditional ImageNet model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Along with including the SR3 model in the cascading pipeline, we also introduce a new data augmentation technique, which we call &lt;em&gt;conditioning augmentation&lt;/em&gt;, that further improves the sample quality results of CDM. While the super-resolution models in CDM are trained on original images from the dataset, during generation they need to perform super-resolution on the images generated by a low-resolution base model, which may not be of sufficiently high quality in comparison to the original images. This leads to a train-test mismatch for the super-resolution models. Conditioning augmentation refers to applying data augmentation to the low-resolution input image of each super-resolution model in the cascading pipeline. These augmentations, which in our case include Gaussian noise and Gaussian blur, prevents each super-resolution model from overfitting to its lower resolution conditioning input, eventually leading to better higher resolution sample quality for CDM. &lt;/p&gt;&lt;p&gt;Altogether, CDM generates high fidelity samples superior to BigGAN-deep and VQ-VAE-2 in terms of both &lt;a href=&quot;https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance&quot;&gt;FID score&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1905.10887&quot;&gt;Classification Accuracy Score&lt;/a&gt; on class-conditional ImageNet generation. CDM is a pure generative model that does not use a classifier to boost sample quality, unlike other models such as &lt;a href=&quot;https://arxiv.org/abs/2105.05233&quot;&gt;ADM&lt;/a&gt; and VQ-VAE-2. See below for quantitative results on sample quality. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-0QCpdlDZTyU/YPG6GJId-tI/AAAAAAAAH7A/Bp2OK_PTrqQBo_1-P2E7KftCCJr7yPa2ACLcBGAsYHQ/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;487&quot; data-original-width=&quot;1999&quot; height=&quot;156&quot; src=&quot;https://1.bp.blogspot.com/-0QCpdlDZTyU/YPG6GJId-tI/AAAAAAAAH7A/Bp2OK_PTrqQBo_1-P2E7KftCCJr7yPa2ACLcBGAsYHQ/w640-h156/image5.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Class-conditional ImageNet FID scores at the 256x256 resolution for methods that do not use extra classifiers to boost sample quality. BigGAN-deep is reported at its best truncation value. (Lower is better.)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-fxFnIOObOFY/YPG6Q2nld6I/AAAAAAAAH7I/mnBOaoQv3IYLS8meip7XrAqpUceBtARLQCLcBGAsYHQ/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;626&quot; data-original-width=&quot;1999&quot; height=&quot;200&quot; src=&quot;https://1.bp.blogspot.com/-fxFnIOObOFY/YPG6Q2nld6I/AAAAAAAAH7I/mnBOaoQv3IYLS8meip7XrAqpUceBtARLQCLcBGAsYHQ/w640-h200/image1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;ImageNet classification accuracy scores at the 256x256 resolution, measuring the validation set accuracy of a classifier trained on generated data. CDM generated data attains significant gains over existing methods, closing the gap in classification accuracy between real and generated data. (Higher is better.)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;With SR3 and CDM, we have pushed the performance of diffusion models to state-of-the-art on super-resolution and class-conditional ImageNet generation benchmarks. We are excited to further test the limits of diffusion models for a wide variety of generative modeling problems. For more information on our work, please visit &lt;a href=&quot;http://iterative-refinement.github.io&quot;&gt;Image Super-Resolution via Iterative Refinement&lt;/a&gt; and &lt;a href=&quot;http://cascaded-diffusion.github.io&quot;&gt;Cascaded Diffusion Models for High Fidelity Image Generation&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements:&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We thank our co-authors William Chan, Mohammad Norouzi, Tim Salimans, and David Fleet, and we are grateful for research discussions and assistance from Ben Poole, Jascha Sohl-Dickstein, Doug Eck, and the rest of the Google Research, Brain Team. Thanks to Tom Small for helping us with the animations.&lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=m7_gu_XWeyA:g_NcGYP94oY:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/m7_gu_XWeyA&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/8564855857063590663/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8564855857063590663"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8564855857063590663"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/m7_gu_XWeyA/high-fidelity-image-generation-using.html" title="High Fidelity Image Generation Using Diffusion Models"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-MRJn88KECNA/YPG5dmVIpYI/AAAAAAAAH6s/cNP2QlhlZz0xJ8yWNbpgr1JJVIXwXJ-3QCLcBGAsYHQ/s72-w640-h326-c/image4.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-6520074264803989229</id><published>2021-07-15T10:34:00.000-07:00</published><updated>2021-07-15T10:34:24.759-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Robotics"/><category scheme="http://www.blogger.com/atom/ns#" term="TPU"/><title type="text">Speeding Up Reinforcement Learning with a New Physics Simulation Engine</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by C. Daniel Freeman, Senior Software Engineer and Erik Frey, Staff Software Engineer, Google Research&lt;/span&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;&gt;Reinforcement learning&lt;/a&gt; (RL) is a &lt;a href=&quot;https://ai.googleblog.com/2019/02/long-range-robotic-navigation-via.html&quot;&gt;popular method&lt;/a&gt; for &lt;a href=&quot;https://ai.googleblog.com/2020/05/agile-and-intelligent-locomotion-via.html&quot;&gt;teaching robots&lt;/a&gt; to navigate and &lt;a href=&quot;https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html&quot;&gt;manipulate&lt;/a&gt; the physical world, which itself can be simplified and expressed as interactions between &lt;em&gt;rigid bodies&lt;/em&gt;&lt;sup id=&quot;fnref1&quot;&gt;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;1&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt; (i.e., solid physical objects that do not deform when a force is applied to them). In order to facilitate the collection of training data in a practical amount of time, RL usually leverages simulation, where approximations of any number of complex objects are composed of many rigid bodies connected by joints and powered by actuators. But this poses a challenge: it frequently takes millions to billions of simulation frames for an RL agent to become proficient at even simple tasks, such as walking, using tools, or assembling toy blocks. &lt;/p&gt;  &lt;p&gt;While progress has been made to &lt;a href=&quot;https://ai.googleblog.com/2020/08/tackling-open-challenges-in-offline.html&quot;&gt;improve training efficiency by recycling simulation frames&lt;/a&gt;, some RL tools instead sidestep this problem by distributing the generation of simulation frames across many simulators. These distributed simulation platforms yield impressive results that train very quickly, but they must run on compute clusters with thousands of CPUs or GPUs which are inaccessible to most researchers. &lt;/p&gt;&lt;p&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2106.13281&quot;&gt;Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation&lt;/a&gt;”, we present a new physics simulation engine that matches the performance of a large compute cluster with just a single TPU or GPU. The engine is designed to both efficiently run thousands of parallel physics simulations alongside a machine learning (ML) algorithm on a single accelerator and scale millions of simulations seamlessly across pods of interconnected accelerators. We’ve &lt;a href=&quot;https://github.com/google/brax&quot;&gt;open sourced the engine&lt;/a&gt; along with reference RL algorithms and simulation environments that are all accessible via &lt;a href=&quot;https://colab.sandbox.google.com/github/google/brax/blob/main/notebooks/training.ipynb&quot;&gt;Colab&lt;/a&gt;. Using this new platform, we demonstrate 100-1000x faster training compared to a traditional workstation setup. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-Za-dyrqXP24/YPBB_LLLRHI/AAAAAAAAH48/2R922TQkSwsh38UEPztNA86DqAZqAMBfACLcBGAsYHQ/s1600/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;817&quot; data-original-width=&quot;1600&quot; height=&quot;326&quot; src=&quot;https://1.bp.blogspot.com/-Za-dyrqXP24/YPBB_LLLRHI/AAAAAAAAH48/2R922TQkSwsh38UEPztNA86DqAZqAMBfACLcBGAsYHQ/w640-h326/image1.gif&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Three typical RL workflows. The&lt;b&gt; left&lt;/b&gt; shows a typical workstation flow: on a single machine, with the environment on CPU, training takes hours or days. The&lt;b&gt; middle&lt;/b&gt; shows a typical distributed simulation flow: training takes minutes by farming simulation out to thousands of machines. The&lt;b&gt; right&lt;/b&gt; shows the Brax flow: learning and large batch simulation occur side by side on a single CPU/GPU chip.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;b&gt;Physics Simulation Engine Design Opportunities&lt;/b&gt;&lt;br/&gt;Rigid body physics are used in video games, robotics, molecular dynamics, biomechanics, graphics and animation, and other domains. In order to accurately model such systems, simulators integrate forces from gravity, motor actuation, joint constraints, object collisions, and others to simulate the motion of a physical system across time. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-xzTE4RTKkHg/YPBCTEmN4CI/AAAAAAAAH5E/vgQLCCI-eKw93j46VtHAyVtgLAde7M0MgCLcBGAsYHQ/s782/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;538&quot; data-original-width=&quot;782&quot; height=&quot;440&quot; src=&quot;https://1.bp.blogspot.com/-xzTE4RTKkHg/YPBCTEmN4CI/AAAAAAAAH5E/vgQLCCI-eKw93j46VtHAyVtgLAde7M0MgCLcBGAsYHQ/w640-h440/image9.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Simulation of three spherical bodies, a wall, two joints, and one actuator. For each simulation timestep, forces and torques are integrated together to update the positions, rotations, and velocities of each physical body.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;Taking a closer look at how most physics simulation engines are designed today, there are a few large opportunities to improve efficiency. As we noted above, a typical robotics learning pipeline places a single learner in a tight feedback with many simulations in parallel, but upon analyzing this architecture, one finds that: &lt;/p&gt;&lt;ol&gt; &lt;li&gt;This layout imposes an enormous latency bottleneck. Because the data must travel over the network within a datacenter, the learner must wait for 10,000+ nanoseconds to fetch experience from the simulator. Were this experience instead already on the same device as the learner’s neural network, latency would drop to &amp;lt;1 nanosecond.  &lt;/li&gt;&lt;li&gt;The computation necessary for training the agent (one simulation step, followed by one update of the agent’s neural network) is overshadowed by the computation spent packaging the data (i.e., marshalling data within the engine, then into a wire format such as &lt;a href=&quot;https://developers.google.com/protocol-buffers&quot;&gt;protobuf&lt;/a&gt;, then into &lt;a href=&quot;https://en.wikipedia.org/wiki/Transmission_Control_Protocol&quot;&gt;TCP&lt;/a&gt; buffers, and then undoing all these steps on the learner side).  &lt;/li&gt;&lt;li&gt;The computations happening within each simulator are remarkably similar, but not exactly the same. &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;Brax Design&lt;/b&gt;&lt;br/&gt;In response to these observations, Brax is designed so that its physics calculations are exactly the same across each of its thousands of parallel environments by ensuring that the simulation is free of &lt;em&gt;branches&lt;/em&gt; (i.e., simulation “&lt;em&gt;if”&lt;/em&gt; logic that diverges as a result of the environment state). An example of a branch in a physics engine is the application of a contact force between a ball and a wall: different code paths will execute depending on whether the ball is touching the wall. That is, &lt;em&gt;if &lt;/em&gt;the ball contacts the wall, separate code for simulating the ball’s bounce off the wall will execute. Brax employs a mix of the following three strategies to avoid branching: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;em&gt;Replace the discrete branching logic with a continuous function&lt;/em&gt;, such as approximating the ball-wall contact force using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Signed_distance_function&quot;&gt;signed distance function&lt;/a&gt;. This approach results in the most efficiency gains.  &lt;li&gt;&lt;em&gt;Evaluate the branch during JAX’s &lt;a href=&quot;https://jax.readthedocs.io/en/latest/jax.html?highlight=jit#jax.jit&quot;&gt;just-in-time compile&lt;/a&gt;.&lt;/em&gt; Many branches based on static properties of the environment, such as whether it’s even possible for two objects to collide, may be evaluated prior to simulation time.  &lt;li&gt;&lt;em&gt;Run both sides of the branch during simulation but then select only the required results.&lt;/em&gt; Because this executes some code that isn’t ultimately used, it wastes operations compared to the above.  &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Once the calculations are guaranteed to be exactly uniform, the entire training architecture can be reduced in complexity to be executed on a single TPU or GPU. Doing so removes the computational overhead and latency of cross-machine communication.  In practice, these changes lower the cost of training by 100x-1000x for comparable workloads. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Brax Environments&lt;/b&gt;&lt;br/&gt;Environments are tiny packaged worlds that define a task for an RL agent to learn. Environments contain not only the means to simulate a world, but also functions, such as how to observe the world and the definition of the goal in that world. &lt;/p&gt;&lt;p&gt;A few standard benchmark environments have emerged in recent years for testing new RL algorithms and for evaluating the impact of those algorithms using metrics commonly understood by research scientists. Brax includes four such ready-to-use environments that come from the popular &lt;a href=&quot;https://gym.openai.com/&quot;&gt;OpenAI gym:&lt;/a&gt; &lt;a href=&quot;https://gym.openai.com/envs/Ant-v2/&quot;&gt;Ant&lt;/a&gt;, &lt;a href=&quot;https://gym.openai.com/envs/HalfCheetah-v2/&quot;&gt;HalfCheetah&lt;/a&gt;, &lt;a href=&quot;https://gym.openai.com/envs/Humanoid-v2/&quot;&gt;Humanoid&lt;/a&gt;, and &lt;a href=&quot;https://gym.openai.com/envs/Reacher-v2/&quot;&gt;Reacher&lt;/a&gt;. &lt;/p&gt;      &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;  &lt;tbody&gt;  &lt;tr&gt;    &lt;td&gt;            &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-w1fngfkPou0/YPBCq9tV5kI/AAAAAAAAH5M/27Xzefx15NYf0TXIcOj47pXl6lDYozIdACLcBGAsYHQ/s414/image5.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;322&quot; data-original-width=&quot;414&quot; src=&quot;https://1.bp.blogspot.com/-w1fngfkPou0/YPBCq9tV5kI/AAAAAAAAH5M/27Xzefx15NYf0TXIcOj47pXl6lDYozIdACLcBGAsYHQ/s320/image5.gif&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;         &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td&gt;           &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-ynkWxgmLucE/YPBCqz_bzII/AAAAAAAAH5U/VnKz2ru6yZAgO8C0IphooJ4zup4G0JUcwCLcBGAsYHQ/s414/image4.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;322&quot; data-original-width=&quot;414&quot; src=&quot;https://1.bp.blogspot.com/-ynkWxgmLucE/YPBCqz_bzII/AAAAAAAAH5U/VnKz2ru6yZAgO8C0IphooJ4zup4G0JUcwCLcBGAsYHQ/s320/image4.gif&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;         &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;     &lt;td&gt;           &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-1aPtvNcWAGk/YPBCqxzL8qI/AAAAAAAAH5Q/036dqbtKAJkVE_G7Yc8rxVc4A_bfJsILgCLcBGAsYHQ/s414/image12.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;322&quot; data-original-width=&quot;414&quot; src=&quot;https://1.bp.blogspot.com/-1aPtvNcWAGk/YPBCqxzL8qI/AAAAAAAAH5Q/036dqbtKAJkVE_G7Yc8rxVc4A_bfJsILgCLcBGAsYHQ/s320/image12.gif&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;         &lt;/div&gt;&lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;     &lt;td&gt;           &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-HiwPUacRKM0/YPBCrAVMpPI/AAAAAAAAH5Y/XThQkNR82poh0HMrTd1_FZ1rzNhBthd1QCLcBGAsYHQ/s414/image8.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;322&quot; data-original-width=&quot;414&quot; src=&quot;https://1.bp.blogspot.com/-HiwPUacRKM0/YPBCrAVMpPI/AAAAAAAAH5Y/XThQkNR82poh0HMrTd1_FZ1rzNhBthd1QCLcBGAsYHQ/s320/image8.gif&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;         &lt;/td&gt;  &lt;/tr&gt;     &lt;/tbody&gt;&lt;/table&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;From left to right: Ant, HalfCheetah, Humanoid, and Reacher are popular baseline environments for RL research.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;Brax also includes three novel environments: dexterous manipulation of an object (a popular challenge in robotics), generalized locomotion (an agent that goes to a target placed anywhere around it), and a simulation of an industrial robot arm. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;  &lt;tbody&gt;  &lt;tr&gt;    &lt;td&gt;           &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-W3lXKYhaOJo/YPBDRhqXwQI/AAAAAAAAH5s/em_t2NhCBR0FY8Z9HAeOHn8NXcyqfoU-ACLcBGAsYHQ/s348/image2.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;249&quot; data-original-width=&quot;348&quot; src=&quot;https://1.bp.blogspot.com/-W3lXKYhaOJo/YPBDRhqXwQI/AAAAAAAAH5s/em_t2NhCBR0FY8Z9HAeOHn8NXcyqfoU-ACLcBGAsYHQ/s320/image2.gif&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;         &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td&gt;                    &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-H9lTyVstDjc/YPBGkE6izeI/AAAAAAAAH6g/7dkkJmXV7-EQEWYPzZZ_KkxsOU6GSprTACLcBGAsYHQ/s348/image13.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;249&quot; data-original-width=&quot;348&quot; src=&quot;https://1.bp.blogspot.com/-H9lTyVstDjc/YPBGkE6izeI/AAAAAAAAH6g/7dkkJmXV7-EQEWYPzZZ_KkxsOU6GSprTACLcBGAsYHQ/s320/image13.gif&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;                  &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;     &lt;td&gt;           &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-RqdH8dLe84Q/YPBDRjNS5TI/AAAAAAAAH5o/ryEmGdS030UeRkD8V7gkUuMLtAMpr0ZZQCLcBGAsYHQ/s348/image6.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;249&quot; data-original-width=&quot;348&quot; src=&quot;https://1.bp.blogspot.com/-RqdH8dLe84Q/YPBDRjNS5TI/AAAAAAAAH5o/ryEmGdS030UeRkD8V7gkUuMLtAMpr0ZZQCLcBGAsYHQ/s320/image6.gif&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;            &lt;/td&gt;       &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Left:&lt;/b&gt; Grasp, a claw hand that learns dexterous manipulation. &lt;b&gt;Middle:&lt;/b&gt; Fetch, a toy, box-like dog learns a general goal-based locomotion policy. &lt;b&gt;Right:&lt;/b&gt; Simulation of &lt;a href=&quot;https://www.universal-robots.com/products/ur5-robot/&quot;&gt;UR5e&lt;/a&gt;, an industrial robot arm.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;b&gt;Performance Benchmarks&lt;/b&gt;&lt;br/&gt;The first step for analyzing Brax’s performance is to measure the speed at which it can simulate large batches of environments, because this is the critical bottleneck to overcome in order for the learner to consume enough experience to learn quickly. &lt;/p&gt;&lt;p&gt;These two graphs below show how many physics steps (updates to the state of the environment) Brax can produce as it is tasked with simulating more and more environments in parallel. The graph on the left shows that Brax scales the number of steps per second linearly with the number of parallel environments, only hitting memory bandwidth bottlenecks at 10,000 environments, which is not only enough for training single agents, but also suitable for &lt;a href=&quot;https://deepmind.com/blog/article/population-based-training-neural-networks&quot;&gt;training entire populations of agents&lt;/a&gt;. The graph on the right shows two things: first, that Brax performs well not only on TPU, but also on high-end GPUs (see the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/v100/&quot;&gt;V100&lt;/a&gt; and &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/tesla-p100/&quot;&gt;P100&lt;/a&gt; curves), and second, that by leveraging JAX’s &lt;a href=&quot;https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap&quot;&gt;device parallelism primitives&lt;/a&gt;, Brax scales seamlessly across multiple devices, reaching hundreds of millions of physics steps per second (see the &lt;em&gt;TPUv3 8x8&lt;/em&gt; curve, which is 64 &lt;a href=&quot;https://cloud.google.com/tpu&quot;&gt;TPUv3&lt;/a&gt; chips directly connected to each other over a high speed interconnect) . &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-2032wGgcsk0/YPBDjRPJaWI/AAAAAAAAH6A/MsDBJTrMH4c2klChA6WEgR1POsSsqqHfgCLcBGAsYHQ/s1418/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;654&quot; data-original-width=&quot;1418&quot; height=&quot;296&quot; src=&quot;https://1.bp.blogspot.com/-2032wGgcsk0/YPBDjRPJaWI/AAAAAAAAH6A/MsDBJTrMH4c2klChA6WEgR1POsSsqqHfgCLcBGAsYHQ/w640-h296/image3.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Left:&lt;/b&gt; Scaling of the simulation steps per second for each Brax environment on a 4x2 TPU v3. &lt;b&gt;Right:&lt;/b&gt; Scaling of the simulation steps per second for several accelerators on the Ant environment.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;Another way to analyze Brax’s performance is to measure its impact on the time it takes to run a reinforcement learning experiment on a single workstation. Here we compare Brax training the popular &lt;a href=&quot;https://github.com/google/brax/blob/main/brax/envs/ant.py&quot;&gt;Ant&lt;/a&gt; benchmark environment to its &lt;a href=&quot;https://github.com/openai/gym/blob/master/gym/envs/mujoco/ant.py&quot;&gt;OpenAI counterpart&lt;/a&gt;, powered by the &lt;a href=&quot;http://www.mujoco.org/&quot;&gt;MuJoCo physics engine&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;In the graph below, the blue line represents a standard workstation setup, where a learner runs on the GPU and the simulator runs on the CPU. We see that the time it takes to train an ant to run with reasonable proficiency (a score of 4000 on the y axis) drops from about 3 hours for the blue line, to about 10 seconds using Brax on accelerator hardware. It’s interesting to note that even on CPU alone (the grey line), Brax performs more than an order of magnitude faster, benefitting from learner and simulator both sitting in the same process. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-KU3l-v2fqf0/YPBDwKsUneI/AAAAAAAAH6E/QjXVk96xT24esu0U7HfsM2Y8XlVx2w8FACLcBGAsYHQ/s1394/image10.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;646&quot; data-original-width=&quot;1394&quot; height=&quot;296&quot; src=&quot;https://1.bp.blogspot.com/-KU3l-v2fqf0/YPBDwKsUneI/AAAAAAAAH6E/QjXVk96xT24esu0U7HfsM2Y8XlVx2w8FACLcBGAsYHQ/w640-h296/image10.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Brax’s optimized &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;&gt;PPO&lt;/a&gt; versus a standard GPU-backed PPO learning the MuJoCo-Ant-v2 environment, evaluated for 10 million steps. Note the x-axis is log-wallclock-time in seconds. Shaded region indicates lowest and highest performing seeds over 5 replicas, and solid line indicates mean.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Physics Fidelity&lt;/b&gt;&lt;br/&gt;Designing a &lt;a href=&quot;https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html&quot;&gt;simulator that matches the behavior of the real world&lt;/a&gt; is a known hard problem that this work does not address. Nevertheless, it is useful to compare Brax to a reference simulator to ensure it is producing output that is at least as valid. In this case, we again compare Brax to &lt;a href=&quot;http://www.mujoco.org/&quot;&gt;MuJoCo&lt;/a&gt;, which is well-regarded for its simulation quality. We expect to see that, all else being equal, a policy has a similar reward trajectory whether trained in MuJoCo or Brax. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-EtkcA1VQsN4/YPBD3uYpd4I/AAAAAAAAH6M/VaaiMN0IH0k4JpIfDdWtsCki7wdJ8CEnwCLcBGAsYHQ/s394/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;265&quot; data-original-width=&quot;394&quot; height=&quot;430&quot; src=&quot;https://1.bp.blogspot.com/-EtkcA1VQsN4/YPBD3uYpd4I/AAAAAAAAH6M/VaaiMN0IH0k4JpIfDdWtsCki7wdJ8CEnwCLcBGAsYHQ/w640-h430/image7.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;MuJoCo-Ant-v2 vs. Brax Ant, showing the number of environment steps plotted against the average episode score achieved for the environment. Both environments were trained with the same standard implementation of &lt;a href=&quot;https://arxiv.org/pdf/1812.05905.pdf&quot;&gt;SAC&lt;/a&gt;. Shaded region indicates lowest and highest performing seeds over five runs, and solid line indicates the mean.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;These curves show that as the reward rises at about the same rate for both simulators, both engines compute physics with a comparable level of complexity or difficulty to solve. And as both curves top out at about the same reward, we have confidence that the same general physical limits apply to agents operating to the best of their ability in either simulation. &lt;/p&gt;&lt;p&gt;We can also measure Brax’s ability to conserve linear momentum, angular momentum, and energy. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-YbL_hH_D8t8/YPBEBWXLoRI/AAAAAAAAH6U/eAo3Drp5iKAn4QNp5PjEKpvMXUuG5t-CQCLcBGAsYHQ/s1072/image11.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;310&quot; data-original-width=&quot;1072&quot; height=&quot;186&quot; src=&quot;https://1.bp.blogspot.com/-YbL_hH_D8t8/YPBEBWXLoRI/AAAAAAAAH6U/eAo3Drp5iKAn4QNp5PjEKpvMXUuG5t-CQCLcBGAsYHQ/w640-h186/image11.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Linear momentum (&lt;b&gt;left&lt;/b&gt;), angular momentum (&lt;b&gt;middle&lt;/b&gt;), and energy (&lt;b&gt;right&lt;/b&gt;) non-conservation scaling for Brax as well as several other physics engines. The y-axis indicates drift from the expected calculation (higher is smaller drift, which is better), and the x axis indicates the amount of time being simulated.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;This measure of physics simulation quality was first &lt;a href=&quot;https://homes.cs.washington.edu/~todorov/papers/ErezICRA15.pdf&quot;&gt;proposed by the authors of MuJoCo&lt;/a&gt; as a way to understand how the simulation drifts off course as it is tasked with computing larger and larger time steps. Here, Brax performs similarly as its neighbors. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br/&gt;We invite researchers to perform a more qualitative measure of Brax’s physics fidelity by training their own policies in the &lt;a href=&quot;https://colab.research.google.com/github/google/brax/blob/main/notebooks/training.ipynb&quot;&gt;Brax Training Colab&lt;/a&gt;. The learned trajectories are recognizably similar to those seen in OpenAI Gym. &lt;/p&gt;&lt;p&gt;Our work makes fast, scalable RL and robotics research much more accessible — what was formerly only possible via large compute clusters can now be run on workstations, or for free &lt;a href=&quot;https://colab.research.google.com/github/google/brax/blob/main/notebooks/training.ipynb&quot;&gt;via hosted Google Colaboratory&lt;/a&gt;. &lt;a href=&quot;https://github.com/google/brax&quot;&gt;Our Github repository&lt;/a&gt; includes not only the Brax simulation engine, but also a host of reference RL algorithms for fast training. We can’t wait to see what kind of new research Brax enables. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br/&gt;&lt;em&gt;We'd like to thank our paper co-authors: Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem.  We also thank Erwin Coumans for advice on building physics engines, Blake Hechtman and James Bradbury for providing optimization help with JAX and XLA, and Luke Metz and Shane Gu for their advice.  We’d also like to thank Vijay Sundaram, Wright Bagwell, Matt Leffler, Gavin Dodd, Brad Mckee, and Logan Olson, for helping to incubate this project.&lt;/em&gt;&lt;/p&gt; &lt;!--Footnotes--&gt;&lt;hr width=&quot;80%&quot; /&gt;&lt;p&gt;  &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;&gt;&lt;sup&gt;&lt;a name=&quot;fn1&quot;&gt;&lt;b&gt;1&lt;/b&gt;&lt;/a&gt;&lt;/sup&gt; Due to the complexity of the real world, there is also ongoing research exploring the physics of &lt;a href=&quot;https://ai.googleblog.com/2021/05/learning-to-manipulate-deformable.html&quot;&gt;deformable bodies&lt;/a&gt;.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;&gt;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=n_Mjs-FnDXU:Rqc7JTNknck:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/n_Mjs-FnDXU&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/6520074264803989229/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/speeding-up-reinforcement-learning-with.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6520074264803989229"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6520074264803989229"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/n_Mjs-FnDXU/speeding-up-reinforcement-learning-with.html" title="Speeding Up Reinforcement Learning with a New Physics Simulation Engine"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-Za-dyrqXP24/YPBB_LLLRHI/AAAAAAAAH48/2R922TQkSwsh38UEPztNA86DqAZqAMBfACLcBGAsYHQ/s72-w640-h326-c/image1.gif" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/07/speeding-up-reinforcement-learning-with.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7000929806584590164</id><published>2021-07-14T11:28:00.001-07:00</published><updated>2021-07-14T11:30:40.317-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Natural Language Processing"/><category scheme="http://www.blogger.com/atom/ns#" term="Search"/><category scheme="http://www.blogger.com/atom/ns#" term="Self-Supervised Learning"/><title type="text">From Vision to Language: Semi-supervised Learning in Action…at Scale</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Thang Luong, Staff Research Scientist, Google Research and Jingcao Hu, Senior Staff Software Engineer, Google Search&lt;/span&gt; &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;&gt;Supervised learning&lt;/a&gt;, the machine learning task of training predictive models using data points with known outcomes (i.e., labeled data), is generally the preferred approach in industry because of its simplicity. However, supervised learning requires accurately labeled data, the collection of which is often labor intensive. In addition, as model efficiency &lt;a href=&quot;https://openai.com/blog/ai-and-efficiency/&quot;&gt;improves&lt;/a&gt; with better architectures, algorithms, and hardware (&lt;a href=&quot;https://en.wikipedia.org/wiki/Graphics_processing_unit&quot;&gt;GPUs&lt;/a&gt; / &lt;a href=&quot;https://en.wikipedia.org/wiki/Tensor_Processing_Unit&quot;&gt;TPUs&lt;/a&gt;), training &lt;a href=&quot;https://arxiv.org/abs/2006.16668&quot;&gt;large models&lt;/a&gt; to achieve better quality becomes more accessible, which, in turn, requires even more labeled data for continued progress.  &lt;/p&gt;&lt;p&gt;To mitigate such data acquisition challenges, &lt;a href=&quot;https://en.wikipedia.org/wiki/Semi-supervised_learning&quot;&gt;semi-supervised learning&lt;/a&gt;, a machine learning paradigm that combines a small amount of labeled data with a large amount of unlabeled data, has recently seen success with methods such as &lt;a href=&quot;https://ai.googleblog.com/2019/07/advancing-semi-supervised-learning-with.html&quot;&gt;UDA&lt;/a&gt;, &lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&quot;&gt;SimCLR&lt;/a&gt;, and many others. In our previous work, we demonstrated for the first time that a semi-supervised learning approach, &lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;&gt;Noisy Student&lt;/a&gt;, can achieve state-of-the-art performance on &lt;a href=&quot;http://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt;, a large-scale academic benchmark for image classification, by utilizing many more unlabeled examples.  &lt;/p&gt;&lt;p&gt;Inspired by these results, today we are excited to present semi-supervised distillation (SSD), a simplified version of Noisy Student, and demonstrate its successful application to the language domain. We apply SSD to language understanding within the context of Google Search, resulting in high performance gains. This is the first successful instance of semi-supervised learning applied at such a large scale and demonstrates the potential impact of such approaches for production-scale systems.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Noisy Student Training&lt;/b&gt;&lt;br /&gt;Prior to our development of Noisy Student, there was a large body of research into semi-supervised learning. In spite of this extensive research, however, such systems typically worked well only in the low-data regime, e.g., &lt;a href=&quot;https://en.wikipedia.org/wiki/CIFAR-10&quot;&gt;CIFAR&lt;/a&gt;, &lt;a href=&quot;http://ufldl.stanford.edu/housenumbers/&quot;&gt;SVHN&lt;/a&gt;, and 10% &lt;a href=&quot;http://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt;. When labeled data were abundant, such models were unable to compete with fully supervised learning systems, which prevented semi-supervised approaches from being applied to important applications in production, such as search engines and self-driving cars. This shortcoming motivated our development of &lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;&gt;Noisy Student Training&lt;/a&gt;, a semi-supervised learning approach that worked well in the high-data regime, and at the time achieved state-of-the-art accuracy on ImageNet using 130M additional unlabeled images.  &lt;/p&gt;&lt;p&gt;Noisy Student Training has 4 simple steps: &lt;/p&gt;&lt;ol&gt;&lt;li&gt;Train a classifier (the teacher) on labeled data. &lt;/li&gt;&lt;li&gt;The teacher then infers pseudo-labels on a much larger unlabeled dataset. &lt;/li&gt;&lt;li&gt;Then, it trains a larger classifier on the combined labeled and pseudo-labeled data, while also adding noise (noisy student). &lt;/li&gt;&lt;li&gt;(Optional) Going back to step 2, the student may be used as a new teacher. &lt;/li&gt;&lt;/ol&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-P84HreAGlaM/YO3u9rxOftI/AAAAAAAAH4Y/ertrhpUxGY0kkWpoAzB0WFcWaxbvGYE5gCLcBGAsYHQ/s1854/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1046&quot; data-original-width=&quot;1854&quot; height=&quot;226&quot; src=&quot;https://1.bp.blogspot.com/-P84HreAGlaM/YO3u9rxOftI/AAAAAAAAH4Y/ertrhpUxGY0kkWpoAzB0WFcWaxbvGYE5gCLcBGAsYHQ/w400-h226/image4.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;text-align: left;&quot;&gt;An illustration of Noisy Student Training through four simple steps. We use two types of noise: model noise (&lt;/span&gt;&lt;a href=&quot;https://jmlr.org/papers/v15/srivastava14a.html&quot; style=&quot;text-align: left;&quot;&gt;Dropout&lt;/a&gt;&lt;span style=&quot;text-align: left;&quot;&gt;,&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.09382&quot; style=&quot;text-align: left;&quot;&gt;Stochastic Depth&lt;/a&gt;&lt;span style=&quot;text-align: left;&quot;&gt;) and input noise (data augmentation, such as&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.13719&quot; style=&quot;text-align: left;&quot;&gt;RandAugment&lt;/a&gt;&lt;span style=&quot;text-align: left;&quot;&gt;).&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  One can view Noisy Student as a form of self-training, because the model generates pseudo-labels with which it retrains itself to improve performance. A surprising property of Noisy Student Training is that the trained models work extremely well on robustness test sets for which it was not optimized, including &lt;a href=&quot;https://arxiv.org/abs/1907.07174&quot;&gt;ImageNet-A&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1903.12261&quot;&gt;ImageNet-C, and ImageNet-P&lt;/a&gt;. We hypothesize that the noise added during training not only helps with the learning, but also makes the model more robust. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-p5-t-HXLa88/YO3vn03u2OI/AAAAAAAAH4g/2QA2YZ1ce3Ig7E8yg_T6spKWhHRGB5EMQCLcBGAsYHQ/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;892&quot; data-original-width=&quot;1999&quot; height=&quot;286&quot; src=&quot;https://1.bp.blogspot.com/-p5-t-HXLa88/YO3vn03u2OI/AAAAAAAAH4g/2QA2YZ1ce3Ig7E8yg_T6spKWhHRGB5EMQCLcBGAsYHQ/w640-h286/image1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Examples of images that are classified incorrectly by the baseline model, but correctly by Noisy Student. &lt;b&gt;Left:&lt;/b&gt; An unmodified image from ImageNet-A. &lt;b&gt;Middle&lt;/b&gt; and &lt;b&gt;Right:&lt;/b&gt; Images with noise added, selected from ImageNet-C. For more examples including ImageNet-P, please see the &lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;&gt;paper&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  &lt;b&gt;Connections to Knowledge Distillation&lt;/b&gt;&lt;br /&gt;Noisy Student is similar to &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;&gt;knowledge distillation&lt;/a&gt;, which is a process of transferring knowledge from a large model (i.e., the teacher) to a smaller model (the student). The goal of distillation is to improve speed in order to build a model that is fast to run in production without sacrificing much in quality compared to the teacher. The simplest setup for distillation involves a single teacher and uses the same data, but in practice, one can use multiple teachers or a separate dataset for the student. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-6Fw3xwmDgZs/YO31Hc9WWJI/AAAAAAAAH4o/woD6ZKVll0wkn71Z_XbHh_x9Ak0YzzaVwCLcBGAsYHQ/s1484/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;1484&quot; height=&quot;226&quot; src=&quot;https://1.bp.blogspot.com/-6Fw3xwmDgZs/YO31Hc9WWJI/AAAAAAAAH4o/woD6ZKVll0wkn71Z_XbHh_x9Ak0YzzaVwCLcBGAsYHQ/w640-h226/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;text-align: left;&quot;&gt;Simple illustrations of Noisy Student and knowledge distillation.&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  Unlike Noisy Student, knowledge distillation does not add noise during training (e.g., data augmentation or model regularization) and typically involves a smaller student model. In contrast, one can think of Noisy Student as the process of “knowledge expansion”.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Semi-Supervised Distillation&lt;/b&gt;&lt;br /&gt;Another strategy for training production models is to apply Noisy Student training twice: first to get a larger teacher model T’ and then to derive a &lt;em&gt;smaller&lt;/em&gt; student S. This approach produces a model that is better than either training with supervised learning or with Noisy Student training alone. Specifically, when applied to the vision domain for a family of &lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;&gt;EfficientNet&lt;/a&gt; models, ranging from EfficientNet-B0 with 5.3M parameters to EfficientNet-B7 with 66M parameters, this strategy achieves much better performance for each given model size  (see Table 9 of the &lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;&gt;Noisy Student paper&lt;/a&gt; for more details). &lt;/p&gt;&lt;p&gt;Noisy Student training needs data augmentation, e.g., &lt;a href=&quot;https://arxiv.org/abs/1909.13719&quot;&gt;RandAugment&lt;/a&gt; (for vision) or &lt;a href=&quot;https://arxiv.org/abs/1904.08779&quot;&gt;SpecAugment&lt;/a&gt; (for speech), to work well. But in certain applications, e.g., natural language processing, such types of input noise are not readily available. For those applications, Noisy Student Training can be simplified to have no noise. In that case, the above two-stage process becomes a simpler method, which we call Semi-Supervised Distillation (SSD). First, the teacher model infers pseudo-labels on the unlabeled dataset from which we then train a new teacher model (T’) that is of &lt;em&gt;equal-or-larger&lt;/em&gt; size than the original teacher model. This step, which is essentially self-training, is then followed by knowledge distillation to produce a smaller  student model for production.  &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-jsvnvPxipNI/YO31TKpwRhI/AAAAAAAAH4s/Tk8NjSteu8A9J-F_xHKHA37QtLVFpcYFACLcBGAsYHQ/s998/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;412&quot; data-original-width=&quot;998&quot; height=&quot;264&quot; src=&quot;https://1.bp.blogspot.com/-jsvnvPxipNI/YO31TKpwRhI/AAAAAAAAH4s/Tk8NjSteu8A9J-F_xHKHA37QtLVFpcYFACLcBGAsYHQ/w640-h264/image5.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;text-align: left;&quot;&gt;An illustration of Semi-Supervised Distillation (SSD), a 2-stage process that self-trains an equal-or-larger teacher (T’) before distilling to a student (S).&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  &lt;b&gt;Improving Search&lt;/b&gt;&lt;br /&gt;Having succeeded in the vision domain, an application in the language understanding domain, like Google Search, is a logical next step with broader user impact. In this case, we focus on an important ranking component in Search, which builds on &lt;a href=&quot;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&quot;&gt;BERT&lt;/a&gt; to &lt;a href=&quot;https://blog.google/products/search/search-language-understanding-bert/&quot;&gt;better understand languages&lt;/a&gt;. This task turns out to be well-suited for SSD. Indeed, applying SSD to the ranking component to better understand the relevance of candidate search results to queries achieved one of the highest performance gains among top launches at Search in 2020. Below is an example of a query where the improved model demonstrates better language understanding.  &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-_H-Li7Myyxo/YO31deYvthI/AAAAAAAAH40/bLLq9E9vEEsnOsSO1t5M7Gcb7S0fERiwgCLcBGAsYHQ/s1435/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;645&quot; data-original-width=&quot;1435&quot; height=&quot;288&quot; src=&quot;https://1.bp.blogspot.com/-_H-Li7Myyxo/YO31deYvthI/AAAAAAAAH40/bLLq9E9vEEsnOsSO1t5M7Gcb7S0fERiwgCLcBGAsYHQ/w640-h288/image3.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;text-align: left;&quot;&gt;With the implementation of SSD, Search is able to find documents that are more relevant to user queries.&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  &lt;b&gt;Future Research &amp;amp; Challenges&lt;/b&gt;&lt;br /&gt;We have presented a successful instance of semi-supervised distillation (SSD) in the production scale setting of Search. We believe SSD will continue changing the landscape of machine learning usage in the industry from predominantly supervised learning to semi-supervised learning. While our results are promising, there is still much research needed in how to efficiently utilize unlabeled examples in the real world, which is often noisy, and apply them to various domains. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;Zhenshuai Ding, Yanping Huang, Elizabeth Tucker, Hai Qian, and Steve He contributed immensely to this successful launch. The project would not have succeeded without contributions from members of both the Brain and Search teams: Shuyuan Zhang, Rohan Anil, Zhifeng Chen, Rigel Swavely, Chris Waterson, Avinash Atreya. Thanks to Qizhe Xie and Zihang Dai for feedback on the work. Also, thanks to Quoc Le, Yonghui Wu, Sundeep Tirumalareddy, Alexander Grushetsky,  Pandu Nayak for their leadership support. &lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=z3nwWEfus8M:D8RmJOVAXUg:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/z3nwWEfus8M&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/7000929806584590164/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/from-vision-to-language-semi-supervised.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7000929806584590164"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7000929806584590164"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/z3nwWEfus8M/from-vision-to-language-semi-supervised.html" title="From Vision to Language: Semi-supervised Learning in Action…at Scale"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-P84HreAGlaM/YO3u9rxOftI/AAAAAAAAH4Y/ertrhpUxGY0kkWpoAzB0WFcWaxbvGYE5gCLcBGAsYHQ/s72-w400-h226-c/image4.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/07/from-vision-to-language-semi-supervised.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-3503000124032758615</id><published>2021-07-13T11:12:00.001-07:00</published><updated>2021-07-13T11:34:15.150-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/><title type="text">Reducing the Computational Cost of Deep Reinforcement Learning Research</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Pablo Samuel Castro, Staff Software Engineer, Google Research&lt;/span&gt; &lt;p&gt;It is widely accepted that the enormous growth of deep reinforcement learning research, which combines traditional &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;&gt;reinforcement learning&lt;/a&gt; with &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks&quot;&gt;deep neural networks&lt;/a&gt;, began with the publication of the seminal &lt;a href=&quot;https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning&quot;&gt;DQN&lt;/a&gt; algorithm. This paper demonstrated the potential of this combination, showing that it could produce agents that could play a number of Atari 2600 games very effectively. Since then, there have been &lt;a href=&quot;https://arxiv.org/abs/1509.06461&quot;&gt;several&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1706.10295&quot;&gt;approaches&lt;/a&gt; that have built on and improved the original DQN. The popular &lt;a href=&quot;https://arxiv.org/abs/1710.02298&quot;&gt;Rainbow algorithm&lt;/a&gt; combined a number of these recent advances to achieve state-of-the-art performance on the &lt;a href=&quot;https://arxiv.org/abs/1207.4708&quot;&gt;ALE benchmark&lt;/a&gt;. This advance, however, came at a very high computational cost, which has the unfortunate side effect of widening the gap between those with ample access to computational resources and those without. &lt;/p&gt;&lt;p&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2011.14826&quot;&gt;Revisiting Rainbow: Promoting more Insightful and Inclusive Deep Reinforcement Learning Research&lt;/a&gt;”, to be presented at &lt;a href=&quot;https://icml.cc/&quot;&gt;ICML 2021&lt;/a&gt;, we revisit this algorithm on a set of small- and medium-sized tasks. We first discuss the computational cost associated with the Rainbow algorithm. We explore how the same conclusions regarding the benefits of combining the various algorithmic components can be reached with smaller-scale experiments, and further generalize that idea to how research done on a smaller computational budget can provide valuable scientific insights. &lt;/p&gt;&lt;p&gt;&lt;b&gt;The Cost of Rainbow&lt;/b&gt;&lt;br/&gt;A major reason for the computational cost of Rainbow is that the standards in academic publishing often require evaluating new algorithms on large benchmarks like &lt;a href=&quot;https://arxiv.org/abs/1207.4708&quot;&gt;ALE&lt;/a&gt;, which consists of 57 Atari 2600 games that reinforcement learning agents may learn to play. For a typical game, it takes roughly five days to train a model using a &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/tesla-p100/&quot;&gt;Tesla P100 GPU&lt;/a&gt;. Furthermore, if one wants to establish meaningful confidence bounds, it is common to perform at least five independent runs. Thus, to train Rainbow on the full suite of 57 games required around 34,200 GPU hours (or 1425 days) in order to provide convincing empirical performance statistics. In other words, such experiments are only feasible if one is able to train on multiple GPUs in parallel, which can be prohibitive for smaller research groups.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Revisiting Rainbow&lt;/b&gt;&lt;br/&gt;As in the original Rainbow paper, we evaluate the effect of adding the following components to the original DQN algorithm: &lt;a href=&quot;https://arxiv.org/abs/1509.06461&quot;&gt;double Q-learning&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1511.05952&quot;&gt;prioritized experience replay&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1511.06581&quot;&gt;dueling networks&lt;/a&gt;, &lt;a href=&quot;https://link.springer.com/article/10.1007/BF00115009&quot;&gt;multi-step learning&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1707.06887&quot;&gt;distributional RL&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/1706.10295&quot;&gt;noisy nets&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;We evaluate on a set of four classic control environments, which can be fully trained in 10-20 minutes (compared to five days for ALE games): &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-V4326gyjRSs/YO2jV38rGvI/AAAAAAAAH3w/2mpLzssovDIrH94hMACOci0Zc8iMNMjMACLcBGAsYHQ/s1065/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;771&quot; data-original-width=&quot;1065&quot; height=&quot;464&quot; src=&quot;https://1.bp.blogspot.com/-V4326gyjRSs/YO2jV38rGvI/AAAAAAAAH3w/2mpLzssovDIrH94hMACOci0Zc8iMNMjMACLcBGAsYHQ/w640-h464/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Upper left: &lt;/b&gt;In &lt;a href=&quot;https://gym.openai.com/envs/CartPole-v1/&quot;&gt;CartPole&lt;/a&gt;, the task is to balance a pole on a cart that the agent can move left and right. &lt;b&gt;Upper right:&lt;/b&gt; In &lt;a href=&quot;https://gym.openai.com/envs/Acrobot-v1/&quot;&gt;Acrobot&lt;/a&gt;, there are two arms and two joints, where the agent applies force to the joint between the two arms in order to raise the lower arm above a threshold. &lt;b&gt;Lower left:&lt;/b&gt; In &lt;a href=&quot;https://gym.openai.com/envs/LunarLander-v2/&quot;&gt;LunarLander&lt;/a&gt;, the agent is meant to land the spaceship between the two flags. &lt;b&gt;Lower right:&lt;/b&gt; In &lt;a href=&quot;https://gym.openai.com/envs/MountainCar-v0/&quot;&gt;MountainCar&lt;/a&gt;, the agent must build up momentum between two hills to drive to the top of the rightmost hill.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;     &lt;p&gt;We investigated the effect of both independently adding each of the components to DQN, as well as removing each from the full Rainbow algorithm. As in the original Rainbow paper, we find that, in aggregate, the addition of each of these algorithms does improve learning over the base DQN. However, we also found some important differences, such as the fact that &lt;a href=&quot;https://arxiv.org/abs/1707.06887&quot;&gt;distributional RL&lt;/a&gt; — commonly thought to be a positive addition on its own — does not always yield improvements on its own. Indeed, in contrast to the ALE results in the Rainbow paper, in the classic control environments, distributional RL only yields an improvement &lt;em&gt;when combined with another component&lt;/em&gt;.  &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-gw5s7FbNvaI/YO2j6DC8UDI/AAAAAAAAH34/ZxJ1D2fJR1EPrcj4cDvEZjRzoMRlXAcugCLcBGAsYHQ/s991/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;952&quot; data-original-width=&quot;991&quot; height=&quot;614&quot; src=&quot;https://1.bp.blogspot.com/-gw5s7FbNvaI/YO2j6DC8UDI/AAAAAAAAH34/ZxJ1D2fJR1EPrcj4cDvEZjRzoMRlXAcugCLcBGAsYHQ/w640-h614/image1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Each plot shows the training progress when adding the various components to DQN. The x-axis is training steps,the y-axis is performance (higher is better).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-86hRRLWGWAs/YO3HAyHEvqI/AAAAAAAAH4A/977Qa_n0KyUoLq0IABie30au1Bo3ZedIQCLcBGAsYHQ/s991/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;952&quot; data-original-width=&quot;991&quot; height=&quot;614&quot; src=&quot;https://1.bp.blogspot.com/-86hRRLWGWAs/YO3HAyHEvqI/AAAAAAAAH4A/977Qa_n0KyUoLq0IABie30au1Bo3ZedIQCLcBGAsYHQ/w640-h614/image3.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Each plot shows the training progress when removing the various components from Rainbow. The x-axis is training steps,the y-axis is performance (higher is better).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;We also re-ran the Rainbow experiments on the &lt;a href=&quot;https://github.com/kenjyoung/MinAtar&quot;&gt;MinAtar environment&lt;/a&gt;, which consists of a set of five miniaturized Atari games, and found qualitatively similar results. The MinAtar games are roughly 10 times faster to train than the regular Atari 2600 games on which the original Rainbow algorithm was evaluated, but still share some interesting aspects, such as game dynamics and having pixel-based inputs to the agent. As such, they provide a challenging mid-level environment, in between the classic control and the full Atari 2600 games. &lt;/p&gt;&lt;p&gt;When viewed in aggregate, we find our results to be consistent with those of &lt;a href=&quot;https://arxiv.org/abs/1710.02298&quot;&gt;the original Rainbow paper&lt;/a&gt; — the impact resulting from each algorithmic component can vary from environment to environment. If we were to suggest a single agent that balances the tradeoffs of the different algorithmic components, our version of Rainbow would likely be consistent with the original, in that combining all components produces a better overall agent. However, there are important details in the variations of the different algorithmic components that merit a more thorough investigation.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Beyond the Rainbow&lt;/b&gt;&lt;br/&gt;When DQN was introduced, it made use of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Huber_loss&quot;&gt;Huber loss&lt;/a&gt; and the &lt;a href=&quot;https://keras.io/api/optimizers/rmsprop/&quot;&gt;RMSProp Optimizer&lt;/a&gt;. It has been common practice for researchers to use these same choices when building on DQN, as most of their effort is spent on other algorithmic design decisions. In the spirit of reassessing these assumptions, we revisited the &lt;a href=&quot;https://en.wikipedia.org/wiki/Loss_function&quot;&gt;loss function&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_optimization&quot;&gt;optimizer&lt;/a&gt; used by &lt;a href=&quot;https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning&quot;&gt;DQN&lt;/a&gt; on a lower-cost, small-scale classic control and MinAtar environments. We ran some initial experiments using the &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam optimizer&lt;/a&gt;, which has lately been the most popular optimizer choice, combined with a simpler loss function, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;mean-squared error loss&lt;/a&gt; (MSE). Since the selection of optimizer and loss function is often overlooked when developing a new algorithm, we were surprised to see that we observed a dramatic improvement on all the classic control and MinAtar environments. &lt;/p&gt;&lt;p&gt;We thus decided to evaluate the different ways of combining the two optimizers (RMSProp and Adam) with the two losses (Huber and MSE) on the full ALE suite (60 Atari 2600 games). We found that Adam+MSE is a superior combination than RMSProp+Huber. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-kTFSYKdJ0Eo/YO3HYhr4x3I/AAAAAAAAH4M/Av5cOBT8adcfej9GMfKBszIKjSRQNSrTwCLcBGAsYHQ/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1999&quot; height=&quot;208&quot; src=&quot;https://1.bp.blogspot.com/-kTFSYKdJ0Eo/YO3HYhr4x3I/AAAAAAAAH4M/Av5cOBT8adcfej9GMfKBszIKjSRQNSrTwCLcBGAsYHQ/w640-h208/image4.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Measuring the improvement Adam+MSE gives over the default DQN settings (RMSProp + Huber); higher is better.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;Additionally, when comparing the various optimizer-loss combinations, we find that when using RMSProp, the Huber loss tends to perform better than MSE (illustrated by the gap between the solid and dotted orange lines). &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-vQSdnchQtik/YO3HStmaeJI/AAAAAAAAH4I/DfEzo0J0hXwPGrG9lEG97LGi28xjXDMXwCLcBGAsYHQ/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;551&quot; data-original-width=&quot;1999&quot; height=&quot;176&quot; src=&quot;https://1.bp.blogspot.com/-vQSdnchQtik/YO3HStmaeJI/AAAAAAAAH4I/DfEzo0J0hXwPGrG9lEG97LGi28xjXDMXwCLcBGAsYHQ/w640-h176/image5.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Normalized scores aggregated over all 60 Atari 2600 games, comparing the different optimizer-loss combinations.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br/&gt;On a limited computational budget we were able to reproduce, at a high-level, the findings of &lt;a href=&quot;https://arxiv.org/abs/1710.02298&quot;&gt;the Rainbow paper&lt;/a&gt; and uncover new and interesting phenomena. Evidently it is much easier to revisit something than to discover it in the first place. Our intent with this work, however, was to argue for the relevance and significance of empirical research on small- and medium-scale environments. We believe that these less computationally intensive environments lend themselves well to a more critical and thorough analysis of the performance, behaviors, and intricacies of new algorithms. &lt;/p&gt;&lt;p&gt;We are by no means calling for less emphasis to be placed on large-scale benchmarks. We are simply urging researchers to consider smaller-scale environments as a valuable tool in their investigations, and reviewers to avoid dismissing empirical work that focuses on smaller-scale environments. By doing so, in addition to reducing the environmental impact of our experiments, we will get both a clearer picture of the research landscape and reduce the barriers for researchers from diverse and often underresourced communities, which can only help make our community and scientific advances stronger. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgments&lt;/b&gt;&lt;br/&gt;&lt;em&gt;Thank you to Johan, the first author of this paper, for his hard work and persistence in seeing this through! We would also like to thank Marlos C. Machado, Sara Hooker, Matthieu Geist, Nino Vieillard, Hado van Hasselt, Eleni Triantafillou, and Brian Tanner for their insightful comments on this work.&lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=CqWWMkz8b0M:WRFG0AlOJeQ:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/CqWWMkz8b0M&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/3503000124032758615/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/reducing-computational-cost-of-deep.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3503000124032758615"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3503000124032758615"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/CqWWMkz8b0M/reducing-computational-cost-of-deep.html" title="Reducing the Computational Cost of Deep Reinforcement Learning Research"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-V4326gyjRSs/YO2jV38rGvI/AAAAAAAAH3w/2mpLzssovDIrH94hMACOci0Zc8iMNMjMACLcBGAsYHQ/s72-w640-h464-c/image2.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/07/reducing-computational-cost-of-deep.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-756990546465936835</id><published>2021-06-29T10:20:00.003-07:00</published><updated>2021-06-30T14:53:23.948-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="API"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/><title type="text">Quickly Training Game-Playing Agents with Machine Learning</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Leopold Haller and Hernan Moraldo, Software Engineers, Google Research&lt;/span&gt; &lt;p&gt;In the last two decades, dramatic advances in compute and connectivity have allowed game developers to create works of ever-increasing scope and complexity. Simple linear levels have evolved into photorealistic open worlds, procedural algorithms have enabled games with unprecedented variety, and expanding internet access has transformed games into dynamic online services. Unfortunately, scope and complexity have grown more rapidly than the size of quality assurance teams or the capabilities of traditional automated testing. This poses a challenge to both product quality (such as delayed releases and post-launch patches) and developer quality of life. &lt;/p&gt;&lt;p&gt;Machine learning (ML) techniques offer a possible solution, as they have demonstrated the potential to profoundly impact game development flows — they can help designers &lt;a href=&quot;https://ai.googleblog.com/2021/03/leveraging-machine-learning-for-game.html&quot;&gt;balance their game&lt;/a&gt; and empower artists to &lt;a href=&quot;https://ai.googleblog.com/2020/11/using-gans-to-create-fantastical.html&quot;&gt;produce high-quality assets&lt;/a&gt; in a fraction of the time traditionally required. Furthermore, they can be used to train challenging opponents that can &lt;a href=&quot;https://openai.com/projects/five/&quot;&gt;compete&lt;/a&gt; at the &lt;a href=&quot;https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii&quot;&gt;highest levels of play&lt;/a&gt;. Yet some ML techniques can pose requirements that currently make them impractical for production game teams, including the design of game-specific network architectures, the development of expertise in implementing ML algorithms, or the generation of billions of frames of training data. Conversely, game developers operate in a setting that offers unique advantages to leverage ML techniques, such as direct access to the game source, an abundance of expert demonstrations, and the uniquely interactive nature of video games. &lt;/p&gt;&lt;p&gt;Today, we present an ML-based system that game developers can use to quickly and efficiently train game-testing agents, helping developers find serious bugs quickly, while allowing human testers to focus on more complex and intricate problems. The resulting solution requires no ML expertise, works on many of the most popular game genres, and can train an ML policy, which generates game actions from the game state, in less than an hour on a single game instance. We have also released an &lt;a href=&quot;https://github.com/google-research/falken&quot;&gt;open source library&lt;/a&gt; that demonstrates a functional application of these techniques. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-dMg144dMPXA/YNsaMJgPaoI/AAAAAAAAH08/NxPKw1dbD1IcVsM4PdE8Sw6HK_ksYuG4QCLcBGAsYHQ/s1974/Screen%2BShot%2B2021-06-29%2Bat%2B9.00.11%2BAM.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;474&quot; data-original-width=&quot;1974&quot; height=&quot;640&quot; src=&quot;https://1.bp.blogspot.com/-dMg144dMPXA/YNsaMJgPaoI/AAAAAAAAH08/NxPKw1dbD1IcVsM4PdE8Sw6HK_ksYuG4QCLcBGAsYHQ/w640-h640/Screen%2BShot%2B2021-06-29%2Bat%2B9.00.11%2BAM.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Supported genres include arcade, action/adventure, and racing games.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;&lt;b&gt;The Right Tool for the Right Job&lt;/b&gt;&lt;br /&gt;The most elemental form of &lt;a href=&quot;https://en.wikipedia.org/wiki/Game_testing&quot;&gt;video game testing&lt;/a&gt; is to simply play the game. A lot. Many of the most serious bugs (such as crashes or falling out of the world) are easy to detect and fix; the challenge is finding them within the vast state space of a modern game. As such, we decided to focus on training a system that could “just play the game” at scale. &lt;/p&gt;&lt;p&gt;We found that the most effective way to do this was not to try to train a single, super-effective agent that could play the entire game from end-to-end, but to provide developers with the ability to train an ensemble of game-testing agents, each of which could effectively accomplish tasks of a few minutes each, which game developers refer to as “gameplay loops”. &lt;/p&gt;      &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-qgidPnQTJBg/YNsajA8IGoI/AAAAAAAAH1E/-P26GB8K1ewoM3JK627vY4aI7Y3yj6OvwCLcBGAsYHQ/s1500/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1024&quot; data-original-width=&quot;1500&quot; height=&quot;436&quot; src=&quot;https://1.bp.blogspot.com/-qgidPnQTJBg/YNsajA8IGoI/AAAAAAAAH1E/-P26GB8K1ewoM3JK627vY4aI7Y3yj6OvwCLcBGAsYHQ/w640-h436/image5.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;   These core gameplay behaviors are often expensive to program through traditional means, but are much more efficient to train than a single end-to-end ML model. In practice, commercial games create longer loops by repeating and remixing core gameplay loops, which means that developers can test large stretches of gameplay by combining ML policies with a small amount of simple scripting. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Simulation-centric, Semantic API&lt;/b&gt;&lt;br /&gt;One of the most fundamental challenges in applying ML to game development is bridging the chasm between the simulation-centric world of video games and the data-centric world of ML. Rather than ask developers to directly convert the game state into custom, low-level ML features (which would be too labor intensive) or attempting to learn from raw pixels (which would require too much data to train), our system provides developers with an idiomatic, game-developer friendly API that allows them to describe their game in terms of the essential state that a player observes and the semantic actions they can perform. All of this information is expressed via concepts that are familiar to game developers, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Entity_component_system&quot;&gt;entities&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Ray_casting#Concept&quot;&gt;raycasts&lt;/a&gt;, 3D positions and rotations, buttons and joysticks.  &lt;/p&gt;&lt;p&gt;As you can see in the example below, the API allows the specification of observations and actions in just a few lines of code. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-XUGqrMT_HT8/YNsa3lQ3WQI/AAAAAAAAH1M/tUIkskRHXaAEMD4j0tyIKjpFV-6VljrjQCLcBGAsYHQ/s1792/Screen%2BShot%2B2021-06-29%2Bat%2B9.00.47%2BAM.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1006&quot; data-original-width=&quot;1792&quot; height=&quot;360&quot; src=&quot;https://1.bp.blogspot.com/-XUGqrMT_HT8/YNsa3lQ3WQI/AAAAAAAAH1M/tUIkskRHXaAEMD4j0tyIKjpFV-6VljrjQCLcBGAsYHQ/w640-h360/Screen%2BShot%2B2021-06-29%2Bat%2B9.00.47%2BAM.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Example actions and observations for a racing game.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;&lt;b&gt;From API to Neural Network&lt;/b&gt;&lt;br /&gt;This high level, semantic API is not just easy to use, but also allows the system to flexibly adapt to the specific game being developed — the specific combination of API building blocks employed by the game developer informs our choice of network architecture, since it provides information about the type of gaming scenario in which the system is deployed. Some examples of this include: handling action outputs differently depending on whether they represent a digital button or analog joystick, or using &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;&gt;techniques from image processing&lt;/a&gt; to handle observations that result from an agent probing its environment with raycasts (similar to how autonomous vehicles probe their environment with &lt;a href=&quot;https://en.wikipedia.org/wiki/Lidar&quot;&gt;LIDAR&lt;/a&gt;).&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Our API is sufficiently general to allow modeling of many common control-schemes (the configuration of action outputs that control movement) in games, such as first-person games, third-person games with camera-relative controls, racing games, twin stick shooters, etc. Since 3D movement and aiming are often an integral aspect of gameplay in general, we create networks that automatically tend towards simple behaviors such as aiming, approach or avoidance in these games. The system accomplishes this by analyzing the game’s control scheme to create neural network layers that perform custom processing of observations and actions in that game. For example, positions and rotations of objects in the world are automatically translated into directions and distances from the point of view of the AI-controlled game entity. This transformation typically increases the speed of learning and helps the learned network generalize better. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-GrV5ey1Djtk/YNzngDvmC9I/AAAAAAAAH1g/M7a2ky1OzXAFsiqU9aUadMBhKlMv6mg1wCLcBGAsYHQ/s525/neural%2Bnetwork%2Bnot%2Btransparent.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;247&quot; data-original-width=&quot;525&quot; height=&quot;302&quot; src=&quot;https://1.bp.blogspot.com/-GrV5ey1Djtk/YNzngDvmC9I/AAAAAAAAH1g/M7a2ky1OzXAFsiqU9aUadMBhKlMv6mg1wCLcBGAsYHQ/w640-h302/neural%2Bnetwork%2Bnot%2Btransparent.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;An example neural network generated for a game with joystick controls and raycast inputs. Depending on the inputs (&lt;b&gt;red&lt;/b&gt;) and the control scheme, the system generates custom pre- and post-processing layers (&lt;b&gt;orange&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Learning From The Experts in Real Time&lt;/b&gt;&lt;br /&gt;After generating a neural network architecture, the network needs to be trained to play the game using an appropriate choice of learning algorithm. &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning#:~:text=Reinforcement%20learning%20(RL)%20is%20an,supervised%20learning%20and%20unsupervised%20learning.&quot;&gt;Reinforcement learning&lt;/a&gt; (RL), in which an ML policy is trained directly to maximize a reward, may seem like the obvious choice since they have been successfully used to train &lt;a href=&quot;https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii&quot;&gt;highly competent ML policies&lt;/a&gt; for games. However, RL algorithms tend to require more data than a single game instance can produce in a reasonable amount of time, and achieving good results in a new domain often requires &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;&gt;hyperparameter tuning&lt;/a&gt; and strong ML domain knowledge. &lt;/p&gt;&lt;p&gt;Instead, we found that &lt;a href=&quot;https://arxiv.org/abs/1811.06711&quot;&gt;imitation learning&lt;/a&gt; (IL), which trains ML policies based  by observing experts play the game, works well for our use case. Unlike RL, where the agent needs to discover a good policy on its own, IL only needs to recreate the behavior of a human expert. Since game developers and testers are experts in their own games, they can easily provide demonstrations of how to play the game. &lt;/p&gt;&lt;p&gt;We use an IL approach inspired by the &lt;a href=&quot;https://arxiv.org/abs/1011.0686&quot;&gt;DAgger algorithm&lt;/a&gt;, which allows us to take advantage of video games’ most compelling quality — interactivity. Thanks to the reductions in training time and data requirements enabled by our semantic API, training is effectively realtime, giving a developer the ability to fluidly switch between providing gameplay demonstrations and watching the system play. This results in a natural feedback loop, in which a developer iteratively provides corrections to a continuous stream of ML policies. &lt;/p&gt;&lt;p&gt;From the developer’s perspective, providing a demonstration or a correction to faulty behavior is as simple as picking up the controller and starting to play the game. Once they are done, they can put the controller down and watch the ML policy play. The result is a training experience that is real-time, interactive, highly experiential, and, very often, more than a little fun.  &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-p3BAq08zio0/YNsbP2z9YwI/AAAAAAAAH1Y/zFdCODpDUewyr2VjAAAwxqB2r8jspqKUgCLcBGAsYHQ/s674/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;674&quot; data-original-width=&quot;674&quot; height=&quot;400&quot; src=&quot;https://1.bp.blogspot.com/-p3BAq08zio0/YNsbP2z9YwI/AAAAAAAAH1Y/zFdCODpDUewyr2VjAAAwxqB2r8jspqKUgCLcBGAsYHQ/w400-h400/image4.gif&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;ML policy for an FPS game, trained with our system.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;We present a system that combines a high-level semantic API with a &lt;a href=&quot;https://arxiv.org/abs/1011.0686&quot;&gt;DAgger&lt;/a&gt;-inspired interactive training flow that enables training of useful ML policies for video game testing in a wide variety of genres.  We have released an &lt;a href=&quot;https://github.com/google-research/falken&quot;&gt;open source library&lt;/a&gt; as a functional illustration of our system. No ML expertise is required and training of agents for test applications often takes less than an hour on a single developer machine. We hope that this work will help inspire the development of ML techniques that can be deployed in real-world game-development flows in ways that are accessible, effective, and fun to use.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We’d like to thank the core members of the project: Dexter Allen, Leopold Haller, Nathan Martz, Hernan Moraldo, Stewart Miles and Hina Sakazaki. Training algorithms are provided by &lt;a href=&quot;https://www.tensorflow.org/agents&quot;&gt;TF Agents&lt;/a&gt;, and on-device inference by &lt;a href=&quot;https://www.tensorflow.org/lite&quot;&gt;TF Lite&lt;/a&gt;. Special thanks to our research advisors, Olivier Bachem, Erik Frey, and Toby Pohlen, and to Eugene Brevdo, Jared Duke, Oscar Ramirez and Neal Wu who provided helpful guidance and support.&lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=NH30BgHCkCw:v5g299ej8TQ:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/NH30BgHCkCw&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/756990546465936835/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/quickly-training-game-playing-agents.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/756990546465936835"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/756990546465936835"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/NH30BgHCkCw/quickly-training-game-playing-agents.html" title="Quickly Training Game-Playing Agents with Machine Learning"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-dMg144dMPXA/YNsaMJgPaoI/AAAAAAAAH08/NxPKw1dbD1IcVsM4PdE8Sw6HK_ksYuG4QCLcBGAsYHQ/s72-w640-h640-c/Screen%2BShot%2B2021-06-29%2Bat%2B9.00.11%2BAM.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/06/quickly-training-game-playing-agents.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-5159347988023824086</id><published>2021-06-28T10:15:00.001-07:00</published><updated>2021-08-27T13:16:51.112-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computational Imaging"/><category scheme="http://www.blogger.com/atom/ns#" term="Computational Photography"/><title type="text">Take All Your Pictures to the Cleaners, with Google Photos Noise and Blur Reduction</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Mauricio Delbracio, Research Scientist and Sungjoon Choi, Software Engineer, Google Research&lt;/span&gt; &lt;p&gt;Despite recent leaps in imaging technology, &lt;a href=&quot;https://arxiv.org/abs/2102.09000&quot;&gt;especially on mobile devices&lt;/a&gt;, image noise and limited sharpness remain two of the most important levers for improving the visual quality of a photograph. These are particularly relevant when taking pictures in poor light conditions, where cameras may compensate by increasing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Film_speed#Digital_camera_ISO_speed_and_exposure_index&quot;&gt;ISO&lt;/a&gt; or slowing the shutter speed, thereby exacerbating the presence of noise and, at times, increasing image blur. Noise can be associated with the particle nature of light (&lt;a href=&quot;https://en.wikipedia.org/wiki/Shot_noise&quot;&gt;shot noise&lt;/a&gt;) or be introduced by electronic components during the readout process (&lt;a href=&quot;https://en.wikipedia.org/wiki/Image_noise#Read_noise&quot;&gt;read noise&lt;/a&gt;). The captured noisy signal is then processed by the camera &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_processor&quot;&gt;image processor&lt;/a&gt; (ISP) and later may be further enhanced, amplified, or distorted by a photographic editing process. Image blur can be caused by a wide variety of phenomena, from inadvertent camera shake during capture, an incorrect setting of the camera’s focus (automatic or not), or due to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Airy_disk&quot;&gt;finite lens aperture&lt;/a&gt;, sensor resolution or the camera’s image processing.  &lt;/p&gt;&lt;p&gt;It is far easier to minimize the effects of noise and blur within a camera pipeline, where details of  the sensor, optical hardware and software blocks are understood. However, when presented with an image produced from an arbitrary (possibly unknown) camera, improving noise and sharpness becomes much more challenging due to the lack of detailed knowledge and access to the internal parameters of the camera. In most situations, these two problems are intrinsically related: noise reduction tends to eliminate fine structures along with unwanted details, while blur reduction seeks to boost structures and fine details. This interconnectedness increases the difficulty of developing image enhancement techniques that are computationally efficient to run on mobile devices. &lt;/p&gt;&lt;p&gt;Today, we present a new approach for &lt;em&gt;camera-agnostic&lt;/em&gt; estimation and elimination of noise and blur that can improve the quality of most images.  We developed a &lt;a href=&quot;https://ieeexplore.ieee.org/document/7532702&quot;&gt;pull-push denoising&lt;/a&gt; algorithm that is paired with a deblurring method, called &lt;a href=&quot;https://ieeexplore.ieee.org/document/9502555&quot;&gt;polyblur&lt;/a&gt;. Both of these components are designed to maximize computational efficiency, so users can successfully enhance the quality of a multi-megapixel image in milliseconds on a mobile device. These noise and blur reduction strategies are critical components of the recent &lt;a href=&quot;https://blog.google/products/photos/new-helpful-editor/&quot;&gt;Google Photos editor&lt;/a&gt; updates, which includes “&lt;em&gt;Denoise&lt;/em&gt;” and “&lt;em&gt;Sharpen&lt;/em&gt;” tools that enable users to enhance images that may have been captured under less than ideal conditions, or with older devices that may have had more noisy sensors or less sharp optics.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-bblz9X-bJro/YNOlX2r8KtI/AAAAAAAAHxI/6HhG4EJDPIokKzB6DUtQh5pvDZLVIUgqwCLcBGAsYHQ/s1520/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1520&quot; data-original-width=&quot;720&quot; height=&quot;640&quot; src=&quot;https://1.bp.blogspot.com/-bblz9X-bJro/YNOlX2r8KtI/AAAAAAAAHxI/6HhG4EJDPIokKzB6DUtQh5pvDZLVIUgqwCLcBGAsYHQ/w304-h640/image3.gif&quot; width=&quot;304&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A demonstration of the “Denoise” and “Sharpen” tools now available in the Google Photos editor.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;How Noisy is An Image? &lt;/b&gt;&lt;br /&gt;In order to accurately process a photographic image and successfully reduce the unwanted effects of noise and blur, it is vitally important to first characterize the types and levels of noise and blur found in the image. So, a camera-agnostic approach for noise reduction begins by formulating  a method to gauge the strength of noise at the pixel level from any given image, regardless of the device that created it. The noise level is modeled as a function of the brightness of the underlying pixel. That is, for each possible brightness level, the model estimates a corresponding noise level in a manner agnostic to either the actual source of the noise or the processing pipeline.  &lt;/p&gt;&lt;p&gt;To estimate this brightness-based noise level, we sample a number of small patches across the image and measure the noise level within each patch, after roughly removing any underlying structure in the image. This process is repeated at multiple scales, making it robust to artifacts that may arise from compression, image resizing, or other non-linear camera processing operations. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-6TDvzbLVI5o/YNOlghgNykI/AAAAAAAAHxM/qBnd7byMgRgPHY_xxM4PzwRzpmExTYRUACLcBGAsYHQ/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;785&quot; data-original-width=&quot;1999&quot; height=&quot;252&quot; src=&quot;https://1.bp.blogspot.com/-6TDvzbLVI5o/YNOlghgNykI/AAAAAAAAHxM/qBnd7byMgRgPHY_xxM4PzwRzpmExTYRUACLcBGAsYHQ/w640-h252/image1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The two segments on the left illustrate signal-dependent noise present in the input image (&lt;b&gt;center&lt;/b&gt;). The noise is more prominent in the bottom, darker crop and is unrelated to the underlying structure, but rather to the light level. Such image segments are sampled and processed to generate the spatially-varying noise map (&lt;b&gt;right&lt;/b&gt;) where red indicates more noise is present.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Reducing Noise Selectively with a Pull-Push Method&lt;/b&gt;&lt;br /&gt;We take advantage of self-similarity of patches across the image to denoise with high fidelity. The general principle behind such so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-local_means&quot;&gt;“non-local” denoising&lt;/a&gt; is that noisy pixels can be denoised by averaging pixels with similar local structure. However, these approaches typically incur high computational costs because they require a brute force search for pixels with similar local structure, making them impractical for on-device use. In our “pull-push” approach&lt;sup id=&quot;fnref1&quot;&gt;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;1&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;, the algorithmic complexity is decoupled from the size of filter footprints thanks to effective information propagation across spatial scales. &lt;/p&gt;&lt;p&gt;The first step in pull-push is to build an image pyramid (i.e., multiscale representation) in which each successive level is generated recursively by a “pull” filter (analogous to &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_scaling&quot;&gt;downsampling&lt;/a&gt;). This filter uses a per-pixel weighting scheme to &lt;em&gt;selectively&lt;/em&gt; combine existing noisy pixels together based on their patch similarities and estimated noise, thus reducing the noise at each successive, “coarser” level. Pixels at coarser levels (i.e., with lower resolution) pull and aggregate only compatible pixels from higher resolution, “finer” levels. In addition to this, each merged pixel in the coarser layers also includes an estimated reliability measure computed from the similarity weights used to generate it. Thus, merged pixels provide a simple per-pixel, per-level characterization of the image and its local statistics. By efficiently propagating this information through each level (i.e., each spatial scale), we are able to track a model of the neighborhood statistics for increasingly larger regions in a multiscale manner. &lt;/p&gt;&lt;p&gt;After the pull stage is evaluated to the coarsest level, the “push” stage fuses the results, starting from the coarsest level and generating finer levels iteratively. At a given scale, the push stage generates “filtered” pixels following a process similar to that of the pull stage, but going from coarse to finer levels. The pixels at each level are fused with those of coarser levels by doing a weighted average of same-level pixels along with coarser-level filtered pixels using the respective reliability weights. This enables us to reduce pixel noise while preserving local structure, because only average reliable information is included. This selective filtering and reliability (i.e. information) multiscale propagation is what makes push-pull different from existing frameworks. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-TD5fmjQFQyA/YNOlmsR7WPI/AAAAAAAAHxQ/ocx9gqSi4DM5Bhd5F0kYUyg_KoKtw8zfACLcBGAsYHQ/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1227&quot; data-original-width=&quot;1999&quot; height=&quot;392&quot; src=&quot;https://1.bp.blogspot.com/-TD5fmjQFQyA/YNOlmsR7WPI/AAAAAAAAHxQ/ocx9gqSi4DM5Bhd5F0kYUyg_KoKtw8zfACLcBGAsYHQ/w640-h392/image6.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;This series of images shows how filtering progresses through the pull-push process. Coarser level pixels pull and aggregate only compatible pixels from finer levels, as opposed to the traditional multiscale approaches using a fixed (non-data dependent) kernel. Notice how the noise is reduced throughout the stages.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The pull-push approach has a low computational cost, because the algorithm to selectively filter similar pixels over a very large neighborhood has a complexity that is only linear with the number of image pixels. In practice, the quality of this denoising approach is comparable to traditional non-local methods with much larger kernel footprints, but operates at a fraction of the computational cost. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-BnzI4F32xGk/YNOlstli8gI/AAAAAAAAHxU/o1nIYzP05RgWID9v4LwfMXpzXBq-gBW1wCLcBGAsYHQ/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1250&quot; data-original-width=&quot;1999&quot; height=&quot;400&quot; src=&quot;https://1.bp.blogspot.com/-BnzI4F32xGk/YNOlstli8gI/AAAAAAAAHxU/o1nIYzP05RgWID9v4LwfMXpzXBq-gBW1wCLcBGAsYHQ/w640-h400/image5.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-XBJuHvtX1Gg/YNOlwM4KmuI/AAAAAAAAHxY/k6WT7xZZ-_Qm54uGRgNwETW4VpBM71GhQCLcBGAsYHQ/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1250&quot; data-original-width=&quot;1999&quot; height=&quot;400&quot; src=&quot;https://1.bp.blogspot.com/-XBJuHvtX1Gg/YNOlwM4KmuI/AAAAAAAAHxY/k6WT7xZZ-_Qm54uGRgNwETW4VpBM71GhQCLcBGAsYHQ/w640-h400/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Image enhanced using the pull-push denoising method.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;How Blurry Is an Image?&lt;/b&gt;&lt;br /&gt;An image with poor sharpness can be thought of as being a more pristine latent image that was operated on by a blur kernel. So, if one can identify the blur kernel, it can be used to reduce the effect. This is referred to as “deblurring”, i.e., the removal or reduction of an undesired blur effect induced by a particular kernel on a particular image. In contrast, “sharpening”  refers to applying a sharpening filter, built from scratch and without reference to any particular image or blur kernel. Typical sharpening filters are also, in general, local operations that do not take account of any other information from other parts of the image, whereas deblurring algorithms estimate the blur from the whole image. Unlike arbitrary sharpening, which can result in worse image quality when applied to an image that is already sharp, deblurring a sharp image with a blur kernel accurately estimated from the image itself will have very little effect. &lt;/p&gt;&lt;p&gt;We specifically target relatively mild blur, as this scenario is more technically tractable, more computationally efficient, and produces consistent results. We model the blur kernel as an anisotropic (elliptical) Gaussian kernel, specified by three parameters that control the strength, direction and aspect ratio of the blur.   &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-NdigDKPMQqg/YNOl3zapNNI/AAAAAAAAHxk/nJrFTISkd3cqPsPh1BSQjWEy3XlIpHxBgCLcBGAsYHQ/s973/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;538&quot; data-original-width=&quot;973&quot; height=&quot;354&quot; src=&quot;https://1.bp.blogspot.com/-NdigDKPMQqg/YNOl3zapNNI/AAAAAAAAHxk/nJrFTISkd3cqPsPh1BSQjWEy3XlIpHxBgCLcBGAsYHQ/w640-h354/image9.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Gaussian blur model and example blur kernels. Each row of the plot on the right represents possible combinations of &lt;em&gt;σ&lt;sub&gt;0&lt;/sub&gt;&lt;/em&gt;, &lt;em&gt;ρ&lt;/em&gt; and &lt;em&gt;θ&lt;/em&gt;. We show three different &lt;em&gt;σ&lt;sub&gt;0&lt;/sub&gt;&lt;/em&gt; values with three different &lt;em&gt;ρ&lt;/em&gt; values for each.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Computing and removing blur without noticeable delay for the user requires an algorithm that is much more computationally efficient than existing approaches, which typically cannot be executed on a mobile device. We rely on an intriguing empirical observation: the maximal value of the image gradient across all directions at any point in a sharp image follows a particular distribution. Finding the maximum gradient value is efficient, and can yield a reliable estimate of the strength of the blur in the given direction. With this information in hand, we can directly recover the parameters that characterize the blur.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Polyblur: Removing Blur by Re-blurring&lt;/b&gt;&lt;br /&gt;To recover the sharp image given the estimated blur, we would (in theory) need to solve a numerically unstable inverse problem (i.e., deblurring). The inversion problem grows exponentially more unstable with the strength of the blur. As such, we target the case of mild blur removal. That is, we &lt;em&gt;assume&lt;/em&gt; that the image at hand is not so blurry as to be beyond practical repair. This enables a more practical approach — by carefully combining different re-applications of an operator we can approximate its inverse. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-uq2YWo1rPnw/YNOl_OctgLI/AAAAAAAAHxs/_6Q3GKI47EcpFHonzjCOLBKzcIGEeSEeQCLcBGAsYHQ/s1999/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; height=&quot;360&quot; src=&quot;https://1.bp.blogspot.com/-uq2YWo1rPnw/YNOl_OctgLI/AAAAAAAAHxs/_6Q3GKI47EcpFHonzjCOLBKzcIGEeSEeQCLcBGAsYHQ/w640-h360/image8.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-UMJT4p3BfZo/YNOmD9BxfCI/AAAAAAAAHx0/jUQ3YWbmniUYV-m9lMWvLbZ9KwO_MBsqgCLcBGAsYHQ/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;888&quot; data-original-width=&quot;1999&quot; height=&quot;284&quot; src=&quot;https://1.bp.blogspot.com/-UMJT4p3BfZo/YNOmD9BxfCI/AAAAAAAAHx0/jUQ3YWbmniUYV-m9lMWvLbZ9KwO_MBsqgCLcBGAsYHQ/w640-h284/image7.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Mild blur, as shown in these examples, can be effectively removed by combining multiple applications of the estimated blur.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This means, rather counterintuitively, that we can &lt;em&gt;deblur&lt;/em&gt; an image by &lt;em&gt;re-blurring&lt;/em&gt; it several times with the estimated blur kernel. Each application of the (estimated) blur corresponds to a first order polynomial, and the repeated applications (adding or subtracting) correspond to higher order terms in a polynomial. A key aspect of this approach, which we call &lt;a href=&quot;https://ieeexplore.ieee.org/document/9502555&quot;&gt;polyblur&lt;/a&gt;, is that it is very fast, because it only requires a few applications of the blur itself. This allows it to operate on megapixel images in a fraction of a second on a typical mobile device. The degree of the polynomial and its coefficients are set to invert the blur without boosting noise and other unwanted artifacts.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-YCtQk62teYY/YNOmJzVVlnI/AAAAAAAAHx8/69Vo-SFE2GEZvzLdz8fb9EcMUxvt7hpeQCLcBGAsYHQ/s788/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;266&quot; data-original-width=&quot;788&quot; height=&quot;216&quot; src=&quot;https://1.bp.blogspot.com/-YCtQk62teYY/YNOmJzVVlnI/AAAAAAAAHx8/69Vo-SFE2GEZvzLdz8fb9EcMUxvt7hpeQCLcBGAsYHQ/w640-h216/image4.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The deblurred image is generated by adding and subtracting multiple re-applications of the estimated blur (polyblur).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Integration with Google Photos&lt;/b&gt;&lt;br /&gt;The innovations described here have been integrated and made available to users in the Google Photos image editor in two new adjustment sliders called “Denoise” and “Sharpen”.  These features allow users to improve the quality of everyday images, from any capture device. The features often complement each other, allowing both denoising to reduce unwanted artifacts, and sharpening to bring clarity to the image subjects. Try using this pair of  tools in tandem in your images for best results. To learn more about the details of the work described here, check out our papers on &lt;a href=&quot;https://ieeexplore.ieee.org/document/9502555&quot;&gt;polyblur&lt;/a&gt; and &lt;a href=&quot;https://ieeexplore.ieee.org/document/7532702&quot;&gt;pull-push denoising&lt;/a&gt;. To see some examples of the effect of our denoising and sharpening up close, have a look at the images in this &lt;a href=&quot;https://photos.app.goo.gl/eV2HhMAXNGpS62ZYA&quot;&gt;album&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;The authors gratefully acknowledge the contributions of Ignacio Garcia-Dorado, Ryan Campbell, Damien Kelly, Peyman Milanfar, and John Isidoro. We are also thankful for support and feedback from Navin Sarma, Zachary Senzer, Brandon Ruffin, and Michael Milne.&lt;/em&gt;&lt;/p&gt; &lt;!--Footnotes--&gt;&lt;hr width=&quot;80%&quot; /&gt;&lt;p&gt;  &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;&gt;&lt;sup&gt;&lt;a name=&quot;fn1&quot;&gt;&lt;b&gt;1&lt;/b&gt;&lt;/a&gt;&lt;/sup&gt; The original &lt;a href=&quot;https://cseweb.ucsd.edu/~ravir/6160/papers/p43-gortler.pdf&quot;&gt;pull-push algorithm&lt;/a&gt; was developed as an efficient scattered data interpolation method to estimate and fill in the missing pixels in an image where only a subset of the pixels are specified. Here, we extend its methodology and present a data-dependent multiscale algorithm for denoising images efficiently.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;&gt;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=YzrGZK_H1dE:kz2iSuHd3gs:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/YzrGZK_H1dE&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/5159347988023824086/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/take-all-your-pictures-to-cleaners-with.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5159347988023824086"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5159347988023824086"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/YzrGZK_H1dE/take-all-your-pictures-to-cleaners-with.html" title="Take All Your Pictures to the Cleaners, with Google Photos Noise and Blur Reduction"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-bblz9X-bJro/YNOlX2r8KtI/AAAAAAAAHxI/6HhG4EJDPIokKzB6DUtQh5pvDZLVIUgqwCLcBGAsYHQ/s72-w304-h640-c/image3.gif" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/06/take-all-your-pictures-to-cleaners-with.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-6206828479795872652</id><published>2021-06-25T09:56:00.000-07:00</published><updated>2021-06-25T09:56:59.720-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Quantum AI"/><category scheme="http://www.blogger.com/atom/ns#" term="Quantum Computing"/><title type="text">Achieving Precision in Quantum Material Simulations</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Charles Neill and Zhang Jiang, Senior Research Scientists, Google Quantum AI&lt;/span&gt; &lt;p&gt;In fall of 2019, &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;&gt;we demonstrated&lt;/a&gt; that the Sycamore quantum processor could &lt;a href=&quot;https://www.nature.com/articles/s41586-019-1666-5&quot;&gt;outperform the most powerful classical computers&lt;/a&gt; when applied to a tailor-made problem. The next challenge is to extend this result to solve practical problems&lt;strong&gt; &lt;/strong&gt;in materials science, chemistry and physics. But going beyond the capabilities of classical computers for these problems is challenging and will require new insights to achieve state-of-the-art accuracy. Generally, the difficulty in performing quantum simulations of such physical problems is rooted in the wave nature of quantum particles, where &lt;a href=&quot;https://ai.googleblog.com/2019/10/improving-quantum-computation-with.html&quot;&gt;deviations in the initial setup, interference from the environment, or small errors in the calculations&lt;/a&gt; can lead to large deviations in the computational result. &lt;/p&gt;&lt;p&gt;In two upcoming publications, we outline a blueprint for achieving record levels of precision for the task of simulating quantum materials. In the first work, we consider one-dimensional systems, like thin wires, and demonstrate how to accurately compute electronic properties, such as current and conductance. In the second work, we show how to map the &lt;a href=&quot;https://arxiv.org/abs/1505.02290&quot;&gt;Fermi-Hubbard model&lt;/a&gt;, which describes interacting electrons, to a quantum processor in order to simulate important physical properties. These works take a significant step towards realizing our long-term goal of simulating more complex systems with practical applications, like batteries and pharmaceuticals. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-nc5W1EQwsFg/YNOtzBn1ccI/AAAAAAAAHyE/GaAwaetZpeMExLp3wPkEFDNfgvS-HgVowCLcBGAsYHQ/s1999/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1331&quot; data-original-width=&quot;1999&quot; height=&quot;426&quot; src=&quot;https://1.bp.blogspot.com/-nc5W1EQwsFg/YNOtzBn1ccI/AAAAAAAAHyE/GaAwaetZpeMExLp3wPkEFDNfgvS-HgVowCLcBGAsYHQ/w640-h426/image3.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A bottom view of one of the quantum &lt;a href=&quot;https://en.wikipedia.org/wiki/Dilution_refrigerator&quot;&gt;dilution refrigerators&lt;/a&gt; during maintenance. During the operation, the microwave wires that are floating in this image are connected to the quantum processor, e.g., the Sycamore chip, bringing the temperature of the lowest stage to a few tens of milli-degrees above absolute zero temperature.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Computing Electronic Properties of Quantum Materials&lt;/b&gt;&lt;br /&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2012.00921&quot;&gt;Accurately computing electronic properties of a quantum ring&lt;/a&gt;”, to be published in &lt;em&gt;&lt;a href=&quot;https://www.nature.com/&quot;&gt;Nature&lt;/a&gt;&lt;/em&gt;, we show how to reconstruct key electronic properties of quantum materials. The focus of this work is on one-dimensional conductors, which we simulate by forming a loop out of 18 qubits on the Sycamore processor in order to mimic a very narrow wire. We illustrate the underlying physics through a series of simple text-book experiments, starting with a computation of the “&lt;a href=&quot;https://en.wikipedia.org/wiki/Electronic_band_structure&quot;&gt;band-structure&lt;/a&gt;” of this wire, which describes the relationship between the energy and momentum of electrons in the metal. Understanding such structure is a key step in computing electronic properties such as current and conductance. Despite being an 18-qubit algorithm consisting of over 1,400 logical operations, a significant computational task for near-term devices, we are able to achieve a total error as low as 1%.  &lt;/p&gt;&lt;p&gt;The key insight enabling this level of accuracy stems from robust properties of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform&quot;&gt;Fourier transform&lt;/a&gt;. The quantum signal that we measure oscillates in time with a small number of frequencies. Taking a Fourier transform of this signal reveals peaks at the oscillation frequencies (in this case, the energy of electrons in the wire). While experimental imperfections affect the height of the observed peaks (corresponding to the strength of the oscillation), the center frequencies are robust to these errors. On the other hand, the center frequencies are especially sensitive to the physical properties of the wire that we hope to study (e.g., revealing small disorders in the local electric field felt by the electrons). The essence of our work is that studying quantum signals in the Fourier domain enables robust protection against experimental errors while providing a sensitive probe of the underlying quantum system. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-OX4fa3f84xE/YNOuP1YmIaI/AAAAAAAAHyY/fwA6DH192zQ1RphxKayjHwSNKSjEQr9NgCLcBGAsYHQ/s1999/image1%2B%25288%2529.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;663&quot; data-original-width=&quot;1999&quot; height=&quot;212&quot; src=&quot;https://1.bp.blogspot.com/-OX4fa3f84xE/YNOuP1YmIaI/AAAAAAAAHyY/fwA6DH192zQ1RphxKayjHwSNKSjEQr9NgCLcBGAsYHQ/w640-h212/image1%2B%25288%2529.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;(Left)&lt;/b&gt; Schematic of the 54-qubit quantum processor, Sycamore. Qubits are shown as gray crosses and tunable couplers as blue squares. Eighteen of the qubits are isolated to form a ring. &lt;b&gt;(Middle)&lt;/b&gt; Fourier transform of the measured quantum signal. Peaks in the Fourier spectrum correspond to the energy of electrons in the ring. Each peak can be associated with a traveling wave that has fixed momentum. &lt;b&gt;(Right)&lt;/b&gt; The center frequency of each peak (corresponding to the energy of electrons in the wire) is plotted versus the peak index (corresponding to the momentum). The measured relationship between energy and momentum is referred to as the ‘band structure’ of the quantum wire and provides valuable information about electronic properties of the material, such as current and conductance.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Quantum Simulation of the Fermi-Hubbard Model&lt;/b&gt;&lt;br /&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2010.07965&quot;&gt;Observation of separated dynamics of charge and spin in the Fermi-Hubbard model&lt;/a&gt;”, we focus on the dynamics of interacting electrons. Interactions between particles give rise to novel phenomena such as &lt;a href=&quot;https://en.wikipedia.org/wiki/High-temperature_superconductivity&quot;&gt;high temperature superconductivity&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Spin%E2%80%93charge_separation&quot;&gt;spin-charge separation&lt;/a&gt;. The simplest model that captures this behavior is known as the &lt;a href=&quot;https://arxiv.org/abs/1505.02290&quot;&gt;Fermi-Hubbard model&lt;/a&gt;. In materials such as metals, the atomic nuclei form a crystalline lattice and electrons hop from lattice site to lattice site carrying electrical current. In order to accurately model these systems, it is necessary to include the repulsion that electrons feel when getting close to one another.  The Fermi-Hubbard model captures this physics with two simple parameters that describe the hopping rate (&lt;em&gt;J&lt;/em&gt;) and the repulsion strength (&lt;em&gt;U&lt;/em&gt;).  &lt;/p&gt;&lt;p&gt;We realize the dynamics of this model by mapping the two physical parameters to logical operations on the qubits of the processor. Using these operations, we simulate a state of the electrons where both the electron charge and spin densities are peaked near the center of the qubit array. As the system evolves, the charge and spin densities spread at different rates due to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Strongly_correlated_material&quot;&gt;strong correlations&lt;/a&gt; between electrons. Our results provide an intuitive picture of interacting electrons and serve as a benchmark for simulating quantum materials with superconducting qubits.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-D_PQaMZF3ys/YNOuVZf5ONI/AAAAAAAAHyc/-OBDaKvQNzQqI0LeAHZs4nO_ovrRJDZyQCLcBGAsYHQ/s1999/image2%2B%25285%2529.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;873&quot; data-original-width=&quot;1999&quot; height=&quot;280&quot; src=&quot;https://1.bp.blogspot.com/-D_PQaMZF3ys/YNOuVZf5ONI/AAAAAAAAHyc/-OBDaKvQNzQqI0LeAHZs4nO_ovrRJDZyQCLcBGAsYHQ/w640-h280/image2%2B%25285%2529.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;(Left top)&lt;/b&gt; Illustration of the one-dimensional Fermi-Hubbard model in a periodic potential. Electrons are shown in blue, with their spin indicated by the connected arrow. &lt;em&gt;J&lt;/em&gt;, the distance between troughs in the electric potential field, reflects the “hopping” rate, i.e., the rate at which electrons transition from one trough in the potential to another, and &lt;em&gt;U&lt;/em&gt;, the amplitude, represents the strength of repulsion between electrons. &lt;b&gt;(Left bottom)&lt;/b&gt; The simulation of the model on a qubit ladder, where each qubit (square) represents a fermionic state with spin-up or spin-down (arrows). &lt;b&gt;(Right)&lt;/b&gt; Time evolution of the model reveals separated spreading rates of charge and spin. Points and solid lines represent experimental and numerical exact results, respectively. At &lt;em&gt;t&lt;/em&gt; = 0, the charge and spin densities are peaked at the middle sites. At later times, the charge density spreads and reaches the boundaries faster than the spin density.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;Quantum processors hold the promise to solve computationally hard tasks beyond the capability of classical approaches. However, in order for these engineered platforms to be considered as serious contenders, they must offer computational accuracy beyond the current state-of-the-art classical methods. In our first experiment, we demonstrate an unprecedented level of accuracy in simulating simple materials, and in our second experiment, we show how to embed realistic models of interacting electrons into a quantum processor. It is our hope that these experimental results help progress the goal of moving beyond the classical computing horizon.  &lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=lFnNMAnC5TY:eI9r5xO35h0:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/lFnNMAnC5TY&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/6206828479795872652/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/achieving-precision-in-quantum-material.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6206828479795872652"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6206828479795872652"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/lFnNMAnC5TY/achieving-precision-in-quantum-material.html" title="Achieving Precision in Quantum Material Simulations"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-nc5W1EQwsFg/YNOtzBn1ccI/AAAAAAAAHyE/GaAwaetZpeMExLp3wPkEFDNfgvS-HgVowCLcBGAsYHQ/s72-w640-h426-c/image3.jpg" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/06/achieving-precision-in-quantum-material.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-819110334029306444</id><published>2021-06-24T12:55:00.015-07:00</published><updated>2021-06-24T15:51:42.509-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="datasets"/><category scheme="http://www.blogger.com/atom/ns#" term="Google Translate"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Translation"/><category scheme="http://www.blogger.com/atom/ns#" term="ML Fairness"/><title type="text">A Dataset for Studying Gender Bias in Translation</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Romina Stella, Product Manager, Google Translate&lt;/span&gt; &lt;p&gt;Advances on &lt;a href=&quot;https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html&quot;&gt;neural machine translation&lt;/a&gt; (NMT) have enabled more natural and fluid translations, but they still can reflect the societal biases and stereotypes of the data on which they're trained. As such, it is an ongoing goal at Google to develop &lt;a href=&quot;https://ai.googleblog.com/2018/12/providing-gender-specific-translations.html&quot;&gt;innovative techniques&lt;/a&gt; to &lt;a href=&quot;https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html&quot;&gt;reduce gender bias&lt;/a&gt; in machine translation, in alignment with our &lt;a href=&quot;https://ai.google/principles/&quot;&gt;AI Principles&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;One research area has been using context from surrounding sentences or passages to improve gender accuracy. This is a challenge because traditional NMT methods translate sentences individually, but gendered information is not always explicitly stated in each individual sentence. For example, in the following passage in Spanish (a language where subjects aren’t always &lt;a href=&quot;https://en.wikipedia.org/wiki/Null-subject_language&quot;&gt;explicitly mentioned&lt;/a&gt;), the first sentence refers explicitly to Marie Curie as the subject, but the second one doesn't explicitly mention the subject. In isolation, this second sentence could refer to a person of any gender. When translating to English, however, a pronoun needs to be picked, and the information needed for an accurate translation is in the first sentence. &lt;/p&gt; &lt;table&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;&lt;em&gt;&lt;b&gt;Spanish Text&lt;/b&gt;&lt;/em&gt;&lt;/td&gt;      &lt;td&gt;&lt;em&gt;&lt;b&gt;Translation to English&lt;/b&gt;&lt;/em&gt;&lt;/td&gt;    &lt;/tr&gt;&lt;tr&gt;   &lt;td&gt;Marie Curie nació en Varsovia. Fue la primera persona en recibir dos premios Nobel en distintas especialidades.    &lt;/td&gt;      &lt;td&gt;Marie Curie was born in Warsaw. &lt;b&gt;She&lt;/b&gt; was the first person to receive two Nobel Prizes in different specialties.    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;Advancing translation techniques beyond single sentences requires new metrics for measuring progress and new datasets with the most common context-related errors. Adding to this challenge is the fact that translation errors related to gender (such as picking the correct pronoun or having gender agreement) are particularly sensitive, because they may directly refer to people and how they self identify. &lt;/p&gt;&lt;p&gt;To help facilitate progress against the common challenges on contextual translation (e.g., pronoun drop, gender agreement and accurate possessives), we are releasing the &lt;a href=&quot;https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/Readme.html&quot;&gt;Translated Wikipedia Biographies&lt;/a&gt;&lt;strong&gt; &lt;/strong&gt;dataset, which can be used to evaluate the gender bias of translation models.&lt;strong&gt; &lt;/strong&gt;Our intent with this release is to support long-term improvements on ML systems focused on pronouns and gender in translation by providing a benchmark in which translations’ accuracy can be measured pre- and post-model changes.&lt;strong&gt;  &lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;b&gt;A Source of Common Translation Errors &lt;/b&gt;&lt;br /&gt;Because they are well-written, geographically diverse, contain multiple sentences, and refer to subjects in the third person (and so contain plenty of pronouns), &lt;a href=&quot;https://en.wikipedia.org/wiki/Wikipedia:Biographies_of_living_persons&quot;&gt;Wikipedia biographies&lt;/a&gt; offer a high potential for common translation errors associated with gender. These often occur when articles refer to a person explicitly in early sentences of a paragraph, but there is no explicit mention of the person in later sentences. Some examples:  &lt;/p&gt;  &lt;table&gt;  &lt;tbody&gt;    &lt;tr&gt;&lt;td&gt;&lt;b&gt;&lt;em&gt;Translation Error&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;&lt;em&gt;Text&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;&lt;em&gt;Translation&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;    &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;b&gt;&lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Pro-drop_language&quot;&gt;Pro-drop&lt;/a&gt; in Spanish →  English&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;   &lt;td&gt;Marie Curie nació en Varsovia. Recibió el Premio Nobel en 1903 y en 1911.    &lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;Marie Curie&lt;/b&gt; was born in Warsaw. &lt;b&gt;He&lt;/b&gt; received the Nobel Prize in 1903 and in 1911.&lt;br /&gt;   &lt;/td&gt;  &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;        &lt;tr&gt;&lt;td&gt;&lt;b&gt;&lt;em&gt;Neutral &lt;a href=&quot;https://en.wikipedia.org/wiki/Possessive_determiner#Other_languages&quot;&gt;possessives&lt;/a&gt; in Spanish →  English&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;   &lt;td&gt;Marie Curie nació en Varsovia. Su carrera profesional fue desarrollada en Francia.    &lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;Marie Curie&lt;/b&gt;  was born in Warsaw. &lt;b&gt;His&lt;/b&gt; professional career was developed in France.&lt;br /&gt;   &lt;/td&gt;  &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;     &lt;tr&gt;&lt;td&gt;&lt;b&gt;&lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Grammatical_gender&quot;&gt;Gender agreement&lt;/a&gt; in English  → German&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;         &lt;td&gt;Marie Curie was born in Warsaw. The distinguished scientist received the Nobel Prize in 1903 and in 1911.    &lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;Marie Curie&lt;/b&gt; wurde in Varsovia geboren. &lt;b&gt;Der&lt;/b&gt; angesehene &lt;b&gt;Wissenschaftler&lt;/b&gt; erhielt 1903 und 1911 den Nobelpreis.&lt;br /&gt;   &lt;/td&gt;  &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;       &lt;tr&gt;&lt;td&gt;&lt;b&gt;&lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Grammatical_gender&quot;&gt;Gender agreement&lt;/a&gt; in English  →  Spanish  &lt;/em&gt;&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;       &lt;td&gt;Marie Curie was born in Warsaw. The distinguished scientist received the Nobel Prize in 1903 and in 1911.    &lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;         &lt;td&gt;&lt;b&gt;Marie Curie&lt;/b&gt; nació en Varsovia. &lt;b&gt;El distinguido científico&lt;/b&gt; recibió el Premio Nobel en 1903 y en 1911.    &lt;/td&gt;  &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Building the Dataset&lt;/b&gt;&lt;br /&gt;The &lt;a href=&quot;https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/Readme.html&quot;&gt;Translated Wikipedia Biographies&lt;/a&gt; dataset has been designed to analyze common gender errors in machine translation, such as those illustrated above. Each instance of the dataset represents a person (identified in the biographies as feminine or masculine), a rock band or a sports team (considered genderless).  Each instance is represented by a long text translation of 8 to 15 connected sentences referring to that central subject (the person, rock band, or sports team).  Articles are written in native English and have been professionally translated to Spanish and German. For Spanish, translations were optimized for pronoun-drop, so the same set could be used to analyze pro-drop (Spanish → English) and gender agreement (English → Spanish).  &lt;/p&gt;&lt;p&gt;The dataset was built by selecting a group of instances that has equal representation across geographies and genders. To do this, we extracted biographies from Wikipedia according to occupation, profession, job and/or activity. To ensure an unbiased selection of occupations, we chose nine occupations that represented a range of stereotypical gender associations (either feminine, masculine, or neither) based on Wikipedia statistics. Then, to mitigate any geography-based bias, we divided all these instances based on geographical diversity. For each occupation category, we looked to have one candidate per region (using regions from &lt;a href=&quot;https://www.census.gov/prod/2004pubs/wp-02.pdf&quot;&gt;census.gov&lt;/a&gt; as a proxy of geographical diversity). When an instance was associated with a region, we checked that the selected person had a relevant relationship with a country that belongs to a designated region (nationality, place of birth, lived for a big portion of their life, etc.). By using this criteria, the dataset contains entries about individuals from more than 90 countries and all regions of the world.  &lt;/p&gt;&lt;p&gt;Although gender is non-binary, we focused on having equal representation of “feminine” and “masculine” entities. It's worth mentioning that because the entities are represented as such on Wikipedia, the set doesn't include individuals that identify as non-binary,  as, unfortunately, there are not enough instances currently represented in Wikipedia to accurately reflect the non-binary community. To label each instance as &quot;feminine&quot; or &quot;masculine&quot; we relied on the biographical information from Wikipedia, which contained gender-specific references to the person (she, he, woman, son, father, etc.).&lt;/p&gt;&lt;p&gt;After applying all these filters, we randomly selected an instance for each occupation-region-gender triplet. For each occupation, there are two biographies (one masculine and one feminine), for each of the seven geographic regions. &lt;/p&gt;&lt;p&gt;Finally, we added 12 instances with no gender. We picked rock bands and sports teams because they are usually referred to by non-gendered third person pronouns (such as “it” or singular “they”). The purpose of including these instances is to study over triggering, i.e., when models learn that they are rewarded for producing gender-specific pronouns, leading them to produce these pronouns in cases where they shouldn't. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Results and Applications&lt;/b&gt;&lt;br /&gt;This dataset enables a new method of evaluation for gender bias reduction in machine translations (introduced in a &lt;a href=&quot;https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html&quot;&gt;previous post&lt;/a&gt;). Because each instance refers to a subject with a known gender, we can compute the accuracy of the gender-specific translations that refer to this subject. This computation is easier when translating into English (cases of languages with pro-drop or neutral pronouns) since computation is mainly based on  gender-specific pronouns in English. In these cases, the gender datasets have resulted in a 67% reduction in errors on context-aware models vs. previous models. As mentioned before, the neutral entities have allowed us to discover cases of over triggering like the usage of feminine or masculine pronouns to refer to genderless entities. This new dataset also enables new research directions into the performance of different models across types of occupations or geographic regions. &lt;/p&gt;&lt;p&gt;As an example, the dataset allowed us to discover the following improvements in an excerpt of the translated biography of Marie Curie from Spanish. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-6gpgZOiYors/YNS7uoth6oI/AAAAAAAAHyo/AgABelsRxJwEJnu1GYeYc3sAcKq0ewD6wCLcBGAsYHQ/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1273&quot; data-original-width=&quot;1999&quot; height=&quot;408&quot; src=&quot;https://1.bp.blogspot.com/-6gpgZOiYors/YNS7uoth6oI/AAAAAAAAHyo/AgABelsRxJwEJnu1GYeYc3sAcKq0ewD6wCLcBGAsYHQ/w640-h408/image1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Translation result with the previous NMT model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-gp3P_kKYfrg/YNS73FUz2fI/AAAAAAAAHys/NA90YKWiyig5rwVSlJzAttquDte1QTxzwCLcBGAsYHQ/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1273&quot; data-original-width=&quot;1999&quot; height=&quot;408&quot; src=&quot;https://1.bp.blogspot.com/-gp3P_kKYfrg/YNS73FUz2fI/AAAAAAAAHys/NA90YKWiyig5rwVSlJzAttquDte1QTxzwCLcBGAsYHQ/w640-h408/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Translation result with the new contextual model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;This &lt;a href=&quot;https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/Readme.html&quot;&gt;Translated Wikipedia Biographies&lt;/a&gt; dataset is the result of our own studies and work on identifying biases associated with gender and machine translation. This set focuses on a specific problem related to gender bias and doesn't aim to cover the whole problem. It's worth mentioning that by releasing this dataset, we don't aim to be prescriptive in determining what's the optimal approach to address gender bias. This contribution aims to foster progress on this challenge across the global research community.   &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;The datasets were built with help from Anja Austermann, Melvin Johnson, Michelle Linch, Mengmeng Niu, Mahima Pushkarna, Apu Shah, Romina Stella, and Kellie Webster.&lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=SqLDbUeZeYM:UOurCmUGih4:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/SqLDbUeZeYM&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/819110334029306444/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/a-dataset-for-studying-gender-bias-in.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/819110334029306444"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/819110334029306444"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/SqLDbUeZeYM/a-dataset-for-studying-gender-bias-in.html" title="A Dataset for Studying Gender Bias in Translation"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-6gpgZOiYors/YNS7uoth6oI/AAAAAAAAHyo/AgABelsRxJwEJnu1GYeYc3sAcKq0ewD6wCLcBGAsYHQ/s72-w640-h408-c/image1.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/06/a-dataset-for-studying-gender-bias-in.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-613959647323225034</id><published>2021-06-23T12:58:00.002-07:00</published><updated>2021-06-24T07:19:21.458-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="AI"/><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Health"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><title type="text">Improving Genomic Discovery with Machine Learning</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Andrew Carroll, Product Manager and Cory McLean, Software Engineer, Google Health&lt;/span&gt; &lt;p&gt;Each person’s genome, which collectively encodes the biochemical machinery they are born with, is composed of over 3 billion letters of DNA. However, only a small subset of the genome (~4-5 million &lt;a href=&quot;https://en.wikipedia.org/wiki/Locus_(genetics)&quot;&gt;positions&lt;/a&gt;) varies between two people. Nonetheless, each person’s unique genome interacts with the environment they experience to determine the &lt;a href=&quot;https://www.healthaffairs.org/doi/10.1377/hlthaff.21.2.78&quot;&gt;majority of their health outcomes&lt;/a&gt;. A key method of understanding the relationship between genetic variants and traits is a &lt;a href=&quot;https://www.genome.gov/genetics-glossary/Genome-Wide-Association-Studies&quot;&gt;genome-wide association study&lt;/a&gt; (GWAS), in which each genetic variant present in a cohort is individually examined for correlation with the trait of interest. GWAS results can be used to identify and prioritize potential therapeutic targets by identifying genes that are strongly associated with a disease of interest, and can also be used to build a &lt;a href=&quot;https://www.genome.gov/Health/Genomics-and-Medicine/Polygenic-risk-scores&quot;&gt;polygenic risk score&lt;/a&gt; (PRS) to predict disease predisposition based on the combined influence of variants present in an individual. However, while accurate measurement of traits in an individual (called phenotyping) is essential to GWAS,  it often requires painstaking expert curation and/or subjective judgment calls. &lt;/p&gt;&lt;p&gt;In “&lt;a href=&quot;https://www.cell.com/ajhg/fulltext/S0002-9297(21)00188-9&quot;&gt;Large-scale machine learning-based phenotyping significantly improves genomic discovery for optic nerve head morphology&lt;/a&gt;”, we demonstrate how using machine learning (ML) models to classify medical imaging data can be used to improve GWAS. We describe how models can be trained for &lt;a href=&quot;https://en.wikipedia.org/wiki/Phenotype&quot;&gt;phenotypes&lt;/a&gt; to generate trait predictions and how these predictions are used to identify novel genetic associations. We then show that the novel associations discovered improve PRS accuracy and, using glaucoma as an example, that the improvements for anatomical eye traits relate to human disease. We have released the model training code and detailed documentation for its use on our &lt;a href=&quot;https://github.com/Google-Health/genomics-research/tree/main/ml-based-vcdr&quot;&gt;Genomics Research GitHub repository&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Identifying genetic variants associated with eye anatomical traits&lt;/b&gt;&lt;br /&gt;Previous work has demonstrated that ML models can identify &lt;a href=&quot;https://ai.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html&quot;&gt;eye diseases&lt;/a&gt;, &lt;a href=&quot;https://blog.google/technology/health/ai-dermatology-preview-io-2021&quot;&gt;skin diseases&lt;/a&gt;, and &lt;a href=&quot;https://blog.google/technology/health/improving-breast-cancer-screening/&quot;&gt;abnormal mammogram results&lt;/a&gt; with accuracy approaching or exceeding state-of-the-art methods by domain experts. Because identifying disease is a subset of phenotyping, we reasoned that ML models could be broadly used to improve the speed and quality of phenotyping for GWAS. &lt;/p&gt;&lt;p&gt;To test this, we chose a model that uses a &lt;a href=&quot;https://en.wikipedia.org/wiki/Fundus_photography&quot;&gt;fundus image&lt;/a&gt; of the eye to accurately &lt;a href=&quot;https://www.aaojournal.org/article/S0161-6420(19)31875-5/fulltext&quot;&gt;predict whether a patient should be referred for assessment for glaucoma&lt;/a&gt;. This model uses the fundus images to predict the diameters of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Optic_disc&quot;&gt;optic disc&lt;/a&gt; (the region where the optic nerve connects to the retina) and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Optic_cup_(anatomical)&quot;&gt;optic cup&lt;/a&gt; (a whitish region in the center of the optic disc). The ratio of the diameters of these two anatomical features (called the vertical cup-to-disc ratio, or VCDR) correlates strongly with glaucoma risk.&lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-5lyl-oGlCtU/YNNk8-JpCEI/AAAAAAAAHww/Fgyb8zfkRncpkaRvpEGzh2W7Wwc1SGh4gCLcBGAsYHQ/s899/genomics%2Bfundus%2Bimage%2BVCDR.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;293&quot; data-original-width=&quot;899&quot; height=&quot;208&quot; src=&quot;https://1.bp.blogspot.com/-5lyl-oGlCtU/YNNk8-JpCEI/AAAAAAAAHww/Fgyb8zfkRncpkaRvpEGzh2W7Wwc1SGh4gCLcBGAsYHQ/w640-h208/genomics%2Bfundus%2Bimage%2BVCDR.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A representative retinal fundus image showing the vertical cup-to-disc ratio, which is an important diagnostic measurement for glaucoma.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;We applied this model to predict VCDR in all fundus images from individuals in the &lt;a href=&quot;https://www.ukbiobank.ac.uk/&quot;&gt;UK Biobank&lt;/a&gt;, which is the world’s largest dataset available to researchers worldwide for health-related research in the public interest, containing extensive phenotyping and genetic data for ~500,000 pseudonymized (the UK Biobank's standard for de-identification) individuals. We then performed GWAS in this dataset to identify genetic variants that are associated with the model-based predictions of VCDR. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-czEja2pkbL8/YNNjJZUCQsI/AAAAAAAAHwU/WqO4yDtk83ow_0IqycLUYps5Sf6-O0IRQCLcBGAsYHQ/s1880/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;964&quot; data-original-width=&quot;1880&quot; height=&quot;328&quot; src=&quot;https://1.bp.blogspot.com/-czEja2pkbL8/YNNjJZUCQsI/AAAAAAAAHwU/WqO4yDtk83ow_0IqycLUYps5Sf6-O0IRQCLcBGAsYHQ/w640-h328/image3.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Applying a VCDR prediction model trained on clinical data to generate predicted values for VCDR to enable discovery of genetic associations for the VCDR trait.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;The ML-based GWAS identified 156 distinct genomic regions associated with VCDR. We compared these results to a VCDR GWAS conducted by another group on the same UK Biobank data, &lt;a href=&quot;https://www.nature.com/articles/s41588-019-0556-y&quot;&gt;Craig et al. 2020&lt;/a&gt;, where experts had painstakingly labeled all images for VCDR. The ML-based GWAS replicates 62 of the 65 associations found in Craig &lt;em&gt;et al.&lt;/em&gt;, which indicates that the model accurately predicts VCDR in the UK Biobank images. Additionally, the ML-based GWAS discovered 93 novel associations. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-sZzLM8IZYcs/YNNlD-28SNI/AAAAAAAAHw0/VpXxgbDgTDkIMYX6307GvbwFOeGmVp0kgCLcBGAsYHQ/s403/genomics%2Bvenn.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;234&quot; data-original-width=&quot;403&quot; height=&quot;233&quot; src=&quot;https://1.bp.blogspot.com/-sZzLM8IZYcs/YNNlD-28SNI/AAAAAAAAHw0/VpXxgbDgTDkIMYX6307GvbwFOeGmVp0kgCLcBGAsYHQ/w400-h233/genomics%2Bvenn.jpg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Number of statistically significant GWAS associations discovered by exhaustive expert labeling approach (Craig &lt;em&gt;et al&lt;/em&gt;., left), and by our ML-based approach (right), with shared associations in the middle.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;The ML-based GWAS improves polygenic model predictions&lt;/b&gt;&lt;br /&gt;To validate that the novel associations discovered in the ML-based GWAS are biologically relevant, we developed independent PRSes using the Craig &lt;em&gt;et al.&lt;/em&gt; and ML-based GWAS results, and tested their ability to predict human-expert-labeled VCDR in a subset of UK Biobank as well as a fully independent cohort (&lt;a href=&quot;https://www.epic-norfolk.org.uk/&quot;&gt;EPIC-Norfolk&lt;/a&gt;). The PRS developed from the ML-based GWAS showed greater predictive ability than the PRS built from the expert labeling approach in both datasets, providing strong evidence that the novel associations discovered by the ML-based method influence VCDR biology, and suggesting that the improved phenotyping accuracy (i.e., more accurate VCDR measurement) of the model translates into a more powerful GWAS. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-UMiJ7ass7y8/YNNo7SboEgI/AAAAAAAAHxA/_JGvVES_fq0akw5KaHj7Ob0pOLDJBirVACLcBGAsYHQ/s938/image4.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;475&quot; data-original-width=&quot;938&quot; height=&quot;324&quot; src=&quot;https://1.bp.blogspot.com/-UMiJ7ass7y8/YNNo7SboEgI/AAAAAAAAHxA/_JGvVES_fq0akw5KaHj7Ob0pOLDJBirVACLcBGAsYHQ/w640-h324/image4.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The correlation between a polygenic risk score (PRS) for VCDR generated from the ML-based approach and the exhaustive expert labeling approach (Craig &lt;em&gt;et al.&lt;/em&gt;). In these plots, higher values on the y-axis indicate a greater correlation and therefore greater prediction from only the genetic data. [* — p ≤ 0.05; *** — p ≤ 0.001]&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;As a second validation, because we know that VCDR is strongly correlated with glaucoma, we also investigated whether the ML-based PRS was correlated with individuals who had either self-reported that they had glaucoma or had medical procedure codes suggestive of glaucoma or glaucoma treatment. We found that the PRS for VCDR determined using our model predictions were also predictive of the probability that an individual had indications of glaucoma. Individuals with a PRS 2.5 or more standard deviations higher than the mean were more than 3 times as likely to have glaucoma in this cohort. We also observed that the VCDR PRS from ML-based phenotypes was more predictive of glaucoma than the VCDR PRS produced from the extensive manual phenotyping.  &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-Qw9SCuYaOjY/YNNjJWGOICI/AAAAAAAAHwQ/e5RE3jUNBjU2lA4tD96eE7QNsQZuWPRXgCLcBGAsYHQ/s1704/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1704&quot; height=&quot;450&quot; src=&quot;https://1.bp.blogspot.com/-Qw9SCuYaOjY/YNNjJWGOICI/AAAAAAAAHwQ/e5RE3jUNBjU2lA4tD96eE7QNsQZuWPRXgCLcBGAsYHQ/w640-h450/image1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The odds ratio of glaucoma (self-report or ICD code) stratified by the PRS for VCDR determined using the ML-based phenotypes (in standard deviations from the mean). In this plot, the y-axis shows the probability that the individual has glaucoma relative to the baseline rate (represented by the dashed line). The x-axis shows standard deviations from the mean for the PRS. Data are visualized as a standard &lt;a href=&quot;https://en.wikipedia.org/wiki/Box_plot&quot;&gt;box plot&lt;/a&gt;, which illustrates values for the mean (the orange line), first and third quartiles, and minimum and maximum.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;We have shown that ML models can be used to quickly phenotype large cohorts for GWAS, and that these models can increase statistical power in such studies. Although these examples were shown for eye traits predicted from retinal imaging, we look forward to exploring how this concept could generally apply to other diseases and data types.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgments&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We would like to especially thank co-author &lt;a href=&quot;https://www.moorfields.nhs.uk/consultant/anthony-khawaja&quot;&gt;Dr. Anthony Khawaja&lt;/a&gt; of &lt;a href=&quot;https://www.moorfields.nhs.uk/&quot;&gt;Moorfields Eye Hospital&lt;/a&gt; for contributing his extensive medical expertise. We also recognize the  efforts of Professor Jamie Craig and colleagues for their exhaustive labeling of UK Biobank images, which allowed us to make comparisons with our method. Several authors of that work, as well as Professor Stuart MacGregor and collaborators in Australia and at Max Kelsen have &lt;a href=&quot;https://www.cell.com/ajhg/fulltext/S0002-9297(21)00189-0&quot;&gt;independently replicated these findings&lt;/a&gt;, and we value these scientific contributions as well. Last, this work summarizes the work of the following Google contributors, who we would like to thank: Babak Alipanahi, Farhad Hormozdiari, Babak Behsaz, Justin Cosentino, Zachary R. McCaw, Emanuel Schorsch, D. Sculley, Elizabeth H. Dorfman, Sonia Phene, Naama Hammel, Andrew Carroll, and Cory Y. McLean&lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=dUKd8nUp3vY:icK6m6lFdqs:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/dUKd8nUp3vY&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/613959647323225034/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/improving-genomic-discovery-with.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/613959647323225034"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/613959647323225034"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/dUKd8nUp3vY/improving-genomic-discovery-with.html" title="Improving Genomic Discovery with Machine Learning"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-5lyl-oGlCtU/YNNk8-JpCEI/AAAAAAAAHww/Fgyb8zfkRncpkaRvpEGzh2W7Wwc1SGh4gCLcBGAsYHQ/s72-w640-h208-c/genomics%2Bfundus%2Bimage%2BVCDR.jpg" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/06/improving-genomic-discovery-with.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-2976383705310409753</id><published>2021-06-22T13:33:00.000-07:00</published><updated>2021-06-22T13:33:53.451-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Quantum AI"/><category scheme="http://www.blogger.com/atom/ns#" term="Quantum Computing"/><title type="text">Quantum Machine Learning and the Power of Data</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Jarrod McClean, Staff Research Scientist and Hsin-Yuan (Robert) Huang&lt;sup id=&quot;fnref1&quot;&gt;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;1&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;, Intern, Google Quantum AI&lt;/span&gt; &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_computing&quot;&gt;Quantum computing&lt;/a&gt; has rapidly advanced in both theory and practice in recent years, and with it the hope for the potential impact in real applications. One key area of interest is how quantum computers might affect machine learning. We recently &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;&gt;demonstrated experimentally&lt;/a&gt; that quantum computers are able to naturally solve certain problems with complex correlations between inputs that can be incredibly hard for traditional, or “classical”, computers. This suggests that learning models made on quantum computers may be dramatically more powerful for select applications, potentially boasting faster computation, better generalization on less data, or both. Hence it is of great interest to understand in what situations such a “quantum advantage” might be achieved. &lt;/p&gt;&lt;p&gt;The idea of quantum advantage is typically phrased in terms of computational advantages. That is, given some task with well defined inputs and outputs, can a quantum computer achieve a more accurate result  than a  classical machine in a comparable runtime? There are a number of algorithms for which quantum computers are suspected to have overwhelming advantages, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Shor%27s_algorithm&quot;&gt;Shor’s factoring algorithm&lt;/a&gt; for factoring products of large primes (relevant to &lt;a href=&quot;https://en.wikipedia.org/wiki/RSA_(cryptosystem)&quot;&gt;RSA encryption&lt;/a&gt;) or the &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.654.7909&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;quantum simulation of quantum systems&lt;/a&gt;. However, the difficulty of solving a problem, and hence the potential advantage for a quantum computer, can be greatly impacted by the &lt;em&gt;availability of data&lt;/em&gt;. As such, understanding when a quantum computer can help in a machine learning task depends not only on the task, but also the data available, and a complete understanding of this must include both. &lt;/p&gt;&lt;p&gt;In “&lt;a href=&quot;https://www.nature.com/articles/s41467-021-22539-9&quot;&gt;Power of data in quantum machine learning&lt;/a&gt;”, published in &lt;em&gt;&lt;a href=&quot;https://www.nature.com/ncomms/&quot;&gt;Nature Communications&lt;/a&gt;&lt;/em&gt;, we dissect the problem of quantum advantage in machine learning to better understand when it will apply. We show how the complexity of a problem formally changes with the availability of data, and how this sometimes has the power to elevate classical learning models to be competitive with quantum algorithms. We then develop a practical method for screening when there may be a quantum advantage for a chosen set of data embeddings in the context of &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_method&quot;&gt;kernel methods&lt;/a&gt;. We use the insights from the screening method and learning bounds to introduce a novel method that projects select aspects of feature maps from a quantum computer back into classical space. This enables us to imbue the quantum approach with additional insights from classical machine learning that shows the best empirical separation in quantum learning advantages to date. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Computational Power of Data&lt;/b&gt;&lt;br /&gt;The idea of quantum advantage over a classical computer is often framed in terms of computational complexity classes. Examples such as factoring large numbers and simulating quantum systems are classified as bounded quantum polynomial time (BQP) problems, which are those thought to be handled more easily by quantum computers than by classical systems. Problems easily solved on classical computers are called bounded probabilistic polynomial (BPP) problems.  &lt;/p&gt;&lt;p&gt;We show that learning algorithms equipped with data from a quantum process, such as a natural process like fusion or chemical reactions, form a new class of problems (which we call BPP/Samp) that can efficiently perform some tasks that traditional algorithms without data cannot, and is a subclass of the problems efficiently solvable with polynomial sized advice (P/poly). This demonstrates that for some machine learning tasks, understanding the quantum advantage requires examination of available data as well. &lt;/p&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-zgqODKx4UZM/YNI5FjTWbYI/AAAAAAAAHv0/j9pRkvsY_WccThdwo_qbhGiepC4S3y2lQCLcBGAsYHQ/s1999/image1.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1710&quot; data-original-width=&quot;1999&quot; height=&quot;343&quot; src=&quot;https://1.bp.blogspot.com/-zgqODKx4UZM/YNI5FjTWbYI/AAAAAAAAHv0/j9pRkvsY_WccThdwo_qbhGiepC4S3y2lQCLcBGAsYHQ/w400-h343/image1.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;b&gt;&lt;br /&gt;Geometric Test for Quantum Learning Advantage&lt;/b&gt;&lt;br /&gt;Informed by the results that the potential for advantage changes depending on the availability of data, one may ask how a practitioner can quickly evaluate if their problem may be well suited for a quantum computer. To help with this, we developed a workflow for assessing the potential for advantage within a kernel learning framework. We examined a number of tests, the most powerful and informative of which was a novel geometric test we developed. &lt;p&gt;&lt;/p&gt;&lt;p&gt;In &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_machine_learning&quot;&gt;quantum machine learning&lt;/a&gt; methods, such as &lt;a href=&quot;https://ai.googleblog.com/2018/12/exploring-quantum-neural-networks.html&quot;&gt;quantum neural networks&lt;/a&gt; or quantum kernel methods, a quantum program is often divided into two parts, a quantum embedding of the data (an embedding map for the feature space using a quantum computer), and the evaluation of a function applied to the data embedding. In the context of quantum computing, quantum kernel methods make use of traditional kernel methods, but use the quantum computer to evaluate part or all of the kernel on the quantum embedding, which has a different geometry than a classical embedding. It was conjectured that a quantum advantage might arise from the quantum embedding, which might be much better suited to a particular problem than any accessible classical geometry.  &lt;/p&gt;&lt;p&gt;We developed a quick and rigorous test that can be used to quickly compare a particular quantum embedding, kernel, and data set to a range of classical kernels and assess if there is any opportunity for quantum advantage across, e.g., possible label functions such as those used for image recognition tasks. We define a geometric constant &lt;em&gt;g&lt;/em&gt;, which quantifies the amount of data that could theoretically close that gap, based on the geometric test. This is an extremely useful technique for deciding, based on data constraints, if a quantum solution is right for the given problem. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Projected Quantum Kernel Approach&lt;/b&gt;&lt;br /&gt;One insight revealed by the geometric test, was that existing quantum kernels often suffered from a geometry that was easy to best classically because they encouraged memorization, instead of understanding. This inspired us to develop a &lt;em&gt;projected&lt;/em&gt; quantum kernel, in which the quantum embedding is projected back to a classical representation.  While this representation is still hard to compute with a classical computer directly, it comes with a number of practical advantages in comparison to staying in the quantum space entirely.   &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-YTYoa70wmQI/YNI5LbKYWvI/AAAAAAAAHv4/chQCZqtHBBIboAApvTYe5fk-59t4GWjUQCLcBGAsYHQ/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1589&quot; data-original-width=&quot;1999&quot; height=&quot;509&quot; src=&quot;https://1.bp.blogspot.com/-YTYoa70wmQI/YNI5LbKYWvI/AAAAAAAAHv4/chQCZqtHBBIboAApvTYe5fk-59t4GWjUQCLcBGAsYHQ/w640-h509/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Geometric quantity &lt;em&gt;g&lt;/em&gt;, which quantifies the potential for quantum advantage, depicted for several embeddings, including the projected quantum kernel introduced here.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;By selectly projecting back to classical space, we can retain aspects of the quantum geometry that are still hard to simulate classically, but now it is much easier to develop distance functions, and hence kernels, that are better behaved with respect to modest changes in the input than was the original quantum kernel. In addition the projected quantum kernel facilitates better integration with powerful non-linear kernels (like a squared exponential) that have been developed classically, which is much more challenging to do in the native quantum space. &lt;/p&gt;&lt;p&gt;This projected quantum kernel has a number of benefits over previous approaches, including an improved ability to describe non-linear functions of the existing embedding, a reduction in the resources needed to process the kernel from quadratic to linear with the number of data points, and the ability to generalize better at larger sizes. The kernel also helps to expand the geometric &lt;em&gt;g&lt;/em&gt;, which helps to ensure the greatest potential for quantum advantage. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Data Sets Exhibit Learning Advantages&lt;/b&gt;&lt;br /&gt;The geometric test quantifies potential advantage for all possible label functions, however in practice we are most often interested in specific label functions.  Using learning theoretic approaches, we also bound the generalization error for specific tasks, including those which are definitively quantum in origin.  As the advantage of a quantum computer relies on its ability to use many qubits simultaneously but previous approaches scale poorly in number of qubits, it is important to verify the tasks at reasonably large qubit sizes ( &amp;gt; 20 ) to ensure a method has the potential to scale to real problems.  For our studies we verified up to 30 qubits, which was enabled by the open source tool, &lt;a href=&quot;https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html&quot;&gt;TensorFlow-Quantum&lt;/a&gt;, enabling scaling to petaflops of compute. &lt;/p&gt;&lt;p&gt;Interestingly, we showed that many naturally quantum problems, even up to 30 qubits, were readily handled by classical learning methods when sufficient data were provided. Hence one conclusion is that even for some problems that look quantum, classical machine learning methods empowered by data can match the power of quantum computers.  However, using the geometric construction in combination with the projected quantum kernel, we were able to construct a data set that exhibited an empirical learning advantage for a quantum model over a classical one. Thus, while it remains an open question to find such data sets in natural problems, we were able to show the existence of label functions where this can be the case. Although this problem was engineered and a quantum computational advantage would require the embeddings to be larger and more challenging, this work represents an important step in understanding the role data plays in quantum machine learning.  &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-LcPnij2Zt_g/YNI5VrtfhyI/AAAAAAAAHwA/B0mtaA3z02ErcKqaY53cqjlQiJ-iRQiFACLcBGAsYHQ/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;996&quot; data-original-width=&quot;1999&quot; height=&quot;318&quot; src=&quot;https://1.bp.blogspot.com/-LcPnij2Zt_g/YNI5VrtfhyI/AAAAAAAAHwA/B0mtaA3z02ErcKqaY53cqjlQiJ-iRQiFACLcBGAsYHQ/w640-h318/image3.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Prediction accuracy as a function of the number of qubits (&lt;i&gt;n&lt;/i&gt;) for a problem engineered to maximize the potential for learning advantage in a quantum model.  The data is shown for two different sizes of training data (&lt;i&gt;N&lt;/i&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;For this problem, we scaled up the number of qubits (&lt;em&gt;n&lt;/em&gt;) and compared the prediction accuracy of the projected quantum kernel to existing kernel approaches and the best classical machine learning model in our dataset. Moreover, a key takeaway from these results is that although we showed the existence of datasets where a quantum computer has an advantage, for many quantum problems, classical learning methods were still the best approach. Understanding how data can affect a given problem is a key factor to consider when discussing quantum advantage in learning problems, unlike traditional computation problems for which that is not a consideration. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusions&lt;/b&gt;&lt;br /&gt;When considering the ability of quantum computers to aid in machine learning, we have shown that the availability of data fundamentally changes the question. In our work, we develop a practical set of tools for examining these questions, and use them to develop a new projected quantum kernel method that has a number of advantages over existing approaches. We build towards the largest numerical demonstration to date, 30 qubits, of potential learning advantages for quantum embeddings. While a complete computational advantage on a real world application remains to be seen, this work helps set the foundation for the path forward. We encourage any interested readers to check out both &lt;a href=&quot;https://www.nature.com/articles/s41467-021-22539-9&quot;&gt;the paper&lt;/a&gt; and &lt;a href=&quot;https://www.tensorflow.org/quantum/tutorials/quantum_data&quot;&gt;related TensorFlow-Quantum tutorials&lt;/a&gt; that make it easy to build on this work. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We would like to acknowledge our co-authors on this paper — Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, and Hartmut Neven, as well as the entirety of the &lt;a href=&quot;https://quantumai.google/&quot;&gt;Google Quantum AI&lt;/a&gt; team. In addition, we acknowledge valuable help and feedback from Richard Kueng, John Platt, John Preskill, Thomas Vidick, Nathan Wiebe, Chun-Ju Wu, and Balint Pato.&lt;/em&gt;&lt;/p&gt;&lt;!--Footnotes--&gt;&lt;hr width=&quot;80%&quot; /&gt;&lt;p&gt;  &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;&gt;&lt;sup&gt;&lt;a name=&quot;fn1&quot;&gt;&lt;b&gt;1&lt;/b&gt;&lt;/a&gt;&lt;/sup&gt;Current affiliation — Institute for Quantum Information and Matter and Department of Computing and Mathematical Sciences, Caltech, Pasadena, CA, USA&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;&gt;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=5mwX1gN-7UE:VMkMsuoXIc4:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/5mwX1gN-7UE&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/2976383705310409753/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/quantum-machine-learning-and-power-of.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2976383705310409753"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2976383705310409753"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/5mwX1gN-7UE/quantum-machine-learning-and-power-of.html" title="Quantum Machine Learning and the Power of Data"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-zgqODKx4UZM/YNI5FjTWbYI/AAAAAAAAHv0/j9pRkvsY_WccThdwo_qbhGiepC4S3y2lQCLcBGAsYHQ/s72-w400-h343-c/image1.png" height="72" width="72"/><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/06/quantum-machine-learning-and-power-of.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7776510198374933258</id><published>2021-06-21T14:04:00.001-07:00</published><updated>2021-07-16T06:54:36.294-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="conferences"/><category scheme="http://www.blogger.com/atom/ns#" term="CVPR"/><title type="text">Google at CVPR 2021</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Emily Knapp and Tim Herrmann, Program Managers&lt;/span&gt; &lt;p&gt;This week marks the start of the &lt;a href=&quot;http://cvpr2021.thecvf.com/&quot;&gt;2021 Conference on Computer Vision and Pattern Recognition&lt;/a&gt; (CVPR 2021), the premier annual computer vision event consisting of the &lt;a href=&quot;http://cvpr2021.thecvf.com/node/141&quot;&gt;main conference&lt;/a&gt;, &lt;a href=&quot;http://cvpr2021.thecvf.com/workshops-schedule&quot;&gt;workshops&lt;/a&gt; and &lt;a href=&quot;http://cvpr2021.thecvf.com/program&quot;&gt;tutorials&lt;/a&gt;. As a leader in computer vision research and a &lt;a href=&quot;http://cvpr2021.thecvf.com/sponsors&quot;&gt;Champion Level Sponsor&lt;/a&gt;, Google will have a strong presence at CVPR 2021, with over 70 publications accepted, along with the organization of and participation in multiple workshops and tutorials. &lt;/p&gt;&lt;p&gt;If you are participating in CVPR this year, please visit our virtual booth to learn about Google research into the next generation of intelligent systems that utilize the latest machine learning techniques applied to various areas of &lt;a href=&quot;https://research.google/research-areas/machine-perception/&quot;&gt;machine perception&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;You can also learn more about our research being presented at CVPR 2021 in the list below (&lt;strong&gt;Google affiliations in bold&lt;/strong&gt;). &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;Organizing Committee Members&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;General Chair: &lt;b&gt;&lt;em&gt;Rahul Sukthankar&lt;/em&gt;&lt;/b&gt;&lt;br/&gt;Finance Chair: &lt;b&gt;&lt;em&gt;Ramin Zabih&lt;/em&gt;&lt;/b&gt;&lt;br/&gt;Workshop Chair: &lt;b&gt;&lt;em&gt;Caroline Pantofaru&lt;/em&gt;&lt;/b&gt;&lt;br/&gt;Area Chairs: &lt;b&gt;&lt;em&gt;Chen Sun, Golnaz Ghiasi, Jonathan Barron, Kostas Rematas, Negar Rostamzadeh, Noah Snavely, Sanmi Koyejo, Tsung-Yi Lin&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;Publications&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2101.04702.pdf&quot;&gt;Cross-Modal Contrastive Learning for Text-to-Image Generation&lt;/a&gt; (see the &lt;a href=&quot;https://ai.googleblog.com/2021/05/cross-modal-contrastive-learning-for.html&quot;&gt;blog post&lt;/a&gt;)&lt;br/&gt;    &lt;em&gt;&lt;b&gt;Han Zhang, Jing Yu Koh, Jason Baldridge&lt;/b&gt;, Honglak Lee*, &lt;b&gt;Yinfei Yang&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2102.01987.pdf&quot;&gt;Learning Graph Embeddings for Compositional Zero-Shot Learning&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Muhammad Ferjad Naeem, Yongqin Xian, &lt;b&gt;Federico Tombari&lt;/b&gt;, Zeynep Akata&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2006.14660.pdf&quot;&gt;SPSG: Self-Supervised Photometric Scene Generation From RGB-D Scans&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Angela Dai, Yawar Siddiqui, Justus Thies, &lt;b&gt;Julien Valentin&lt;/b&gt;, Matthias Nießner&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.16054.pdf&quot;&gt;3D-MAN: 3D Multi-Frame Attention Network for Object Detection&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Zetong Yang*, Yin Zhou, &lt;b&gt;Zhifeng Chen, Jiquan Ngiam&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.10725.pdf&quot;&gt;MIST: Multiple Instance Spatial Transformer&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Baptiste Angles, Yuhe Jin, &lt;b&gt;Simon Kornblith, Andrea Tagliasacchi&lt;/b&gt;, Kwang Moo Yi&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://people.csail.mit.edu/celiu/pdfs/CVPR21_SemanticUncrop.pdf&quot;&gt;OCONet: Image Extrapolation by Object Completion&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Richard Strong Bowen*, &lt;b&gt;Huiwen Chang&lt;/b&gt;, Charles Herrmann*, Piotr Teterwak*, &lt;b&gt;Ce Liu, Ramin Zabih&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2011.11200.pdf&quot;&gt;Ranking Neural Checkpoints&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Yandong Li, Xuhui Jia, Ruoxin Sang, Yukun Zhu, Bradley Green, Liqiang Wang, Boqing Gong&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.04185.pdf&quot;&gt;LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces From Video Using Pose and Lighting Normalization&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Avisek Lahiri, Vivek Kwatra, Christian Frueh, John Lewis, Chris Bregler&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.03059.pdf&quot;&gt;Differentiable Patch Selection for Image Recognition&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Jean-Baptiste Cordonnier*, &lt;b&gt;Aravindh Mahendran, Alexey Dosovitskiy, Dirk Weissenborn, Jakob Uszkoreit, Thomas Unterthiner&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.15573.pdf&quot;&gt;HumanGPS: Geodesic PreServing Feature for Dense Human Correspondences&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Feitong Tan, Danhang Tang, Mingsong Dou, Kaiwen Guo, Rohit Pandey, Cem Keskin, Ruofei Du, Deqing Sun, Sofien Bouaziz, Sean Fanello, Ping Tan, Yinda Zhang&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.05258.pdf&quot;&gt;VIP-DeepLab: Learning Visual Perception With Depth-Aware Video Panoptic Segmentation&lt;/a&gt; (see the &lt;a href=&quot;https://ai.googleblog.com/2021/04/holistic-video-scene-understanding-with.html&quot;&gt;blog post&lt;/a&gt;)&lt;br/&gt;  &lt;em&gt;Siyuan Qiao*, &lt;b&gt;Yukun Zhu, Hartwig Adam&lt;/b&gt;, Alan Yuille, &lt;b&gt;Liang-Chieh Chen&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.00595.pdf&quot;&gt;DeFMO: Deblurring and Shape Recovery of Fast Moving Objects&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Denys Rozumnyi, Martin R. Oswald, &lt;b&gt;Vittorio Ferrari&lt;/b&gt;, Jiri Matas, Marc Pollefeys&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/papers/Mi_HDMapGen_A_Hierarchical_Graph_Generative_Model_of_High_Definition_Maps_CVPR_2021_paper.pdf&quot;&gt;HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Lu Mi, Hang Zhao, Charlie Nash&lt;b&gt;, &lt;/b&gt;Xiaohan Jin, Jiyang Gao,&lt;b&gt; Chen Sun, Cordelia Schmid, &lt;/b&gt;Nir Shavit,&lt;b&gt; &lt;/b&gt;Yuning Chai, Dragomir Anguelov&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.03336.pdf&quot;&gt;Wide-Baseline Relative Camera Pose Estimation With Directional Learning&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Kefan Chen, Noah Snavely, Ameesh Makadia&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2004.14525.pdf&quot;&gt;MobileDets: Searching for Object Detection Architectures for Mobile Accelerators&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Yunyang Xiong, &lt;b&gt;Hanxiao Liu, Suyog Gupta, Berkin Akin, Gabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans, Mingxing Tan&lt;/b&gt;, Vikas Singh, &lt;b&gt;Bo Chen&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2105.07014.pdf&quot;&gt;SMURF: Self-Teaching Multi-Frame Unsupervised RAFT With Full-Image Warping&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Austin Stone, Daniel Maurer, Alper Ayvaci, Anelia Angelova, Rico Jonschkowski&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2102.08981.pdf&quot;&gt;Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.06777.pdf&quot;&gt;Uncalibrated Neural Inverse Rendering for Photometric Stereo of General Surfaces&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Berk Kaya, Suryansh Kumar, Carlos Oliveira,&lt;b&gt; Vittorio Ferrari&lt;/b&gt;, Luc Van Gool&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.00303.pdf&quot;&gt;MeanShift++: Extremely Fast Mode-Seeking With Applications to Segmentation and Object Tracking&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Jennifer Jang,&lt;b&gt; Heinrich Jiang&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.16183.pdf&quot;&gt;Repopulating Street Scenes&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Yifan Wang*, &lt;b&gt;Andrew Liu, Richard Tucker&lt;/b&gt;, Jiajun Wu, &lt;b&gt;Brian L. Curless, Steven M. Seitz, Noah Snavely&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.00759.pdf&quot;&gt;MaX-DeepLab: End-to-End Panoptic Segmentation With Mask Transformers&lt;/a&gt; (see the &lt;a href=&quot;https://ai.googleblog.com/2021/04/max-deeplab-dual-path-transformers-for.html&quot;&gt;blog post&lt;/a&gt;)&lt;br/&gt;  &lt;em&gt;Huiyu Wang*, &lt;b&gt;Yukun Zhu, Hartwig Adam&lt;/b&gt;, Alan Yuille, &lt;b&gt;Liang-Chieh Chen&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2102.13090.pdf&quot;&gt;IBRNet: Learning Multi-View Image-Based Rendering&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.11575.pdf&quot;&gt;From Points to Multi-Object 3D Reconstruction&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Francis Engelmann*, &lt;b&gt;Konstantinos Rematas&lt;/b&gt;, Bastian Leibe, &lt;b&gt;Vittorio Ferrari&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.08271.pdf&quot;&gt;Learning Compositional Representation for 4D Captures With Neural ODE&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Boyan Jiang, &lt;b&gt;Yinda Zhang&lt;/b&gt;, Xingkui Wei, Xiangyang Xue, Yanwei Fu&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/papers/Kapishnikov_Guided_Integrated_Gradients_An_Adaptive_Path_Method_for_Removing_Noise_CVPR_2021_paper.pdf&quot;&gt;Guided Integrated Gradients: An Adaptive Path Method for Removing Noise&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Andrei Kapishnikov, Subhashini Venugopalan, Besim Avci, Ben Wedin, Michael Terry, Tolga Bolukbasi&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.03954.pdf&quot;&gt;De-Rendering the World’s Revolutionary Artefacts&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Shangzhe Wu*, &lt;b&gt;Ameesh Makadia&lt;/b&gt;, Jiajun Wu, &lt;b&gt;Noah Snavely, Richard Tucker, Angjoo Kanazawa&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2008.03800.pdf&quot;&gt;Spatiotemporal Contrastive Video Representation Learning&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, Yin Cui&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.14107.pdf&quot;&gt;Decoupled Dynamic Filter Networks&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Jingkai Zhou, &lt;b&gt;Varun Jampani&lt;/b&gt;, Zhixiong Pi, Qiong Liu, &lt;b&gt;Ming-Hsuan Yang&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.07700.pdf&quot;&gt;NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering Using RGB Cameras&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, &lt;b&gt;Kaiwen Guo&lt;/b&gt;, Minye Wu, Lan Xu&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.03310.pdf&quot;&gt;Regularizing Generative Adversarial Networks Under Limited Data&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Hung-Yu Tseng*,&lt;b&gt; Lu Jiang, Ce Liu, Ming-Hsuan Yang, Weilong Yang&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.14898.pdf&quot;&gt;SceneGraphFusion: Incremental 3D Scene Graph Prediction From RGB-D Sequences&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Shun-Cheng Wu, Johanna Wald, &lt;b&gt;Keisuke Tateno&lt;/b&gt;, Nassir Navab, &lt;b&gt;Federico Tombari&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.03927.pdf&quot;&gt;NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, Jonathan T. Barron&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.01899.pdf&quot;&gt;Adversarially Adaptive Normalization for Single Domain Generalization&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Xinjie Fan*, &lt;b&gt;Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong&lt;/b&gt;, Mingyuan Zhou&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.01893.pdf&quot;&gt;Adaptive Prototype Learning and Allocation for Few-Shot Segmentation&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Gen Li, &lt;b&gt;Varun Jampani&lt;/b&gt;, Laura Sevilla-Lara, &lt;b&gt;Deqing Sun&lt;/b&gt;, Jonghyun Kim, Joongkyu Kim&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.00802.pdf&quot;&gt;Adversarial Robustness Across Representation Spaces&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Pranjal Awasthi, George Yu, Chun-Sung Ferng, Andrew Tomkins, Da-Cheng Juan&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2008.12873.pdf&quot;&gt;Background Splitting: Finding Rare Classes in a Sea of Background&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Ravi Teja Mullapudi, Fait Poms, &lt;b&gt;William R. Mark&lt;/b&gt;, Deva Ramanan, Kayvon Fatahalian&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2102.05610.pdf&quot;&gt;Searching for Fast Model Families on Datacenter Accelerators&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Sheng Li, Mingxing Tan, Ruoming Pang, Andrew Li, Liqun Cheng, Quoc Le, Norman P. Jouppi&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.09988.pdf&quot;&gt;Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild With Pose Annotations&lt;/a&gt; (see the &lt;a href=&quot;https://ai.googleblog.com/2020/11/announcing-objectron-dataset.html&quot;&gt;blog post&lt;/a&gt;)&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Adel Ahmadyan, Liangkai Zhang, Jianing Wei, Artsiom Ablavatski, Matthias Grundmann&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.04015.pdf&quot;&gt;CutPaste: Self-Supervised Learning for Anomaly Detection and Localization&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, Tomas Pfister&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.03375.pdf&quot;&gt;Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Quin Thames, Arjun Karpur, Wade Norris, Fangting Xia, Liviu Panait, Tobias Weyand, Jack Sim&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2102.09559.pdf&quot;&gt;CReST: A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Chen Wei*, &lt;b&gt;Kihyuk Sohn, Clayton Mellina&lt;/b&gt;, Alan Yuille, &lt;b&gt;Fan Yang&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2006.02334.pdf&quot;&gt;DetectoRS: Detecting Objects With Recursive Feature Pyramid and Switchable Atrous Convolution&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Siyuan Qiao, &lt;b&gt;Liang-Chieh Chen&lt;/b&gt;, Alan Yuille&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2011.12490.pdf&quot;&gt;DeRF: Decomposed Radiance Fields&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Daniel Rebain, Wei Jiang, &lt;b&gt;Soroosh Yazdani, Ke Li&lt;/b&gt;, Kwang Moo Yi,&lt;b&gt; Andrea Tagliasacchi&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.02416.pdf&quot;&gt;Variational Transformer Networks for Layout Generation&lt;/a&gt; (see the &lt;a href=&quot;https://ai.googleblog.com/2021/06/using-variational-transformer-networks.html&quot;&gt;blog post&lt;/a&gt;)&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Diego Martin Arroyo, Janis Postels, Federico Tombari&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Rich_Features_for_Perceptual_Quality_Assessment_of_UGC_Videos_CVPR_2021_paper.pdf&quot;&gt;Rich Features for Perceptual Quality Assessment of UGC Videos&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Yilin Wang, Junjie Ke, Hossein Talebi, Joong Gon Yim, Neil Birkbeck, Balu Adsumilli, Peyman Milanfar, Feng Yang&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2007.08488.pdf&quot;&gt;Complete &amp; Label: A Domain Adaptation Approach to Semantic Segmentation of LiDAR Point Clouds&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Li Yi, Boqing Gong, Thomas Funkhouser&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2008.06910.pdf&quot;&gt;Neural Descent for Visual 3D Human Pose and Shape&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Zanfir, William T. Freeman, Rahul Sukthankar, Cristian Sminchisescu&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2102.12145.pdf&quot;&gt;GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Gu Wang, Fabian Manhardt, &lt;b&gt;Federico Tombari&lt;/b&gt;, Xiangyang Ji&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.05710.pdf&quot;&gt;Look Before You Speak: Visually Contextualized Utterances&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Paul Hongsuck Seo, Arsha Nagrani, Cordelia Schmid&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2105.02976.pdf&quot;&gt;LASR: Learning Articulated Shape Reconstruction From a Monocular Video&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Gengshan Yang*,&lt;b&gt; Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, &lt;/b&gt;Deva Ramanan&lt;b&gt;, William T. Freeman, Ce Liu&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.11511.pdf&quot;&gt;MoViNets: Mobile Video Networks for Efficient Video Recognition&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew Brown, Boqing Gong&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.10565.pdf&quot;&gt;No Shadow Left Behind: Removing Objects and Their Shadows Using Approximate Lighting and Geometry&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Edward Zhang,&lt;b&gt; Ricardo Martin-Brualla, Janne Kontkanen, Brian Curless&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2007.08558.pdf&quot;&gt;On Robustness and Transferability of Convolutional Neural Networks&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D'Amour, Dan Moldovan, Sylvain Gelly, Neil Houlsby, Xiaohua Zhai, Mario Lucic&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.13886.pdf&quot;&gt;Robust and Accurate Object Detection via Adversarial Learning&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Xiangning Chen, Cihang Xie, Mingxing Tan, Li Zhang, Cho-Jui Hsieh, Boqing Gong&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/papers/Chai_To_the_Point_Efficient_3D_Object_Detection_in_the_Range_CVPR_2021_paper.pdf&quot;&gt;To the Point: Efficient 3D Object Detection in the Range Image With Graph Convolution Kernels&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Yuning Chai, Pei Sun, &lt;b&gt;Jiquan Ngiam&lt;/b&gt;, Weiyue Wang, &lt;b&gt;Benjamin Caine, Vijay Vasudevan&lt;/b&gt;, Xiao Zhang, Dragomir Anguelov&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2101.11605.pdf&quot;&gt;Bottleneck Transformers for Visual Recognition&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Aravind Srinivas,&lt;b&gt; Tsung-Yi Lin, Niki Parmar, Jonathon Shlens&lt;/b&gt;, Pieter Abbeel, &lt;b&gt;Ashish Vaswani&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.15092.pdf&quot;&gt;Faster Meta Update Strategy for Noise-Robust Deep Learning&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Youjiang Xu, Linchao Zhu, &lt;b&gt;Lu Jiang&lt;/b&gt;, Yi Yang&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2105.10305.pdf&quot;&gt;Correlated Input-Dependent Label Noise in Large-Scale Image Classification&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Mark Collier, Basil Mustafa, Efi Kokiopoulou, Rodolphe Jenatton, Jesse Berent&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.02189.pdf&quot;&gt;Learned Initializations for Optimizing Coordinate-Based Neural Representations&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, &lt;b&gt;Pratul P. Srinivasan, Jonathan T. Barron&lt;/b&gt;, Ren Ng&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.07177.pdf&quot;&gt;Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Golnaz Ghiasi, Yin Cui, Aravind Srinivas*, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, Barret Zoph&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2105.01859.pdf&quot;&gt;Function4D: Real-Time Human Volumetric Capture From Very Sparse Consumer RGBD Sensors&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Tao Yu, Zerong Zheng, &lt;b&gt;Kaiwen Guo&lt;/b&gt;, Pengpeng Liu, Qionghai Dai, Yebin Liu&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_RSN_Range_Sparse_Net_for_Efficient_Accurate_LiDAR_3D_Object_CVPR_2021_paper.pdf&quot;&gt;RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Pei Sun, Weiyue Wang, Yuning Chai, &lt;b&gt;Gamaleldin Elsayed&lt;/b&gt;, &lt;b&gt;Alex Bewley&lt;/b&gt;, Xiao Zhang, &lt;b&gt;Cristian Sminchisescu&lt;/b&gt;, Dragomir Anguelov&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2008.02268.pdf&quot;&gt;NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.04746.pdf&quot;&gt;Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Siyan Dong, Qingnan Fan, He Wang, Ji Shi, &lt;b&gt;Li Yi, Thomas Funkhouser&lt;/b&gt;, Baoquan Chen, Leonidas Guibas&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2005.07289.pdf&quot;&gt;Taskology: Utilizing Task Relations at Scale&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Yao Lu, Sören Pirk, Jan Dlabal, Anthony Brohan, Ankita Pasad*, Zhao Chen, Vincent Casser, Anelia Angelova, Ariel Gordon&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2105.06993.pdf&quot;&gt;Omnimatte: Associating Objects and Their Effects in Video&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William T. Freeman, Michael Rubinstein&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.14544.pdf&quot;&gt;AutoFlow: Learning a Better Training Set for Optical Flow&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Deqing Sun, Daniel Vlasic, Charles Herrmann, Varun Jampani, Michael Krainin, Huiwen Chang, Ramin Zabih, William T. Freeman, and Ce Liu&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.01845.pdf&quot;&gt;Unsupervised Multi-Source Domain Adaptation Without Access to Source Data&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Sk Miraj Ahmed, Dripta S. Raychaudhuri, &lt;b&gt;Sujoy Paul&lt;/b&gt;, Samet Oymak, Amit K. Roy-Chowdhury&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2003.10580.pdf&quot;&gt;Meta Pseudo Labels&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, Quoc V. Le&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.04160.pdf&quot;&gt;Spatially-Varying Outdoor Lighting Estimation From Intrinsics&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Yongjie Zhu, &lt;b&gt;Yinda Zhang&lt;/b&gt;, Si Li, Boxin Shi&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.01405.pdf&quot;&gt;Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Long Zhao*, &lt;b&gt;Yuxiao Wang, Jiaping Zhao, Liangzhe Yuan, Jennifer J. Sun, Florian Schroff, Hartwig Adam&lt;/b&gt;, Xi Peng, Dimitris Metaxas, &lt;b&gt;Ting Liu&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.16483.pdf&quot;&gt;Benchmarking Representation Learning for Natural World Image Collections&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Grant Van Horn, Elijah Cole, Sara Beery, &lt;b&gt;Kimberly Wilber, Serge Belongie&lt;/b&gt;, Oisin Mac Aodha&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.12731.pdf&quot;&gt;Scaling Local Self-Attention for Parameter Efficient Visual Backbones&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, Jonathon Shlens&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.11224.pdf&quot;&gt;KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Tomas Jakab*&lt;b&gt;, Richard Tucker, Ameesh Makadia&lt;/b&gt;, Jiajun Wu, &lt;b&gt;Noah Snavely, Angjoo Kanazawa&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2007.12140.pdf&quot;&gt;HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Vladimir Tankovich, Christian Häne, Yinda Zhang, Adarsh Kowdle, Sean Fanello, Sofien Bouaziz&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.15331.pdf&quot;&gt;POSEFusion: Pose-Guided Selective Fusion for Single-View Human Volumetric Capture&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Zhe Li, Tao Yu, Zerong Zheng, &lt;b&gt;Kaiwen Guo&lt;/b&gt;, Yebin Liu&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;Workshops&lt;/span&gt;&lt;/b&gt; (&lt;em&gt;only Google affiliations are noted&lt;/em&gt;)   &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/view/mediaforensics2021&quot;&gt;Media Forensics&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Christoph Bregler&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/saiad2021/home?authuser=0&quot;&gt;Safe Artificial Intelligence for Automated Driving&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;b&gt;&lt;em&gt;Been Kim&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://vizwiz.org/workshops/2021-workshop/&quot;&gt;VizWiz Grand Challenge&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Meredith Morris&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/cvpr2021-3d-vision-robotics&quot;&gt;3D Vision and Robotics&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Andy Zeng&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://data.vision.ee.ethz.ch/cvl/ntire21/&quot;&gt;New Trends in Image Restoration and Enhancement Workshop and Challenges on Image and Video Processing&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;em&gt; &lt;b&gt;Ming-Hsuan Yang&lt;/b&gt;&lt;/em&gt;  Program Committee:&lt;b&gt; &lt;em&gt;George Toderici, Ming-Hsuan Yang&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/extremevision-v2/home?authuser=0&quot;&gt;2nd Workshop on Extreme Vision Modeling&lt;/a&gt;&lt;br/&gt;  Invited Speakers:&lt;b&gt; &lt;em&gt;Quoc Le, Chen Sun&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/auvi-cvpr2021/home?authuser=0&quot;&gt;First International Workshop on Affective Understanding in Video&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Gautam Prasad, Ting Liu&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aisecure-workshop.github.io/amlcvpr2021/&quot;&gt;Adversarial Machine Learning in Real-World Computer Vision Systems and Online Challenges&lt;/a&gt;&lt;br/&gt;  Program Committee:&lt;b&gt; &lt;em&gt;Nicholas Carlini, Nicolas Papernot&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/ec3v-cvpr2021/home?authuser=0&quot;&gt;Ethical Considerations in Creative Applications of Computer Vision&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Alex Hanna&lt;/em&gt;&lt;/b&gt;  Organizers:&lt;b&gt; &lt;em&gt;Negar Rostamzadeh, Emily Denton, Linda Petrini&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://visualqa.org/workshop&quot;&gt;Visual Question Answering Workshop&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;b&gt;&lt;em&gt;Vittorio Ferrari&lt;/em&gt;&lt;/b&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://workshop2021.isic-archive.com/&quot;&gt;Sixth International Skin Imaging Collaboration (ISIC) Workshop on Skin Image Analysis&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;b&gt;&lt;em&gt;Sandra Avila&lt;/em&gt;&lt;/b&gt;  Organizers:&lt;b&gt; &lt;em&gt;Yuan Liu&lt;/em&gt;&lt;/b&gt;  Steering Committee:&lt;b&gt; &lt;em&gt;Yuan Liu, Dale Webster&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://cvpr2021.ug2challenge.org/&quot;&gt;The 4th Workshop and Prize Challenge: Bridging the Gap between Computational Photography and Visual Recognition (UG2+) in Conjunction with IEEE CVPR 2021&lt;/a&gt;&lt;br/&gt;  Invited Speakers:&lt;b&gt; &lt;em&gt;Peyman Milanfar, Chelsea Finn&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://scene-understanding.com/&quot;&gt;The 3rd CVPR Workshop on 3D Scene Understanding for Vision, Graphics, and Robotics&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Andrea Tagliasacchi&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://eval.vision.rwth-aachen.de/rvsu-workshop21/&quot;&gt;Robust Video Scene Understanding: Tracking and Video Segmentation&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;b&gt; &lt;em&gt;Jordi Pont-Tuset, Sergi Caelles, Jack Valmadre, Alex Bewley&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://cvpr2021.ug2challenge.org/&quot;&gt;4th Workshop and Challenge on Learned Image Compression&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Rianne van den Berg&lt;/em&gt;&lt;/b&gt;  Organizers: &lt;b&gt;&lt;em&gt;George Toderici, Lucas Theis, Johannes Ballé, Eirikur Agustsson, Nick Johnston, Fabian Mentzer&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/ieeecvf-cvpr2021-precognition/&quot;&gt;The Third Workshop on Precognition: Seeing Through the Future&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Anelia Angelova&lt;/em&gt;&lt;br&gt;&lt;/b&gt;Organizers: &lt;b&gt;&lt;em&gt;Utsav Prabhu&lt;/em&gt;&lt;/b&gt;  Program Committee:&lt;b&gt; &lt;em&gt;Chen Sun, David Ross&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://visual.ee.ucla.edu/ccd2021.htm/&quot;&gt;Computational Cameras and Displays&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Tali Dekel&lt;/em&gt;&lt;/b&gt;  Keynote Talks: &lt;b&gt;&lt;em&gt;Paul Debevec&lt;/em&gt;&lt;/b&gt;  Program Committee:&lt;b&gt; &lt;em&gt;Ayan Chakrabarti, Tali Dekel&lt;/em&gt; &lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://embodied-ai.org/&quot;&gt;2nd Embodied AI Workshop&lt;/a&gt;&lt;br/&gt;  Organizing Committee:&lt;b&gt; &lt;em&gt;Anthony Francis&lt;/em&gt;&lt;/b&gt;  Challenge Organizers:&lt;b&gt; &lt;em&gt;Peter Anderson, Anthony Francis, Alex Ku, Alexander Toshev&lt;/em&gt;&lt;/b&gt;  Scientific Advisory Board:&lt;b&gt; &lt;em&gt;Alexander Toshev&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/rcv-cvpr2021/home?authuser=0&quot;&gt;Responsible Computer Vision&lt;/a&gt;&lt;br/&gt;  Program Committee: &lt;b&gt;&lt;em&gt;Caroline Pantofaru, Utsav Prabhu, Susanna Ricco, Negar Rostamzadeh, Candice Schumann&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/cvpr2021-dnetcv/home?authuser=0&quot;&gt;Dynamic Neural Networks Meets Computer Vision&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;b&gt;&lt;em&gt;Azalia Mirhoseini&lt;/em&gt;&lt;/b&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://cmmc-cvpr21.com/&quot;&gt;Interactive Workshop on Bridging the Gap between Subjective and Computational Measurements of Machine Creativity&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;David Bau&lt;/em&gt;&lt;/b&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://gazeworkshop.github.io/2021/&quot;&gt;GAZE 2021: The 3rd International Workshop on Gaze Estimation and Prediction in the Wild&lt;/a&gt;&lt;br/&gt;  Organizer: &lt;b&gt;&lt;em&gt;Thabo Beeler&lt;/em&gt;&lt;/b&gt;  Program Committee:&lt;b&gt; &lt;em&gt;Thabo Beeler&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://sightsound.org/&quot;&gt;Sight and Sound&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;b&gt; &lt;em&gt;William Freeman&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://visualai.princeton.edu/fcvd/&quot;&gt;Future of Computer Vision Datasets&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Emily Denton, Caroline Pantofaru&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~shuk/open-world-vision.html&quot;&gt;Open World Vision&lt;/a&gt;&lt;br/&gt;  Invited Speakers:&lt;b&gt; &lt;em&gt;Rahul Sukthankar&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/luv2021/home?authuser=0&quot;&gt;The 3rd Workshop on Learning from Unlabeled Videos&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;b&gt; &lt;em&gt;Anelia Angelova, Honglak Lee&lt;/em&gt;&lt;/b&gt;  Program Committee:&lt;b&gt; &lt;em&gt;AJ Piergiovanni&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://www.cis.rit.edu/~glpci/vocvalc2021/&quot;&gt;4th International Workshop on Visual Odometry and Computer Vision Applications Based on Location Clues — With a Focus on Mobile Platform Applications&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Anelia Angelova&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/ecv2021/home?authuser=0&quot;&gt;4th Workshop on Efficient Deep Learning for Computer Vision&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Andrew Howard&lt;/em&gt;&lt;br&gt;&lt;/b&gt;Organizers: &lt;b&gt;&lt;em&gt;Pete Warden, Andrew Howard&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://holistic-video-understanding.github.io/workshops/cvpr2021.html&quot;&gt;Second International Workshop on Large Scale Holistic  Video Understanding&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;b&gt;&lt;em&gt;Cordelia Schmid&lt;/em&gt;&lt;/b&gt;  Program Committee: &lt;b&gt;&lt;em&gt;AJ Piergiovanni&lt;/em&gt;&lt;/b&gt;  Organizers:&lt;b&gt; &lt;em&gt;David Ross&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://cvpr21-nas.com/program&quot;&gt;Neural Architecture Search 1st Lightweight NAS Challenge and Moving Beyond&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;b&gt;&lt;em&gt;Sara Sabour&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://fadetrcv.github.io/2021/&quot;&gt;The Second Workshop on Fair, Data-Efficient, and Trusted Computer Vision&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;b&gt;&lt;em&gt;Gaurav Aggarwal&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://embeddedvisionworkshop.wordpress.com/&quot;&gt;The 17th Embedded Vision Workshop&lt;/a&gt;&lt;br/&gt;  General Chair:&lt;b&gt; &lt;em&gt;Anelia Angelova&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/fgvc8/home?authuser=0&quot;&gt;8th Workshop on Fine-Grained Visual Categorization&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;b&gt; &lt;em&gt;Christine Kaeser-Chen, Kimberly Wilber&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://visual.cs.brown.edu/aicc2021&quot;&gt;AI for Content Creation&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;b&gt;&lt;em&gt;Tali Dekel, Jon Barron, Emily Denton&lt;/em&gt;&lt;/b&gt;  Organizers: &lt;b&gt;&lt;em&gt;Deqing Sun&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/mono3d-workshop&quot;&gt;Frontiers of Monocular 3D Perception&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;b&gt;&lt;em&gt;Anelia Angelova, Cordelia Schmid, Noah Snavely&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/beyond-fairness-cv/home?authuser=0&quot;&gt;Beyond Fairness: Towards a Just, Equitable, and Accountable Computer Vision&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;b&gt; &lt;em&gt;Emily Denton&lt;/em&gt;&lt;/b&gt;    &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://fvc-workshop.github.io/index.html&quot;&gt;The 1st Workshop on Future Video Conferencing&lt;/a&gt;&lt;br/&gt;  Invited Speakers:&lt;b&gt; &lt;em&gt;Chuo-Ling Chang, Sergi Caelles&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;Tutorials&lt;/span&gt;&lt;/b&gt; &lt;em&gt;(only Google affiliations are noted)&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/fatecv-tutorial-2021/home?authuser=0&quot;&gt;Tutorial on Fairness Accountability Transparency and Ethics in Computer Vision&lt;/a&gt;&lt;br/&gt;  Organizer:&lt;b&gt;&lt;em&gt; Emily Denton&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://vita-group.github.io/cvpr_2021_data_efficient_tutorial.html&quot;&gt;Data-Efficient Learning in An Imperfect World&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Boqing Gong, Ting Chen&lt;/em&gt;&lt;/b&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://d2ch.dii.univpm.it/#A_Home&quot;&gt;Semantic Segmentation of Point Clouds: a Deep Learning Framework for Cultural Heritage&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt;&lt;em&gt; Manzil Zaheer&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://vqa2vln-tutorial.github.io/&quot;&gt;From VQA to VLN: Recent Advances in Vision-and-Language Research&lt;/a&gt;&lt;br/&gt;	   Organizer: &lt;b&gt;&lt;em&gt;Peter Anderson&lt;/em&gt;&lt;/b&gt; &lt;/p&gt;&lt;p&gt;  &lt;em&gt;* Indicates work done while at Google&lt;/em&gt;&lt;/p&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?a=TrNMZxLRY78:qaWq24C_WpI:yIl2AUoC8zA&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA&quot; border=&quot;0&quot;&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src=&quot;http://feeds.feedburner.com/~r/blogspot/gJZg/~4/TrNMZxLRY78&quot; height=&quot;1&quot; width=&quot;1&quot; alt=&quot;&quot;/&gt;</content><link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/7776510198374933258/comments/default" title="Post Comments"><link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/google-at-cvpr-2021.html#comment-form" title="0 Comments"><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7776510198374933258"><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7776510198374933258"><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/TrNMZxLRY78/google-at-cvpr-2021.html" title="Google at CVPR 2021"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/></author><thr:total>0</thr:total><feedburner:origLink>http://ai.googleblog.com/2021/06/google-at-cvpr-2021.html</feedburner:origLink></entry></feed>
