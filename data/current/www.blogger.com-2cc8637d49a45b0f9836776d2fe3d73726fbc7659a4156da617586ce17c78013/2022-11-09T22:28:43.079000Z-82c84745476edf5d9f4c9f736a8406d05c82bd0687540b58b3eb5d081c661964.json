{
  "title":"Multi-layered Mapping of Brain Tissue via Segmentation Guided Contrastive Learning",
  "date":"2022-11-09T22:28:43.079000Z",
  "author":"Google AI",
  "id":"tag:blogger.com,1999:blog-8474926331452026626.post-1592161586308962338",
  "link":"http://ai.googleblog.com/2022/11/multi-layered-mapping-of-brain-tissue.html",
  "content":"<span class=\"byline-author\">Posted by Peter H. Li, Research Scientist, and Sven Dorkenwald, Student Researcher, Connectomics at Google</span> <img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGcjzK2k-GUyCJ3tjBj6RkXS3hblnCdrP0-QFl9tvhGZON-IqTSNlB6jNbCa2aMr561M_Qc1gIXNZAArnk0uZQw8RVupbfHUxjMY9GtiChJTdBR6yBPmhAY80xmnWaVSgAueOMu368GuI_mFoK4SKDYQMBUIPcny5iDjvgFimZj8NWabLxy4V-QxUoCw/s320/image5.gif\"> <p>Mapping the wiring and firing activity of the human brain is fundamental to deciphering how we think — how we sense the world, learn, decide, remember, and create — as well as what issues can arise in brain disease or dysfunction. Recent efforts have delivered publicly available brain maps (high-resolution 3D mapping of brain cells and their connectivities) at unprecedented quality and scale, such as <a href=\"https://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html\">H01, a 1.4 petabyte nanometer-scale digital reconstruction</a> of a sample of human brain tissue from Harvard / Google, and the <a href=\"https://www.microns-explorer.org/cortical-mm3\">cubic millimeter mouse cortex dataset</a> from our colleagues at the <a href=\"https://www.microns-explorer.org/team\">MICrONS consortium</a>. </p><a name=\"more\"></a> <p>To interpret brain maps at this scale requires multiple layers of analysis, including the identification of <a href=\"https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/gallery/ei_point.json\">synaptic connections</a>, <a href=\"https://h01-dot-neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B3.3e-8%2C%22m%22%5D%7D%2C%22position%22:%5B253279%2C187478%2C1836.5%5D%2C%22crossSectionScale%22:9.65671770692589%2C%22projectionScale%22:63363.54435174225%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/4nm_raw%22%2C%22tab%22:%22source%22%2C%22name%22:%224nm%20EM%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%7B%22url%22:%22precomputed://gs://h01-release/data/20210601/c2%22%2C%22subsources%22:%7B%22default%22:true%2C%22bounds%22:true%2C%22properties%22:true%2C%22mesh%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22tab%22:%22source%22%2C%22meshSilhouetteRendering%22:2%2C%22segments%22:%5B%224737616850%22%5D%2C%22colorSeed%22:3673823978%2C%22name%22:%22c2%20segmentation%22%7D%2C%7B%22type%22:%22annotation%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/c2/subcompartments/annotations%22%2C%22tab%22:%22rendering%22%2C%22ignoreNullSegmentFilter%22:false%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20switch%20%28prop_class_label%28%29%29%20%7B%5Cn%20%20case%200:%20%20//%20axon%5Cn%20%20%20%20setColor%28vec3%280%2C%200%2C%201%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%201:%20%20//%20dendrite%5Cn%20%20%20%20setColor%28vec3%281%2C%200%2C%200%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%202:%20%20//%20astrocyte%5Cn%20%20%20%20setColor%28vec3%280%2C%201%2C%200%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%203:%20%20//%20soma%5Cn%20%20%20%20setColor%28vec3%281%2C%201%2C%201%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%204:%20%20//%20cilium%5Cn%20%20%20%20setColor%28vec3%280.5%2C%200.5%2C%200%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%205:%20%20//%20AIS%5Cn%20%20%20%20setColor%28vec3%280.5%2C%200.5%2C%201%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20%20%20%5Cn%20%20case%201000:%20%20//%20myelinated%20axon%5Cn%20%20case%201001:%5Cn%20%20%20%20setColor%28vec3%281%2C%200.25%2C%200.75%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%201004:%20%20//%20fragments%5Cn%20%20case%201005:%5Cn%20%20default:%5Cn%20%20%20%20discard%3B%5Cn%20%20%7D%5Cn%7D%5Cn%22%2C%22linkedSegmentationLayer%22:%7B%22skeleton_id%22:%22c2%20segmentation%22%7D%2C%22filterBySegmentation%22:%5B%22skeleton_id%22%5D%2C%22name%22:%226-class%20annotations%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/c2/subcompartments%22%2C%22tab%22:%22source%22%2C%22segmentColors%22:%7B%22100%22:%22#0000ff%22%2C%22101%22:%22#ff0000%22%2C%22102%22:%22#00ff00%22%2C%22103%22:%22#ffffff%22%2C%22104%22:%22#7f7f00%22%2C%22105%22:%22#7f7fff%22%2C%221100%22:%22#ff3fbf%22%2C%221101%22:%22#ff3fbf%22%2C%221102%22:%22#000000%22%2C%221103%22:%22#000000%22%2C%221104%22:%22#ff3fbf%22%2C%221105%22:%22#ff3fbf%22%7D%2C%22name%22:%226-class%20render%22%7D%5D%2C%22showAxisLines%22:false%2C%22showSlices%22:false%2C%22selectedLayer%22:%7B%22layer%22:%226-class%20render%22%7D%2C%22layout%22:%22xy-3d%22%7D\">cellular subcompartments</a>, and <a href=\"https://h01-dot-neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B3.3e-8%2C%22m%22%5D%7D%2C%22position%22:%5B379548.5%2C137610.125%2C2660.5%5D%2C%22crossSectionScale%22:5.776802800212544%2C%22projectionScale%22:261570.24038807175%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/4nm_raw%22%2C%22tab%22:%22source%22%2C%22name%22:%224nm%20EM%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%5B%7B%22url%22:%22precomputed://gs://h01-release/data/20210601/c3%22%2C%22subsources%22:%7B%22default%22:true%2C%22bounds%22:true%2C%22properties%22:true%2C%22mesh%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22precomputed://gs://lichtman-h01-49eee972005c8846803ef58fbd36e049/goog14r0s5c3_new_props/segment_properties%22%5D%2C%22panels%22:%5B%7B%22flex%22:1.55%2C%22size%22:366%2C%22tab%22:%22segments%22%7D%5D%2C%22colorSeed%22:4270253886%2C%22name%22:%22c3%20segmentation%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/layers%22%2C%22tab%22:%22source%22%2C%22selectedAlpha%22:0.3%2C%22objectAlpha%22:0.2%2C%22segments%22:%5B%221%22%2C%222%22%2C%223%22%2C%224%22%2C%225%22%2C%226%22%2C%227%22%5D%2C%22segmentQuery%22:%221%2C2%2C3%2C4%2C5%2C6%2C7%22%2C%22name%22:%22cortical%20layers%22%2C%22visible%22:false%7D%5D%2C%22showSlices%22:false%2C%22prefetch%22:false%2C%22selectedLayer%22:%7B%22row%22:1%2C%22flex%22:1.55%2C%22size%22:309%2C%22layer%22:%224nm%20EM%22%7D%2C%22layout%22:%7B%22type%22:%22xy-3d%22%2C%22orthographicProjection%22:true%7D%2C%22selection%22:%7B%22row%22:2%2C%22flex%22:0.45%2C%22size%22:309%2C%22visible%22:false%7D%7D\">cell types</a>. Machine learning and computer vision technology have played a central role in enabling these analyses, but deploying such systems is still a laborious process, requiring hours of manual ground truth labeling by expert annotators and significant computational resources. Moreover, some important tasks, such as identifying the cell type from only a small fragment of <a href=\"https://en.wikipedia.org/wiki/Axon\">axon</a> or <a href=\"https://en.wikipedia.org/wiki/Dendrite\">dendrite</a>, can be challenging even for human experts, and have not yet been effectively automated. </p><p>Today, in “<a href=\"https://www.biorxiv.org/content/10.1101/2022.03.29.486320\">Multi-Layered Maps of Neuropil with Segmentation-Guided Contrastive Learning</a>”, we are announcing Segmentation-Guided Contrastive Learning of Representations (SegCLR), a method for training rich, generic representations of cellular morphology (the cell’s shape) and ultrastructure (the cell’s internal structure) without laborious manual effort. SegCLR produces compact vector representations (i.e., embeddings) that are applicable across diverse downstream tasks (e.g., local classification of cellular subcompartments, unsupervised clustering), and are even able to identify cell types from only small fragments of a cell. We trained SegCLR on both the H01 human cortex dataset and the MICrONS mouse cortex dataset, and we are releasing the <a href=\"https://h01-release.storage.googleapis.com/embeddings.html\">resulting embedding vectors</a>, about 8 billion in total, for researchers to explore. </p> <br><br> <div class=\"separator\"> <video loop=\"\" width=\"80%\"> <source src=\"https://storage.googleapis.com/h01-release/data/20220326/c3/embeddings/segclr_14.mp4\" type=\"video/mp4\">&lt;source&gt; </video></div><br> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\"><tbody><tr><td class=\"tr-caption\">From brain cells segmented out of a 3D block of tissue, SegCLR embeddings capture cellular morphology and ultrastructure and can be used to distinguish cellular subcompartments (e.g., <a href=\"https://en.wikipedia.org/wiki/Dendritic_spine\">dendritic spine</a> versus dendrite shaft) or cell types (e.g., <a href=\"https://en.wikipedia.org/wiki/Pyramidal_cell\">pyramidal</a> versus <a href=\"https://en.wikipedia.org/wiki/Microglia\">microglia</a> cell).</td></tr></tbody></table>  <div>    <br></div><h2>Representing Cellular Morphology and Ultrastructure</h2><p>SegCLR builds on <a href=\"https://ai.googleblog.com/2021/10/self-supervised-learning-advances.html\">recent advances in self-supervised contrastive learning</a>. We use a standard deep network architecture to encode inputs comprising local 3D blocks of electron <a href=\"https://en.wikipedia.org/wiki/Microscopy\">microscopy</a> data (about 4 micrometers on a side) into 64-dimensional embedding vectors. The network is trained via a <a href=\"https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html\">contrastive loss</a> to map semantically related inputs to similar coordinates in the embedding space. This is close to the <a href=\"https://arxiv.org/abs/2002.05709\">popular SimCLR setup</a>, except that we also require an <a href=\"https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html\">instance segmentation</a> of the volume (tracing out individual cells and cell fragments), which we use in two important ways. </p><p>First, the input 3D electron microscopy data are explicitly masked by the segmentation, forcing the network to focus only on the central cell within each block. Second, we leverage the segmentation to automatically define which inputs are semantically related: positive pairs for the contrastive loss are drawn from nearby locations on the same segmented cell and trained to have similar representations, while inputs drawn from different cells are trained to have dissimilar representations. Importantly, publicly available automated segmentations of the human and mouse datasets were sufficiently accurate to train SegCLR without requiring laborious review and correction by human experts. </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\"><tbody><tr><td><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW9HNCvQvWIHGSf4m4kcgmSNmyqYnQUjm4clwOg9zoJz6NOkR2_x68HEdep2srVXkWXVz2QS6-PdwQVbV0M25PNUSEm7IhiAIWCkfwTYmUoXOW3A7IrZ__CyH_-T18IKkU1hbI5G5YsKm4DDxX2itFvKielNwqgUnEFmlXWjZvioTaqTPLyf3ueeaYnQ/s1999/image1.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW9HNCvQvWIHGSf4m4kcgmSNmyqYnQUjm4clwOg9zoJz6NOkR2_x68HEdep2srVXkWXVz2QS6-PdwQVbV0M25PNUSEm7IhiAIWCkfwTYmUoXOW3A7IrZ__CyH_-T18IKkU1hbI5G5YsKm4DDxX2itFvKielNwqgUnEFmlXWjZvioTaqTPLyf3ueeaYnQ/s16000/image1.png\"></a></td></tr><tr><td class=\"tr-caption\">SegCLR is trained to represent rich cellular features without manual labeling. <b>Top</b>: The SegCLR architecture maps local masked 3D views of electron microscopy data to embedding vectors. Only the microscopy volume and a draft automated instance segmentation are required. <b>Bottom</b>: The segmentation is also used to define positive versus negative example pairs, whose representations are pushed closer together (<b>positives</b>, <b>blue arrows</b>) or further apart (<b>negatives</b>, <b>red arrows</b>) during training.</td></tr></tbody></table>   <div>    <br></div><h2>Reducing Annotation Training Requirements by Three Orders of Magnitude</h2><p>SegCLR embeddings can be used in diverse downstream settings, whether supervised (e.g., training classifiers) or unsupervised (e.g., clustering or content-based image retrieval). In the supervised setting, embeddings simplify the training of classifiers, and can greatly reduce ground truth labeling requirements. For example, we found that for identifying cellular subcompartments (axon, dendrite, <a href=\"https://en.wikipedia.org/wiki/Soma_(biology)\">soma</a>, etc.) a simple linear classifier trained on top of SegCLR embeddings outperformed a fully supervised deep network trained on the same task, while using only about one thousand labeled examples instead of millions. </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\"><tbody><tr><td><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbYdSshuRcTBFCB-zmDY7C31mUpyyXP98JEOz9ZsvXMy3a23l5mEBoamtlz7geaoBh8s_PXPi61SNUfbUbIbWBqBcm2JjD7pZD-q5rx4g6psp54DLoPSwdz2TQRAu4vGRFAMui1VRQwCAudV3Ff5rcjEOWwSPsMcYE4II6zvfetG_X7CF4bgHXMPzhPw/s1999/image2.png\"><img border=\"0\" height=\"480\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbYdSshuRcTBFCB-zmDY7C31mUpyyXP98JEOz9ZsvXMy3a23l5mEBoamtlz7geaoBh8s_PXPi61SNUfbUbIbWBqBcm2JjD7pZD-q5rx4g6psp54DLoPSwdz2TQRAu4vGRFAMui1VRQwCAudV3Ff5rcjEOWwSPsMcYE4II6zvfetG_X7CF4bgHXMPzhPw/w640-h480/image2.png\" width=\"640\"></a></td></tr><tr><td class=\"tr-caption\">We assessed the classification performance for axon, dendrite, soma, and <a href=\"https://en.wikipedia.org/wiki/Astrocyte\">astrocyte</a> subcompartments in the human cortex dataset via mean <a href=\"https://en.wikipedia.org/wiki/F-score\">F1-Score</a>, while varying the number of training examples used. Linear classifiers trained on top of SegCLR embeddings matched or exceeded the performance of a fully supervised deep classifier (horizontal line), while using a fraction of the training data.</td></tr></tbody></table>   <div>    <br></div><h2>Distinguishing Cell Types, Even from Small Fragments</h2><p>Distinguishing different cell types is an important step towards understanding how brain circuits develop and function in health and disease. Human experts can learn to identify some cortical cell types based on morphological features, but manual cell typing is laborious and ambiguous cases are common. Cell typing also becomes more difficult when only small fragments of cells are available, which is common for many cells in current <a href=\"https://research.google/teams/connectomics/\">connectomic reconstructions</a>. </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\"><tbody><tr><td><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjgfqCNS8o0guumswH1girDbY9ys-MyDwnJZGyluaNE91q2S55gBX2A0bD5TwHbWw58dcET7_Q_lCMGbOx_9ETd3_sNggS45pO5jH40sAc2Cer8slSyq-Y9zLZqR1hEyeQW0ho3fAaWraCOau6xmjkSrkIenX9b3OSFAvPXzmKePW9xpA4qMXdSiWDIA/s1999/image4.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjgfqCNS8o0guumswH1girDbY9ys-MyDwnJZGyluaNE91q2S55gBX2A0bD5TwHbWw58dcET7_Q_lCMGbOx_9ETd3_sNggS45pO5jH40sAc2Cer8slSyq-Y9zLZqR1hEyeQW0ho3fAaWraCOau6xmjkSrkIenX9b3OSFAvPXzmKePW9xpA4qMXdSiWDIA/s16000/image4.png\"></a></td></tr><tr><td class=\"tr-caption\">Human experts manually labeled cell types for a small number of proofread cells in each dataset. In the mouse cortex dataset, experts labeled six neuron types (<b>top</b>) and four <a href=\"https://en.wikipedia.org/wiki/Glia\">glia</a> types (not shown). In the human cortex dataset, experts labeled two neuron types (not shown) and four glia types (<b>bottom</b>). (Rows not to scale with each other.)</td></tr></tbody></table>   <p>We found that SegCLR accurately infers human and mouse cell types, even for small fragments. Prior to classification, we collected and averaged embeddings within each cell over a set aggregation distance, defined as the radius from a central point. We found that human cortical cell types can be identified with high accuracy for aggregation radii as small as 10 micrometers, even for types that experts <a href=\"https://www.biorxiv.org/content/10.1101/2021.05.29.446289v4\">find difficult to distinguish</a>, such as microglia (MGC) versus <a href=\"https://en.wikipedia.org/wiki/Oligodendrocyte_progenitor_cell\">oligodendrocyte precursor cells</a> (OPC). </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\"><tbody><tr><td><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy_9tPcai_9WSDpruyAshMw2mgr-LPkD62ZKFRzbjhyhqipGmAkFzSy9t_XRwP8Edp1gXmcg269CUzAj8AjMR0sIqwpFJPGkTLbgpGgFf0Kit-jOopjS4preHP72LBK7D50Qs8gXXRL3aEFU9XF1iNF286urh3M9BSlkOAB12-yarRfwfcPUAXev-AIQ/s1999/image3.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy_9tPcai_9WSDpruyAshMw2mgr-LPkD62ZKFRzbjhyhqipGmAkFzSy9t_XRwP8Edp1gXmcg269CUzAj8AjMR0sIqwpFJPGkTLbgpGgFf0Kit-jOopjS4preHP72LBK7D50Qs8gXXRL3aEFU9XF1iNF286urh3M9BSlkOAB12-yarRfwfcPUAXev-AIQ/s16000/image3.png\"></a></td></tr><tr><td class=\"tr-caption\">SegCLR can classify cell types, even from small fragments. <b>Left</b>: Classification performance over six human cortex cell types for shallow <a href=\"https://en.wikipedia.org/wiki/Residual_neural_network\">ResNet</a> models trained on SegCLR embeddings for different sized cell fragments. Aggregation radius zero corresponds to very small fragments with only a single embedding. Cell type performance reaches high accuracy (0.938 mean F1-Score) for fragments with aggregation radii of only 10 micrometers (<b>boxed point</b>). <b>Right</b>: Class-wise <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">confusion matrix</a> at 10 micrometers aggregation radius. Darker shading along the diagonal indicates that predicted cell types agree with expert labels in most cases. AC: astrocyte; MGC: microglia cell; OGC: <a href=\"https://en.wikipedia.org/wiki/Oligodendrocyte\">oligodendrocyte</a> cell; OPC: oligodendrocyte precursor cell; E: excitatory neuron; I: inhibitory neuron.</td></tr></tbody></table>   <p>In the mouse cortex, ten cell types could be distinguished with high accuracy at aggregation radii of 25 micrometers. </p>   <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\"><tbody><tr><td><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiETN568Ipc4zmD21q2KmbRjcHn4vTjJHAd5PmPr342h_y-38b8K3rgTa2YR9Xs85Hv7rlogyaBLGcMajs94L5sTt4OrsRaNNy000TRAXHKZDxPOTAz70Ar3y47EbpBrIBBDmKzuIxNwuYZX7CXzmdNI-_AJFycmRXqsiy66_lp17GwwdxlhMBN7AgZ-g/s1999/image6.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiETN568Ipc4zmD21q2KmbRjcHn4vTjJHAd5PmPr342h_y-38b8K3rgTa2YR9Xs85Hv7rlogyaBLGcMajs94L5sTt4OrsRaNNy000TRAXHKZDxPOTAz70Ar3y47EbpBrIBBDmKzuIxNwuYZX7CXzmdNI-_AJFycmRXqsiy66_lp17GwwdxlhMBN7AgZ-g/s16000/image6.png\"></a></td></tr><tr><td class=\"tr-caption\"><b>Left</b>: Classification performance over the ten mouse cortex cell types reaches 0.832 mean F1-Score for fragments with aggregation radius 25 micrometers (<b>boxed point</b>). <b>Right</b>: The class-wise confusion matrix at 25 micrometers aggregation radius. Boxes indicate broad groups (glia, excitatory neurons, and inhibitory interneurons). P: pyramidal cell; THLC: <a href=\"https://en.wikipedia.org/wiki/Thalamocortical_radiations\">thalamocortical axon</a>; BC: <a href=\"https://en.wikipedia.org/wiki/Basket_cell\">basket cell</a>; BPC: <a href=\"https://link.springer.com/article/10.1007/BF01258522\">bipolar cell</a>; MC: <a href=\"https://en.wikipedia.org/wiki/Martinotti_cell\">Martinotti cell</a>; NGC: <a href=\"https://en.wikipedia.org/wiki/Neurogliaform_cell\">neurogliaform cell</a>.</td></tr></tbody></table>   <p>In additional cell type applications, we <a href=\"https://www.biorxiv.org/content/10.1101/2022.03.29.486320\">used unsupervised clustering of SegCLR embeddings</a> to reveal further neuronal subtypes, and demonstrated how <a href=\"https://arxiv.org/abs/2006.10108\">uncertainty estimation</a> can be used to <a href=\"https://www.tensorflow.org/tutorials/understanding/sngp\">restrict classification to high confidence subsets</a> of the dataset, e.g., when only a few cell types have expert labels. </p><div>    <br></div><h2>Revealing Patterns of Brain Connectivity</h2><p>Finally, we showed how SegCLR can be used for automated analysis of brain connectivity by cell typing the <a href=\"https://en.wikipedia.org/wiki/Synapse\">synaptic partners</a> of reconstructed cells throughout the mouse cortex dataset. Knowing the connectivity patterns between specific cell types is fundamental to interpreting large-scale connectomic reconstructions of brain wiring, but this typically requires manual tracing to identify partner cell types. Using SegCLR, we replicated brain connectivity findings that previously relied on intensive manual tracing, while extending their scale in terms of the number of synapses, cell types, and brain areas analyzed. (See <a href=\"https://www.biorxiv.org/content/10.1101/2022.03.29.486320\">the paper</a> for further details.) </p>   <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\"><tbody><tr><td><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4DXv06GyF551IlYe9F1_07xE6MIn5WdzlPiNaVvsDAfV-nFrdA9y8JA9kRUjixtZUFqVNEH0n_Q5lmi2dD69z0LoDTbZRd9jE5wtfVHIBbofT-INGg6IAvQbzeCiKlvC_vBdyCm9RrgT0NR10cHLqYDpWmRtM9OewFqoSksHGSQiTyUPZCbEn_rZm-w/s1999/image7.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4DXv06GyF551IlYe9F1_07xE6MIn5WdzlPiNaVvsDAfV-nFrdA9y8JA9kRUjixtZUFqVNEH0n_Q5lmi2dD69z0LoDTbZRd9jE5wtfVHIBbofT-INGg6IAvQbzeCiKlvC_vBdyCm9RrgT0NR10cHLqYDpWmRtM9OewFqoSksHGSQiTyUPZCbEn_rZm-w/s16000/image7.png\"></a></td></tr><tr><td class=\"tr-caption\">SegCLR automated analysis of brain connectivity. <b>Top</b>: An example mouse pyramidal cell, with synapse locations color-coded according to whether the synaptic partner was classified as inhibitory (<b>blue</b>), excitatory (<b>red</b>), or unknown (<b>black</b>). Inset shows higher detail of the soma and proximal dendrites. <b>Bottom</b>: We counted how many upstream synaptic partners were classified as thalamocortical axons, which bring input from sensory systems to the cortex. We found that thalamic input arrives primarily at cortical layer L4, the canonical cortical input layer, and preferentially targets primary visual area V1, rather than higher visual areas (HVA).</td></tr></tbody></table>   <div>    <br></div><h2>What’s Next?</h2><p>SegCLR captures rich cellular features and can greatly simplify downstream analyses compared to working directly with raw image and segmentation data. We are excited to see what the community can discover using the <a href=\"https://h01-release.storage.googleapis.com/embeddings.html\">~8 billion embeddings we are releasing</a> for the human and mouse cortical datasets (<a href=\"https://colab.sandbox.google.com/gist/chinasaur/63f15b3f37b35b5bb27de31ba0a0087f/segclr-sharding.ipynb\">example access code</a>; browsable <a href=\"https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B3.3e-8%2C%22m%22%5D%7D%2C%22position%22:%5B329992.21875%2C105731.6171875%2C2191.5%5D%2C%22crossSectionScale%22:410.82862069592164%2C%22projectionScale%22:126684.8939275032%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/4nm_raw%22%2C%22tab%22:%22source%22%2C%22name%22:%224nm%20EM%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%7B%22url%22:%22precomputed://gs://h01-release/data/20210601/c3%22%2C%22subsources%22:%7B%22default%22:true%2C%22bounds%22:true%2C%22properties%22:true%2C%22mesh%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22tab%22:%22segments%22%2C%22meshSilhouetteRendering%22:2%2C%22segments%22:%5B%222746434850%22%2C%2246794629299%22%2C%22925441648%22%5D%2C%22segmentQuery%22:%22925441648%2C%202746434850%2C%2046794629299%22%2C%22colorSeed%22:4270253886%2C%22name%22:%22c3%20segmentation%22%7D%2C%7B%22type%22:%22annotation%22%2C%22source%22:%22precomputed://gs://h01-release/data/20220326/c3/embeddings/segclr%22%2C%22panels%22:%5B%7B%22flex%22:0.65%2C%22tab%22:%22rendering%22%7D%5D%2C%22shader%22:%22#uicontrol%20invlerp%20red%5Cn#uicontrol%20invlerp%20green%5Cn#uicontrol%20invlerp%20blue%5Cn%5Cnvoid%20main%28%29%20%7B%5Cn%20%20setColor%28vec3%28red%28%29%2C%20green%28%29%2C%20blue%28%29%29%29%3B%5Cn%20%20setPointMarkerSize%284.0%29%3B%5Cn%20%20setPointMarkerBorderWidth%280.%29%3B%5Cn%7D%22%2C%22shaderControls%22:%7B%22red%22:%7B%22range%22:%5B-30%2C30%5D%2C%22window%22:%5B-30%2C30%5D%7D%2C%22green%22:%7B%22range%22:%5B-25%2C30%5D%2C%22window%22:%5B-30%2C30%5D%2C%22property%22:%22e1%22%7D%2C%22blue%22:%7B%22range%22:%5B-40%2C0%5D%2C%22window%22:%5B-50%2C10%5D%2C%22property%22:%22e6%22%7D%7D%2C%22linkedSegmentationLayer%22:%7B%22skeleton_id%22:%22c3%20segmentation%22%7D%2C%22filterBySegmentation%22:%5B%22skeleton_id%22%5D%2C%22name%22:%22segclr%22%7D%5D%2C%22showAxisLines%22:false%2C%22showDefaultAnnotations%22:false%2C%22showSlices%22:false%2C%22prefetch%22:false%2C%22selectedLayer%22:%7B%22row%22:1%2C%22flex%22:0.65%2C%22visible%22:true%2C%22layer%22:%22c3%20segmentation%22%7D%2C%22layout%22:%7B%22type%22:%22xy-3d%22%2C%22orthographicProjection%22:true%7D%2C%22helpPanel%22:%7B%22flex%22:1.04%7D%2C%22selection%22:%7B%22row%22:2%2C%22flex%22:0.45%2C%22size%22:309%2C%22visible%22:false%7D%2C%22layerListPanel%22:%7B%22flex%22:0.96%7D%7D\">human</a> and <a href=\"https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B4e-8%2C%22m%22%5D%7D%2C%22position%22:%5B125184.765625%2C90334.2734375%2C20655.470703125%5D%2C%22crossSectionScale%22:175.91483748406438%2C%22projectionScale%22:159795.35592227764%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em%22%2C%22tab%22:%22source%22%2C%22name%22:%22em65%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://iarpa_microns/minnie/minnie65/seg_m343%22%2C%22tab%22:%22segments%22%2C%22meshSilhouetteRendering%22:2%2C%22segments%22:%5B%22864691135337845734%22%2C%22864691135737765745%22%5D%2C%22segmentQuery%22:%22864691135337845734%2C%20864691135737765745%22%2C%22name%22:%22seg_m343%22%7D%2C%7B%22type%22:%22annotation%22%2C%22source%22:%7B%22url%22:%22precomputed://gs://iarpa_microns/minnie/minnie65/embeddings_m343/segclr%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C13824%5D%2C%5B0%2C1%2C0%2C13824%5D%2C%5B0%2C0%2C1%2C14816%5D%5D%2C%22outputDimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B4e-8%2C%22m%22%5D%7D%7D%7D%2C%22panels%22:%5B%7B%22size%22:353%2C%22tab%22:%22rendering%22%7D%5D%2C%22shader%22:%22#uicontrol%20invlerp%20red%5Cn#uicontrol%20invlerp%20green%5Cn#uicontrol%20invlerp%20blue%5Cn%5Cnvoid%20main%28%29%20%7B%5Cn%20%20setColor%28vec3%28red%28%29%2C%20green%28%29%2C%20blue%28%29%29%29%3B%5Cn%20%20setPointMarkerSize%284.0%29%3B%5Cn%20%20setPointMarkerBorderWidth%280.%29%3B%5Cn%7D%22%2C%22shaderControls%22:%7B%22red%22:%7B%22range%22:%5B-7.8687191836535355%2C11.536377197734613%5D%2C%22window%22:%5B-38.720551224447334%2C52.72395267839594%5D%7D%2C%22green%22:%7B%22range%22:%5B-7.099167230561023%2C8.861439622293307%5D%2C%22window%22:%5B-10%2C10%5D%2C%22property%22:%22e1%22%7D%2C%22blue%22:%7B%22range%22:%5B23.624030954467138%2C1.4089367275395688%5D%2C%22window%22:%5B-38.67762518591486%2C61.42514715610742%5D%2C%22property%22:%22e16%22%7D%7D%2C%22linkedSegmentationLayer%22:%7B%22skeleton_id%22:%22seg_m343%22%7D%2C%22filterBySegmentation%22:%5B%22skeleton_id%22%5D%2C%22name%22:%22segclr_m343%22%7D%5D%2C%22showSlices%22:false%2C%22selectedLayer%22:%7B%22row%22:1%2C%22size%22:353%2C%22visible%22:true%2C%22layer%22:%22seg_m343%22%7D%2C%22layout%22:%7B%22type%22:%22xy-3d%22%2C%22orthographicProjection%22:true%7D%2C%22selection%22:%7B%22row%22:2%2C%22visible%22:false%7D%7D\">mouse</a> views in <a href=\"https://ai.googleblog.com/2019/08/an-interactive-automated-3d.html\">Neuroglancer</a>). By reducing complex microscopy data to rich and compact embedding representations, SegCLR opens many novel avenues for biological insight, and may serve as a link to <a href=\"https://www.cell.com/cell/pdf/S0092-8674(22)00783-8.pdf\">complementary modalities</a> for high-dimensional characterization at the cellular and subcellular levels, such as <a href=\"https://www.nature.com/articles/s41592-020-01033-y\">spatially-resolved transcriptomics</a>. </p>"
}