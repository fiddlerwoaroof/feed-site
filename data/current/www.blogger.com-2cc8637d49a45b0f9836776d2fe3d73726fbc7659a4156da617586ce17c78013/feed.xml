<?xml version="1.0" encoding="UTF-8" standalone="no"?><?xml-stylesheet  href="http://www.blogger.com/styles/atom.css" type="text/css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>tag:blogger.com,1999:blog-8474926331452026626</id><updated>2022-11-16T20:00:41.849-08:00</updated><category term="Machine Learning"/><category term="Deep Learning"/><category term="Computer Vision"/><category term="Google Brain"/><category term="Natural Language Processing"/><category term="open source"/><category term="Publications"/><category term="Research"/><category term="TensorFlow"/><category term="Machine Perception"/><category term="Education"/><category term="Natural Language Understanding"/><category term="conference"/><category term="datasets"/><category term="Neural Networks"/><category term="University Relations"/><category term="conferences"/><category term="Reinforcement Learning"/><category term="Robotics"/><category term="Health"/><category term="AI"/><category term="NLP"/><category term="CVPR"/><category term="Research Awards"/><category term="Algorithms"/><category term="Computational Photography"/><category term="Speech"/><category term="Computer Science"/><category term="MOOC"/><category term="Machine Intelligence"/><category term="Quantum Computing"/><category term="On-device Learning"/><category term="Machine Translation"/><category term="AI for Social Good"/><category term="ICLR"/><category term="Image Classification"/><category term="Multimodal Learning"/><category term="Pixel"/><category term="Visualization"/><category term="YouTube"/><category term="HCI"/><category term="Self-Supervised Learning"/><category term="Hardware"/><category term="Security and Privacy"/><category term="accessibility"/><category term="optimization"/><category term="AutoML"/><category term="Android"/><category term="Audio"/><category term="Awards"/><category term="NeurIPS"/><category term="Quantum AI"/><category term="TPU"/><category term="ACL"/><category term="EMNLP"/><category term="ICML"/><category term="Information Retrieval"/><category term="Image Processing"/><category term="ML"/><category term="Physics"/><category term="Structured Data"/><category term="TTS"/><category term="Google Accelerated Science"/><category term="ML Fairness"/><category term="Search"/><category term="Speech Recognition"/><category term="Google Translate"/><category term="Graph Mining"/><category term="User Experience"/><category term="distributed systems"/><category term="video"/><category term="ACM"/><category term="Automatic Speech Recognition"/><category term="Earth Engine"/><category term="Google Maps"/><category term="K-12"/><category term="Video Analysis"/><category term="statistics"/><category term="Chemistry"/><category term="Collaboration"/><category term="DeepMind"/><category term="Diversity"/><category term="Vision Research"/><category term="Voice Search"/><category term="ph.d. fellowship"/><category term="Cloud Computing"/><category term="Environment"/><category term="Interspeech"/><category term="NIPS"/><category term="Software"/><category term="Supervised Learning"/><category term="UI"/><category term="data science"/><category term="grants"/><category term="market algorithms"/><category term="Compression"/><category term="Faculty Summit"/><category term="Google Cloud Platform"/><category term="Google Genomics"/><category term="ICCV"/><category term="Machine Hearing"/><category term="Semi-supervised Learning"/><category term="Translate"/><category term="crowd-sourcing"/><category term="Acoustic Modeling"/><category term="Art"/><category term="Augmented Reality"/><category term="Course Builder"/><category term="Google Photos"/><category term="Google+"/><category term="PhD Fellowship"/><category term="Recommender Systems"/><category term="Systems"/><category term="Unsupervised Learning"/><category term="WWW"/><category term="renewable energy"/><category term="Computational Imaging"/><category term="Data Discovery"/><category term="Europe"/><category term="Expander"/><category term="Fusion Tables"/><category term="Google Books"/><category term="Moore's Law"/><category term="Ngram"/><category term="Semantic Models"/><category term="Social Networks"/><category term="Year in Review"/><category term="schema.org"/><category term="API"/><category term="App Engine"/><category term="Gmail"/><category term="Google Play Apps"/><category term="High Dynamic Range Imaging"/><category term="Image Annotation"/><category term="India"/><category term="Internet of Things"/><category term="Kaggle"/><category term="NAACL"/><category term="Networks"/><category term="Optical Character Recognition"/><category term="Virtual Reality"/><category term="ads"/><category term="economics"/><category term="internationalization"/><category term="publication"/><category term="resource optimization"/><category term="search ads"/><category term="wikipedia"/><category term="Adaptive Data Analysis"/><category term="Africa"/><category term="App Inventor"/><category term="China"/><category term="DeepDream"/><category term="EMEA"/><category term="Exacycle"/><category term="Gboard"/><category term="Google Docs"/><category term="Google Drive"/><category term="Google Science Fair"/><category term="Google Sheets"/><category term="Graph"/><category term="Inbox"/><category term="KDD"/><category term="Keyboard Input"/><category term="Labs"/><category term="Low-Light Photography"/><category term="MapReduce"/><category term="Policy"/><category term="Proposals"/><category term="Responsible AI"/><category term="Style Transfer"/><category term="TensorBoard"/><category term="VLDB"/><category term="electronics"/><category term="osdi"/><category term="patents"/><category term="trends"/><category term="Android Wear"/><category term="April Fools"/><category term="Australia"/><category term="BigQuery"/><category term="Biology"/><category term="Cantonese"/><category term="Chrome"/><category term="Conservation"/><category term="Data Center"/><category term="ECCV"/><category term="Electronic Commerce and Algorithms"/><category term="Encryption"/><category term="Entity Salience"/><category term="Faculty Institute"/><category term="Flu Trends"/><category term="Google Trips"/><category term="Google Voice Search"/><category term="Government"/><category term="ICSE"/><category term="IPython"/><category term="Journalism"/><category term="Klingon"/><category term="Korean"/><category term="Linear Optimization"/><category term="Magenta"/><category term="Market Research"/><category term="Mixed Reality"/><category term="Network Management"/><category term="Nexus"/><category term="Peer Review"/><category term="PhotoScan"/><category term="PiLab"/><category term="Professional Development"/><category term="Public Data Explorer"/><category term="SIGCOMM"/><category term="SIGMOD"/><category term="Site Reliability Engineering"/><category term="Sound Search"/><category term="TV"/><category term="UNIX"/><category term="Visiting Faculty"/><category term="Wiki"/><category term="adsense"/><category term="adwords"/><category term="correlate"/><category term="entities"/><category term="gamification"/><category term="jsm"/><category term="jsm2011"/><category term="localization"/><category term="materials science"/><category term="operating systems"/><category term="osdi10"/><title type="text">Google AI Blog</title><subtitle type="html">The latest news from Google AI.</subtitle><link href="http://ai.googleblog.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default?alt=atom&amp;redirect=false" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/" rel="alternate" type="text/html"><link href="http://pubsubhubbub.appspot.com/" rel="hub"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default?alt=atom&amp;start-index=26&amp;max-results=25&amp;redirect=false" rel="next" type="application/atom+xml"><author><name>ewood</name><uri>http://www.blogger.com/profile/12341551220176883769</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><generator uri="http://www.blogger.com" version="7.00">Blogger</generator><openSearch:totalResults>1148</openSearch:totalResults><openSearch:startIndex>1</openSearch:startIndex><openSearch:itemsPerPage>25</openSearch:itemsPerPage><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-5847621573881286256</id><published>2022-11-16T11:32:00.005-08:00</published><updated>2022-11-16T14:36:01.980-08:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="NeurIPS"/><title type="text">Mixture-of-Experts with Expert Choice Routing</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Yanqi Zhou, Research Scientist, Google Research Brain Team&lt;/span&gt; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhChCBacAgwbusy1z7uSUrOH57-cKAmiPxR4UN60ohiPId91XlgMe_RI1ZMxehi_MsbcZPU7P8CacBmLOk4gusgq6JRH-speCQsditvXpqiHlzN3qUFHoeyX4IYD-PWj3Rw868wbAz8vuLOI5P8S99jKd6Cei-CP-J1IhATQBnKrEzKTLmwnBZYVvRj7Q/s1040/image2.jpg&quot; style=&quot;display: none;&quot; /&gt; &lt;p&gt;The capacity of a neural network to absorb information is limited by the number of its parameters, and as a consequence, finding more effective ways to increase model parameters has become a trend in deep learning research. &lt;a href=&quot;https://arxiv.org/abs/1701.06538&quot;&gt;Mixture-of-experts&lt;/a&gt; (MoE), a type of conditional computation where parts of the network are activated on a per-example basis, has been proposed as a way of dramatically increasing model capacity without a proportional increase in computation. In sparsely-activated variants of MoE models (e.g., &lt;a href=&quot;https://arxiv.org/abs/2101.03961&quot;&gt;Switch Transformer&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2112.06905&quot;&gt;GLaM&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2106.05974&quot;&gt;V-MoE&lt;/a&gt;), a subset of experts is selected on a per-token or per-example basis, thus creating sparsity in the network. Such models have demonstrated better scaling in multiple domains and better retention capability in a continual learning setting (e.g., &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/papers/Aljundi_Expert_Gate_Lifelong_CVPR_2017_paper.pdf&quot;&gt;Expert Gate&lt;/a&gt;). However, a poor expert routing strategy can cause certain experts to be under-trained, leading to an expert being under or over-specialized.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2202.09368&quot;&gt;Mixture-of-Experts with Expert Choice Routing&lt;/a&gt;”, presented at &lt;a href=&quot;https://nips.cc/Conferences/2022&quot;&gt;NeurIPS 2022&lt;/a&gt;, we introduce a novel MoE routing algorithm called Expert Choice (EC). We discuss how this novel approach can achieve optimal load balancing in an MoE system while allowing heterogeneity in token-to-expert mapping. Compared to token-based routing and other routing methods in traditional MoE networks, EC demonstrates very strong training efficiency and downstream task scores. Our method resonates with one of the vision for &lt;a href=&quot;https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/&quot;&gt;Pathways&lt;/a&gt;, which is to enable heterogeneous mixture-of-experts via Pathways MPMD (multi program, multi data) support.&lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Overview of MoE Routing&lt;/h2&gt;&lt;p&gt;MoE operates by adopting a number of experts, each as a sub-network, and activating only one or a few experts for each input token. A gating network must be chosen and optimized in order to route each token to the most suited expert(s). Depending on how tokens are mapped to experts, MoE can be sparse or dense. Sparse MoE only selects a subset of experts when routing each token, reducing computational cost as compared to a dense MoE. For example, recent work has implemented sparse routing via &lt;em&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.06363&quot;&gt;k-means clustering&lt;/a&gt;&lt;/em&gt;, &lt;a href=&quot;https://arxiv.org/abs/2106.04426&quot;&gt;linear assignment to maximize token-expert affinities&lt;/a&gt;, or &lt;a href=&quot;https://arxiv.org/abs/2106.04426&quot;&gt;hashing&lt;/a&gt;. Google also recently announced &lt;a href=&quot;https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html&quot;&gt;GLaM&lt;/a&gt; and &lt;a href=&quot;https://ai.googleblog.com/2022/01/scaling-vision-with-sparse-mixture-of.html&quot;&gt;V-MoE&lt;/a&gt;, both of which advance the state of the art in natural language processing and computer vision via sparsely gated MoE with top-&lt;em&gt;k&lt;/em&gt; token routing, demonstrating better performance scaling with sparsely activated MoE layers. Many of these prior works used a &lt;em&gt;token choice&lt;/em&gt; routing strategy in which the routing algorithm picks the best one or two experts for each token. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbsjq4iseHi-Azxcj0irBjGkma0yd4geSPPombnJSdd5dyzTguUU2pdFfZu4G38G4F4TiymUOaIkQnXGVAix5x8wF3-9Ov3NJwWaEZNvJY84CWCgU5MbUYI_DjKa_BvalTHu3eyfCJGR89UqwskKngsppDy94Gahz3HAoKLh2vmh-Jzb7ZedRI91OwFw/s960/image1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;509&quot; data-original-width=&quot;960&quot; height=&quot;339&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbsjq4iseHi-Azxcj0irBjGkma0yd4geSPPombnJSdd5dyzTguUU2pdFfZu4G38G4F4TiymUOaIkQnXGVAix5x8wF3-9Ov3NJwWaEZNvJY84CWCgU5MbUYI_DjKa_BvalTHu3eyfCJGR89UqwskKngsppDy94Gahz3HAoKLh2vmh-Jzb7ZedRI91OwFw/w640-h339/image1.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Token Choice Routing. The routing algorithm picks the top-1 or top-2 experts with highest affinity scores for each token. The affinity scores can be trained together with model parameters.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The independent token choice approach often leads to an imbalanced load of experts and under-utilization. In order to mitigate this, previous sparsely gated networks introduced additional auxiliary losses as &lt;a href=&quot;https://en.wikipedia.org/wiki/Regularization_(mathematics)&quot;&gt;regularization&lt;/a&gt; to prevent too many tokens being routed to a single expert, but the effectiveness was limited. As a result, token choice routings need to overprovision expert capacity by a significant margin (2x–8x of the calculated capacity) to avoid dropping tokens when there is a buffer overflow.   &lt;/p&gt;&lt;p&gt;In addition to load imbalance, most prior works allocate a fixed number of experts to each token using a top-&lt;em&gt;k&lt;/em&gt; function, regardless of the relative importance of different tokens. We argue that different tokens should be received by a variable number of experts, conditioned on token importance or difficulty.  &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Expert Choice Routing&lt;/h2&gt;&lt;p&gt;To address the above issues, we propose a heterogeneous MoE that employs the &lt;a href=&quot;https://arxiv.org/abs/2202.09368&quot;&gt;expert choice&lt;/a&gt; routing method illustrated below. Instead of having tokens select the top-&lt;em&gt;k&lt;/em&gt; experts, the experts with predetermined buffer capacity are assigned to the top-&lt;em&gt;k&lt;/em&gt; tokens. This method guarantees even load balancing, allows a variable number of experts for each token, and achieves substantial gains in training efficiency and downstream performance. EC routing speeds up training convergence by over 2x in an 8B/64E (8 billion activated parameters, 64 experts) model, compared to the top-1 and top-2 gating counterparts in &lt;a href=&quot;https://arxiv.org/abs/2101.03961&quot;&gt;Switch Transformer&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2006.16668&quot;&gt;GShard&lt;/a&gt;, and &lt;a href=&quot;https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html&quot;&gt;GLaM&lt;/a&gt;. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicHrweArY_U7OQVacpijOJ5Gm8L8D58anL2OcQRZaZ21JY5Qxr_V5xyxt1vWDuLbI8CaMc2tbDTkiiv8uKmvFPN0CKyeXUa7xPNVREVYxH-DNs0-yEawWhwwz44QmSBzTP6fwIAgiEUlcSddFGaimPsw1Pqy_FTS7BnyXjOj5DoxLCWQMufqzYfPtvDA/s960/image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;570&quot; data-original-width=&quot;960&quot; height=&quot;380&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicHrweArY_U7OQVacpijOJ5Gm8L8D58anL2OcQRZaZ21JY5Qxr_V5xyxt1vWDuLbI8CaMc2tbDTkiiv8uKmvFPN0CKyeXUa7xPNVREVYxH-DNs0-yEawWhwwz44QmSBzTP6fwIAgiEUlcSddFGaimPsw1Pqy_FTS7BnyXjOj5DoxLCWQMufqzYfPtvDA/w640-h380/image2.jpg&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Expert Choice Routing. Experts with predetermined buffer capacity are assigned top-&lt;em&gt;k&lt;/em&gt; tokens, thus guaranteeing even load balancing. Each token can be received by a variable number of experts.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;In EC routing, we set expert capacity&lt;em&gt; k&lt;/em&gt; as the average tokens per expert in a batch of input sequences multiplied by a &lt;em&gt;capacity factor&lt;/em&gt;, which determines the average number of experts that can be received by each token. To learn the token-to-expert affinity, our method produces a token-to-expert score matrix that is used to make routing decisions. The score matrix indicates the likelihood of a given token in a batch of input sequences being routed to a given expert.  &lt;/p&gt;&lt;p&gt;Similar to Switch Transformer and GShard, we apply an MoE and gating function in the dense &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;&gt;feedforward&lt;/a&gt; (FFN) layer, as it is the most computationally expensive part of a Transformer-based network. After producing the token-to-expert score matrix, a top-&lt;em&gt;k&lt;/em&gt; function is applied along the token dimension for each expert to pick the most relevant tokens. A permutation function is then applied based on the generated indexes of the token, to create a hidden value with an additional expert dimension. The data is split across multiple experts such that all experts can execute the same computational kernel concurrently on a subset of tokens. Because a fixed expert capacity can be determined, we no longer overprovision expert capacity due to load imbalancing, thus significantly reducing training and inference step time by around 20% compared to GLaM.  &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Evaluation&lt;/h2&gt;&lt;p&gt;To illustrate the effectiveness of Expert Choice routing, we first look at training efficiency and convergence. We use EC with a capacity factor of 2 (EC-CF2) to match the activated parameter size and computational cost on a per-token basis to GShard top-2 gating and run both for a fixed number of steps. EC-CF2 reaches the same perplexity as GShard top-2 in less than half the steps and, in addition, we find that each GShard top-2 step is 20% slower than our method. &lt;/p&gt;&lt;p&gt;We also scale the number of experts while fixing the expert size to 100M parameters for both EC and GShard top-2 methods. We find that both work well in terms of perplexity on the evaluation dataset during pre-training — having more experts consistently improves training perplexity. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0AgndpPfn58cV1_jNZceDLClnlmbxBGW_jQME3uqu_wnawotdKtJor4B0JFOaN_ovZqITmkIW25lUi37eODeoVDZggqXXMMid2tbFZ4pcMs6WRLVpdPTQncyTBcEVzb3SH9FSSuxDIXqkaMe61sAddIFORzxejv_5YAcSgNUf5rZrJJJwdShH36vUsg/s1134/Screenshot%202022-11-16%204.29.47%20PM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1134&quot; data-original-width=&quot;950&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0AgndpPfn58cV1_jNZceDLClnlmbxBGW_jQME3uqu_wnawotdKtJor4B0JFOaN_ovZqITmkIW25lUi37eODeoVDZggqXXMMid2tbFZ4pcMs6WRLVpdPTQncyTBcEVzb3SH9FSSuxDIXqkaMe61sAddIFORzxejv_5YAcSgNUf5rZrJJJwdShH36vUsg/w536-h640/Screenshot%202022-11-16%204.29.47%20PM.png&quot; width=&quot;536&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Evaluation results on training convergence: EC routing yields 2x faster convergence at 8B/64E scale compared to top-2 gating used in GShard and GLaM (&lt;b&gt;top&lt;/b&gt;). EC training perplexity scales better with the scaling of number of experts (&lt;b&gt;bottom&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;To validate whether improved perplexity directly translates to better performance in downstream tasks, we perform fine-tuning on 11 selected tasks from &lt;a href=&quot;https://gluebenchmark.com/&quot;&gt;GLUE&lt;/a&gt; and &lt;a href=&quot;https://super.gluebenchmark.com/&quot;&gt;SuperGLUE&lt;/a&gt;. We compare three MoE methods including Switch Transformer top-1 gating (ST Top-1), GShard top-2 gating (GS Top-2) and a version of our method (EC-CF2) that matches the activated parameters and computational cost of GS Top-2. The EC-CF2 method consistently outperforms the related methods and yields an average accuracy increase of more than 2% in a large 8B/64E setting. Comparing our 8B/64E model against its dense counterpart, our method achieves better fine-tuning results, increasing the average score by 3.4 points. &lt;/p&gt;&lt;p&gt;Our empirical results indicate that capping the number of experts for each token hurts the fine-tuning score by 1 point on average. This study confirms that allowing a variable number of experts per token is indeed helpful. On the other hand, we compute statistics on token-to-expert routing, particularly on the ratio of tokens that have been routed to a certain number of experts. We find that a majority of tokens have been routed to one or two experts while 23% have been routed to three or four experts and only about 3% tokens have been routed to more than four experts, thus verifying our hypothesis that expert choice routing learns to allocate a variable number of experts to tokens. &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Final Thoughts&lt;/h2&gt;&lt;p&gt;We propose a new routing method for sparsely activated mixture-of-experts models. This method addresses load imbalance and under-utilization of experts in conventional MoE methods, and enables the selection of different numbers of experts for each token. Our model demonstrates more than 2x training efficiency improvement when compared to the state-of-the-art GShard and Switch Transformer models, and achieves strong gains when fine-tuning on 11 datasets in the GLUE and SuperGLUE benchmark. &lt;/p&gt;&lt;p&gt;Our approach for expert choice routing enables heterogeneous MoE with straightforward algorithmic innovations. We hope that this may lead to more advances in this space at both the application and system levels.   &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;Many collaborators across google research supported this work. We particularly thank Nan Du, Andrew Dai, Yanping Huang, and Zhifeng Chen for the initial ground work on MoE infrastructure and Tarzan datasets. We greatly appreciate Hanxiao Liu and Quoc Le for contributing the initial ideas and discussions. Tao Lei, Vincent Zhao, Da Huang, Chang Lan, Daiyi Peng, and Yifeng Lu contributed significantly on implementations and evaluations. Claire Cui, James Laudon, Martin Abadi, and Jeff Dean provided invaluable feedback and resource support. &lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/5847621573881286256/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5847621573881286256" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5847621573881286256" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html" rel="alternate" title="Mixture-of-Experts with Expert Choice Routing" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhChCBacAgwbusy1z7uSUrOH57-cKAmiPxR4UN60ohiPId91XlgMe_RI1ZMxehi_MsbcZPU7P8CacBmLOk4gusgq6JRH-speCQsditvXpqiHlzN3qUFHoeyX4IYD-PWj3Rw868wbAz8vuLOI5P8S99jKd6Cei-CP-J1IhATQBnKrEzKTLmwnBZYVvRj7Q/s72-c/image2.jpg" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-732464377672747583</id><published>2022-11-10T10:05:00.005-08:00</published><updated>2022-11-14T08:58:44.554-08:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="NLP"/><category scheme="http://www.blogger.com/atom/ns#" term="Publications"/><category scheme="http://www.blogger.com/atom/ns#" term="Research"/><title type="text">Characterizing Emergent Phenomena in Large Language Models</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Jason Wei and Yi Tay, Research Scientists, Google Research, Brain Team&lt;/span&gt;  &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJoQyQGqe67BekuWDn30rdtn9W1Lf7RvMeUKtoK2aoYziWIDOMAitiRYViKdyK-KYexgbDxExs3rxw2v306JK-tyZLNd5_6Y5LanpyDux6oiH7NhExBXwPUnJnUnrHUyjF5Y9k_ekMu6y0ZUPNbbn7tkQ7NVOXTMz2w2-3oDs_Pj-Ll64LyB4tkWLVqQ/s732/image1.png&quot; style=&quot;display: none;&quot; /&gt; &lt;p&gt;The field of natural language processing (NLP) has been revolutionized by language models trained on large amounts of text data. Scaling up the size of language models often leads to improved performance and sample efficiency on a range of downstream NLP tasks. In many cases, the performance of a large language model can be predicted by extrapolating the performance trend of smaller models. For instance, the effect of scale on language model &lt;a href=&quot;https://en.wikipedia.org/wiki/Perplexity&quot;&gt;perplexity&lt;/a&gt; has been empirically shown to span more than &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;seven orders of magnitude&lt;/a&gt;. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;On the other hand, performance for certain other tasks does not improve in a predictable fashion. For example, the &lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;&gt;GPT-3 paper&lt;/a&gt; showed that the ability of language models to perform multi-digit addition has a flat scaling curve (approximately random performance) for models from 100M to 13B parameters, at which point the performance jumped substantially. Given the growing use of language models in NLP research and applications, it is important to better understand abilities such as these that can arise unexpectedly. &lt;/p&gt;&lt;p&gt;In “&lt;a href=&quot;https://openreview.net/forum?id=yzkSU5zdwD&quot;&gt;Emergent Abilities of Large Language Models&lt;/a&gt;,” recently published in the &lt;em&gt;&lt;a href=&quot;https://www.jmlr.org/tmlr/&quot;&gt;Transactions on Machine Learning Research&lt;/a&gt;&lt;/em&gt; (TMLR), we discuss the phenomena of &lt;em&gt;emergent abilities&lt;/em&gt;, which we define as abilities that are not present in small models but are present in larger models. More specifically, we study emergence by analyzing the performance of language models as a function of language model scale, as measured by total &lt;a href=&quot;https://en.wikipedia.org/wiki/Floating-point_arithmetic&quot;&gt;floating point operations&lt;/a&gt; (FLOPs), or how much compute was used to train the language model. However, we also explore emergence as a function of other variables, such as dataset size or number of model parameters (see the paper for full details). Overall, we present dozens of examples of emergent abilities that result from scaling up language models. The existence of such emergent abilities raises the question of whether additional scaling could potentially further expand the range of capabilities of language models. &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Emergent Prompted Tasks&lt;/h2&gt; &lt;p&gt;First we discuss emergent abilities that may arise in prompted tasks. In such tasks, a pre-trained language model is given a prompt for a task framed as next word prediction, and it performs the task by completing the response. Without any further fine-tuning, language models can often perform tasks that were not seen during training.  &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigsk_RT3zBrzSTftNq6czTHYkv3izej5wCEhxNrjnoUrvIPt0aJLsV8s4zIgpnyoPysHobWFhHuzCU-B30AItGMAmYRMEWY_Pp--lLmQ6--oMMWrRciyDDv7qD1zf4Y--i7avr9EHv2nsz4Q7hHTY5-JeXFKHhbUttmVruMd8Py_fqCUtaAKCwHyOF_A/s1288/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;341&quot; data-original-width=&quot;1288&quot; height=&quot;169&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigsk_RT3zBrzSTftNq6czTHYkv3izej5wCEhxNrjnoUrvIPt0aJLsV8s4zIgpnyoPysHobWFhHuzCU-B30AItGMAmYRMEWY_Pp--lLmQ6--oMMWrRciyDDv7qD1zf4Y--i7avr9EHv2nsz4Q7hHTY5-JeXFKHhbUttmVruMd8Py_fqCUtaAKCwHyOF_A/w640-h169/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Example of few-shot prompting on movie review sentiment classification. The model is given one example of a task (classifying a movie review as positive or negative) and then performs the task on an unseen example.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We call a prompted task emergent when it unpredictably surges from random performance to above-random at a specific scale threshold. Below we show three examples of prompted tasks with emergent performance: &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/modified_arithmetic&quot;&gt;multi-step arithmetic&lt;/a&gt;, taking &lt;a href=&quot;https://arxiv.org/abs/2009.03300&quot;&gt;college-level exams&lt;/a&gt;, and &lt;a href=&quot;https://pilehvar.github.io/wic/&quot;&gt;identifying the intended meaning of a word&lt;/a&gt;. In each case, language models perform poorly with very little dependence on model size up to a threshold at which point their performance suddenly begins to excel.  &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhl5PSqGGHMWNxwav2cdB6GaoiHCrKESFwkRXQ6VJmJxVGCjcuQhqJsey9EiCQW6WUKaHDaMCmYj9LGxZaVuU5DpHTh9-Wl0pRzlTybDC2WES0_jSjmyGHcHKku9XZECXceG1TCtH5DNocVj-0PQHTztf_5Zzo7Ijrj8jlT_kClaW72fxzj4-3SQOwtNQ/s1013/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;487&quot; data-original-width=&quot;1013&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhl5PSqGGHMWNxwav2cdB6GaoiHCrKESFwkRXQ6VJmJxVGCjcuQhqJsey9EiCQW6WUKaHDaMCmYj9LGxZaVuU5DpHTh9-Wl0pRzlTybDC2WES0_jSjmyGHcHKku9XZECXceG1TCtH5DNocVj-0PQHTztf_5Zzo7Ijrj8jlT_kClaW72fxzj4-3SQOwtNQ/s16000/image4.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The ability to perform multi-step arithmetic (&lt;strong&gt;left&lt;/strong&gt;), succeed on college-level exams (&lt;strong&gt;middle&lt;/strong&gt;), and identify the intended meaning of a word in context (&lt;strong&gt;right&lt;/strong&gt;) all emerge only for models of sufficiently large scale. The models shown include &lt;a href=&quot;https://arxiv.org/abs/2201.08239&quot;&gt;LaMDA&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;GPT-3&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2112.11446&quot;&gt;Gopher&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;&gt;PaLM&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Performance on these tasks only becomes non-random for models of sufficient scale — for instance, above 10&lt;sup&gt;22&lt;/sup&gt; training FLOPs for the arithmetic and multi-task NLU tasks, and above 10&lt;sup&gt;24&lt;/sup&gt; training FLOPs for the word in context tasks. Note that although the scale at which emergence occurs can be different for different tasks and models, no model showed smooth improvement in behavior on any of these tasks. Dozens of other emergent prompted tasks are listed &lt;a href=&quot;https://openreview.net/forum?id=yzkSU5zdwD&quot;&gt;in our paper&lt;/a&gt;. &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Emergent Prompting Strategies&lt;/h2&gt;  &lt;p&gt;The second class of emergent abilities encompasses &lt;em&gt;prompting strategies&lt;/em&gt; that augment the capabilities of language models. Prompting strategies are broad paradigms for prompting that can be applied to a range of different tasks. They are considered emergent when they fail for small models and can only be used by a sufficiently-large model. &lt;/p&gt;&lt;p&gt;One example of an emergent prompting strategy is called “&lt;a href=&quot;https://twitter.com/Google/status/1525188695875366912&quot;&gt;chain-of-thought prompting&lt;/a&gt;”, for which the model is prompted to generate a series of intermediate steps before giving the final answer. Chain-of-thought prompting enables language models to perform tasks requiring complex reasoning, such as a multi-step math word problem. Notably, models acquire the ability to do chain-of-thought reasoning without being explicitly trained to do so. An example of chain-of-thought prompting is shown in the figure below. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjO6jryw6Oesg8YVxsa2hl2b5FSoBVfzuDou3U9LA9U6cBAIMbV1MZs5ZX5XLMHGg2jd29FPYpabC9hn7PgfC1qLDKMS7sWz6ay8XTKupyB0cB4EHu8ZpRkftQTMP5gFxyXiAPQ-dBscd6-QFEdp_P1qaUADthj0DOZ8zrZb1dBNd6nbzy4tFR-rtjkCw/s793/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;370&quot; data-original-width=&quot;793&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjO6jryw6Oesg8YVxsa2hl2b5FSoBVfzuDou3U9LA9U6cBAIMbV1MZs5ZX5XLMHGg2jd29FPYpabC9hn7PgfC1qLDKMS7sWz6ay8XTKupyB0cB4EHu8ZpRkftQTMP5gFxyXiAPQ-dBscd6-QFEdp_P1qaUADthj0DOZ8zrZb1dBNd6nbzy4tFR-rtjkCw/s16000/image3.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Chain of thought prompting enables sufficiently large models to solve multi-step reasoning problems.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The empirical results of chain-of-thought prompting are shown below. For smaller models, applying chain-of-thought prompting does not outperform standard prompting, for example, when applied to &lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;&gt;GSM8K&lt;/a&gt;, a challenging benchmark of math word problems. However, for large models (10&lt;sup&gt;24&lt;/sup&gt; FLOPs), chain-of-thought prompting substantially improves performance in our tests, reaching a 57% solve rate on GSM8K. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDr6dLlvyfWclgmTuqlzEQ0Ge-x2CUywoKXoozXO5wMJgw5ZERIzqRy59_aDr_P9YOC3XEZ1wFqoPWmGgP26-DvdJUMzHx9-i2Nc8fyDGIwu9s5kYyhDkadS8s4azusiper7nDPk7fgUe4dNM9KVgbQkZoO3AiXQ8-rIJ4CN3YY4US2g3Us-oMNr9gPQ/s732/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;561&quot; data-original-width=&quot;732&quot; height=&quot;306&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDr6dLlvyfWclgmTuqlzEQ0Ge-x2CUywoKXoozXO5wMJgw5ZERIzqRy59_aDr_P9YOC3XEZ1wFqoPWmGgP26-DvdJUMzHx9-i2Nc8fyDGIwu9s5kYyhDkadS8s4azusiper7nDPk7fgUe4dNM9KVgbQkZoO3AiXQ8-rIJ4CN3YY4US2g3Us-oMNr9gPQ/w400-h306/image1.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Chain-of-thought prompting is an emergent ability — it fails to improve performance for small language models, but substantially improves performance for large models. Here we illustrate the difference between standard and chain-of-thought prompting at different scales for two language models, &lt;a href=&quot;https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html&quot;&gt;LaMDA&lt;/a&gt; and &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;&gt;PaLM&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Implications of Emergent Abilities&lt;/h2&gt;  &lt;p&gt;The existence of emergent abilities has a range of implications. For example, because emergent few-shot prompted abilities and strategies are not explicitly encoded in pre-training, researchers may not know the full scope of few-shot prompted abilities of current language models. Moreover, the emergence of new abilities as a function of model scale raises the question of whether further scaling will potentially endow even larger models with new emergent abilities. &lt;/p&gt;&lt;p&gt;Identifying emergent abilities in large language models is a first step in understanding such phenomena and their potential impact on future model capabilities. Why does scaling unlock emergent abilities? Because computational resources are expensive, can emergent abilities be unlocked via other methods without increased scaling (e.g., better model architectures or training techniques)? Will new real-world applications of language models become unlocked when certain abilities emerge? Analyzing and understanding the behaviors of language models, including emergent behaviors that arise from scaling, is an important research question as the field of NLP continues to grow.  &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;  &lt;p&gt;&lt;em&gt;It was an honor and privilege to work with Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/732464377672747583/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/732464377672747583" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/732464377672747583" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html" rel="alternate" title="Characterizing Emergent Phenomena in Large Language Models" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJoQyQGqe67BekuWDn30rdtn9W1Lf7RvMeUKtoK2aoYziWIDOMAitiRYViKdyK-KYexgbDxExs3rxw2v306JK-tyZLNd5_6Y5LanpyDux6oiH7NhExBXwPUnJnUnrHUyjF5Y9k_ekMu6y0ZUPNbbn7tkQ7NVOXTMz2w2-3oDs_Pj-Ll64LyB4tkWLVqQ/s72-c/image1.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-1592161586308962338</id><published>2022-11-09T14:16:00.003-08:00</published><updated>2022-11-09T14:28:43.079-08:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Self-Supervised Learning"/><title type="text">Multi-layered Mapping of Brain Tissue via Segmentation Guided Contrastive Learning</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Peter H. Li, Research Scientist, and Sven Dorkenwald, Student Researcher, Connectomics at Google&lt;/span&gt; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGcjzK2k-GUyCJ3tjBj6RkXS3hblnCdrP0-QFl9tvhGZON-IqTSNlB6jNbCa2aMr561M_Qc1gIXNZAArnk0uZQw8RVupbfHUxjMY9GtiChJTdBR6yBPmhAY80xmnWaVSgAueOMu368GuI_mFoK4SKDYQMBUIPcny5iDjvgFimZj8NWabLxy4V-QxUoCw/s320/image5.gif&quot; style=&quot;display: none;&quot; /&gt; &lt;p&gt;Mapping the wiring and firing activity of the human brain is fundamental to deciphering how we think — how we sense the world, learn, decide, remember, and create — as well as what issues can arise in brain disease or dysfunction. Recent efforts have delivered publicly available brain maps (high-resolution 3D mapping of brain cells and their connectivities) at unprecedented quality and scale, such as &lt;a href=&quot;https://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html&quot;&gt;H01, a 1.4 petabyte nanometer-scale digital reconstruction&lt;/a&gt; of a sample of human brain tissue from Harvard / Google, and the &lt;a href=&quot;https://www.microns-explorer.org/cortical-mm3&quot;&gt;cubic millimeter mouse cortex dataset&lt;/a&gt; from our colleagues at the &lt;a href=&quot;https://www.microns-explorer.org/team&quot;&gt;MICrONS consortium&lt;/a&gt;. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt; &lt;p&gt;To interpret brain maps at this scale requires multiple layers of analysis, including the identification of &lt;a href=&quot;https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/gallery/ei_point.json&quot;&gt;synaptic connections&lt;/a&gt;, &lt;a href=&quot;https://h01-dot-neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B3.3e-8%2C%22m%22%5D%7D%2C%22position%22:%5B253279%2C187478%2C1836.5%5D%2C%22crossSectionScale%22:9.65671770692589%2C%22projectionScale%22:63363.54435174225%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/4nm_raw%22%2C%22tab%22:%22source%22%2C%22name%22:%224nm%20EM%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%7B%22url%22:%22precomputed://gs://h01-release/data/20210601/c2%22%2C%22subsources%22:%7B%22default%22:true%2C%22bounds%22:true%2C%22properties%22:true%2C%22mesh%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22tab%22:%22source%22%2C%22meshSilhouetteRendering%22:2%2C%22segments%22:%5B%224737616850%22%5D%2C%22colorSeed%22:3673823978%2C%22name%22:%22c2%20segmentation%22%7D%2C%7B%22type%22:%22annotation%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/c2/subcompartments/annotations%22%2C%22tab%22:%22rendering%22%2C%22ignoreNullSegmentFilter%22:false%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20switch%20%28prop_class_label%28%29%29%20%7B%5Cn%20%20case%200:%20%20//%20axon%5Cn%20%20%20%20setColor%28vec3%280%2C%200%2C%201%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%201:%20%20//%20dendrite%5Cn%20%20%20%20setColor%28vec3%281%2C%200%2C%200%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%202:%20%20//%20astrocyte%5Cn%20%20%20%20setColor%28vec3%280%2C%201%2C%200%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%203:%20%20//%20soma%5Cn%20%20%20%20setColor%28vec3%281%2C%201%2C%201%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%204:%20%20//%20cilium%5Cn%20%20%20%20setColor%28vec3%280.5%2C%200.5%2C%200%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%205:%20%20//%20AIS%5Cn%20%20%20%20setColor%28vec3%280.5%2C%200.5%2C%201%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20%20%20%5Cn%20%20case%201000:%20%20//%20myelinated%20axon%5Cn%20%20case%201001:%5Cn%20%20%20%20setColor%28vec3%281%2C%200.25%2C%200.75%29%29%3B%5Cn%20%20%20%20break%3B%5Cn%20%20case%201004:%20%20//%20fragments%5Cn%20%20case%201005:%5Cn%20%20default:%5Cn%20%20%20%20discard%3B%5Cn%20%20%7D%5Cn%7D%5Cn%22%2C%22linkedSegmentationLayer%22:%7B%22skeleton_id%22:%22c2%20segmentation%22%7D%2C%22filterBySegmentation%22:%5B%22skeleton_id%22%5D%2C%22name%22:%226-class%20annotations%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/c2/subcompartments%22%2C%22tab%22:%22source%22%2C%22segmentColors%22:%7B%22100%22:%22#0000ff%22%2C%22101%22:%22#ff0000%22%2C%22102%22:%22#00ff00%22%2C%22103%22:%22#ffffff%22%2C%22104%22:%22#7f7f00%22%2C%22105%22:%22#7f7fff%22%2C%221100%22:%22#ff3fbf%22%2C%221101%22:%22#ff3fbf%22%2C%221102%22:%22#000000%22%2C%221103%22:%22#000000%22%2C%221104%22:%22#ff3fbf%22%2C%221105%22:%22#ff3fbf%22%7D%2C%22name%22:%226-class%20render%22%7D%5D%2C%22showAxisLines%22:false%2C%22showSlices%22:false%2C%22selectedLayer%22:%7B%22layer%22:%226-class%20render%22%7D%2C%22layout%22:%22xy-3d%22%7D&quot;&gt;cellular subcompartments&lt;/a&gt;, and &lt;a href=&quot;https://h01-dot-neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B3.3e-8%2C%22m%22%5D%7D%2C%22position%22:%5B379548.5%2C137610.125%2C2660.5%5D%2C%22crossSectionScale%22:5.776802800212544%2C%22projectionScale%22:261570.24038807175%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/4nm_raw%22%2C%22tab%22:%22source%22%2C%22name%22:%224nm%20EM%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%5B%7B%22url%22:%22precomputed://gs://h01-release/data/20210601/c3%22%2C%22subsources%22:%7B%22default%22:true%2C%22bounds%22:true%2C%22properties%22:true%2C%22mesh%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22precomputed://gs://lichtman-h01-49eee972005c8846803ef58fbd36e049/goog14r0s5c3_new_props/segment_properties%22%5D%2C%22panels%22:%5B%7B%22flex%22:1.55%2C%22size%22:366%2C%22tab%22:%22segments%22%7D%5D%2C%22colorSeed%22:4270253886%2C%22name%22:%22c3%20segmentation%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/layers%22%2C%22tab%22:%22source%22%2C%22selectedAlpha%22:0.3%2C%22objectAlpha%22:0.2%2C%22segments%22:%5B%221%22%2C%222%22%2C%223%22%2C%224%22%2C%225%22%2C%226%22%2C%227%22%5D%2C%22segmentQuery%22:%221%2C2%2C3%2C4%2C5%2C6%2C7%22%2C%22name%22:%22cortical%20layers%22%2C%22visible%22:false%7D%5D%2C%22showSlices%22:false%2C%22prefetch%22:false%2C%22selectedLayer%22:%7B%22row%22:1%2C%22flex%22:1.55%2C%22size%22:309%2C%22layer%22:%224nm%20EM%22%7D%2C%22layout%22:%7B%22type%22:%22xy-3d%22%2C%22orthographicProjection%22:true%7D%2C%22selection%22:%7B%22row%22:2%2C%22flex%22:0.45%2C%22size%22:309%2C%22visible%22:false%7D%7D&quot;&gt;cell types&lt;/a&gt;. Machine learning and computer vision technology have played a central role in enabling these analyses, but deploying such systems is still a laborious process, requiring hours of manual ground truth labeling by expert annotators and significant computational resources. Moreover, some important tasks, such as identifying the cell type from only a small fragment of &lt;a href=&quot;https://en.wikipedia.org/wiki/Axon&quot;&gt;axon&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Dendrite&quot;&gt;dendrite&lt;/a&gt;, can be challenging even for human experts, and have not yet been effectively automated. &lt;/p&gt;&lt;p&gt;Today, in “&lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2022.03.29.486320&quot;&gt;Multi-Layered Maps of Neuropil with Segmentation-Guided Contrastive Learning&lt;/a&gt;”, we are announcing Segmentation-Guided Contrastive Learning of Representations (SegCLR), a method for training rich, generic representations of cellular morphology (the cell’s shape) and ultrastructure (the cell’s internal structure) without laborious manual effort. SegCLR produces compact vector representations (i.e., embeddings) that are applicable across diverse downstream tasks (e.g., local classification of cellular subcompartments, unsupervised clustering), and are even able to identify cell types from only small fragments of a cell. We trained SegCLR on both the H01 human cortex dataset and the MICrONS mouse cortex dataset, and we are releasing the &lt;a href=&quot;https://h01-release.storage.googleapis.com/embeddings.html&quot;&gt;resulting embedding vectors&lt;/a&gt;, about 8 billion in total, for researchers to explore. &lt;/p&gt; &lt;br /&gt;&lt;br /&gt; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;&gt; &lt;source src=&quot;https://storage.googleapis.com/h01-release/data/20220326/c3/embeddings/segclr_14.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt; &lt;/video&gt;&lt;/div&gt;&lt;br /&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;From brain cells segmented out of a 3D block of tissue, SegCLR embeddings capture cellular morphology and ultrastructure and can be used to distinguish cellular subcompartments (e.g., &lt;a href=&quot;https://en.wikipedia.org/wiki/Dendritic_spine&quot;&gt;dendritic spine&lt;/a&gt; versus dendrite shaft) or cell types (e.g., &lt;a href=&quot;https://en.wikipedia.org/wiki/Pyramidal_cell&quot;&gt;pyramidal&lt;/a&gt; versus &lt;a href=&quot;https://en.wikipedia.org/wiki/Microglia&quot;&gt;microglia&lt;/a&gt; cell).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Representing Cellular Morphology and Ultrastructure&lt;/h2&gt;&lt;p&gt;SegCLR builds on &lt;a href=&quot;https://ai.googleblog.com/2021/10/self-supervised-learning-advances.html&quot;&gt;recent advances in self-supervised contrastive learning&lt;/a&gt;. We use a standard deep network architecture to encode inputs comprising local 3D blocks of electron &lt;a href=&quot;https://en.wikipedia.org/wiki/Microscopy&quot;&gt;microscopy&lt;/a&gt; data (about 4 micrometers on a side) into 64-dimensional embedding vectors. The network is trained via a &lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&quot;&gt;contrastive loss&lt;/a&gt; to map semantically related inputs to similar coordinates in the embedding space. This is close to the &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;popular SimCLR setup&lt;/a&gt;, except that we also require an &lt;a href=&quot;https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html&quot;&gt;instance segmentation&lt;/a&gt; of the volume (tracing out individual cells and cell fragments), which we use in two important ways. &lt;/p&gt;&lt;p&gt;First, the input 3D electron microscopy data are explicitly masked by the segmentation, forcing the network to focus only on the central cell within each block. Second, we leverage the segmentation to automatically define which inputs are semantically related: positive pairs for the contrastive loss are drawn from nearby locations on the same segmented cell and trained to have similar representations, while inputs drawn from different cells are trained to have dissimilar representations. Importantly, publicly available automated segmentations of the human and mouse datasets were sufficiently accurate to train SegCLR without requiring laborious review and correction by human experts. &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW9HNCvQvWIHGSf4m4kcgmSNmyqYnQUjm4clwOg9zoJz6NOkR2_x68HEdep2srVXkWXVz2QS6-PdwQVbV0M25PNUSEm7IhiAIWCkfwTYmUoXOW3A7IrZ__CyH_-T18IKkU1hbI5G5YsKm4DDxX2itFvKielNwqgUnEFmlXWjZvioTaqTPLyf3ueeaYnQ/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1708&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW9HNCvQvWIHGSf4m4kcgmSNmyqYnQUjm4clwOg9zoJz6NOkR2_x68HEdep2srVXkWXVz2QS6-PdwQVbV0M25PNUSEm7IhiAIWCkfwTYmUoXOW3A7IrZ__CyH_-T18IKkU1hbI5G5YsKm4DDxX2itFvKielNwqgUnEFmlXWjZvioTaqTPLyf3ueeaYnQ/s16000/image1.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;SegCLR is trained to represent rich cellular features without manual labeling. &lt;b&gt;Top&lt;/b&gt;: The SegCLR architecture maps local masked 3D views of electron microscopy data to embedding vectors. Only the microscopy volume and a draft automated instance segmentation are required. &lt;b&gt;Bottom&lt;/b&gt;: The segmentation is also used to define positive versus negative example pairs, whose representations are pushed closer together (&lt;b&gt;positives&lt;/b&gt;, &lt;b&gt;blue arrows&lt;/b&gt;) or further apart (&lt;b&gt;negatives&lt;/b&gt;, &lt;b&gt;red arrows&lt;/b&gt;) during training.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Reducing Annotation Training Requirements by Three Orders of Magnitude&lt;/h2&gt;&lt;p&gt;SegCLR embeddings can be used in diverse downstream settings, whether supervised (e.g., training classifiers) or unsupervised (e.g., clustering or content-based image retrieval). In the supervised setting, embeddings simplify the training of classifiers, and can greatly reduce ground truth labeling requirements. For example, we found that for identifying cellular subcompartments (axon, dendrite, &lt;a href=&quot;https://en.wikipedia.org/wiki/Soma_(biology)&quot;&gt;soma&lt;/a&gt;, etc.) a simple linear classifier trained on top of SegCLR embeddings outperformed a fully supervised deep network trained on the same task, while using only about one thousand labeled examples instead of millions. &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbYdSshuRcTBFCB-zmDY7C31mUpyyXP98JEOz9ZsvXMy3a23l5mEBoamtlz7geaoBh8s_PXPi61SNUfbUbIbWBqBcm2JjD7pZD-q5rx4g6psp54DLoPSwdz2TQRAu4vGRFAMui1VRQwCAudV3Ff5rcjEOWwSPsMcYE4II6zvfetG_X7CF4bgHXMPzhPw/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1500&quot; data-original-width=&quot;1999&quot; height=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbYdSshuRcTBFCB-zmDY7C31mUpyyXP98JEOz9ZsvXMy3a23l5mEBoamtlz7geaoBh8s_PXPi61SNUfbUbIbWBqBcm2JjD7pZD-q5rx4g6psp54DLoPSwdz2TQRAu4vGRFAMui1VRQwCAudV3Ff5rcjEOWwSPsMcYE4II6zvfetG_X7CF4bgHXMPzhPw/w640-h480/image2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;We assessed the classification performance for axon, dendrite, soma, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Astrocyte&quot;&gt;astrocyte&lt;/a&gt; subcompartments in the human cortex dataset via mean &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;&gt;F1-Score&lt;/a&gt;, while varying the number of training examples used. Linear classifiers trained on top of SegCLR embeddings matched or exceeded the performance of a fully supervised deep classifier (horizontal line), while using a fraction of the training data.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Distinguishing Cell Types, Even from Small Fragments&lt;/h2&gt;&lt;p&gt;Distinguishing different cell types is an important step towards understanding how brain circuits develop and function in health and disease. Human experts can learn to identify some cortical cell types based on morphological features, but manual cell typing is laborious and ambiguous cases are common. Cell typing also becomes more difficult when only small fragments of cells are available, which is common for many cells in current &lt;a href=&quot;https://research.google/teams/connectomics/&quot;&gt;connectomic reconstructions&lt;/a&gt;. &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjgfqCNS8o0guumswH1girDbY9ys-MyDwnJZGyluaNE91q2S55gBX2A0bD5TwHbWw58dcET7_Q_lCMGbOx_9ETd3_sNggS45pO5jH40sAc2Cer8slSyq-Y9zLZqR1hEyeQW0ho3fAaWraCOau6xmjkSrkIenX9b3OSFAvPXzmKePW9xpA4qMXdSiWDIA/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1355&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjgfqCNS8o0guumswH1girDbY9ys-MyDwnJZGyluaNE91q2S55gBX2A0bD5TwHbWw58dcET7_Q_lCMGbOx_9ETd3_sNggS45pO5jH40sAc2Cer8slSyq-Y9zLZqR1hEyeQW0ho3fAaWraCOau6xmjkSrkIenX9b3OSFAvPXzmKePW9xpA4qMXdSiWDIA/s16000/image4.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Human experts manually labeled cell types for a small number of proofread cells in each dataset. In the mouse cortex dataset, experts labeled six neuron types (&lt;b&gt;top&lt;/b&gt;) and four &lt;a href=&quot;https://en.wikipedia.org/wiki/Glia&quot;&gt;glia&lt;/a&gt; types (not shown). In the human cortex dataset, experts labeled two neuron types (not shown) and four glia types (&lt;b&gt;bottom&lt;/b&gt;). (Rows not to scale with each other.)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;We found that SegCLR accurately infers human and mouse cell types, even for small fragments. Prior to classification, we collected and averaged embeddings within each cell over a set aggregation distance, defined as the radius from a central point. We found that human cortical cell types can be identified with high accuracy for aggregation radii as small as 10 micrometers, even for types that experts &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2021.05.29.446289v4&quot;&gt;find difficult to distinguish&lt;/a&gt;, such as microglia (MGC) versus &lt;a href=&quot;https://en.wikipedia.org/wiki/Oligodendrocyte_progenitor_cell&quot;&gt;oligodendrocyte precursor cells&lt;/a&gt; (OPC). &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy_9tPcai_9WSDpruyAshMw2mgr-LPkD62ZKFRzbjhyhqipGmAkFzSy9t_XRwP8Edp1gXmcg269CUzAj8AjMR0sIqwpFJPGkTLbgpGgFf0Kit-jOopjS4preHP72LBK7D50Qs8gXXRL3aEFU9XF1iNF286urh3M9BSlkOAB12-yarRfwfcPUAXev-AIQ/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1030&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy_9tPcai_9WSDpruyAshMw2mgr-LPkD62ZKFRzbjhyhqipGmAkFzSy9t_XRwP8Edp1gXmcg269CUzAj8AjMR0sIqwpFJPGkTLbgpGgFf0Kit-jOopjS4preHP72LBK7D50Qs8gXXRL3aEFU9XF1iNF286urh3M9BSlkOAB12-yarRfwfcPUAXev-AIQ/s16000/image3.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;SegCLR can classify cell types, even from small fragments. &lt;b&gt;Left&lt;/b&gt;: Classification performance over six human cortex cell types for shallow &lt;a href=&quot;https://en.wikipedia.org/wiki/Residual_neural_network&quot;&gt;ResNet&lt;/a&gt; models trained on SegCLR embeddings for different sized cell fragments. Aggregation radius zero corresponds to very small fragments with only a single embedding. Cell type performance reaches high accuracy (0.938 mean F1-Score) for fragments with aggregation radii of only 10 micrometers (&lt;b&gt;boxed point&lt;/b&gt;). &lt;b&gt;Right&lt;/b&gt;: Class-wise &lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;confusion matrix&lt;/a&gt; at 10 micrometers aggregation radius. Darker shading along the diagonal indicates that predicted cell types agree with expert labels in most cases. AC: astrocyte; MGC: microglia cell; OGC: &lt;a href=&quot;https://en.wikipedia.org/wiki/Oligodendrocyte&quot;&gt;oligodendrocyte&lt;/a&gt; cell; OPC: oligodendrocyte precursor cell; E: excitatory neuron; I: inhibitory neuron.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;In the mouse cortex, ten cell types could be distinguished with high accuracy at aggregation radii of 25 micrometers. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiETN568Ipc4zmD21q2KmbRjcHn4vTjJHAd5PmPr342h_y-38b8K3rgTa2YR9Xs85Hv7rlogyaBLGcMajs94L5sTt4OrsRaNNy000TRAXHKZDxPOTAz70Ar3y47EbpBrIBBDmKzuIxNwuYZX7CXzmdNI-_AJFycmRXqsiy66_lp17GwwdxlhMBN7AgZ-g/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1015&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiETN568Ipc4zmD21q2KmbRjcHn4vTjJHAd5PmPr342h_y-38b8K3rgTa2YR9Xs85Hv7rlogyaBLGcMajs94L5sTt4OrsRaNNy000TRAXHKZDxPOTAz70Ar3y47EbpBrIBBDmKzuIxNwuYZX7CXzmdNI-_AJFycmRXqsiy66_lp17GwwdxlhMBN7AgZ-g/s16000/image6.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Left&lt;/b&gt;: Classification performance over the ten mouse cortex cell types reaches 0.832 mean F1-Score for fragments with aggregation radius 25 micrometers (&lt;b&gt;boxed point&lt;/b&gt;). &lt;b&gt;Right&lt;/b&gt;: The class-wise confusion matrix at 25 micrometers aggregation radius. Boxes indicate broad groups (glia, excitatory neurons, and inhibitory interneurons). P: pyramidal cell; THLC: &lt;a href=&quot;https://en.wikipedia.org/wiki/Thalamocortical_radiations&quot;&gt;thalamocortical axon&lt;/a&gt;; BC: &lt;a href=&quot;https://en.wikipedia.org/wiki/Basket_cell&quot;&gt;basket cell&lt;/a&gt;; BPC: &lt;a href=&quot;https://link.springer.com/article/10.1007/BF01258522&quot;&gt;bipolar cell&lt;/a&gt;; MC: &lt;a href=&quot;https://en.wikipedia.org/wiki/Martinotti_cell&quot;&gt;Martinotti cell&lt;/a&gt;; NGC: &lt;a href=&quot;https://en.wikipedia.org/wiki/Neurogliaform_cell&quot;&gt;neurogliaform cell&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;In additional cell type applications, we &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2022.03.29.486320&quot;&gt;used unsupervised clustering of SegCLR embeddings&lt;/a&gt; to reveal further neuronal subtypes, and demonstrated how &lt;a href=&quot;https://arxiv.org/abs/2006.10108&quot;&gt;uncertainty estimation&lt;/a&gt; can be used to &lt;a href=&quot;https://www.tensorflow.org/tutorials/understanding/sngp&quot;&gt;restrict classification to high confidence subsets&lt;/a&gt; of the dataset, e.g., when only a few cell types have expert labels. &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Revealing Patterns of Brain Connectivity&lt;/h2&gt;&lt;p&gt;Finally, we showed how SegCLR can be used for automated analysis of brain connectivity by cell typing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Synapse&quot;&gt;synaptic partners&lt;/a&gt; of reconstructed cells throughout the mouse cortex dataset. Knowing the connectivity patterns between specific cell types is fundamental to interpreting large-scale connectomic reconstructions of brain wiring, but this typically requires manual tracing to identify partner cell types. Using SegCLR, we replicated brain connectivity findings that previously relied on intensive manual tracing, while extending their scale in terms of the number of synapses, cell types, and brain areas analyzed. (See &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2022.03.29.486320&quot;&gt;the paper&lt;/a&gt; for further details.) &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4DXv06GyF551IlYe9F1_07xE6MIn5WdzlPiNaVvsDAfV-nFrdA9y8JA9kRUjixtZUFqVNEH0n_Q5lmi2dD69z0LoDTbZRd9jE5wtfVHIBbofT-INGg6IAvQbzeCiKlvC_vBdyCm9RrgT0NR10cHLqYDpWmRtM9OewFqoSksHGSQiTyUPZCbEn_rZm-w/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1956&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4DXv06GyF551IlYe9F1_07xE6MIn5WdzlPiNaVvsDAfV-nFrdA9y8JA9kRUjixtZUFqVNEH0n_Q5lmi2dD69z0LoDTbZRd9jE5wtfVHIBbofT-INGg6IAvQbzeCiKlvC_vBdyCm9RrgT0NR10cHLqYDpWmRtM9OewFqoSksHGSQiTyUPZCbEn_rZm-w/s16000/image7.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;SegCLR automated analysis of brain connectivity. &lt;b&gt;Top&lt;/b&gt;: An example mouse pyramidal cell, with synapse locations color-coded according to whether the synaptic partner was classified as inhibitory (&lt;b&gt;blue&lt;/b&gt;), excitatory (&lt;b&gt;red&lt;/b&gt;), or unknown (&lt;b&gt;black&lt;/b&gt;). Inset shows higher detail of the soma and proximal dendrites. &lt;b&gt;Bottom&lt;/b&gt;: We counted how many upstream synaptic partners were classified as thalamocortical axons, which bring input from sensory systems to the cortex. We found that thalamic input arrives primarily at cortical layer L4, the canonical cortical input layer, and preferentially targets primary visual area V1, rather than higher visual areas (HVA).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;What’s Next?&lt;/h2&gt;&lt;p&gt;SegCLR captures rich cellular features and can greatly simplify downstream analyses compared to working directly with raw image and segmentation data. We are excited to see what the community can discover using the &lt;a href=&quot;https://h01-release.storage.googleapis.com/embeddings.html&quot;&gt;~8 billion embeddings we are releasing&lt;/a&gt; for the human and mouse cortical datasets (&lt;a href=&quot;https://colab.sandbox.google.com/gist/chinasaur/63f15b3f37b35b5bb27de31ba0a0087f/segclr-sharding.ipynb&quot;&gt;example access code&lt;/a&gt;; browsable &lt;a href=&quot;https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B3.3e-8%2C%22m%22%5D%7D%2C%22position%22:%5B329992.21875%2C105731.6171875%2C2191.5%5D%2C%22crossSectionScale%22:410.82862069592164%2C%22projectionScale%22:126684.8939275032%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://h01-release/data/20210601/4nm_raw%22%2C%22tab%22:%22source%22%2C%22name%22:%224nm%20EM%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%7B%22url%22:%22precomputed://gs://h01-release/data/20210601/c3%22%2C%22subsources%22:%7B%22default%22:true%2C%22bounds%22:true%2C%22properties%22:true%2C%22mesh%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22tab%22:%22segments%22%2C%22meshSilhouetteRendering%22:2%2C%22segments%22:%5B%222746434850%22%2C%2246794629299%22%2C%22925441648%22%5D%2C%22segmentQuery%22:%22925441648%2C%202746434850%2C%2046794629299%22%2C%22colorSeed%22:4270253886%2C%22name%22:%22c3%20segmentation%22%7D%2C%7B%22type%22:%22annotation%22%2C%22source%22:%22precomputed://gs://h01-release/data/20220326/c3/embeddings/segclr%22%2C%22panels%22:%5B%7B%22flex%22:0.65%2C%22tab%22:%22rendering%22%7D%5D%2C%22shader%22:%22#uicontrol%20invlerp%20red%5Cn#uicontrol%20invlerp%20green%5Cn#uicontrol%20invlerp%20blue%5Cn%5Cnvoid%20main%28%29%20%7B%5Cn%20%20setColor%28vec3%28red%28%29%2C%20green%28%29%2C%20blue%28%29%29%29%3B%5Cn%20%20setPointMarkerSize%284.0%29%3B%5Cn%20%20setPointMarkerBorderWidth%280.%29%3B%5Cn%7D%22%2C%22shaderControls%22:%7B%22red%22:%7B%22range%22:%5B-30%2C30%5D%2C%22window%22:%5B-30%2C30%5D%7D%2C%22green%22:%7B%22range%22:%5B-25%2C30%5D%2C%22window%22:%5B-30%2C30%5D%2C%22property%22:%22e1%22%7D%2C%22blue%22:%7B%22range%22:%5B-40%2C0%5D%2C%22window%22:%5B-50%2C10%5D%2C%22property%22:%22e6%22%7D%7D%2C%22linkedSegmentationLayer%22:%7B%22skeleton_id%22:%22c3%20segmentation%22%7D%2C%22filterBySegmentation%22:%5B%22skeleton_id%22%5D%2C%22name%22:%22segclr%22%7D%5D%2C%22showAxisLines%22:false%2C%22showDefaultAnnotations%22:false%2C%22showSlices%22:false%2C%22prefetch%22:false%2C%22selectedLayer%22:%7B%22row%22:1%2C%22flex%22:0.65%2C%22visible%22:true%2C%22layer%22:%22c3%20segmentation%22%7D%2C%22layout%22:%7B%22type%22:%22xy-3d%22%2C%22orthographicProjection%22:true%7D%2C%22helpPanel%22:%7B%22flex%22:1.04%7D%2C%22selection%22:%7B%22row%22:2%2C%22flex%22:0.45%2C%22size%22:309%2C%22visible%22:false%7D%2C%22layerListPanel%22:%7B%22flex%22:0.96%7D%7D&quot;&gt;human&lt;/a&gt; and &lt;a href=&quot;https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B4e-8%2C%22m%22%5D%7D%2C%22position%22:%5B125184.765625%2C90334.2734375%2C20655.470703125%5D%2C%22crossSectionScale%22:175.91483748406438%2C%22projectionScale%22:159795.35592227764%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em%22%2C%22tab%22:%22source%22%2C%22name%22:%22em65%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://iarpa_microns/minnie/minnie65/seg_m343%22%2C%22tab%22:%22segments%22%2C%22meshSilhouetteRendering%22:2%2C%22segments%22:%5B%22864691135337845734%22%2C%22864691135737765745%22%5D%2C%22segmentQuery%22:%22864691135337845734%2C%20864691135737765745%22%2C%22name%22:%22seg_m343%22%7D%2C%7B%22type%22:%22annotation%22%2C%22source%22:%7B%22url%22:%22precomputed://gs://iarpa_microns/minnie/minnie65/embeddings_m343/segclr%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C13824%5D%2C%5B0%2C1%2C0%2C13824%5D%2C%5B0%2C0%2C1%2C14816%5D%5D%2C%22outputDimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B4e-8%2C%22m%22%5D%7D%7D%7D%2C%22panels%22:%5B%7B%22size%22:353%2C%22tab%22:%22rendering%22%7D%5D%2C%22shader%22:%22#uicontrol%20invlerp%20red%5Cn#uicontrol%20invlerp%20green%5Cn#uicontrol%20invlerp%20blue%5Cn%5Cnvoid%20main%28%29%20%7B%5Cn%20%20setColor%28vec3%28red%28%29%2C%20green%28%29%2C%20blue%28%29%29%29%3B%5Cn%20%20setPointMarkerSize%284.0%29%3B%5Cn%20%20setPointMarkerBorderWidth%280.%29%3B%5Cn%7D%22%2C%22shaderControls%22:%7B%22red%22:%7B%22range%22:%5B-7.8687191836535355%2C11.536377197734613%5D%2C%22window%22:%5B-38.720551224447334%2C52.72395267839594%5D%7D%2C%22green%22:%7B%22range%22:%5B-7.099167230561023%2C8.861439622293307%5D%2C%22window%22:%5B-10%2C10%5D%2C%22property%22:%22e1%22%7D%2C%22blue%22:%7B%22range%22:%5B23.624030954467138%2C1.4089367275395688%5D%2C%22window%22:%5B-38.67762518591486%2C61.42514715610742%5D%2C%22property%22:%22e16%22%7D%7D%2C%22linkedSegmentationLayer%22:%7B%22skeleton_id%22:%22seg_m343%22%7D%2C%22filterBySegmentation%22:%5B%22skeleton_id%22%5D%2C%22name%22:%22segclr_m343%22%7D%5D%2C%22showSlices%22:false%2C%22selectedLayer%22:%7B%22row%22:1%2C%22size%22:353%2C%22visible%22:true%2C%22layer%22:%22seg_m343%22%7D%2C%22layout%22:%7B%22type%22:%22xy-3d%22%2C%22orthographicProjection%22:true%7D%2C%22selection%22:%7B%22row%22:2%2C%22visible%22:false%7D%7D&quot;&gt;mouse&lt;/a&gt; views in &lt;a href=&quot;https://ai.googleblog.com/2019/08/an-interactive-automated-3d.html&quot;&gt;Neuroglancer&lt;/a&gt;). By reducing complex microscopy data to rich and compact embedding representations, SegCLR opens many novel avenues for biological insight, and may serve as a link to &lt;a href=&quot;https://www.cell.com/cell/pdf/S0092-8674(22)00783-8.pdf&quot;&gt;complementary modalities&lt;/a&gt; for high-dimensional characterization at the cellular and subcellular levels, such as &lt;a href=&quot;https://www.nature.com/articles/s41592-020-01033-y&quot;&gt;spatially-resolved transcriptomics&lt;/a&gt;. &lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/1592161586308962338/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/multi-layered-mapping-of-brain-tissue.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1592161586308962338" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1592161586308962338" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/multi-layered-mapping-of-brain-tissue.html" rel="alternate" title="Multi-layered Mapping of Brain Tissue via Segmentation Guided Contrastive Learning" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGcjzK2k-GUyCJ3tjBj6RkXS3hblnCdrP0-QFl9tvhGZON-IqTSNlB6jNbCa2aMr561M_Qc1gIXNZAArnk0uZQw8RVupbfHUxjMY9GtiChJTdBR6yBPmhAY80xmnWaVSgAueOMu368GuI_mFoK4SKDYQMBUIPcny5iDjvgFimZj8NWabLxy4V-QxUoCw/s72-c/image5.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-8090716463694605174</id><published>2022-11-08T10:50:00.004-08:00</published><updated>2022-11-08T10:50:43.488-08:00</updated><title type="text">ReAct: Synergizing Reasoning and Acting in Language Models</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Shunyu Yao, Student Researcher, and Yuan Cao, Research Scientist, Google Research, Brain Team&lt;/span&gt;  &lt;!--&lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuuYg9Pduep9GkUfjloNVOiy3qjpPbT017GKlgGEGMaLNu_TCheEeJ7r8Qok6-0BK3KMfLvsN2vSgFQ8xOvnHM9CAb4Ix4I62bcN2oXFWfqAJzGAGbVqbeCyVktu3h9Dyf5ameRe54LEr32Emp0nG52iofpNOTXCxMY12K7fvmDZNPPmfJaT5zo1OBQA/s595/Screen%20Shot%202022-11-08%20at%208.53.49%20AM.png&quot; style=&quot;display: none;&quot; /&gt;--&gt;&lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuuYg9Pduep9GkUfjloNVOiy3qjpPbT017GKlgGEGMaLNu_TCheEeJ7r8Qok6-0BK3KMfLvsN2vSgFQ8xOvnHM9CAb4Ix4I62bcN2oXFWfqAJzGAGbVqbeCyVktu3h9Dyf5ameRe54LEr32Emp0nG52iofpNOTXCxMY12K7fvmDZNPPmfJaT5zo1OBQA/s16000/Screen%20Shot%202022-11-08%20at%208.53.49%20AM.png&quot; style=&quot;display: none;&quot; /&gt;   &lt;p&gt;Recent advances have expanded the applicability of language models (LM) to downstream tasks. On one hand, existing language models that are properly prompted, via &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;&gt;chain-of-thought&lt;/a&gt;, demonstrate emergent capabilities that carry out self-conditioned reasoning traces to derive answers from questions, excelling at various arithmetic, commonsense, and symbolic reasoning tasks. However, with chain-of-thought prompting, a model is not grounded in the external world and uses its own internal representations to generate reasoning traces, limiting its ability to reactively explore and reason or update its knowledge. On the other hand, recent work uses pre-trained language models for planning and acting in various interactive environments (e.g., &lt;a href=&quot;https://arxiv.org/pdf/2010.02903.pdf&quot;&gt;text games&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2112.09332.pdf&quot;&gt;web navigation&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2201.07207.pdf&quot;&gt;embodied tasks&lt;/a&gt;, &lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;&gt;robotics&lt;/a&gt;), with a focus on mapping text contexts to text actions via the language model’s internal knowledge. However, they do not reason abstractly about high-level goals or maintain a working memory to support acting over long horizons. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;In “&lt;a href=&quot;https://arxiv.org/pdf/2210.03629.pdf&quot;&gt;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a&gt;”, we propose a general paradigm that combines reasoning and acting advances to enable language models to solve various language reasoning and decision making tasks. We demonstrate that the &lt;em&gt;Reason+Act &lt;/em&gt;(ReAct) paradigm systematically outperforms reasoning and acting only paradigms, when prompting bigger language models and fine-tuning smaller language models. The tight integration of reasoning and acting also presents human-aligned task-solving trajectories that improve interpretability, diagnosability, and controllability.. &lt;/p&gt;  &lt;div style=&quot;line-height: 120%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Model Overview &lt;/h2&gt;&lt;p&gt;ReAct enables language models to generate both verbal reasoning traces and text actions in an interleaved manner. While actions lead to observation feedback from an external environment (“Env” in the figure below), reasoning traces do not affect the external environment. Instead, they affect the internal state of the model by reasoning over the context and updating it with useful information to support future reasoning and acting.  &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuuYg9Pduep9GkUfjloNVOiy3qjpPbT017GKlgGEGMaLNu_TCheEeJ7r8Qok6-0BK3KMfLvsN2vSgFQ8xOvnHM9CAb4Ix4I62bcN2oXFWfqAJzGAGbVqbeCyVktu3h9Dyf5ameRe54LEr32Emp0nG52iofpNOTXCxMY12K7fvmDZNPPmfJaT5zo1OBQA/s595/Screen%20Shot%202022-11-08%20at%208.53.49%20AM.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;595&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuuYg9Pduep9GkUfjloNVOiy3qjpPbT017GKlgGEGMaLNu_TCheEeJ7r8Qok6-0BK3KMfLvsN2vSgFQ8xOvnHM9CAb4Ix4I62bcN2oXFWfqAJzGAGbVqbeCyVktu3h9Dyf5ameRe54LEr32Emp0nG52iofpNOTXCxMY12K7fvmDZNPPmfJaT5zo1OBQA/s16000/Screen%20Shot%202022-11-08%20at%208.53.49%20AM.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Previous methods prompt language models (LM) to either generate self-conditioned reasoning traces or task-specific actions. We propose ReAct, a new paradigm that combines reasoning and acting advances in language models.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;div style=&quot;line-height: 120%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;ReAct Prompting&lt;/h2&gt;  &lt;p&gt;We focus on the setup where a frozen language model, &lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM-540B&lt;/a&gt;, is prompted with few-shot in-context examples to generate both domain-specific actions (e.g., “search” in question answering, and “go to” in room navigation), and free-form language reasoning traces (e.g., “Now I need to find a cup, and put it on the table”) for task solving.  &lt;/p&gt; &lt;p&gt;For tasks where reasoning is of primary importance, we alternate the generation of reasoning traces and actions so that the task-solving trajectory consists of multiple reasoning-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions, reasoning traces only need to appear sparsely in the most relevant positions of a trajectory, so we write prompts with sparse reasoning and let the language model decide the asynchronous occurrence of reasoning traces and actions for itself.  &lt;/p&gt; &lt;p&gt;As shown below, there are various types of useful reasoning traces, e.g., decomposing task goals to create action plans, injecting commonsense knowledge relevant to task solving, extracting important parts from observations, tracking task progress while maintaining plan execution, handling exceptions by adjusting action plans, and so on. &lt;/p&gt; &lt;p&gt;The synergy between reasoning and acting allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interacting with the external environments (e.g., Wikipedia) to incorporate additional information into reasoning (act to reason). &lt;/p&gt; &lt;div style=&quot;line-height: 120%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;ReAct Fine-tuning &lt;/h2&gt;&lt;p&gt;We also explore fine-tuning smaller language models using ReAct-format trajectories. To reduce the need for large-scale human annotation, we use the ReAct prompted PaLM-540B model to generate trajectories, and use trajectories with task success to fine-tune smaller language models (PaLM-8/62B).  &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoAazr9qsoobs5Nkp7_uxjml4AEWA9iwUfoNfJpcJEnj2ZOdrTXptaf9R2CyRK7Qif64zcPbywR6AeIOaeZs19vQ7OH6n-6vEyh1exiHXC965OSoNX4bsGjuIZ3Po9CuJb-LhDYyYTQr1rZum-FZ285gi11jsuiAG58C8MzifUPj8VCC_-2N3k3Fsosg/s776/HotPotQA.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;776&quot; data-original-width=&quot;629&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoAazr9qsoobs5Nkp7_uxjml4AEWA9iwUfoNfJpcJEnj2ZOdrTXptaf9R2CyRK7Qif64zcPbywR6AeIOaeZs19vQ7OH6n-6vEyh1exiHXC965OSoNX4bsGjuIZ3Po9CuJb-LhDYyYTQr1rZum-FZ285gi11jsuiAG58C8MzifUPj8VCC_-2N3k3Fsosg/s16000/HotPotQA.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Comparison of four prompting methods, (a) Standard, (b) Chain of thought (CoT, Reason Only), (c) Act-only, and (d) ReAct, solving a &lt;a href=&quot;https://arxiv.org/abs/1809.09600&quot;&gt;HotpotQA&lt;/a&gt; question. In-context examples are omitted, and only the task trajectory is shown. ReAct is able to retrieve information to support reasoning, while also using reasoning to target what to retrieve next, demonstrating a synergy of reasoning and acting.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;      &lt;div style=&quot;line-height: 120%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Results &lt;/h2&gt;&lt;p&gt;We conduct empirical evaluations of ReAct and state-of-the-art baselines across four different benchmarks: question answering (HotPotQA), fact verification (&lt;a href=&quot;https://arxiv.org/abs/1803.05355&quot;&gt;Fever&lt;/a&gt;), text-based game (&lt;a href=&quot;https://arxiv.org/abs/2010.03768&quot;&gt;ALFWorld&lt;/a&gt;), and web page navigation (&lt;a href=&quot;https://arxiv.org/abs/2207.01206&quot;&gt;WebShop&lt;/a&gt;). For HotPotQA and Fever, with access to a &lt;a href=&quot;https://en.wikipedia.org/api/rest_v1/&quot;&gt;Wikipedia API&lt;/a&gt; with which the model can interact, ReAct outperforms vanilla action generation models while being competitive with chain of thought reasoning (CoT) performance. The approach with the best results is a combination of ReAct and CoT that uses both internal knowledge and externally obtained information during reasoning. &lt;/p&gt; &lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;  &lt;tbody&gt;&lt;tr&gt;   &lt;td&gt;   &lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;HotpotQA (exact match, 6-shot)&lt;/b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;    &lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;FEVER (accuracy, 3-shot)&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Standard    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;28.7    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;57.1    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Reason-only (CoT)    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;29.4    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;56.3    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Act-only    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;25.7    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;58.9    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;ReAct    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;27.4    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;60.9    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Best ReAct + CoT Method    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;35.1&lt;/b&gt;   &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;64.6&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Supervised SoTA    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;67.5 (using ~140k samples)    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;89.5 (using ~90k samples)    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;PaLM-540B prompting results on HotpotQA and Fever.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;      &lt;p&gt;On ALFWorld and WebShop, ReAct with both one-shot and two-shot prompting outperforms imitation and reinforcement learning methods trained with ~105 task instances, with an absolute improvement of 34% and 10% in success rates, respectively, over existing baselines. &lt;/p&gt; &lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;  &lt;tbody&gt;&lt;tr&gt;   &lt;td&gt;   &lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;AlfWorld (2-shot)&lt;/b&gt;   &lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;WebShop (1-shot)&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Act-only    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;45    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;30.1    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;ReAct    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;71&lt;/b&gt;   &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;40&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Imitation Learning Baselines &amp;nbsp;&amp;nbsp;&amp;nbsp;    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;37 (using ~100k samples) &amp;nbsp;&amp;nbsp;&amp;nbsp;    &lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;29.1 (using ~90k samples)    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;PaLM-540B prompting task success rate results on AlfWorld and WebShop.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;      &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_0lCKXSvFq4yyjM5PGdM27OF6LWco9qFGQS1dwa3DtEF8AnAuXg9Q_nPDVyAArYwl9sGsB000-iuKJuSsNjo--fi1ZCJbrj-KwsZ6M569nWg-h2xRGHkdvQobUY9RiIr4MYkathIFyiAHZSnHAwVUfeijU-tCLyaHRgqXQah1XObtE71a00IbGdywVw/s839/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;300&quot; data-original-width=&quot;839&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_0lCKXSvFq4yyjM5PGdM27OF6LWco9qFGQS1dwa3DtEF8AnAuXg9Q_nPDVyAArYwl9sGsB000-iuKJuSsNjo--fi1ZCJbrj-KwsZ6M569nWg-h2xRGHkdvQobUY9RiIr4MYkathIFyiAHZSnHAwVUfeijU-tCLyaHRgqXQah1XObtE71a00IbGdywVw/s16000/image1.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Scaling results for prompting and fine-tuning on HotPotQA with ReAct and different baselines. ReAct consistently achieves best fine-tuning performances.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;  &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgP1HCCuyIgO9D3UQKQSKFAth_Xbtqke0UO0rVbAHYA3tmbGjC6wt_du2bEm12RxFx4uWQs1LxpqaFgmHExL8QRfnPJXHVgmy-TRU3yvsDpHa-oxiX8AzmaWsm92y0J2hxdJdsjxmvFqUyYIdLIfhlr2JOIQzuaXml5YXlrF7MxC22B6thYBl72mNMKvg/s1212/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;243&quot; data-original-width=&quot;1212&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgP1HCCuyIgO9D3UQKQSKFAth_Xbtqke0UO0rVbAHYA3tmbGjC6wt_du2bEm12RxFx4uWQs1LxpqaFgmHExL8QRfnPJXHVgmy-TRU3yvsDpHa-oxiX8AzmaWsm92y0J2hxdJdsjxmvFqUyYIdLIfhlr2JOIQzuaXml5YXlrF7MxC22B6thYBl72mNMKvg/s16000/image6.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi41aji28YNe7jqjXOC0-bdWL6nFc6jlrVXOyVD7v15lYMEJ1JNzV-Q9V1Fh-GpX5iW_gH6CWnnvGyECHQkZF33H9E3RI-GTRKA7ZhaSPjyN2rbniob0_biOcP89qZYtGMpQiodO52CJ5iauN11aitR5brKbYIdB349vFMMwqirnZ2TdufpyHz9QbOyDA/s1216/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;124&quot; data-original-width=&quot;1216&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi41aji28YNe7jqjXOC0-bdWL6nFc6jlrVXOyVD7v15lYMEJ1JNzV-Q9V1Fh-GpX5iW_gH6CWnnvGyECHQkZF33H9E3RI-GTRKA7ZhaSPjyN2rbniob0_biOcP89qZYtGMpQiodO52CJ5iauN11aitR5brKbYIdB349vFMMwqirnZ2TdufpyHz9QbOyDA/s16000/image2.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A comparison of the ReAct (&lt;b&gt;top&lt;/b&gt;) and CoT (&lt;b&gt;bottom&lt;/b&gt;) reasoning trajectories on an example from Fever (observation for ReAct is omitted to reduce space). In this case ReAct provided the right answer, and it can be seen that the reasoning trajectory of ReAct is more grounded on facts and knowledge, in contrast to CoT’s hallucination behavior.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;     &lt;p&gt;We also explore human-in-the-loop interactions with ReAct by allowing a human inspector to edit ReAct’s reasoning traces. We demonstrate that by simply replacing a hallucinating sentence with inspector hints, ReAct can change its behavior to align with inspector edits and successfully complete a task. Solving tasks becomes significantly easier when using ReAct as it only requires the manual editing of a few thoughts, which enables new forms of human-machine collaboration.  &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgORrqQ_PMp1JiljcjCXK3BqVHFR5kJ1mUxISgURlkRa6RH2fCaP3HT6rALL453TM_wD3wyKhJrfAlqlgG6jEU-RsvQsNfb02PNzqgvDLwK1XyZPaaFyc9dGRzkQzLcGGWitXzf2Mthf3YymP-0w09-pxMJxrCScFIfKxDAyFUWQCV7tR8YGGeuiNqiKA/s790/AlfWorld.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;790&quot; data-original-width=&quot;603&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgORrqQ_PMp1JiljcjCXK3BqVHFR5kJ1mUxISgURlkRa6RH2fCaP3HT6rALL453TM_wD3wyKhJrfAlqlgG6jEU-RsvQsNfb02PNzqgvDLwK1XyZPaaFyc9dGRzkQzLcGGWitXzf2Mthf3YymP-0w09-pxMJxrCScFIfKxDAyFUWQCV7tR8YGGeuiNqiKA/s16000/AlfWorld.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A human-in-the-loop behavior correction example with ReAct on AlfWorld. (a) ReAct trajectory fails due to a hallucinating reasoning trace (Act 17). (b) A human inspector edits two reasoning traces (Act 17, 23), ReAct then produces desirable reasoning traces and actions to complete the task.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;div style=&quot;line-height: 120%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We present ReAct, a simple yet effective method for synergizing reasoning and acting in language models. Through various experiments that focus on multi-hop question-answering, fact checking, and interactive decision-making tasks, we show that ReAct leads to superior performance with interpretable decision traces. &lt;/p&gt; &lt;p&gt;ReAct demonstrates the feasibility of jointly modeling thought, actions and feedback from the environment within a language model, making it a versatile agent that is capable of solving tasks that require interactions with the environment. We plan to further extend this line of research and leverage the strong potential of the language model for tackling broader embodied tasks, via approaches like massive multitask training and coupling ReAct with equally strong reward models. &lt;/p&gt; &lt;div style=&quot;line-height: 120%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;i&gt;We would like to thank Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran and Karthik Narasimhan for their great contribution in this work. We would also like to thank Google’s Brain team and the Princeton NLP Group for their joint support and feedback, including project scoping, advising and insightful discussions.&lt;/i&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/8090716463694605174/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/react-synergizing-reasoning-and-acting.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8090716463694605174" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8090716463694605174" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/react-synergizing-reasoning-and-acting.html" rel="alternate" title="ReAct: Synergizing Reasoning and Acting in Language Models" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuuYg9Pduep9GkUfjloNVOiy3qjpPbT017GKlgGEGMaLNu_TCheEeJ7r8Qok6-0BK3KMfLvsN2vSgFQ8xOvnHM9CAb4Ix4I62bcN2oXFWfqAJzGAGbVqbeCyVktu3h9Dyf5ameRe54LEr32Emp0nG52iofpNOTXCxMY12K7fvmDZNPPmfJaT5zo1OBQA/s72-c/Screen%20Shot%202022-11-08%20at%208.53.49%20AM.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7514729444885986403</id><published>2022-11-07T12:42:00.003-08:00</published><updated>2022-11-16T15:55:34.244-08:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computational Photography"/><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Perception"/><title type="text">Infinite Nature: Generating 3D Flythroughs from Still Photos</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Noah Snavely and Zhengqi Li, Research Scientists, Google Research&lt;/span&gt; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcaExGymnLPSg-keBVSNJO5EbxE1otdQHc48pzNg1sH9bOeZs78wfl9kO3il1N7E7oPbc2GJqBp3jWVQftwQtNrNJr6NzcGowmtQm_IGJ0UW8mNwKnHtmhx4fadhYlnvFLbpRrUY7agBg47tH9YRmOCtMLyemtyEsUFFgPb6B-Tc8hYGrEP5jKwEsm8g/s720/InfNatResults_v3.gif&quot; style=&quot;display: none;&quot; /&gt; &lt;p&gt;We live in a world of great natural beauty — of majestic mountains, dramatic seascapes, and serene forests. Imagine seeing this beauty as a bird does, flying past richly detailed, three-dimensional landscapes. Can computers learn to synthesize this kind of visual experience? Such a capability would allow for new kinds of content for games and virtual reality experiences: for instance, relaxing within an immersive flythrough of an infinite nature scene. But existing methods that synthesize &lt;a href=&quot;https://ai.googleblog.com/2021/02/the-technology-behind-cinematic-photos.html&quot;&gt;new views from images&lt;/a&gt; tend to allow for only limited camera motion. &lt;/p&gt; &lt;a name='more'&gt;&lt;/a&gt; &lt;p&gt;In a research effort we call &lt;em&gt;Infinite Nature&lt;/em&gt;, we show that computers can learn to generate such rich 3D experiences simply by viewing nature videos and photographs. Our latest work on this theme, &lt;a href=&quot;https://infinite-nature-zero.github.io/&quot;&gt;InfiniteNature-Zero&lt;/a&gt; (presented at &lt;a href=&quot;https://eccv2022.ecva.net/&quot;&gt;ECCV 2022&lt;/a&gt;) can produce high-resolution, high-quality flythroughs starting from a single seed image, using a system trained only on still photographs, a breakthrough capability not seen before. We call the underlying research problem &lt;em&gt;perpetual view generation&lt;/em&gt;: given a single input view of a scene, how can we synthesize a photorealistic set of output views corresponding to an arbitrarily long, user-controlled 3D path through that scene? Perpetual view generation is very challenging because the system must generate new content on the other side of large landmarks (e.g., mountains), and render that new content with high realism and in high resolution. &lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;&gt; &lt;source src=&quot;https://infinite-nature-zero.github.io/static/videos/PerpetualViewGenerationHD.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt; &lt;/video&gt;&lt;/div&gt; &lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Example flythrough generated with InfiniteNature-Zero. It takes a single input image of a natural scene and synthesizes a long camera path flying into that scene, generating new scene content as it goes.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;br&gt; &lt;h2&gt;Background: Learning 3D Flythroughs from Videos&lt;/h2&gt;&lt;p&gt;To establish the basics of how such a system could work, we’ll describe our first version, “&lt;a href=&quot;https://infinite-nature.github.io/&quot;&gt;Infinite Nature: Perpetual View Generation of Natural Scenes from a Single Image&lt;/a&gt;” (presented at &lt;a href=&quot;https://iccv2021.thecvf.com/home&quot;&gt;ICCV 2021&lt;/a&gt;). In that work we explored a “learn from video” approach, where we collected a set of online videos captured from drones flying along coastlines, with the idea that we could learn to synthesize new flythroughs that resemble these real videos. This set of online videos is called the &lt;a href=&quot;https://infinite-nature.github.io/&quot;&gt;Aerial Coastline Imagery Dataset&lt;/a&gt; (ACID). In order to learn how to synthesize scenes that respond dynamically to any desired 3D camera path, however, we couldn’t simply treat these videos as raw collections of pixels; we also had to compute their underlying 3D geometry, including the camera position at each frame. &lt;/p&gt;&lt;p&gt;The basic idea is that we learn to generate flythroughs step-by-step. Given a starting view, like the first image in the figure below, we first compute a &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth_map&quot;&gt;depth map&lt;/a&gt; using &lt;a href=&quot;https://ai.googleblog.com/2021/02/the-technology-behind-cinematic-photos.html&quot;&gt;single-image depth prediction methods&lt;/a&gt;. We then use that depth map to &lt;em&gt;render&lt;/em&gt; the image forward to a new camera viewpoint, shown in the middle, resulting in a new image and depth map from that new viewpoint. &lt;/p&gt;&lt;p&gt;However, this intermediate image has some problems — it has holes where we can see behind objects into regions that weren’t visible in the starting image. It is also blurry, because we are now closer to objects, but are stretching the pixels from the previous frame to render these now-larger objects.  &lt;/p&gt;&lt;p&gt;To handle these problems, we learn a neural &lt;em&gt;image refinement&lt;/em&gt; network that takes this low-quality intermediate image and outputs a complete, high-quality image and corresponding depth map. These steps can then be repeated, with this synthesized image as the new starting point. Because we refine both the image and the depth map, this process can be iterated as many times as desired — the system automatically learns to generate new scenery, like mountains, islands, and oceans, as the camera moves further into the scene. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqVNDQtTkXSpotcE_u3PVX4TdQWEfFB4IdWMx90BTEY7fLwVx-54T1_1e912Ym_yUdSImgc84oVJ1DzYe-KfPfMeVhxbzVYlhi6Z1AdBrffa1W_zWEUom4-eA4udAbellv_Ubh49WAMJ4S3J4_OwA-mGgKzDOxfxTBEHm5KN-4RBB2j_vEoKhIXG0LUw/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;746&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqVNDQtTkXSpotcE_u3PVX4TdQWEfFB4IdWMx90BTEY7fLwVx-54T1_1e912Ym_yUdSImgc84oVJ1DzYe-KfPfMeVhxbzVYlhi6Z1AdBrffa1W_zWEUom4-eA4udAbellv_Ubh49WAMJ4S3J4_OwA-mGgKzDOxfxTBEHm5KN-4RBB2j_vEoKhIXG0LUw/s16000/image4.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Our Infinite Nature methods take an input view and its corresponding depth map (&lt;b&gt;left&lt;/b&gt;). Using this depth map, the system renders the input image to a new desired viewpoint (&lt;b&gt;center&lt;/b&gt;). This intermediate image has problems, such as missing pixels revealed behind foreground content (&lt;b&gt;shown in magenta&lt;/b&gt;). We learn a deep network that refines this image to produce a new high-quality image (&lt;b&gt;right&lt;/b&gt;). This process can be repeated to produce a long trajectory of views. We thus call this approach “render-refine-repeat”.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;We train this &lt;em&gt;render-refine-repeat &lt;/em&gt;synthesis approach using the ACID dataset. In particular, we sample a video from the dataset and then a frame from that video. We then use this method to render several new views moving into the scene along the same camera trajectory as the ground truth video, as shown in the figure below, and compare these rendered frames to the corresponding ground truth video frames to derive a training signal. We also include an &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_adversarial_network&quot;&gt;adversarial setup&lt;/a&gt; that tries to distinguish synthesized frames from real images, encouraging the generated imagery to appear more realistic. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYiQncG1u9JcVDn-mbmxUVGVttHLuQ-zFIZCg7isifUTozbfdLuVPIWAwziyhnyTScgqaO4BKKKlEytPORvJcF8tlUyGReUZqYw0U0n12mNLiBNL8T_9C5cAjMxCzwB7wONgvTawVCPOsbUUi16qY9uzSLWxJWOCOcagEEfgqGVdgAAczCXCCLH366Ug/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;816&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYiQncG1u9JcVDn-mbmxUVGVttHLuQ-zFIZCg7isifUTozbfdLuVPIWAwziyhnyTScgqaO4BKKKlEytPORvJcF8tlUyGReUZqYw0U0n12mNLiBNL8T_9C5cAjMxCzwB7wONgvTawVCPOsbUUi16qY9uzSLWxJWOCOcagEEfgqGVdgAAczCXCCLH366Ug/s16000/image2.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Infinite Nature can synthesize views corresponding to any camera trajectory. During training, we run our system for &lt;em&gt;T&lt;/em&gt; steps to generate &lt;em&gt;T&lt;/em&gt; views along a camera trajectory calculated from a training video sequence, then compare the resulting synthesized views to the ground truth ones. In the figure, each camera viewpoint is generated from the previous one by performing a warp operation &lt;em&gt;R&lt;/em&gt;, followed by the neural refinement operation &lt;em&gt;g&lt;sub&gt;θ&lt;/sub&gt;&lt;/em&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;The resulting system can generate compelling flythroughs, as featured on the &lt;a href=&quot;https://infinite-nature.github.io/&quot;&gt;project webpage&lt;/a&gt;, along with a “flight simulator” Colab demo. Unlike prior methods on video synthesis, this method allows the user to interactively control the camera and can generate much longer camera paths. &lt;/p&gt; &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;InfiniteNature-Zero: Learning Flythroughs from Still Photos&lt;/h2&gt;&lt;p&gt;One problem with this first approach is that video is difficult to work with as training data. High-quality video with the right kind of camera motion is challenging to find, and the aesthetic quality of an individual video frame generally cannot compare to that of an intentionally captured nature photograph. Therefore, in “&lt;a href=&quot;https://arxiv.org/abs/2207.11148&quot;&gt;InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images&lt;/a&gt;”, we build on the &lt;em&gt;render-refine-repeat&lt;/em&gt; strategy above, but devise a way to learn perpetual view synthesis from collections of &lt;em&gt;still photos&lt;/em&gt; — no videos needed. We call this method &lt;em&gt;&lt;a href=&quot;http://infinite-nature-zero.github.io/&quot;&gt;InfiniteNature-Zero&lt;/a&gt;&lt;/em&gt; because it learns from “zero” videos. At first, this might seem like an impossible task — how can we train a model to generate video flythroughs of scenes when all it’s ever seen are isolated photos? &lt;/p&gt;&lt;p&gt;To solve this problem, we had the key insight that if we take an image and render a camera path that forms a &lt;em&gt;cycle&lt;/em&gt; — that is, where the path loops back such that the last image is from the same viewpoint as the first — then we know that the last synthesized image along this path should be the same as the input image. Such &lt;em&gt;cycle consistency&lt;/em&gt; provides a training constraint that helps the model learn to fill in missing regions and increase image resolution during each step of view generation. &lt;/p&gt;&lt;p&gt;However, training with these camera cycles is insufficient for generating long and stable view sequences, so as in our original work, we include an adversarial strategy that considers long, non-cyclic camera paths, like the one shown in the figure above. In particular, if we render &lt;em&gt;T &lt;/em&gt;frames from a starting frame, we optimize our &lt;em&gt;render-refine-repeat&lt;/em&gt; model such that a discriminator network can’t tell which was the starting frame and which was the final synthesized frame. Finally, we add a component trained to generate high-quality sky regions to increase the perceived realism of the results. &lt;/p&gt;&lt;p&gt;With these insights, we trained InfiniteNature-Zero on collections of landscape photos, which are available in large quantities online. Several resulting videos are shown below — these demonstrate beautiful, diverse natural scenery that can be explored along arbitrarily long camera paths. Compared to our prior work — and to prior video synthesis methods — these results exhibit significant improvements in quality and diversity of content (details available in &lt;a href=&quot;https://arxiv.org/abs/2207.11148&quot;&gt;the paper&lt;/a&gt;). &lt;/p&gt; &lt;br /&gt;&lt;br /&gt; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;&gt; &lt;source src=&quot;https://infinite-nature-zero.github.io/static/videos/InfNatZeroResults.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt; &lt;/video&gt;&lt;/div&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Several nature flythroughs generated by InfiniteNature-Zero from single starting photos.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;There are a number of exciting future directions for this work. For instance, our methods currently synthesize scene content based only on the previous frame and its depth map; there is no persistent underlying 3D representation. Our work points towards future algorithms that can generate complete, photorealistic, and consistent 3D worlds. &lt;/p&gt;  &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;Infinite Nature and InfiniteNature-Zero are the result of a collaboration between researchers at Google Research, UC Berkeley, and Cornell University. The key contributors to the work represented in this post include Angjoo Kanazawa, Andrew Liu, Richard Tucker, Zhengqi Li, Noah Snavely, Qianqian Wang, Varun Jampani, and Ameesh Makadia.&lt;/em&gt;&lt;/p&gt;   </content><link href="http://ai.googleblog.com/feeds/7514729444885986403/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/infinite-nature-generating-3d.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7514729444885986403" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7514729444885986403" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/infinite-nature-generating-3d.html" rel="alternate" title="Infinite Nature: Generating 3D Flythroughs from Still Photos" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcaExGymnLPSg-keBVSNJO5EbxE1otdQHc48pzNg1sH9bOeZs78wfl9kO3il1N7E7oPbc2GJqBp3jWVQftwQtNrNJr6NzcGowmtQm_IGJ0UW8mNwKnHtmhx4fadhYlnvFLbpRrUY7agBg47tH9YRmOCtMLyemtyEsUFFgPb6B-Tc8hYGrEP5jKwEsm8g/s72-c/InfNatResults_v3.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7557864009939969171</id><published>2022-11-03T09:56:00.009-07:00</published><updated>2022-11-03T12:50:01.144-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/><title type="text">Beyond Tabula Rasa: Reincarnating Reinforcement Learning</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Rishabh Agarwal, Senior Research Scientist, and Max Schwarzer, Student Researcher, Google Research, Brain Team&lt;/span&gt; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0b-yGyOyTPS2exZHwK8RnAbJH4fsnBI9RLf02LDILhgdIoveBa8kIo4F8PJRnenUeJLKs0zJtsr6_lW5O7hIjcupoR5E-4eNKBQ2bYG7G25mqkVf-fZLyN6Wh5kjU0FSaRDELXKJ1emPN0SilgCkVzDF-wh-qaLsesKU2VrJw78wpyvQ675SK3O_Q0Q/s750/RRL-small.gif&quot; style=&quot;display: none;&quot; /&gt; &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;&gt;Reinforcement learning&lt;/a&gt; (RL) is an area of &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;machine learning&lt;/a&gt; that focuses on training intelligent agents using related experiences so they can learn to solve decision making tasks, such as &lt;a href=&quot;https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html&quot;&gt;playing video games&lt;/a&gt;, &lt;a href=&quot;http://rdcu.be/cbBRc&quot;&gt;flying stratospheric balloons&lt;/a&gt;, and &lt;a href=&quot;https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html&quot;&gt;designing hardware chips&lt;/a&gt;. Due to the generality of RL, the prevalent trend in RL research is to develop agents that can efficiently learn &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Tabula_rasa&quot;&gt;tabula rasa&lt;/a&gt;&lt;/em&gt;, that is, from scratch without using previously learned knowledge about the problem. However, in practice, tabula rasa RL systems are typically the exception rather than the norm for solving large-scale RL problems. Large-scale RL&lt;strong&gt; &lt;/strong&gt;systems, such as &lt;a href=&quot;https://openai.com/five/&quot;&gt;OpenAI Five&lt;/a&gt;, which achieves human-level performance on &lt;a href=&quot;https://en.wikipedia.org/wiki/Dota_2&quot;&gt;Dota 2&lt;/a&gt;, undergo multiple design changes (e.g., algorithmic or architectural changes) during their developmental cycle. This modification process can last months and necessitates incorporating such changes without re-training from scratch, which would be prohibitively expensive.&amp;nbsp;&lt;/p&gt; &lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;Furthermore, the inefficiency of tabula rasa RL research can exclude many researchers from tackling computationally-demanding problems. For example, the quintessential benchmark of training a deep RL agent on 50+ Atari 2600 games in &lt;a href=&quot;https://arxiv.org/abs/1207.4708&quot;&gt;ALE&lt;/a&gt; for 200M frames (the standard protocol) requires 1,000+ GPU days. As deep RL moves towards more complex and challenging problems, the computational barrier to entry in RL research will likely become even higher. &lt;/p&gt;&lt;p&gt;To address the inefficiencies of tabula rasa RL, we present “&lt;a href=&quot;https://agarwl.github.io/reincarnating_rl/&quot;&gt;Reincarnating Reinforcement Learning: Reusing Prior Computation To Accelerate Progress&lt;/a&gt;” at &lt;a href=&quot;https://openreview.net/forum?id=t3X5yMI_4G2&quot;&gt;NeurIPS 2022&lt;/a&gt;. Here, we propose an alternative approach to RL research, where prior computational work, such as learned models, policies, logged data, etc., is reused or transferred between design iterations of an RL agent or from one agent to another. While some sub-areas of RL leverage prior computation, most RL agents are still largely trained from scratch. Until now, there has been no broader effort to leverage prior computational work for the training workflow in RL research. We have also released our &lt;a href=&quot;https://agarwl.github.io/reincarnating_rl/&quot;&gt;code&lt;/a&gt; and &lt;a href=&quot;https://colab.research.google.com/drive/1ktlNni_vwFpFtCgUez-RHW0OdGc2U_Wv?usp=sharing&quot;&gt;trained agents&lt;/a&gt; to enable researchers to build on this work. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjw7y_DM1drfYM19tqNiaKfxa6L-clo6_QlDS9pWS70TfajO4A8PpMfuRPVEhFsTcrokjCctGvit_QtWji5vsgI3byl9rP1MH6BkFna0MbxT2RgDAnMjhGePaP3v77Nkw6VmrPg-q5-alItMlyiUWHZU2TyA6AllLmobmQGTu1g6MKkXBjcgA5oPFlIsg/s1398/RRL.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;725&quot; data-original-width=&quot;1398&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjw7y_DM1drfYM19tqNiaKfxa6L-clo6_QlDS9pWS70TfajO4A8PpMfuRPVEhFsTcrokjCctGvit_QtWji5vsgI3byl9rP1MH6BkFna0MbxT2RgDAnMjhGePaP3v77Nkw6VmrPg-q5-alItMlyiUWHZU2TyA6AllLmobmQGTu1g6MKkXBjcgA5oPFlIsg/s16000/RRL.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Tabula rasa RL vs. Reincarnating RL (RRL). While tabula rasa RL focuses on learning from scratch, RRL is based on the premise of reusing prior computational work (e.g., prior learned agents) when training new agents or improving existing agents, even in the same environment. In RRL, new agents need not be trained from scratch, except for initial forays into new problems.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Why Reincarnating RL? &lt;/h2&gt;&lt;p&gt;Reincarnating RL (RRL) is a more compute and sample-efficient workflow than training from scratch. RRL can democratize research by allowing the broader community to tackle complex RL problems without requiring excessive computational resources. Furthermore, RRL can enable a benchmarking paradigm where researchers continually improve and update existing trained agents, especially on problems where improving performance has real-world impact, such as &lt;a href=&quot;https://www.nature.com/articles/s41586-020-2939-8.epdf?sharing_token=JYZ0ZlvEivoTq9RkGfWPQtRgN0jAjWel9jnR3ZoTv0Mh-6OgaxBwChMnw6EOI9v07nMOMJGBruSSDc8BFPfwkG1QQ0R-p9CwTuKA6ZO41aQ8e-Y-ffoWrsFX1cztOZfL5cL1mwXL8qU58Plz4GAzu_SLyawhPWS5QV6GieUEDig%3D&quot;&gt;balloon navigation&lt;/a&gt; or &lt;a href=&quot;https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html&quot;&gt;chip design&lt;/a&gt;. Finally, real-world RL use cases will likely be in scenarios where prior computational work is available (e.g., existing deployed RL policies).  &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm5Y13mKOIt4y1ni6M7gJhOcwxKPQuu3kpQMnXxG-SunbQfFhIHcVzyikw5OCYt1U_9Fn0-zKoLSHhvyUD-Q4c8DhKuTBzrSvIZQzsmp-Isam4HitAJZFNKsrd96DvVJ4e5I-Mhpsc9xV-SUSM1dQ7wGaonHmvJYLDQpYlrqO5GqQc40rsL4ROeyb-cA/s960/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;960&quot; height=&quot;360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm5Y13mKOIt4y1ni6M7gJhOcwxKPQuu3kpQMnXxG-SunbQfFhIHcVzyikw5OCYt1U_9Fn0-zKoLSHhvyUD-Q4c8DhKuTBzrSvIZQzsmp-Isam4HitAJZFNKsrd96DvVJ4e5I-Mhpsc9xV-SUSM1dQ7wGaonHmvJYLDQpYlrqO5GqQc40rsL4ROeyb-cA/w640-h360/image3.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;RRL as an alternative research workflow. Imagine a researcher who has trained an agent A&lt;sub&gt;1&lt;/sub&gt; for some time, but now wants to experiment with better architectures or algorithms. While the tabula rasa workflow requires retraining another agent from scratch, RRL provides the more viable option of transferring the existing agent A&lt;sub&gt;1&lt;/sub&gt; to another agent and training this agent further, or simply fine-tuning A&lt;sub&gt;1&lt;/sub&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;p&gt;While there have been some ad hoc large-scale reincarnation efforts with limited applicability, e.g., &lt;a href=&quot;https://arxiv.org/abs/1912.06680&quot;&gt;model surgery in Dota2&lt;/a&gt;, &lt;a href=&quot;https://openai.com/blog/solving-rubiks-cube/&quot;&gt;policy distillation in Rubik’s cube&lt;/a&gt;, &lt;a href=&quot;https://www.nature.com/articles/s41586-019-1724-z.epdf?author_access_token=lZH3nqPYtWJXfDA10W0CNNRgN0jAjWel9jnR3ZoTv0PSZcPzJFGNAZhOlk4deBCKzKm70KfinloafEF1bCCXL6IIHHgKaDkaTkBcTEv7aT-wqDoG1VeO9-wO3GEoAMF9bAOt7mJ0RWQnRVMbyfgH9A%3D%3D&quot;&gt;PBT in AlphaStar&lt;/a&gt;, RL fine-tuning a behavior-cloned policy in &lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/unformatted_final_mastering_go.pdf&quot;&gt;AlphaGo&lt;/a&gt; / &lt;a href=&quot;https://openai.com/blog/vpt/&quot;&gt;Minecraft&lt;/a&gt;, RRL has not been studied as a research problem in its own right. To this end, we argue for developing general-purpose RRL approaches as opposed to prior ad-hoc solutions. &lt;/p&gt; &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Case Study: Policy to Value Reincarnating RL&lt;/h2&gt;&lt;p&gt;Different RRL problems can be instantiated depending on the kind of prior computational work provided. As a step towards developing broadly applicable RRL approaches, we present a case study on the setting of Policy to Value reincarnating RL (PVRL) for efficiently transferring an existing sub-optimal policy (teacher) to a standalone value-based RL agent (student). While a policy directly maps a given environment state (e.g., a game screen in Atari) to an action, value-based agents estimate the effectiveness of an action at a given state in terms of achievable future rewards, which allows them to learn from &lt;a href=&quot;https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html&quot;&gt;previously collected data&lt;/a&gt;.  &lt;/p&gt; &lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2nodipCSsg4tWrNcxdG-YO0shx4xyakwqPlRIKHqGN1o8Kd-SVWEtFdIIcBgToSlnqJPJq3oktHsQL6VGgyiSPmeAAtGCOv63mIsKL8A6NX-4utJ0tp8UOBcIcCyMDI5EXDFc6FArzym-kxzJYrUeFLmOi5jAINIiT2IPilQ2h39eG_dwyq9ZW6wr9w/s960/image4.png&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;960&quot; height=&quot;360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2nodipCSsg4tWrNcxdG-YO0shx4xyakwqPlRIKHqGN1o8Kd-SVWEtFdIIcBgToSlnqJPJq3oktHsQL6VGgyiSPmeAAtGCOv63mIsKL8A6NX-4utJ0tp8UOBcIcCyMDI5EXDFc6FArzym-kxzJYrUeFLmOi5jAINIiT2IPilQ2h39eG_dwyq9ZW6wr9w/w640-h360/image4.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;  For a PVRL algorithm to be broadly useful, it should satisfy the following requirements:  &lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;em&gt;Teacher Agnostic&lt;/em&gt;: The student shouldn’t be constrained by the existing teacher policy’s architecture or training algorithm.   &lt;/li&gt;&lt;li&gt;&lt;em&gt;Weaning off the teacher&lt;/em&gt;: It is undesirable to maintain dependency on past suboptimal teachers for successive reincarnations.  &lt;/li&gt;&lt;li&gt;&lt;em&gt;Compute / Sample Efficient&lt;/em&gt;: Reincarnation is only useful if it is cheaper than training from scratch. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Given the PVRL algorithm requirements, we evaluate whether existing approaches, designed with closely related goals, will suffice. We find that such approaches either result in small improvements over tabula rasa RL or degrade in performance when weaning off the teacher.  &lt;/p&gt;&lt;p&gt;To address these limitations, we introduce a simple method, &lt;em&gt;QDagger&lt;/em&gt;, in which the agent distills knowledge from the suboptimal teacher via an &lt;a href=&quot;https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf&quot;&gt;imitation algorithm&lt;/a&gt; while simultaneously using its environment interactions for RL. We start with a &lt;a href=&quot;https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning&quot;&gt;deep Q-network&lt;/a&gt; (DQN) agent trained for 400M environment frames (a week of single-GPU training) and use it as the teacher for reincarnating student agents trained on only 10M frames (a few hours of training), where the teacher is weaned off over the first 6M frames. For benchmark evaluation, we report the &lt;a href=&quot;https://ai.googleblog.com/2021/11/rliable-towards-reliable-evaluation.html&quot;&gt;interquartile mean&lt;/a&gt;&amp;nbsp;(IQM) metric from the &lt;a href=&quot;https://github.com/google-research/rliable&quot;&gt;RLiable library&lt;/a&gt;.&lt;em&gt; &lt;/em&gt;As shown below for the PVRL setting on Atari games, we find that the QDagger RRL method outperforms prior approaches.  &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7IMcUj_NtK7YxrLzbDbxQYGzgjdviQSVSJH9qYCisFLYu72pXPDjoc5vxeN1rgkELigD_0Jlh67DoaDkj734WCk2hKEuwv7gQo2yS1F_JUKZWoqTBJgCwOvZxOUzHuRWqErY3vFozmLPFAWIDL_I4IT3X6olvMYbFq_4fD1Fdpn7yeEMW27p8tNxZwg/s1168/Screenshot%202022-11-02%2010.33.40%20AM.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;538&quot; data-original-width=&quot;1168&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7IMcUj_NtK7YxrLzbDbxQYGzgjdviQSVSJH9qYCisFLYu72pXPDjoc5vxeN1rgkELigD_0Jlh67DoaDkj734WCk2hKEuwv7gQo2yS1F_JUKZWoqTBJgCwOvZxOUzHuRWqErY3vFozmLPFAWIDL_I4IT3X6olvMYbFq_4fD1Fdpn7yeEMW27p8tNxZwg/s16000/Screenshot%202022-11-02%2010.33.40%20AM.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Benchmarking PVRL algorithms on Atari, with teacher-normalized scores aggregated across 10 games. Tabula rasa DQN (–·–) obtains a normalized score of 0.4. Standard baseline approaches include &lt;a href=&quot;https://arxiv.org/abs/1803.03835&quot;&gt;kickstarting&lt;/a&gt;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/efficiently-initializing-reinforcement.html&quot;&gt;JSRL&lt;/a&gt;, &lt;a href=&quot;https://www.deepmind.com/publications/making-efficient-use-of-demonstrations-to-solve-hard-exploration-problems&quot;&gt;rehearsal&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2006.04779&quot;&gt;offline RL pre-training&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1704.03732&quot;&gt;DQfD&lt;/a&gt;. Among all methods, only QDagger surpasses teacher performance within 10 million frames and outperforms the teacher in 75% of the games.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Reincarnating RL in Practice&lt;/h2&gt;&lt;p&gt;We further examine the RRL approach on the &lt;a href=&quot;https://arxiv.org/abs/1709.06009&quot;&gt;Arcade Learning Environment&lt;/a&gt;, a widely used deep RL benchmark. First, we take a &lt;a href=&quot;https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning&quot;&gt;Nature DQN&lt;/a&gt; agent that uses the &lt;a href=&quot;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&quot;&gt;RMSProp&lt;/a&gt; optimizer and fine-tune it with the &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt; optimizer to create a DQN (Adam) agent. While it is possible to train a DQN (Adam) agent from scratch, we demonstrate that fine-tuning Nature DQN with the Adam optimizer matches the from-scratch performance using 40x less data and compute.&lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKJUYfsufdLqDPvuu-EZdz-Wpi3t69nl2i10Qb5lCjp3pIJjqse7jqNEhoh0WZgM4yNt7c6Fdr_Hz1xQJIir-9mtoaEDoQ2DuVo4RqL9d9xHP8nHHMdfJm8RsOVmd-_V2N1CKuF79mZ5ZWbAPyVH1TcjRO0-_vMZeeBvV4BUZrsLmbn6b-epf4saTLow/s1168/Screenshot%202022-11-02%2010.36.36%20AM.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;519&quot; data-original-width=&quot;1168&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKJUYfsufdLqDPvuu-EZdz-Wpi3t69nl2i10Qb5lCjp3pIJjqse7jqNEhoh0WZgM4yNt7c6Fdr_Hz1xQJIir-9mtoaEDoQ2DuVo4RqL9d9xHP8nHHMdfJm8RsOVmd-_V2N1CKuF79mZ5ZWbAPyVH1TcjRO0-_vMZeeBvV4BUZrsLmbn6b-epf4saTLow/s16000/Screenshot%202022-11-02%2010.36.36%20AM.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Reincarnating DQN (Adam) via Fine-Tuning. The vertical separator corresponds to loading network weights and replay data for fine-tuning. &lt;strong&gt;Left:&lt;/strong&gt; Tabula rasa Nature DQN nearly converges in performance after 200M environment frames.&lt;strong&gt; Right:&lt;/strong&gt; Fine-tuning this Nature DQN agent using a reduced learning rate with the Adam optimizer for 20 million frames obtains similar results to DQN (Adam) trained from scratch for 400M frames.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Given the DQN (Adam) agent as a starting point, fine-tuning is restricted to the 3-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;&gt;convolutional&lt;/a&gt; architecture. So, we consider a more general reincarnation approach that leverages recent architectural and algorithmic advances without training from scratch. Specifically, we use QDagger to reincarnate another RL agent that uses a more advanced RL algorithm (&lt;a href=&quot;https://arxiv.org/pdf/1710.02298.pdf&quot;&gt;Rainbow&lt;/a&gt;) and a better neural network architecture (&lt;a href=&quot;https://arxiv.org/abs/1802.01561&quot;&gt;Impala-CNN ResNet&lt;/a&gt;) from the fine-tuned DQN (Adam) agent. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg7BUJU7A9Iv0jwOGjZn_ZOQzy2CK8eby2xRTAVa9ju336McINSwj95E78RmE-jImJII8YBgeIvC4A6huvdXZnsQQvl_jFXu3o3-bI3XQ4yE_VK1wrCqTfDc4pR7Gh6KY05U-ydBQMWlncZE3ev4cKlnA5mOGHLC9UWf188nf4yttZb9hj3OowZRTfaVg/s1168/Screenshot%202022-11-02%2010.37.34%20AM.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;521&quot; data-original-width=&quot;1168&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg7BUJU7A9Iv0jwOGjZn_ZOQzy2CK8eby2xRTAVa9ju336McINSwj95E78RmE-jImJII8YBgeIvC4A6huvdXZnsQQvl_jFXu3o3-bI3XQ4yE_VK1wrCqTfDc4pR7Gh6KY05U-ydBQMWlncZE3ev4cKlnA5mOGHLC9UWf188nf4yttZb9hj3OowZRTfaVg/s16000/Screenshot%202022-11-02%2010.37.34%20AM.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Reincarnating a different architecture / algorithm via QDagger. The vertical separator is the point at which we apply offline pre-training using QDagger for reincarnation. &lt;strong&gt;Left:&lt;/strong&gt; Fine-tuning DQN with Adam.&lt;strong&gt; Right: &lt;/strong&gt;Comparison of a tabula rasa Impala-CNN Rainbow agent (sky blue) to an Impala-CNN Rainbow agent (pink) trained using QDagger RRL from the fine-tuned DQN (Adam). The reincarnated Impala-CNN Rainbow agent consistently outperforms its scratch counterpart. Note that further fine-tuning DQN (Adam) results in diminishing returns (yellow).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Overall, these results indicate that past research could have been accelerated by incorporating a RRL approach to designing agents, instead of re-training agents from scratch. Our &lt;a href=&quot;https://arxiv.org/pdf/2206.01626.pdf&quot;&gt;paper&lt;/a&gt; also contains results on the &lt;a href=&quot;https://ai.googleblog.com/2022/02/the-balloon-learning-environment.html&quot;&gt;Balloon Learning Environment&lt;/a&gt;, where we demonstrate that RRL allows us to make progress on the problem of navigating stratospheric balloons using only a few hours of TPU-compute by reusing a &lt;a href=&quot;https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html&quot;&gt;distributed RL&lt;/a&gt; agent trained on TPUs for more than a month.  &lt;/p&gt; &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Discussion&lt;/h2&gt;&lt;p&gt;Fairly comparing reincarnation approaches involves using the exact same computational work and workflow. Furthermore, the research findings in RRL that broadly generalize would be about how effective an algorithm is given access to existing computational work, e.g., we successfully applied QDagger developed using Atari for reincarnation on Balloon Learning Environment. As such, we speculate that research in reincarnating RL can branch out in two directions: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Standardized benchmarks with open-sourced computational work:&lt;/strong&gt; Akin to &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;&gt;NLP&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_vision&quot;&gt;vision&lt;/a&gt;, where typically a small set of pre-trained models are common, research in RRL may also converge to a small set of open-sourced computational work (e.g., pre-trained teacher policies) on a given benchmark.  &lt;/li&gt;&lt;li&gt;&lt;strong&gt;Real-world domains:&lt;/strong&gt; Since obtaining higher performance has real-world impact in some domains, it incentivizes the community to reuse state-of-the-art agents and try to improve their performance.  &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;See our &lt;a href=&quot;https://arxiv.org/pdf/2206.01626.pdf&quot;&gt;paper&lt;/a&gt; for a broader discussion on scientific comparisons, generalizability and reproducibility in RRL. Overall, we hope that this work motivates researchers to release computational work (e.g., model checkpoints) on which others could directly build. In this regard, we have open-sourced &lt;a href=&quot;https://github.com/google-research/reincarnating_rl&quot;&gt;our code&lt;/a&gt; and &lt;a href=&quot;https://colab.research.google.com/drive/1ktlNni_vwFpFtCgUez-RHW0OdGc2U_Wv?usp=sharing&quot;&gt;trained agents&lt;/a&gt; with their final replay buffers. We believe that reincarnating RL can substantially accelerate research progress by building on prior computational work, as opposed to always starting from scratch. &lt;/p&gt; &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;This work was done in collaboration with Pablo Samuel Castro, Aaron Courville and Marc Bellemare. We’d like to thank Tom Small for the animated figure used in this post. We are also grateful for feedback by the anonymous NeurIPS reviewers and several members of the Google Research team, DeepMind and Mila.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/7557864009939969171/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/beyond-tabula-rasa-reincarnating.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7557864009939969171" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7557864009939969171" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/beyond-tabula-rasa-reincarnating.html" rel="alternate" title="Beyond Tabula Rasa: Reincarnating Reinforcement Learning" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0b-yGyOyTPS2exZHwK8RnAbJH4fsnBI9RLf02LDILhgdIoveBa8kIo4F8PJRnenUeJLKs0zJtsr6_lW5O7hIjcupoR5E-4eNKBQ2bYG7G25mqkVf-fZLyN6Wh5kjU0FSaRDELXKJ1emPN0SilgCkVzDF-wh-qaLsesKU2VrJw78wpyvQ675SK3O_Q0Q/s72-c/RRL-small.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-5096531161675554410</id><published>2022-11-02T06:00:00.005-07:00</published><updated>2022-11-02T09:28:47.110-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="AI"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Natural Language Processing"/><category scheme="http://www.blogger.com/atom/ns#" term="Robotics"/><title type="text">Robots That Write Their Own Code</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Jacky Liang, Research Intern, and Andy Zeng, Research Scientist, Robotics at Google &lt;/span&gt; &lt;!--&lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg55A1wBMlb52PDrG7xhiw3ZwWZvwcjj-p6IR4d9UxKielgLwnqz9Pr0juFl3ZbIkxFr-7v2hku-v8zFJP6pR9G8ZvnWpgUNTiXl5TAxhYawXox-TBFge-G0VxPGuRrzcs-schpa-SnBCa_VM_r3tYC7BYa_MBO6eymXOyKHIigav8WWAxK0yubYo7_OQ/s1616/image3.png&quot; style=&quot;display: none;&quot; /&gt;--&gt;&lt;!--&lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixMObp6xEWLGL7v9THvcTNNRcWm84Zva-aITJP5L4uarQfLwjyurL5kgplTkPM3KG7dlihAAvj9ooaqCWn0Lu1BYVveA_X2aZ_thEQdXgidKc5GEsSc24TCEODiULY2P_frB1M4W0DRL4jv07zdkV4NSMEMjkiPQ8e5hKgoXgx4CiZPuWeg_nJbCSOpg/s1170/CodeAsPolicies.png&quot; style=&quot;display: none;&quot; /&gt;--&gt;&lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJs5xuCtPfu7bzm4c7-5osumFCp5bvKmguRmt_y3fgQbbUoc_37a-ezjjAEH0u1Nb4Am6-zDJi4-z5ZWOn1Io174DEYDNCjYnBQX_2izJloFoO3pbJI7ibYSz2q4_gDMmasq8YFTFdb-4UcObIZykvLviCh3TNIiAw5umN5dRg8V0ZblRQ4ibBvW2gKw/s1229/CodeAsPolicies-2.png&quot; style=&quot;display: none;&quot; /&gt; &lt;p&gt;A common approach used to control robots is to program them with code to detect objects, sequencing commands to move actuators, and feedback loops to specify how the robot should perform a task. While these programs can be &lt;a href=&quot;https://homes.cs.washington.edu/~ztatlock/599z-17sp/papers/robot-programming-lozano-perez-83.pdf&quot;&gt;expressive&lt;/a&gt;, re-programming policies for each new task can be time consuming, and requires domain expertise.&lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;What if when given instructions from people, robots could autonomously write their own code to interact with the world? It turns out that the latest generation of language models, such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;&gt;PaLM&lt;/a&gt;, are capable of complex &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;&gt;reasoning&lt;/a&gt; and have also been trained on millions of lines of code. Given natural language instructions, current language models are &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3520312.3534862&quot;&gt;highly proficient&lt;/a&gt; at writing not only &lt;a href=&quot;https://arxiv.org/pdf/2107.03374.pdf&quot;&gt;generic code&lt;/a&gt; but, as we’ve discovered, code that can control robot actions as well. When provided with several example instructions (formatted as comments) paired with corresponding code (via &lt;a href=&quot;http://ai.stanford.edu/blog/understanding-incontext/&quot;&gt;in-context learning&lt;/a&gt;), language models can take in new instructions and autonomously generate new code that re-composes API calls, synthesizes new functions, and expresses feedback loops to assemble new behaviors at runtime. More broadly, this suggests an alternative approach to using machine learning for robots that (i) pursues generalization through modularity and (ii) leverages the abundance of open-source code and data available on the Internet. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiE55-x4Nq_pV-hdSw8M45MlYZFoyX9Q0nv7b0LgZ5V-0HBiBw-s_SvcIkKJ4tWoVxu0QgmPxO792fsZ0vg6qSU42ps8clbdCHzJoH_3dZRc8dZ9HtF6a1P3kjr0gKGfb581san33sLFuXlsrCYSk2_6l-1UssiePuUZny0LmIEd8WX1if6CShyYcEsHw/s1276/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;105&quot; data-original-width=&quot;1276&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiE55-x4Nq_pV-hdSw8M45MlYZFoyX9Q0nv7b0LgZ5V-0HBiBw-s_SvcIkKJ4tWoVxu0QgmPxO792fsZ0vg6qSU42ps8clbdCHzJoH_3dZRc8dZ9HtF6a1P3kjr0gKGfb581san33sLFuXlsrCYSk2_6l-1UssiePuUZny0LmIEd8WX1if6CShyYcEsHw/s16000/image4.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Given code for an example task (&lt;b&gt;left&lt;/b&gt;), language models can re-compose API calls to assemble new robot behaviors for new tasks (&lt;b&gt;right&lt;/b&gt;) that use the same functions but in different ways.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;To explore this possibility, we developed &lt;a href=&quot;https://code-as-policies.github.io/&quot;&gt;Code as Policies&lt;/a&gt; (CaP), a robot-centric formulation of language model-generated programs executed on physical systems. CaP extends our &lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;&gt;prior work&lt;/a&gt;, &lt;a href=&quot;https://sites.research.google/palm-saycan&quot;&gt;PaLM-SayCan&lt;/a&gt;, by enabling language models to complete even more complex robotic tasks with the full expression of general-purpose Python code. With CaP, we propose using language models to directly write robot code through few-shot prompting. Our experiments demonstrate that outputting code led to improved generalization and task performance over directly learning robot tasks and outputting natural language actions. CaP allows a single system to perform a variety of complex and varied robotic tasks without task-specific training. &lt;/p&gt;  &lt;br&gt;  &lt;br&gt; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;  &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;70%&quot;&gt;&lt;source src=&quot;https://code-as-policies.github.io/videos/tasks_2_5.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;&lt;/video&gt;&lt;/div&gt;&lt;br&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;We demonstrate, across several robot systems, including a robot from &lt;a href=&quot;https://everydayrobots.com/&quot;&gt;Everyday Robots&lt;/a&gt;, that language models can autonomously interpret language instructions to generate and execute CaPs that represent reactive low-level policies (e.g., &lt;a href=&quot;https://en.wikipedia.org/wiki/PID_controller&quot;&gt;proportional-derivative&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Impedance_control&quot;&gt;impedance controllers&lt;/a&gt;) and waypoint-based policies (e.g., &lt;a href=&quot;https://ai.googleblog.com/2021/02/rearranging-visual-world.html&quot;&gt;vision-based pick and place&lt;/a&gt;, &lt;a href=&quot;https://peract.github.io/&quot;&gt;trajectory-based control&lt;/a&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;div style=&quot;line-height: 120%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;A Different Way to Think about Robot Generalization&lt;/h2&gt;&lt;p&gt;To generate code for a new task given natural language instructions, CaP uses a code-writing language model that, when prompted with hints (i.e., import statements that inform which APIs are available) and examples (instruction-to-code pairs that present few-shot &quot;demonstrations&quot; of how instructions should be converted into code), writes new code for new instructions. Central to this approach is &lt;i&gt;hierarchical code generation&lt;/i&gt;, which prompts language models to recursively define new functions, accumulate their own libraries over time, and self-architect a dynamic codebase. Hierarchical code generation improves state-of-the-art on both robotics as well as standard code-gen benchmarks in natural language processing (NLP) subfields, with 39.8% &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;&gt;pass@1 on HumanEval&lt;/a&gt;, a benchmark of hand-written coding problems used to measure the functional correctness of synthesized programs. &lt;/p&gt; &lt;p&gt;Code-writing language models can express a variety of arithmetic operations and feedback loops grounded in language. Pythonic language model programs can use classic logic structures, e.g., sequences, selection (if/else), and loops (for/while), to assemble new behaviors at runtime. They can also use third-party libraries to interpolate points (&lt;a href=&quot;https://numpy.org/&quot;&gt;NumPy&lt;/a&gt;), analyze and generate shapes (&lt;a href=&quot;https://shapely.readthedocs.io/en/stable/&quot;&gt;Shapely&lt;/a&gt;) for spatial-geometric reasoning, etc. These models not only generalize to new instructions, but they can also translate precise values (e.g., velocities) to ambiguous descriptions (&quot;faster&quot; and &quot;to the left&quot;) depending on the context to elicit behavioral commonsense. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg55A1wBMlb52PDrG7xhiw3ZwWZvwcjj-p6IR4d9UxKielgLwnqz9Pr0juFl3ZbIkxFr-7v2hku-v8zFJP6pR9G8ZvnWpgUNTiXl5TAxhYawXox-TBFge-G0VxPGuRrzcs-schpa-SnBCa_VM_r3tYC7BYa_MBO6eymXOyKHIigav8WWAxK0yubYo7_OQ/s1616/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1056&quot; data-original-width=&quot;1616&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg55A1wBMlb52PDrG7xhiw3ZwWZvwcjj-p6IR4d9UxKielgLwnqz9Pr0juFl3ZbIkxFr-7v2hku-v8zFJP6pR9G8ZvnWpgUNTiXl5TAxhYawXox-TBFge-G0VxPGuRrzcs-schpa-SnBCa_VM_r3tYC7BYa_MBO6eymXOyKHIigav8WWAxK0yubYo7_OQ/s16000/image3.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Code as Policies uses code-writing language models to map natural language instructions to robot code to complete tasks. Generated code can call existing perception action APIs, third party libraries, or write new functions at runtime.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;CaP generalizes at a specific layer in the robot: interpreting natural language instructions, processing perception outputs (e.g., from off-the-shelf object detectors), and then parameterizing control primitives. This fits into systems with factorized perception and control, and imparts a degree of generalization (acquired from pre-trained language models) without the magnitude of data collection needed for &lt;a href=&quot;https://ai.googleblog.com/2021/11/decisiveness-in-imitation-learning-for.html&quot;&gt;end-to-end robot learning&lt;/a&gt;. CaP also inherits language model capabilities that are unrelated to code writing, such as supporting instructions with non-English languages and emojis. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1hSkZCuIOL133ABYTricR_mN2eZfUUTn-N7CtjCqfFZYsRks_si09niSb8cgKmJFXLGzShrI3fxVCT5dtW95zsYAs1hYsIVGB0ged_IenhUx7fB2v20HehSJckAd0_NcLQuq99VYGnNcwt1cv_yDOPPgI1YdGB5KjCVczqQ5rGp7KmM4jHARbAdeUJg/s1096/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;311&quot; data-original-width=&quot;1096&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1hSkZCuIOL133ABYTricR_mN2eZfUUTn-N7CtjCqfFZYsRks_si09niSb8cgKmJFXLGzShrI3fxVCT5dtW95zsYAs1hYsIVGB0ged_IenhUx7fB2v20HehSJckAd0_NcLQuq99VYGnNcwt1cv_yDOPPgI1YdGB5KjCVczqQ5rGp7KmM4jHARbAdeUJg/s16000/image5.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;CaP inherits the capabilities of language models, such as multilingual and emoji support.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;By characterizing the &lt;a href=&quot;https://arxiv.org/abs/1908.08351&quot;&gt;types of generalization&lt;/a&gt; encountered in code generation problems, we can also study how hierarchical code generation improves generalization. For example, &quot;&lt;a href=&quot;https://arxiv.org/abs/1908.08351&quot;&gt;systematicity&lt;/a&gt;&quot; evaluates the ability to recombine known parts to form new sequences, &quot;substitutivity&quot; evaluates robustness to synonymous code snippets, while &quot;productivity&quot; evaluates the ability to write policy code longer than those seen in the examples (e.g., for new long horizon tasks that may require defining and nesting new functions). Our &lt;a href=&quot;https://arxiv.org/abs/2209.07753&quot;&gt;paper&lt;/a&gt; presents a new open-source &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/code_as_policies&quot;&gt;benchmark&lt;/a&gt; to evaluate language models on a set of robotics-related code generation problems. Using this benchmark, we find that, in general, bigger models perform better across most metrics, and that hierarchical code generation improves &quot;productivity&quot; generalization the most. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjdCuJbsXAprg3wz78KiJG9nTC_a1itNEEHgtHP9cdskIrO-WQgpBZ1WHCNTHtHMvYG5p53o1qx5EJsVbVHVdJQGryQPGawiCEGQE4DZET3wWVfXWyhGLLlNPY9gv3R5SOd9dBw2k7NldQShcZ713sZtdaY84c0mulLCbFxq3EoneoG5-_EEOGwdI3EA/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1105&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjdCuJbsXAprg3wz78KiJG9nTC_a1itNEEHgtHP9cdskIrO-WQgpBZ1WHCNTHtHMvYG5p53o1qx5EJsVbVHVdJQGryQPGawiCEGQE4DZET3wWVfXWyhGLLlNPY9gv3R5SOd9dBw2k7NldQShcZ713sZtdaY84c0mulLCbFxq3EoneoG5-_EEOGwdI3EA/s16000/image1.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Performance on our RoboCodeGen Benchmark across different generalization types. The larger model (&lt;a href=&quot;https://beta.openai.com/docs/models/davinci&quot;&gt;Davinci&lt;/a&gt;) performs better than the smaller model (&lt;a href=&quot;https://openai.com/blog/openai-codex/&quot;&gt;Cushman&lt;/a&gt;), with hierarchical code generation improving productivity the most.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;We're also excited about the potential for code-writing models to express cross-embodied plans for robots with different morphologies that perform the same task differently depending on the available APIs (perception action spaces), which is an important aspect of any robotics foundation model. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGlv5NF0wNHS6vOId8jZsAyGIZgvtYgMtwHeRlfW9KpdlujQhHlyjXrn2ybzuaXd4w1fHj6qxsDnbZegDiGk78oyBeXT3QwJUfUvNEMKhvP9q_lMuTwQvgSsQcJ9cnGzvAKtWHhMH_Q8YXcRzgWV-Dc43CoO9ERw36hBAvLJ8P8DHu8WgSjkEyHFJrTQ/s1716/image2.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;832&quot; data-original-width=&quot;1716&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGlv5NF0wNHS6vOId8jZsAyGIZgvtYgMtwHeRlfW9KpdlujQhHlyjXrn2ybzuaXd4w1fHj6qxsDnbZegDiGk78oyBeXT3QwJUfUvNEMKhvP9q_lMuTwQvgSsQcJ9cnGzvAKtWHhMH_Q8YXcRzgWV-Dc43CoO9ERw36hBAvLJ8P8DHu8WgSjkEyHFJrTQ/s16000/image2.jpeg&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Language model code-generation exhibits cross-embodiment capabilities, completing the same task in different ways depending on the available APIs (that define perception action spaces).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Limitations&lt;/h2&gt;&lt;p&gt;Code as policies today are restricted by the scope of (i) what the perception APIs can describe (e.g., few visual-language models to date can describe whether a trajectory is &quot;bumpy&quot; or &quot;more C-shaped&quot;), and (ii) which control primitives are available. Only a handful of named primitive parameters can be adjusted without over-saturating the prompts. Our approach also assumes all given instructions are feasible, and we cannot tell if generated code will be useful &lt;em&gt;a priori&lt;/em&gt;. CaPs also struggle to interpret instructions that are significantly more complex or operate at a different abstraction level than the few-shot examples provided to the language model prompts. Thus, for example, in the tabletop domain, it would be difficult for our specific instantiation of CaPs to &quot;build a house with the blocks&quot; since there are no examples of building complex 3D structures. These limitations point to avenues for future work, including extending visual language models to describe low-level robot behaviors (e.g., trajectories) or combining CaPs with exploration algorithms that can autonomously add to the set of control primitives. &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Open-Source Release&lt;/h2&gt;&lt;p&gt;We have released the code needed to reproduce our experiments and an interactive simulated robot demo on the &lt;a href=&quot;https://code-as-policies.github.io/&quot;&gt;project website&lt;/a&gt;, which also contains additional real-world demos with videos and generated code. &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Conclusion &lt;/h2&gt;&lt;p&gt;Code as policies is a step towards robots that can modify their behaviors and expand their capabilities accordingly. This can be enabling, but the flexibility also raises potential risks since synthesized programs (unless manually checked per runtime) may result in unintended behaviors with physical hardware. We can mitigate these risks with built-in safety checks that bound the control primitives that the system can access, but more work is needed to ensure new combinations of known primitives are equally safe. We welcome broad discussion on how to minimize these risks while maximizing the potential positive impacts towards more general-purpose robots. &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;i&gt;This research was done by Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng. Special thanks to Vikas Sindhwani, Vincent Vanhoucke for helpful feedback on writing, Chad Boodoo for operations and hardware support. An early &lt;a href=&quot;https://arxiv.org/abs/2209.07753&quot;&gt;preprint&lt;/a&gt; is available on arXiv.&lt;/i&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/5096531161675554410/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/robots-that-write-their-own-code.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5096531161675554410" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5096531161675554410" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/11/robots-that-write-their-own-code.html" rel="alternate" title="Robots That Write Their Own Code" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJs5xuCtPfu7bzm4c7-5osumFCp5bvKmguRmt_y3fgQbbUoc_37a-ezjjAEH0u1Nb4Am6-zDJi4-z5ZWOn1Io174DEYDNCjYnBQX_2izJloFoO3pbJI7ibYSz2q4_gDMmasq8YFTFdb-4UcObIZykvLviCh3TNIiAw5umN5dRg8V0ZblRQ4ibBvW2gKw/s72-c/CodeAsPolicies-2.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7123003530389096578</id><published>2022-10-26T10:18:00.002-07:00</published><updated>2022-11-01T10:45:54.347-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="AI for Social Good"/><category scheme="http://www.blogger.com/atom/ns#" term="Education"/><category scheme="http://www.blogger.com/atom/ns#" term="ML"/><category scheme="http://www.blogger.com/atom/ns#" term="Natural Language Understanding"/><title type="text">Natural Language Assessment: A New Framework to Promote Education</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Kedem Snir, Software Engineer, and Gal Elidan, Senior Staff Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;Whether it's a professional honing their skills or a child learning to read, coaches and educators play a key role in assessing the learner's answer to a question in a given context and guiding them towards a goal. These interactions have unique characteristics that set them apart from other forms of dialogue, yet are not available when learners practice alone at home. In the field of natural language processing, this type of capability has not received much attention and is technologically challenging. We set out to explore how we can use machine learning to assess answers in a way that facilitates learning. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;In this blog, we introduce an important natural language understanding (NLU) capability called Natural Language Assessment (NLA), and discuss how it can be helpful in the context of education. While typical NLU tasks focus on the user's intent, NLA allows for the assessment of an answer from multiple perspectives. In situations where a user wants to know how good their answer is, NLA can offer an analysis of how close the answer is to what is expected. In situations where there may not be a “correct” answer, NLA can offer subtle insights that include topicality, relevance, verbosity, and beyond. We formulate the scope of NLA, present a practical model for carrying out topicality NLA, and showcase how NLA has been used to help job seekers practice answering interview questions with Google's new interview prep tool, &lt;a href=&quot;https://blog.google/outreach-initiatives/grow-with-google/interview-warmup/&quot;&gt;Interview Warmup&lt;/a&gt;. &lt;/p&gt;  &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Overview of Natural Language Assessment (NLA)&lt;/h2&gt;&lt;p&gt;The goal of NLA is to evaluate the user's answer against a set of expectations. Consider the following components for an NLA system interacting with students: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;em&gt;A question&lt;/em&gt; presented to the student  &lt;/li&gt;&lt;li&gt;&lt;em&gt;Expectations&lt;/em&gt; that define what we expect to find in the answer (e.g., a concrete textual answer, a set of topics we expect the answer to cover, conciseness)  &lt;/li&gt;&lt;li&gt;&lt;em&gt;An answer&lt;/em&gt; provided by the student  &lt;/li&gt;&lt;li&gt;&lt;em&gt;An assessment output&lt;/em&gt; (e.g., correctness, missing information, too specific or general, stylistic feedback, pronunciation, etc.)  &lt;/li&gt;&lt;li&gt;[Optional] &lt;em&gt;A context&lt;/em&gt; (e.g., a chapter in a book or an article) &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;With NLA, both the expectations about the answer and the assessment of the answer can be very broad. This enables teacher-student interactions that are more expressive and subtle. Here are two examples: &lt;/p&gt;&lt;ol&gt; &lt;li&gt;&lt;em&gt;A question with a concrete correct answer&lt;/em&gt;: Even in situations where there is a clear correct answer, it can be helpful to assess the answer more subtly than simply correct or incorrect. Consider the following:  &lt;blockquote style=&quot;border: none; margin: -10px 0px 0px 40px; padding: 0px; text-align: left;&quot;&gt;&lt;p&gt;   &lt;b&gt;Context&lt;/b&gt;: &lt;em&gt;Harry Potter and the Philosopher's Stone&lt;/em&gt;&lt;br /&gt;  &lt;b&gt;Question&lt;/b&gt;: &lt;em&gt;“What is Hogwarts?”&lt;/em&gt;&lt;br /&gt;  &lt;b&gt;Expectation&lt;/b&gt;: &lt;em&gt;“Hogwarts is a school of Witchcraft and Wizardry” [expectation is given as text]&lt;/em&gt;&lt;br /&gt;  &lt;b&gt;Answer&lt;/b&gt;: &lt;em&gt;“I am not exactly sure, but I think it is a school.”&lt;/em&gt;  &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The answer may be missing salient details but labeling it as incorrect wouldn’t be entirely true or useful to a user. NLA can offer a more subtle understanding by, for example, identifying that the student’s answer is too general, and also that the student is uncertain. &lt;/p&gt;      &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEju84Gt0ur2tmd85d8LqvubtTOZ8cEDPrSeMoCNbMMhdQHwKHyhLsT5KUjGPSsq8H3W99qjQn3h_a2OIFrW7sAhBVLMQyQMKnSvGJKxwVQAFF-DI5cVGj8iXfcwxbuT__0BduM9C8Rw8lEewaqvQPQvuX04grdZW0dvCVXGbhBS7NWXc_AqQoWPVf7uhg/s1082/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;266&quot; data-original-width=&quot;1082&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEju84Gt0ur2tmd85d8LqvubtTOZ8cEDPrSeMoCNbMMhdQHwKHyhLsT5KUjGPSsq8H3W99qjQn3h_a2OIFrW7sAhBVLMQyQMKnSvGJKxwVQAFF-DI5cVGj8iXfcwxbuT__0BduM9C8Rw8lEewaqvQPQvuX04grdZW0dvCVXGbhBS7NWXc_AqQoWPVf7uhg/s16000/image3.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Illustration of the NLA process from input question, answer and expectation to assessment output.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;This kind of subtle assessment, along with noting the uncertainty the student expressed, can be important in helping students build skills in conversational settings. &lt;/p&gt; &lt;/li&gt;&lt;li&gt;&lt;em&gt;Topicality expectations&lt;/em&gt;: There are many situations in which a concrete answer is not expected. For example, if a student is asked an opinion question, there is no concrete textual expectation. Instead, there's an expectation of relevance and opinionation, and perhaps some level of succinctness and fluency. Consider the following interview practice setup: &lt;blockquote style=&quot;border: none; margin: -10px 0px 0px 40px; padding: 0px; text-align: left;&quot;&gt;&lt;p&gt;         &lt;b&gt;Question&lt;/b&gt;: &lt;em&gt;“Tell me a little about yourself?”&lt;/em&gt;&lt;br /&gt;  &lt;b&gt;Expectations&lt;/b&gt;: &lt;em&gt;{ “Education”, “Experience”, “Interests” } (a set of topics)&lt;/em&gt;&lt;br /&gt;        &lt;b&gt;Answer&lt;/b&gt;: &lt;em&gt;“Let’s see. I grew up in the Salinas valley in California and went to Stanford where I majored in economics but then got excited about technology so next I ….”&lt;/em&gt;  &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;In this case, a useful assessment output would map the user’s answer to a subset of the topics covered, possibly along with a markup of which parts of the text relate to which topic. This can be challenging from an NLP perspective as answers can be long, topics can be mixed, and each topic on its own can be multi-faceted.    &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;  &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;A Topicality NLA Model&lt;/h2&gt;&lt;p&gt;In principle, topicality NLA is a standard multi-class task for which one can readily train a classifier using standard techniques. However, training data for such scenarios is scarce and it would be costly and time consuming to collect for each question and topic. Our solution is to break each topic into granular components that can be identified using large language models (LLMs) with a straightforward generic tuning.  &lt;/p&gt;&lt;p&gt;We map each topic to a list of underlying questions and define that if the sentence contains an answer to one of those underlying questions, then it covers that topic. For the topic “Experience” we might choose underlying questions such as: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;Where did you work?  &lt;/li&gt;&lt;li&gt;What did you study?  &lt;/li&gt;&lt;li&gt;… &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;While for the topic “Interests” we might choose underlying questions such as: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;What are you interested in?  &lt;/li&gt;&lt;li&gt;What do you enjoy doing?  &lt;/li&gt;&lt;li&gt;… &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These underlying questions are designed through an iterative manual process. Importantly, since these questions are sufficiently granular, current language models (see details below) can capture their semantics. This allows us to offer a zero-shot setting for the NLA topicality task: once trained (more on the model below), it is easy to add new questions and new topics, or adapt existing topics by modifying their underlying content expectation without the need to collect topic specific data. See below the model’s predictions for the sentence “&lt;em&gt;I’ve worked in retail for 3 years”&lt;/em&gt; for the two topics described above: &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhOjKY5O1qBOyFYNcboWvDaDnZj9CpBhZNGkODUMh-oF7F01FcUlaVtZJ9vOfkmgRJKBuRVJk0wk1VNl-VWVLfkfkWAIKALH3dkFmeiJgd6AuCULJT-WDTfRN8qLV6z4mpvCUGTsDJIalOx5zEbpuMvosCGEOiuXSGQ2B55nB03R1K0bFJdLLnhkid_w/s1680/Diagram%2010%20(1).png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;660&quot; data-original-width=&quot;1680&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhOjKY5O1qBOyFYNcboWvDaDnZj9CpBhZNGkODUMh-oF7F01FcUlaVtZJ9vOfkmgRJKBuRVJk0wk1VNl-VWVLfkfkWAIKALH3dkFmeiJgd6AuCULJT-WDTfRN8qLV6z4mpvCUGTsDJIalOx5zEbpuMvosCGEOiuXSGQ2B55nB03R1K0bFJdLLnhkid_w/s16000/Diagram%2010%20(1).png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A diagram of how the model uses underlying questions to predict the topic most likely to be covered by the user’s answer.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;Since an underlying question for the topic “Experience” was matched, the sentence would be classified as “Experience”. &lt;/p&gt; &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Application: Helping Job Seekers Prepare for Interviews&lt;/h2&gt;  &lt;p&gt;&lt;a href=&quot;https://blog.google/outreach-initiatives/grow-with-google/interview-warmup/&quot;&gt;Interview Warmup&lt;/a&gt; is a new tool developed in collaboration with job seekers to help them prepare for interviews in fast-growing fields of employment such as IT Support and UX Design. It allows job seekers to practice answering questions selected by industry experts and to become more confident and comfortable with interviewing. As we worked with job seekers to understand their challenges in preparing for interviews and how an interview practice tool could be most useful, it inspired our research and the application of topicality NLA. &lt;/p&gt;&lt;p&gt;We build the topicality NLA model (once for all questions and topics) as follows: we train an encoder-only T5 model (&lt;a href=&quot;https://arxiv.org/pdf/2110.08426.pdf&quot;&gt;EncT5&lt;/a&gt; architecture) with 350 million parameters on Question-Answers data to predict the compatibility of an &lt;code&gt;&amp;lt;underlying question, answer&amp;gt;&lt;/code&gt; pair. We rely on data from &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD 2.0&lt;/a&gt; which was processed to produce &lt;code&gt;&amp;lt;question, answer, label&amp;gt;&lt;/code&gt; triplets. &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirInL-Tue-1LTGLBbt_o_uXoDMr8Hl8ERvuxYUt9hrT95-rptv-VORYi6ZZq4jLnGsGk2wEGLF7bnT8Vqgo74uO4ElEqSU9atwQn2-FqFKMRS1302FrLib7xCz4IEEHPrs49MAvKKYnB8glHUdCb2ECg3Te7fB4H8oZW38er5WV1UV0Ku7W3buEZmEHA/s1069/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;668&quot; data-original-width=&quot;1069&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirInL-Tue-1LTGLBbt_o_uXoDMr8Hl8ERvuxYUt9hrT95-rptv-VORYi6ZZq4jLnGsGk2wEGLF7bnT8Vqgo74uO4ElEqSU9atwQn2-FqFKMRS1302FrLib7xCz4IEEHPrs49MAvKKYnB8glHUdCb2ECg3Te7fB4H8oZW38er5WV1UV0Ku7W3buEZmEHA/s16000/image4.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;In the Interview Warmup tool, users can switch between talking points to see which ones were detected in their answer.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;The tool does not grade or judge answers. Instead it enables users to practice and identify ways to improve on their own. After a user replies to an interview question, their answer is parsed sentence-by-sentence with the Topicality NLA model. They can then switch between different talking points to see which ones were detected in their answer. We know that there are many potential pitfalls in signaling to a user that their response is “good”, especially as we only detect a limited set of topics.  Instead, we keep the control in the user’s hands and only use ML to help users make &lt;em&gt;their own discoveries&lt;/em&gt; about how to improve.  &lt;/p&gt;&lt;p&gt;So far, the tool has had great results helping job seekers around the world, including in the US, and we have recently expanded it to &lt;a href=&quot;https://blog.google/around-the-globe/google-africa/how-ai-is-helping-african-communities-and-businesses/&quot;&gt;Africa&lt;/a&gt;. We plan to continue working with job seekers to iterate and make the tool even more helpful to the millions of people searching for new jobs.&lt;br /&gt;&lt;/p&gt; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; rel=&quot;0&amp;amp;&quot; src=&quot;https://www.youtube.com/embed/KKfAuQrwzTY&quot; width=&quot;640&quot; youtube-src-id=&quot;KKfAuQrwzTY&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;     &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A short film showing how Interview Warmup and its NLA capabilities were developed in collaboration with job seekers.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Natural Language Assessment (NLA) is a technologically challenging and interesting research area. It paves the way for new conversational applications that promote learning by enabling the nuanced assessment and analysis of answers from multiple perspectives. Working together with communities, from job seekers and businesses to classroom teachers and students, we can identify situations where NLA has the potential to help people learn, engage, and develop skills across an array of subjects, and we can build applications in a responsible way that empower users to assess their own abilities and discover ways to improve. &lt;/p&gt;  &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;This work is made possible through a collaboration spanning several teams across Google. We’d like to acknowledge contributions from Google Research Israel, Google Creative Lab, and Grow with Google teams among others. &lt;/em&gt;&lt;/p&gt;    </content><link href="http://ai.googleblog.com/feeds/7123003530389096578/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/natural-language-assessment-new.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7123003530389096578" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7123003530389096578" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/natural-language-assessment-new.html" rel="alternate" title="Natural Language Assessment: A New Framework to Promote Education" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEju84Gt0ur2tmd85d8LqvubtTOZ8cEDPrSeMoCNbMMhdQHwKHyhLsT5KUjGPSsq8H3W99qjQn3h_a2OIFrW7sAhBVLMQyQMKnSvGJKxwVQAFF-DI5cVGj8iXfcwxbuT__0BduM9C8Rw8lEewaqvQPQvuX04grdZW0dvCVXGbhBS7NWXc_AqQoWPVf7uhg/s72-c/image3.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-3606260649484710280</id><published>2022-10-25T12:52:00.000-07:00</published><updated>2022-10-25T12:52:17.444-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="datasets"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Perception"/><title type="text">Open Images V7 — Now Featuring Point Labels</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Rodrigo Benenson, Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html&quot;&gt;Open Images&lt;/a&gt; is a computer vision dataset covering ~9 million images with labels spanning thousands of object categories. Researchers around the world use Open Images to train and evaluate computer vision models. Since the &lt;a href=&quot;https://ai.googleblog.com/2016/09/introducing-open-images-dataset.html&quot;&gt;initial release&lt;/a&gt; of Open Images in 2016, which included image-level labels covering 6k categories, we have provided multiple updates to enrich annotations and expand the potential use cases of the dataset. Through several releases, we have added &lt;a href=&quot;https://ai.googleblog.com/2017/07/an-update-to-open-images-now-with.html&quot;&gt;image-level labels for over 20k categories&lt;/a&gt; on all images and &lt;a href=&quot;https://ai.googleblog.com/2017/07/an-update-to-open-images-now-with.html&quot;&gt;bounding box annotations&lt;/a&gt;, &lt;a href=&quot;https://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html&quot;&gt;visual relations&lt;/a&gt;, &lt;a href=&quot;https://ai.googleblog.com/2019/05/announcing-open-images-v5-and-iccv-2019.html&quot;&gt;instance segmentations&lt;/a&gt;, and &lt;a href=&quot;https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html&quot;&gt;localized narratives&lt;/a&gt; (synchronized voice, mouse trace, and text caption) on a subset of 1.9M images.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;  &lt;p&gt;Today, we are happy to announce the release of &lt;a href=&quot;https://storage.googleapis.com/openimages/web/index.html&quot;&gt;Open Images V7&lt;/a&gt;, which expands the Open Images dataset even further with a new annotation type called &lt;em&gt;point-level labels&lt;/em&gt; and includes a new all-in-one visualization tool that allows a better exploration of the rich data available. &lt;/p&gt; &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Point Labels&lt;/h2&gt;&lt;p&gt;The main strategy used to collect the new point-level label annotations leveraged suggestions from a machine learning (ML) model and human verification. First, the ML model selected points of interest and asked a yes or no question, e.g., “is this point on a pumpkin?”. Then, human annotators spent an average of 1.1 seconds answering the yes or no questions. We aggregated the answers from different annotators over the same question and assigned a final “yes”, “no”, or “unsure” label to each annotated point. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVHhZfSUBgnAKpUO9qhA0jtxKym_bRLFaZR-zpVaCab_t1-tFYVjslsZf9OcQpfOPppcKqWNKh78ojdYsF0lDilnTfN_GfOytHRPT6gA1ho7uRMmFOoTPeW1XM19eekxXxI3dwk2JAJHKrk7Jp2QNpmNTfS0hrNVTOPM5QU3FYxEXGWWN9G-odpMLMuw/s474/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;396&quot; data-original-width=&quot;474&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVHhZfSUBgnAKpUO9qhA0jtxKym_bRLFaZR-zpVaCab_t1-tFYVjslsZf9OcQpfOPppcKqWNKh78ojdYsF0lDilnTfN_GfOytHRPT6gA1ho7uRMmFOoTPeW1XM19eekxXxI3dwk2JAJHKrk7Jp2QNpmNTfS0hrNVTOPM5QU3FYxEXGWWN9G-odpMLMuw/s16000/image3.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Illustration of the annotations interface. &lt;br /&gt;(&lt;a href=&quot;https://c2.staticflickr.com/9/8329/8140854650_2a93ae7105_z.jpg&quot;&gt;Image&lt;/a&gt; by &lt;a href=&quot;https://www.flickr.com/people/lenore-m/&quot;&gt;Lenore Edman&lt;/a&gt;, under &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;&gt;CC BY 2.0 license&lt;/a&gt;)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For each annotated image, we provide a collection of points, each with a “yes” or “no” label for a given class. These points provide sparse information that can be used for the semantic segmentation task. We collected a total of 38.6M new point annotations (12.4M with “yes” labels) that cover 5.8 thousand classes and 1.4M images. &lt;/p&gt; &lt;p&gt;By focusing on point labels, we expanded the number of images annotated and categories covered. We also concentrated the efforts of our annotators on efficiently collecting useful information. Compared to our instance segmentation, the new points include 16x more classes and cover more images. The new points also cover 9x more classes than our box annotations. Compared to existing segmentation datasets, like &lt;a href=&quot;https://paperswithcode.com/dataset/pascal-voc&quot;&gt;PASCAL VOC&lt;/a&gt;, &lt;a href=&quot;https://cocodataset.org/#home&quot;&gt;COCO&lt;/a&gt;, &lt;a href=&quot;https://www.cityscapes-dataset.com/&quot;&gt;Cityscapes&lt;/a&gt;, &lt;a href=&quot;https://www.lvisdataset.org/&quot;&gt;LVIS&lt;/a&gt;, or &lt;a href=&quot;https://groups.csail.mit.edu/vision/datasets/ADE20K/&quot;&gt;ADE20K&lt;/a&gt;, our annotations cover more classes and more images than previous work. The new point label annotations are the first type of annotation in Open Images that provides localization information for both things (countable objects, like cars, cats, and catamarans), and stuff categories (uncountable objects like grass, granite, and gravel). Overall, the newly collected data is roughly equivalent to two years of human annotation effort. &lt;/p&gt; &lt;p&gt;Our initial experiments show that this type of sparse data is suitable for both training and evaluating segmentation models. Training a model directly on sparse data allows us to reach comparable quality to training on dense annotations. Similarly, we show that one can directly compute the traditional semantic segmentation &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;intersection-over-union&lt;/a&gt; (IoU) metric over sparse data. The ranking across different methods is preserved, and the sparse IoU values are an accurate estimate of its dense version. See our &lt;a href=&quot;https://storage.googleapis.com/openimages/web_v7/2022_pointillism_arxiv.pdf&quot;&gt;paper&lt;/a&gt; for more details.  &lt;/p&gt; &lt;p&gt;Below, we show four example images with their point-level labels, illustrating the rich and diverse information these annotations provide. Circles ⭘ are “yes” labels, and squares &lt;b&gt;☐&lt;/b&gt; are “no” labels. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;  &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi54uBkZ9pta9G1pdLBtSLeUd1ll64NbfCMt7TFj-auHmlA_4FED5zmAIXjU7yXHmGeun1OaUtUEhmo0sDT5qB9Dn259wal1HM0fnu5fzM_pjrvhyTUfiN0_brbZuzt4LkJnkLlMm6_1P_GX8zzkLqvvJRm1hsbdPgvwJ7xfi2bo-BVJiC5wWmBP1yhTA/s1338/image1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1309&quot; data-original-width=&quot;1338&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi54uBkZ9pta9G1pdLBtSLeUd1ll64NbfCMt7TFj-auHmlA_4FED5zmAIXjU7yXHmGeun1OaUtUEhmo0sDT5qB9Dn259wal1HM0fnu5fzM_pjrvhyTUfiN0_brbZuzt4LkJnkLlMm6_1P_GX8zzkLqvvJRm1hsbdPgvwJ7xfi2bo-BVJiC5wWmBP1yhTA/s16000/image1.jpg&quot; /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdzdC9OBwyNDBCF7E0isRWF2vFusLpNYaBM-lXS34mUJ5AS-97GFS1vd-eOkuyy_CQMtV4Hv0NuQ4IXScJYiAroQn1CDIQurxDbEF5UrP-5ImLKcGGCWbrHLOgR_BxOfR3_foNiIpSUFy4f8AiuE5FlIbOo2GvQhdMwybLSnxhNZrVQLk1DeHcHwlOwg/s1527/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1252&quot; data-original-width=&quot;1527&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdzdC9OBwyNDBCF7E0isRWF2vFusLpNYaBM-lXS34mUJ5AS-97GFS1vd-eOkuyy_CQMtV4Hv0NuQ4IXScJYiAroQn1CDIQurxDbEF5UrP-5ImLKcGGCWbrHLOgR_BxOfR3_foNiIpSUFy4f8AiuE5FlIbOo2GvQhdMwybLSnxhNZrVQLk1DeHcHwlOwg/s16000/image7.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlvNblnDZa_eOUixtNRquY1E_cwQrgcWRVFqIaXQravN-HjmeMTjVDbZkRunVQUQ9iQCkXq_KnOh5LrmVJbKoGZH87qI9BeaJjgnw5tNpCao-o0pPuTLmAfkTKRKPt5T5b4L8bGxQYP6fG4CyzFIxFkYyNJdohcdWMBZaPvx3JEiDAa_Zczz4e8yyd4g/s1650/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1650&quot; data-original-width=&quot;1544&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlvNblnDZa_eOUixtNRquY1E_cwQrgcWRVFqIaXQravN-HjmeMTjVDbZkRunVQUQ9iQCkXq_KnOh5LrmVJbKoGZH87qI9BeaJjgnw5tNpCao-o0pPuTLmAfkTKRKPt5T5b4L8bGxQYP6fG4CyzFIxFkYyNJdohcdWMBZaPvx3JEiDAa_Zczz4e8yyd4g/s16000/image6.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQMEhuUdYlNl4rf8RrlDg7IVznnwk3nfg2WgzEXSAMYOAhJuhFWpLGOsbTgG_5gYfMcA5JHfUJtncic3w450mWwtivUmGfoIDN3-LeYnQ9GlNvC3X2LB5xcqA456nnxLWAx1bvExBhyMiqCvhLzBzCxsaU64MbMcYorqmGTp2k2hsxYoS08DaTpmInUA/s1684/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1650&quot; data-original-width=&quot;1684&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQMEhuUdYlNl4rf8RrlDg7IVznnwk3nfg2WgzEXSAMYOAhJuhFWpLGOsbTgG_5gYfMcA5JHfUJtncic3w450mWwtivUmGfoIDN3-LeYnQ9GlNvC3X2LB5xcqA456nnxLWAx1bvExBhyMiqCvhLzBzCxsaU64MbMcYorqmGTp2k2hsxYoS08DaTpmInUA/s16000/image4.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Four example images with point-level labels. &lt;br /&gt;Images by &lt;a href=&quot;https://c7.staticflickr.com/5/4088/5070100626_57e898dfdf_z.jpg&quot;&gt;Richie&lt;/a&gt; &lt;a href=&quot;https://www.flickr.com/people/puroticorico/&quot;&gt;Diesterheft&lt;/a&gt;, &lt;a href=&quot;https://c3.staticflickr.com/4/3928/15385724218_0a4b86d2f9_o.jpg&quot;&gt;John AM&lt;/a&gt; &lt;a href=&quot;https://www.flickr.com/people/nuevajam/&quot;&gt;Nueva&lt;/a&gt;, &lt;a href=&quot;https://farm8.staticflickr.com/4073/4793785065_bd8509a087_o.jpg&quot;&gt;Sarah&lt;/a&gt; &lt;a href=&quot;https://www.flickr.com/people/sackerman519/&quot;&gt;Ackerman&lt;/a&gt;, and &lt;a href=&quot;https://c5.staticflickr.com/5/4101/4908677641_458e9a060f_o.jpg&quot;&gt;C&lt;/a&gt; &lt;a href=&quot;https://www.flickr.com/people/madmarlin_/&quot;&gt;Thomas&lt;/a&gt;, all under &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;&gt;CC BY 2.0 license.&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;New Visualizers&lt;/h2&gt;&lt;p&gt;In addition to the new data release, we also expanded &lt;a href=&quot;https://storage.googleapis.com/openimages/web/visualizer/index.html&quot;&gt;the available visualizations&lt;/a&gt; of the Open Images annotations. The Open Images website now includes dedicated visualizers to explore the localized narratives annotations, the new point-level annotations, and a new all-in-one view. This new all-in-one view is available for the subset of 1.9M densely annotated images and allows one to explore the rich annotations that Open Images has accumulated over seven releases. On average these images have annotations for 6.7 image-labels (classes), 8.3 boxes, 1.7 relations, 1.5 masks, 0.4 localized narratives and 34.8 point-labels per image. &lt;/p&gt; &lt;p&gt;Below, we show two example images with various annotations in the all-in-one visualizer. The figures show the image-level labels, bounding boxes, box relations, instance masks, localized narrative mouse trace and caption, and point-level labels. The &lt;b&gt;+&lt;/b&gt; classes have positive annotations (of any kind), while &lt;b&gt;–&lt;/b&gt; classes have only negative annotations (image-level or point-level).  &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;  &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm9yNIIz238CuHM_7zLK8RWs5H4tF8Kt6WPu5Pdjk9bdKeOGL89nvDUwIaaIIrSPcL-G0eEye9cbz6vpwIX0Fnntwq-WnPY0Cxz_hvX6-tHTPNQUtUnWQDNxnANcaARn0ZWpfjp3OXVUq6cm4IFMhCaas5tuZ2Uh_gToAsHdXbbjF4T4c90M3Swyj_FQ/s1571/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1439&quot; data-original-width=&quot;1571&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm9yNIIz238CuHM_7zLK8RWs5H4tF8Kt6WPu5Pdjk9bdKeOGL89nvDUwIaaIIrSPcL-G0eEye9cbz6vpwIX0Fnntwq-WnPY0Cxz_hvX6-tHTPNQUtUnWQDNxnANcaARn0ZWpfjp3OXVUq6cm4IFMhCaas5tuZ2Uh_gToAsHdXbbjF4T4c90M3Swyj_FQ/s16000/image9.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNCFmwrp7dp8JxSO4qVJHVBbnEQugQPVT_b80AYWsqKGdg1to0u_eLf5K1_BBQFt8bqH7b2NsTSit9Jq4UYe_-23NQ-uBiPplHhIqU7fMKA5a3msPDEpmgi9yaqaTQJBlV6Z3yfmI7lFTqdbpIqlzNi_NxB2kGlERRI1fS10iDug-v_OxAmWez_Cedgw/s1433/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1433&quot; data-original-width=&quot;1132&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNCFmwrp7dp8JxSO4qVJHVBbnEQugQPVT_b80AYWsqKGdg1to0u_eLf5K1_BBQFt8bqH7b2NsTSit9Jq4UYe_-23NQ-uBiPplHhIqU7fMKA5a3msPDEpmgi9yaqaTQJBlV6Z3yfmI7lFTqdbpIqlzNi_NxB2kGlERRI1fS10iDug-v_OxAmWez_Cedgw/s16000/image2.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Two example images with various annotations in the all-in-one visualizer. &lt;br /&gt;Images by &lt;a href=&quot;https://c1.staticflickr.com/4/3835/15014236762_04033666d5_z.jpg&quot;&gt;Jason&lt;/a&gt; &lt;a href=&quot;https://www.flickr.com/people/jasonparis/&quot;&gt;Paris&lt;/a&gt;, and &lt;a href=&quot;https://c7.staticflickr.com/5/4015/4359099851_701ccfd762_z.jpg&quot;&gt;Rubén&lt;/a&gt; &lt;a href=&quot;https://www.flickr.com/people/vike/&quot;&gt;Vique&lt;/a&gt;, all under &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;&gt;CC BY 2.0 license.&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We hope that this new data release will enable computer vision research to cover ever more diverse and challenging scenarios. As the quality of automated semantic segmentation models improves over common classes, we want to move towards the long tail of visual concepts, and sparse point annotations are a step in that direction. More and more works are exploring how to use such sparse annotations (e.g., as &lt;a href=&quot;https://openreview.net/forum?id=wt6QxYgddsl&quot;&gt;supervision&lt;/a&gt; for &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Pointly-Supervised_Instance_Segmentation_CVPR_2022_paper.html&quot;&gt;instance&lt;/a&gt; &lt;a href=&quot;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880599.pdf&quot;&gt;segmentation&lt;/a&gt; or &lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Shin_All_You_Need_Are_a_Few_Pixels_Semantic_Segmentation_With_ICCVW_2021_paper.html&quot;&gt;semantic segmentation&lt;/a&gt;), and Open Images V7 contributes to this research direction. We are looking forward to seeing what you will build next. &lt;/p&gt; &lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;i&gt;Thanks to &lt;a href=&quot;https://sites.google.com/corp/view/vittoferrari&quot;&gt;Vittorio Ferrari&lt;/a&gt;, &lt;a href=&quot;https://jponttuset.cat/&quot;&gt;Jordi Pont-Tuset&lt;/a&gt;, &lt;a href=&quot;https://akuznetso.github.io/&quot;&gt;Alina Kuznetsova&lt;/a&gt;, Ashlesha Sadras, and the annotators team for their support creating this new data release.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/3606260649484710280/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/open-images-v7-now-featuring-point.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3606260649484710280" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3606260649484710280" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/open-images-v7-now-featuring-point.html" rel="alternate" title="Open Images V7 — Now Featuring Point Labels" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVHhZfSUBgnAKpUO9qhA0jtxKym_bRLFaZR-zpVaCab_t1-tFYVjslsZf9OcQpfOPppcKqWNKh78ojdYsF0lDilnTfN_GfOytHRPT6gA1ho7uRMmFOoTPeW1XM19eekxXxI3dwk2JAJHKrk7Jp2QNpmNTfS0hrNVTOPM5QU3FYxEXGWWN9G-odpMLMuw/s72-c/image3.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-4407074621491257938</id><published>2022-10-22T20:30:00.004-07:00</published><updated>2022-10-24T08:40:18.077-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="conference"/><category scheme="http://www.blogger.com/atom/ns#" term="ECCV"/><title type="text">Google at ECCV 2022 </title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Shaina Mehta, Program Manager, Google&lt;/span&gt; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWmLScVcpCyi8puS5FYC4xNN88CWeK0-77JiMlowYPlxCPym5T4zX7iKIuzIjwtDnUubUJ0yM8Xa6Mt83N_-YoBgyrZH_dwSL6YgoxUEgGf1tNuR5fSWeRY61Ut0TOhlzUuTJlKs6fyBF3JTVzAlbEOUgtGb_5gdgFBIXWFCJ0utT8kuUh3rKIjw4L1g/s2880/about_hero.png&quot; style=&quot;display: none;&quot; /&gt; &lt;p&gt;Google is proud to be a &lt;a href=&quot;https://eccv2022.ecva.net/sponsors/&quot;&gt;Platinum Sponsor&lt;/a&gt; of the &lt;a href=&quot;https://eccv2022.ecva.net/&quot;&gt;European Conference on Computer Vision&lt;/a&gt; (ECCV 2022), a premier forum for the dissemination of research in computer vision and machine learning (ML). This year, ECCV 2022 will be held as a hybrid event, in person in Tel Aviv, Israel with virtual attendance as an option. Google has a strong presence at this year’s conference with over 60 accepted publications and active involvement in a number of workshops and tutorials. We look forward to sharing some of our extensive research and expanding our partnership with the broader ML research community.&amp;nbsp;&lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;  &lt;p&gt;Registered for ECCV 2022? We hope you’ll visit our on-site or virtual booths to learn more about the research we’re presenting at ECCV 2022, including several demos and opportunities to connect with our researchers. Learn more about Google's research being presented at ECCV 2022 below (Google affiliations in &lt;b&gt;bold&lt;/b&gt;). &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Organizing Committee&lt;/h2&gt;&lt;p&gt; &lt;/p&gt;&lt;p&gt;Program Chairs include:&lt;b&gt; &lt;i&gt;Moustapha Cissé&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;Awards Paper Committee:&lt;b&gt; &lt;i&gt;Todd Zickler&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;Area Chairs include:&lt;b&gt; &lt;i&gt;Ayan Chakrabarti&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Tali Dekel&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Alireza Fathi&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vittorio Ferrari&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;David Fleet&lt;/i&gt;, &lt;i&gt;Dilip Krishnan&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michael Rubinstein&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Cordelia Schmid&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Deqing Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Jasper Uijlings&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; T&lt;i&gt;odd Zickler&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Accepted Publications&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.11911.pdf&quot;&gt;NeuMesh: Learning Disentangled Neural Mesh-Based Implicit Field for Geometry and Texture Editing&lt;/a&gt;&lt;br /&gt;Bangbang Yang, Chong Bao, Junyi Zeng, Hujun Bao, &lt;b&gt;&lt;i&gt;Yinda Zhang&lt;/i&gt;&lt;/b&gt;, Zhaopeng Cui, Guofeng Zhang &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2109.09023.pdf&quot;&gt;Anti-Neuron Watermarking: Protecting Personal Data Against Unauthorized Neural Networks&lt;/a&gt;&lt;br /&gt;Zihang Zou, &lt;b&gt;&lt;i&gt;Boqing Gong&lt;/i&gt;&lt;/b&gt;, Liqiang Wang &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.08954.pdf&quot;&gt;Exploiting Unlabeled Data with Vision and Language Models for Object Detection&lt;/a&gt;&lt;br /&gt;Shiyu Zhao, Zhixing Zhang, Samuel Schulter, &lt;b&gt;&lt;i&gt;Long Zhao&lt;/i&gt;&lt;/b&gt;, Vijay Kumar B G, Anastasis Stathopoulos, Manmohan Chandraker, Dimitris N. Metaxas &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2206.07704.pdf&quot;&gt;Waymo Open Dataset: Panoramic Video Panoptic Segmentation&lt;/a&gt;&lt;br /&gt;Jieru Mei, Alex Zhu, Xinchen Yan, Hang Yan, &lt;b&gt;&lt;i&gt;Siyuan Qiao&lt;/i&gt;&lt;/b&gt;, Yukun Zhu, &lt;b&gt;&lt;i&gt;Liang-Chieh Chen&lt;/i&gt;&lt;/b&gt;, Henrik Kretzschmar &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.06143.pdf&quot;&gt;PRIF: Primary Ray-Based Implicit Function&lt;/a&gt;&lt;br /&gt;Brandon Yushan Feng, &lt;b&gt;&lt;i&gt;Yinda Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Danhang Tang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ruofei Du&lt;/i&gt;&lt;/b&gt;, Amitabh Varshney &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.08622.pdf&quot;&gt;LoRD: Local 4D Implicit Representation for High-Fidelity Dynamic Human Modeling&lt;/a&gt;&lt;br /&gt;Boyan Jiang, Xinlin Ren, &lt;b&gt;&lt;i&gt;Mingsong Dou&lt;/i&gt;&lt;/b&gt;, Xiangyang Xue, Yanwei Fu, &lt;b&gt;&lt;i&gt;Yinda Zhang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.04044.pdf&quot;&gt;k-Means Mask Transformer&lt;/a&gt; (see &lt;a href=&quot;https://ai.googleblog.com/2022/07/revisiting-mask-transformer-from.html&quot;&gt;blog post&lt;/a&gt;) &lt;br /&gt;Qihang Yu&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Siyuan Qiao&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Maxwell D Collins&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yukun Zhu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Hartwig Adam&lt;/i&gt;&lt;/b&gt;, Alan Yuille, &lt;b&gt;&lt;i&gt;Liang-Chieh Chen&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.01697.pdf&quot;&gt;MaxViT: Multi-Axis Vision Transformer&lt;/a&gt; (see &lt;a href=&quot;https://ai.googleblog.com/2022/09/a-multi-axis-approach-for-vision.html&quot;&gt;blog post&lt;/a&gt;) &lt;br /&gt;&lt;b&gt;&lt;i&gt;Zhengzhong Tu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Hossein Talebi&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Han Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Feng Yang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Peyman Milanfar&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Alan Bovik&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Yinxiao Li&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.10008.pdf&quot;&gt;E-Graph: Minimal Solution for Rigid Rotation with Extensibility Graphs&lt;/a&gt;&lt;br /&gt;Yanyan Li, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.00237.pdf&quot;&gt;RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation&lt;/a&gt;&lt;br /&gt;Ruida Zhang, Yan Di, Zhiqiang Lou, &lt;b&gt;&lt;i&gt;Fabian Manhardt&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;, Xiangyang Ji &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.10158.pdf&quot;&gt;GOCA: Guided Online Cluster Assignment for Self-Supervised Video Representation Learning&lt;/a&gt;&lt;br /&gt;Huseyin Coskun, Alireza Zareian, Joshua L Moore, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;, Chen Wang &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2112.12143.pdf&quot;&gt;Scaling Open-Vocabulary Image Segmentation with Image-Level Labels&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Golnaz Ghiasi&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiuye Gu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yin Cui&lt;/i&gt;&lt;/b&gt;, Tsung-Yi Lin&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.12175.pdf&quot;&gt;Adaptive Transformers for Robust Few-Shot Cross-Domain Face Anti-spoofing&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Hsin-Ping Huang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Deqing Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yaojie Liu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Wen-Sheng Chu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Taihong Xiao&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Jinwei Yuan&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Hartwig Adam&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.04799.pdf&quot;&gt;DualPrompt: Complementary Prompting for Rehearsal-Free Continual Learning&lt;/a&gt;&lt;br /&gt;Zifeng Wang&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Zizhao Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Sayna Ebrahimi&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ruoxi Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Han Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Chen-Yu Lee&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiaoqi Ren&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Guolong Su&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vincent Perot&lt;/i&gt;&lt;/b&gt;, Jennifer Dy, &lt;b&gt;&lt;i&gt;Tomas Pfister&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2112.05112.pdf&quot;&gt;BLT: Bidirectional Layout Transformer for Controllable Layout Generation&lt;/a&gt;&lt;br /&gt;Xiang Kong, &lt;b&gt;&lt;i&gt;Lu Jiang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Huiwen Chang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Han Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yuan Hao&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Haifeng Gong&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Irfan Essa&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.10638.pdf&quot;&gt;V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer&lt;/a&gt;&lt;br /&gt;Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;, Jiaqi Ma &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.10652.pdf&quot;&gt;Learning Visibility for Robust Dense Human Body Estimation&lt;/a&gt;&lt;br /&gt;Chun-Han Yao, Jimei Yang, Duygu Ceylan, Yi Zhou, Yang Zhou, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2111.10659.pdf&quot;&gt;Are Vision Transformers Robust to Patch Perturbations?&lt;/a&gt;&lt;br /&gt;Jindong Gu, Volker Tresp, &lt;b&gt;Yao Qin&lt;/b&gt;&lt;/p&gt;   &lt;p&gt;&lt;a href=&quot;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910542.pdf&quot;&gt;PseudoAugment: Learning to Use Unlabeled Data for Data Augmentation in Point Clouds&lt;/a&gt;&lt;br /&gt;Zhaoqi Leng, Shuyang Cheng, &lt;b&gt;&lt;i&gt;Ben Caine&lt;/i&gt;&lt;/b&gt;, Weiyue Wang, Xiao Zhang, &lt;b&gt;&lt;i&gt;Jonathon Shlens&lt;/i&gt;&lt;/b&gt;, Mingxing Tan, Dragomir Anguelov &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930020.pdf&quot;&gt;Structure and Motion from Casual Videos&lt;/a&gt;&lt;br /&gt;Zhoutong Zhang, &lt;b&gt;&lt;i&gt;Forrester Cole&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Zhengqi Li&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Noah Snavely&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michael Rubinstein&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;William T. Freeman&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.10435.pdf&quot;&gt;PreTraM: Self-Supervised Pre-training via Connecting Trajectory and Map&lt;/a&gt;&lt;br /&gt;Chenfeng Xu, Tian Li, Chen Tang, Lingfeng Sun, Kurt Keutzer, Masayoshi Tomizuka, &lt;b&gt;&lt;i&gt;Alireza Fathi&lt;/i&gt;&lt;/b&gt;, Wei Zhan &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.10659.pdf&quot;&gt;Novel Class Discovery Without Forgetting&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Joseph K J&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Sujoy Paul&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Gaurav Aggarwal&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Soma Biswas&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Piyush Rai&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Kai Han&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vineeth N Balasubramanian&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.09644.pdf&quot;&gt;Hierarchically Self-Supervised Transformer for Human Skeleton Representation Learning&lt;/a&gt;&lt;br /&gt;Yuxiao Chen, &lt;b&gt;&lt;i&gt;Long Zhao&lt;/i&gt;&lt;/b&gt;, Jianbo Yuan, Yu Tian, Zhaoyang Xia, Shijie Geng, Ligong Han, Dimitris N. Metaxas &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.05126.pdf&quot;&gt;PACTran: PAC-Bayesian Metrics for Estimating the Transferability of Pretrained Models to Classification Tasks&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Nan Ding&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xi Chen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Tomer Levinboim&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Soravit Changpinyo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Radu Soricut&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://infinite-nature-zero.github.io/&quot;&gt;InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Zhengqi Li&lt;/i&gt;&lt;/b&gt;, Qianqian Wang&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Noah Snavely&lt;/i&gt;&lt;/b&gt;, Angjoo Kanazawa&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;  &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.10662.pdf&quot;&gt;Generalizable Patch-Based Neural Rendering&lt;/a&gt; (see &lt;a href=&quot;https://ai.googleblog.com/2022/09/view-synthesis-with-transformers.html&quot;&gt;blog post&lt;/a&gt;) &lt;br /&gt;Mohammed Suhail&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Carlos Esteves&lt;/i&gt;&lt;/b&gt;, Leonid Sigal, &lt;b&gt;&lt;i&gt;Ameesh Makadia&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://cseweb.ucsd.edu/~mil070/projects/ECCV2022/paper.pdf&quot;&gt;LESS: Label-Efficient Semantic Segmentation for LiDAR Point Clouds&lt;/a&gt;&lt;br /&gt;Minghua Liu, Yin Zhou, Charles R. Qi, &lt;b&gt;&lt;i&gt;Boqing Gong&lt;/i&gt;&lt;/b&gt;, Hao Su, Dragomir Anguelov &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2206.04453.pdf&quot;&gt;The Missing Link: Finding Label Relations Across Datasets&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Jasper Uijlings&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Thomas Mensink&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vittorio Ferrari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.16530.pdf&quot;&gt;Learning Instance-Specific Adaptation for Cross-Domain Segmentation&lt;/a&gt;&lt;br /&gt;Yuliang Zou, &lt;b&gt;&lt;i&gt;Zizhao Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Chun-Liang Li&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Han Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Tomas Pfister&lt;/i&gt;&lt;/b&gt;, Jia-Bin Huang &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.00679v1.pdf&quot;&gt;Learning Audio-Video Modalities from Image Captions&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Arsha Nagrani&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Paul Hongsuck Seo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Bryan Seybold&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Anja Hauth&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Santiago Manen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Chen Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Cordelia Schmid&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.06773.pdf&quot;&gt;TL;DW? Summarizing Instructional Videos with Task Relevance &amp;amp; Cross-Modal Saliency&lt;/a&gt;&lt;br /&gt;Medhini Narasimhan&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Arsha Nagrani&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Chen Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michael Rubinstein&lt;/i&gt;&lt;/b&gt;, Trevor Darrell, Anna Rohrbach, &lt;b&gt;&lt;i&gt;Cordelia Schmid&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.10225.pdf&quot;&gt;On Label Granularity and Object Localization&lt;/a&gt;&lt;br /&gt;Elijah Cole, &lt;b&gt;&lt;i&gt;Kimberly Wilber&lt;/i&gt;&lt;/b&gt;, Grant Van Horn, &lt;b&gt;&lt;i&gt;Xuan Yang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Marco Fornoni&lt;/i&gt;&lt;/b&gt;, Pietro Perona, Serge Belongie, &lt;b&gt;&lt;i&gt;Andrew Howard&lt;/i&gt;&lt;/b&gt;, Oisin Mac Aodha &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.10712.pdf&quot;&gt;Disentangling Architecture and Training for Optical Flow&lt;/a&gt;&lt;br /&gt;  &lt;b&gt;&lt;i&gt;Deqing Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Charles Herrmann&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Fitsum Reda&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michael Rubinstein&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;David J. Fleet&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;William T. Freeman&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.13061.pdf&quot;&gt;NewsStories: Illustrating Articles with Visual Summaries&lt;/a&gt;&lt;br /&gt;Reuben Tan, Bryan Plummer, Kate Saenko, &lt;b&gt;&lt;i&gt;J.P. Lewis&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Avneesh Sud&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Thomas Leung&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.09932.pdf&quot;&gt;Improving GANs for Long-Tailed Data Through Group Spectral Regularization&lt;/a&gt;&lt;br /&gt;Harsh Rangwani, Naman Jaswani, &lt;b&gt;&lt;i&gt;Tejan Karmali&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Varun Jampani&lt;/i&gt;&lt;/b&gt;, Venkatesh Babu Radhakrishnan &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.10235.pdf&quot;&gt;Planes vs. Chairs: Category-Guided 3D Shape Learning Without Any 3D Cues&lt;/a&gt;&lt;br /&gt;Zixuan Huang, Stefan Stojanov, Anh Thai, &lt;b&gt;&lt;i&gt;Varun Jampani&lt;/i&gt;&lt;/b&gt;, James Rehg &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.03354.pdf&quot;&gt;A Sketch Is Worth a Thousand Words: Image Retrieval with Text and Sketch&lt;/a&gt;&lt;br /&gt;Patsorn Sangkloy, &lt;b&gt;&lt;i&gt;Wittawat Jitkrittum&lt;/i&gt;&lt;/b&gt;, Diyi Yang, James Hays &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.09171.pdf&quot;&gt;Learned Monocular Depth Priors in Visual-Inertial Initialization&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Yunwen Zhou&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Abhishek Kar&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Eric L. Turner&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Adarsh Kowdle&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Chao Guo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ryan DuToit&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Konstantine Tsotsos&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.01403.pdf&quot;&gt;How Stable are Transferability Metrics Evaluations?&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Andrea Agostinelli&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michal Pandy&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Jasper Uijlings&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Thomas Mensink&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vittorio Ferrari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2112.02086.pdf&quot;&gt;Data-Free Neural Architecture Search via Recursive Label Calibration&lt;/a&gt;&lt;br /&gt;Zechun Liu&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, Zhiqiang Shen, &lt;b&gt;&lt;i&gt;Yun Long&lt;/i&gt;&lt;/b&gt;, Eric Xing, Kwang-Ting Cheng, &lt;b&gt;&lt;i&gt;Chas H. Leichner&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2201.00392.pdf&quot;&gt;Fast and High Quality Image Denoising via Malleable Convolution&lt;/a&gt;&lt;br /&gt;Yifan Jiang&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Bartlomiej Wronski&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ben Mildenhall&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Jonathan T. Barron&lt;/i&gt;&lt;/b&gt;, Zhangyang Wang, &lt;b&gt;&lt;i&gt;Tianfan Xue&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.13247.pdf&quot;&gt;Concurrent Subsidiary Supervision for Unsupervised Source-Free Domain Adaptation&lt;/a&gt;&lt;br /&gt;Jogendra Nath Kundu, Suvaansh Bhambri, Akshay R Kulkarni, Hiran Sarkar, &lt;br /&gt;&lt;b&gt;&lt;i&gt;Varun Jampani&lt;/i&gt;&lt;/b&gt;, Venkatesh Babu Radhakrishnan &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.03353.pdf&quot;&gt;Learning Online Multi-Sensor Depth Fusion&lt;/a&gt;&lt;br /&gt;Erik Sandström, Martin R. Oswald, Suryansh Kumar, Silvan Weder, Fisher Yu, &lt;b&gt;&lt;i&gt;Cristian Sminchisescu&lt;/i&gt;&lt;/b&gt;, Luc Van Gool &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.03764.pdf&quot;&gt;Hierarchical Semantic Regularization of Latent Spaces in StyleGANs&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Tejan Karmali&lt;/i&gt;&lt;/b&gt;, Rishubh Parihar, Susmit Agrawal, Harsh Rangwani, &lt;b&gt;&lt;i&gt;Varun Jampani&lt;/i&gt;&lt;/b&gt;, Maneesh K Singh, Venkatesh Babu Radhakrishnan &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.13296.pdf&quot;&gt;RayTran: 3D Pose Estimation and Shape Reconstruction of Multiple Objects from Videos with Ray-Traced Transformers&lt;/a&gt;&lt;br /&gt;Michał J Tyszkiewicz, &lt;b&gt;&lt;i&gt;Kevis-Kokitsi Maninis&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Stefan Popov&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vittorio Ferrari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;   &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2107.12038.pdf&quot;&gt;Neural Video Compression Using GANs for Detail Synthesis and Propagation&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Fabian Mentzer&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Eirikur Agustsson&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Johannes Ballé&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;David Minnen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Nick Johnston&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;George Toderici&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.10664.pdf&quot;&gt;Exploring Fine-Grained Audiovisual Categorization with the SSW60 Dataset&lt;/a&gt;&lt;br /&gt;Grant Van Horn, Rui Qian, &lt;b&gt;&lt;i&gt;Kimberly Wilber&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Hartwig Adam&lt;/i&gt;&lt;/b&gt;, Oisin Mac Aodha, Serge Belongie &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2112.04267.pdf&quot;&gt;Implicit Neural Representations for Image Compression&lt;/a&gt;&lt;br /&gt;Yannick Strümpler, Janis Postels, Ren Yang, Luc Van Gool, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2111.14673.pdf&quot;&gt;3D Compositional Zero-Shot Learning with DeCompositional Consensus&lt;/a&gt;&lt;br /&gt;Muhammad Ferjad Naeem, Evin Pınar Örnek, Yongqin Xian, Luc Van Gool, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.17273.pdf&quot;&gt;FindIt: Generalized Localization with Natural Language Queries&lt;/a&gt; (see &lt;a href=&quot;https://ai.googleblog.com/2022/09/findit-generalized-object-localization.html&quot;&gt;blog post&lt;/a&gt;) &lt;br /&gt;&lt;b&gt;&lt;i&gt;Weicheng Kuo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Fred Bertsch&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Wei Li&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;AJ Piergiovanni&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Mohammad Saffar&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Anelia Angelova&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2112.09747.pdf&quot;&gt;A Simple Single-Scale Vision Transformer for Object Detection and Instance Segmentation&lt;/a&gt;&lt;br /&gt;Wuyang Chen&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Xianzhi Du&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Fan Yang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Lucas Beyer&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiaohua Zhai&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Tsung-Yi Lin&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Huizhong Chen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Jing Li&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiaodan Song&lt;/i&gt;&lt;/b&gt;, Zhangyang Wang, &lt;b&gt;&lt;i&gt;Denny Zhou&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;   &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2209.04439.pdf&quot;&gt;Improved Masked Image Generation with Token-Critic&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Jose Lezama&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Huiwen Chang&lt;/i&gt;&lt;/b&gt;, &lt;i&gt;&lt;b&gt;Lu Jiang&lt;/b&gt;&lt;/i&gt;, &lt;b&gt;&lt;i&gt;Irfan Essa&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2111.13876.pdf&quot;&gt;Learning Discriminative Shrinkage Deep Networks for Image Deconvolution&lt;/a&gt;&lt;br /&gt;Pin-Hung Kuo, Jinshan Pan, Shao-Yi Chien, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.10141.pdf&quot;&gt;AudioScopeV2: Audio-Visual Attention Architectures for Calibrated Open-Domain On-Screen Sound Separation&lt;/a&gt;&lt;br /&gt;Efthymios Tzinis&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Scott Wisdom&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Tal Remez&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;John Hershey&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2205.06230.pdf&quot;&gt;Simple Open-Vocabulary Object Detection with Vision Transformers&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Matthias Minderer&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Alexey Gritsenko&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Austin C Stone&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Maxim Neumann&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Dirk Weißenborn&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Alexey Dosovitskiy&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Aravindh Mahendran&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Anurag Arnab&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Mostafa Dehghani&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Zhuoran Shen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiao Wang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiaohua Zhai&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Thomas Kipf&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Neil Houlsby&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;   &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2112.05892.pdf&quot;&gt;COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality&lt;/a&gt;&lt;br /&gt;Honglu Zhou, Asim Kadav, Aviv Shamsian, Shijie Geng, Farley Lai, &lt;b&gt;&lt;i&gt;Long Zhao&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ting Liu&lt;/i&gt;&lt;/b&gt;, Mubbasir Kapadia, Hans Peter Graf &lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.00934.pdf&quot;&gt;Video Question Answering with Iterative Video-Text Co-tokenization&lt;/a&gt; (see &lt;a href=&quot;https://ai.googleblog.com/2022/08/efficient-video-text-learning-with.html&quot;&gt;blog post&lt;/a&gt;) &lt;br /&gt;&lt;b&gt;&lt;i&gt;AJ Piergiovanni&lt;/i&gt;&lt;/b&gt;, Kairo Morton&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Weicheng Kuo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michael S. Ryoo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Anelia Angelova&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2111.11430.pdf&quot;&gt;Class-Agnostic Object Detection with Multi-modal Transformer&lt;/a&gt;&lt;br /&gt;Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Fahad Shahbaz Khan, Rao Muhammad Anwer, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2202.04901.pdf&quot;&gt;FILM: Frame Interpolation for Large Motion&lt;/a&gt;&lt;b&gt; &lt;/b&gt;(see &lt;a href=&quot;https://ai.googleblog.com/2022/10/large-motion-frame-interpolation.html&quot;&gt;blog post&lt;/a&gt;) &lt;br /&gt;&lt;b&gt;&lt;i&gt;Fitsum Reda&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Janne Kontkanen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Eric Tabellion&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Deqing Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Caroline Pantofaru&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Brian Curless&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.12824.pdf&quot;&gt;Compositional Human-Scene Interaction Synthesis with Semantic Control&lt;/a&gt;&lt;br /&gt;Kaifeng Zhao, Shaofei Wang, Yan Zhang, &lt;b&gt;&lt;i&gt;Thabo Beeler,&lt;/i&gt;&lt;/b&gt; Siyu Tang &lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Workshops&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.latinxinai.org/eccv-2022&quot;&gt;LatinX in AI&lt;/a&gt;&lt;br /&gt;Mentors include: &lt;i&gt;&lt;b&gt;José Lezama&lt;/b&gt;&lt;/i&gt;&lt;br /&gt;Keynote Speakers include: &lt;b&gt;&lt;i&gt;Andre Araujo&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://cveu.github.io/&quot;&gt;AI for Creative Video Editing and Understanding&lt;/a&gt;&lt;br /&gt;Keynote Speakers include: &lt;b&gt;&lt;i&gt;Tali Dekel&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Negar Rostamzadeh&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://l2id.github.io/l2id2022/&quot;&gt;Learning With Limited and Imperfect Data (L2ID)&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Xiuye Gu&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Sadeep Jayasumana&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://campworkshop.org/&quot;&gt;International Challenge on Compositional and Multimodal Perception (CAMP)&lt;/a&gt;&lt;br /&gt;Program Committee includes: &lt;b&gt;&lt;i&gt;Edward Vendrow&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://sslwin.org/&quot;&gt;Self-Supervised Learning: What is Next?&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Mathilde Caron&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Arsha Nagrani&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;   Organizers include: &lt;b&gt;&lt;i&gt;Andrew Zisserman&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://eccv22-arow.github.io/&quot;&gt;3rd Workshop on Adversarial Robustness In the Real World&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Ekin Dogus Cubuk&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Xinyun Chen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Alexander Robey&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Nataniel Ruiz&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yutong Bai&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;       &lt;p&gt;&lt;a href=&quot;https://av4d.org/#speakers&quot;&gt;AV4D: Visual Learning of Sounds in Spaces&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;John Hershey&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;http://mipi-challenge.org/index.html&quot;&gt;Challenge on Mobile Intelligent Photography and Imaging (MIPI)&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Peyman Milanfar&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;http://www.robustvision.net/&quot;&gt;Robust Vision Challenge 2022&lt;/a&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Alina Kuznetsova&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://computer-vision-in-the-wild.github.io/eccv-2022/&quot;&gt;Computer Vision in the Wild&lt;/a&gt;&lt;br /&gt;Challenge Organizers include: &lt;b&gt;&lt;i&gt;Yi-Ting Chen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ye Xia&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Yin Cui&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yongqin Xian&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Neil Houlsby&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://sslad2022.github.io/pages/organizers.html&quot;&gt;Self-Supervised Learning for Next-Generation Industry-Level Autonomous Driving (SSLAD)&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Fisher Yu&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/rcv-at-eccv-2022/home&quot;&gt;Responsible Computer Vision&lt;/a&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Been Kim&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Emily Denton&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://cross-modal-human-robot-interaction.github.io/&quot;&gt;Cross-Modal Human-Robot Interaction&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Peter Anderson&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://workshop2022.isic-archive.com/#invited_speakers&quot;&gt;ISIC Skin Image Analysis&lt;/a&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Yuan Liu&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Steering Committee includes: &lt;b&gt;&lt;i&gt;Yuan Liu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Dale Webster&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Yuan Liu&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/hands2022/home&quot;&gt;Observing and Understanding Hands in Action&lt;/a&gt;&lt;br /&gt;Sponsored by Google &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://avvision.xyz/eccv22/&quot;&gt;Autonomous Vehicle Vision (AVVision)&lt;/a&gt;&lt;br /&gt;Speakers include: &lt;b&gt;&lt;i&gt;Fisher Yu&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://jrdb.erc.monash.edu/workshops/eccv2022&quot;&gt;Visual Perception for Navigation in Human Environments: The JackRabbot Human Body Pose Dataset and Benchmark&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;i&gt;&lt;b&gt;Edward Vendrow&lt;/b&gt;&lt;/i&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://languagefor3dscenes.github.io/ECCV2022/&quot;&gt;Language for 3D Scenes&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Jason Baldridge&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Leonidas Guibas&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://computerperception.github.io/&quot;&gt;Designing and Evaluating Computer Perception Systems (CoPe)&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Andrew Zisserman&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://learn3dg.github.io/&quot;&gt;Learning To Generate 3D Shapes and Scenes&lt;/a&gt;&lt;br /&gt;Panelists include: &lt;b&gt;&lt;i&gt;Pete Florence&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://data.vision.ee.ethz.ch/cvl/aim22/&quot;&gt;Advances in Image Manipulation&lt;/a&gt;&lt;br /&gt;Program Committee includes: &lt;b&gt;&lt;i&gt;George Toderici&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/tie-eccv2022&quot;&gt;TiE: Text in Everything&lt;/a&gt;&lt;br /&gt;Challenge Organizers include: &lt;b&gt;&lt;i&gt;Shangbang Long&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Siyang Qin&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Tali Dekel&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Aishwarya Agrawal&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://ilr-workshop.github.io/ECCVW2022/&quot;&gt;Instance-Level Recognition&lt;/a&gt;&lt;br /&gt;Organizing Committee: &lt;b&gt;&lt;i&gt;Andre Araujo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Bingyi Cao&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Tobias Weyand&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Mathilde Caron&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://what-is-motion-for.github.io/&quot;&gt;What Is Motion For?&lt;/a&gt;&lt;br /&gt;Organizing Committee: &lt;b&gt;&lt;i&gt;Deqing Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Fitsum Reda&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Charles Herrmann&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Tali Dekel&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://ngr-co3d.github.io/&quot;&gt;Neural Geometry and Rendering: Advances and the Common Objects in 3D Challenge&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Ben Mildenhall&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://geometry.stanford.edu/voli/&quot;&gt;Visual Object-Oriented Learning Meets Interaction: Discovery, Representations, and Applications&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Klaus Greff&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Thomas Kipf&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Leonidas Guibas&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://wvbsd.github.io/2022/index.html&quot;&gt;Vision with Biased or Scarce Data (VBSD)&lt;/a&gt;&lt;br /&gt;Program Committee includes: &lt;b&gt;&lt;i&gt;Yizhou Wang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://motcomplex.github.io/&quot;&gt;Multiple Object Tracking and Segmentation in Complex Environments&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Xingyi Zhou&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Fisher Yu&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://vipriors.github.io/&quot;&gt;3rd Visual Inductive Priors for Data-Efficient Deep Learning Workshop&lt;/a&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Ekin Dogus Cubuk&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://deeperaction.github.io/&quot;&gt;DeeperAction: Detailed Video Action Understanding and Anomaly Recognition&lt;/a&gt;&lt;br /&gt;Advisors include: &lt;b&gt;&lt;i&gt;Rahul Sukthankar&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://signlanguageworkshop.github.io/&quot;&gt;Sign Language Understanding Workshop and Sign Language Recognition, Translation &amp;amp; Production Challenge&lt;/a&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Andrew Zisserman&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Speakers include: &lt;b&gt;&lt;i&gt;Andrew Zisserman&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://ego4d-data.org/workshops/eccv22/&quot;&gt;Ego4D: First-Person Multi-Modal Video Understanding&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Michal Irani&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://vcmi.inesctec.pt/aimia_eccv/&quot;&gt;AI-Enabled Medical Image Analysis: Digital Pathology &amp;amp; Radiology/COVID19&lt;/a&gt;&lt;br /&gt;Program Chairs include: &lt;b&gt;&lt;i&gt;Po-Hsuan Cameron Chen&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Workshop Partner: &lt;b&gt;&lt;i&gt;Google Health&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://www.votchallenge.net/vot2022/index.html&quot;&gt;Visual Object Tracking Challenge (VOT 2022)&lt;/a&gt;&lt;br /&gt;Technical Committee includes: &lt;b&gt;&lt;i&gt;Christoph Mayer&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://iplab.dmi.unict.it/acvr2022/&quot;&gt;Assistive Computer Vision and Robotics&lt;/a&gt;&lt;br /&gt;Technical Committee includes: &lt;b&gt;&lt;i&gt;Maja Mataric&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/egocentric-hand-body-activity&quot;&gt;Human Body, Hands, and Activities from Egocentric and Multi-View Cameras&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Francis Engelmann&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/view/mono3d-eccv-workshop&quot;&gt;Frontiers of Monocular 3D Perception: Implicit x Explicit&lt;/a&gt;&lt;br /&gt;Panelists include: &lt;b&gt;&lt;i&gt;Pete Florence&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;div style=&quot;line-height: 40%;&quot;&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Tutorials&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://feichtenhofer.github.io/eccv2022-ssl-tutorial/&quot;&gt;Self-Supervised Representation Learning in Computer Vision&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Ting Chen&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/berkeley.edu/nerf-tutorial/home&quot;&gt;Neural Volumetric Rendering for Computer Vision&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Ben Mildenhall&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Pratul Srinivasan&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Jon Barron&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Presenters include: &lt;b&gt;&lt;i&gt;Ben Mildenhall&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Pratul Srinivasan&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href=&quot;https://sites.google.com/corp/g.ucla.edu/eccv2022-nas/home&quot;&gt;New Frontiers in Efficient Neural Architecture Search!&lt;/a&gt;&lt;br /&gt;Speakers include: &lt;b&gt;&lt;i&gt;Ruochen Wang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;!--Footnotes themselves at the bottom.--&gt;&lt;hr width=&quot;80%&quot; /&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;&gt;&lt;br /&gt;  &lt;a name=&quot;1&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt;Work done while at Google.&lt;a href=&quot;#top1&quot;&gt; &amp;nbsp;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;p&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/4407074621491257938/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/google-at-eccv-2022.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4407074621491257938" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4407074621491257938" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/google-at-eccv-2022.html" rel="alternate" title="Google at ECCV 2022 " type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWmLScVcpCyi8puS5FYC4xNN88CWeK0-77JiMlowYPlxCPym5T4zX7iKIuzIjwtDnUubUJ0yM8Xa6Mt83N_-YoBgyrZH_dwSL6YgoxUEgGf1tNuR5fSWeRY61Ut0TOhlzUuTJlKs6fyBF3JTVzAlbEOUgtGb_5gdgFBIXWFCJ0utT8kuUh3rKIjw4L1g/s72-c/about_hero.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-553665404692333245</id><published>2022-10-20T13:39:00.015-07:00</published><updated>2022-10-21T09:27:14.310-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Robotics"/><title type="text">PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Wenhao Yu, Research Scientist, Robotics at Google, and Kuang-Huei Lee, Research Engineer, Google Research, Brain team&lt;/span&gt; &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Evolution_strategy&quot;&gt;Evolution strategy&lt;/a&gt; (ES) is a family of optimization techniques inspired by the ideas of &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_selection&quot;&gt;natural selection&lt;/a&gt;: a population of candidate solutions are usually evolved over generations to better adapt to an optimization objective. ES has been applied to a variety of challenging decision making problems, such as &lt;a href=&quot;https://openreview.net/forum?id=NDYbXf-DvwZ&quot;&gt;legged locomotion&lt;/a&gt;, &lt;a href=&quot;https://ieeexplore.ieee.org/document/9307102&quot;&gt;quadcopter control&lt;/a&gt;, and even &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9477182&quot;&gt;power system control&lt;/a&gt;.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;Compared to gradient-based &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;&gt;reinforcement learning&lt;/a&gt; (RL) methods like &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;&gt;proximal policy optimization&lt;/a&gt; (PPO) and &lt;a href=&quot;https://ai.googleblog.com/2019/01/soft-actor-critic-deep-reinforcement.html&quot;&gt;soft actor-critic&lt;/a&gt; (SAC), ES has several advantages. First, ES directly explores in the space of controller parameters, while gradient-based methods often explore within a limited action space, which indirectly influences the controller parameters. More direct exploration has been shown to &lt;a href=&quot;https://openai.com/blog/better-exploration-with-parameter-noise/&quot;&gt;boost learning performance&lt;/a&gt; and enable large scale data collection with parallel computation. Second, a major challenge in RL is long-horizon credit assignment, e.g., when a robot accomplishes a task in the end, determining which actions it performed in the past were the most critical and should be assigned a greater reward. Since ES directly considers the total reward, it relieves researchers from needing to explicitly handle credit assignment. In addition, because ES does not rely on gradient information, it can naturally handle highly non-smooth objectives or controller architectures where gradient computation is non-trivial, such as &lt;a href=&quot;https://ai.googleblog.com/2020/04/exploring-evolutionary-meta-learning-in.html&quot;&gt;meta–reinforcement learning&lt;/a&gt;. However, a major weakness of ES-based algorithms is their difficulty in scaling to problems that require high-dimensional sensory inputs to encode the environment dynamics, such as training robots with complex vision inputs. &lt;/p&gt;&lt;p&gt;In this work, we propose “&lt;a href=&quot;https://arxiv.org/abs/2207.13224&quot;&gt;PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations&lt;/a&gt;”, a learning algorithm that combines &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;&gt;representation learning&lt;/a&gt; and ES to effectively solve high dimensional problems in a scalable way. The core idea is to leverage &lt;a href=&quot;https://arxiv.org/abs/cond-mat/9902341&quot;&gt;predictive information&lt;/a&gt;, a representation learning objective, to obtain a compact representation of the high-dimensional environment dynamics, and then apply &lt;a href=&quot;https://arxiv.org/abs/1803.07055&quot;&gt;Augmented Random Search&lt;/a&gt; (ARS), a popular ES algorithm, to transform the learned compact representation into robot actions. We tested PI-ARS on the challenging problem of visual-locomotion for legged robots. PI-ARS enables fast training of performant vision-based locomotion controllers that can traverse a variety of difficult environments. Furthermore, the controllers trained in simulated environments successfully transfer to a real quadruped robot. &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9-FBUlQSRlE2rFmeloCUXYYfBngkIWz6RIDURErqOxjFSd785fpbkjetz4lkfbUhxY8rDB88yOy26ml669f2WcP16SkXH8uZfy60jCgMq28Ggm__ombQNfdhvYe-1qsOF1imso96y26PjhT3pSukVmy__RHkVdKOJRbMTTS_k0hEJfWexdcbY4QJ2QA/s480/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9-FBUlQSRlE2rFmeloCUXYYfBngkIWz6RIDURErqOxjFSd785fpbkjetz4lkfbUhxY8rDB88yOy26ml669f2WcP16SkXH8uZfy60jCgMq28Ggm__ombQNfdhvYe-1qsOF1imso96y26PjhT3pSukVmy__RHkVdKOJRbMTTS_k0hEJfWexdcbY4QJ2QA/s16000/image3.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;PI-ARS trains reliable visual-locomotion policies that are transferable to the real world.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Predictive Information&lt;/h2&gt;&lt;p&gt;A good representation for policy learning should be both &lt;em&gt;compressive&lt;/em&gt;, so that ES can focus on solving a much lower dimensional problem than learning from raw observations would entail, and &lt;em&gt;task-critical&lt;/em&gt;, so the learned controller has all the necessary information needed to learn the optimal behavior. For robotic control problems with high-dimensional input space, it is critical for the policy to understand the environment, including the dynamic information of both the robot itself and its surrounding objects.  &lt;/p&gt;&lt;p&gt;As such, we propose an observation encoder that preserves information from the raw input observations that allows the policy to predict the future states of the environment, thus the name &lt;em&gt;predictive information&lt;/em&gt; (PI). More specifically, we optimize the encoder such that the encoded version of what the robot has seen and planned in the past can accurately predict what the robot might see and be rewarded in the future. One mathematical tool to describe such a property is that of &lt;a href=&quot;https://en.wikipedia.org/wiki/Mutual_information&quot;&gt;mutual information&lt;/a&gt;, which measures the amount of information we obtain about one random variable &lt;em&gt;X&lt;/em&gt; by observing another random variable &lt;em&gt;Y&lt;/em&gt;. In our case, &lt;em&gt;X&lt;/em&gt; and &lt;em&gt;Y&lt;/em&gt; would be what the robot saw and planned in the past, and what the robot sees and is rewarded in the future. Directly optimizing the mutual information objective is &lt;a href=&quot;https://arxiv.org/pdf/1905.06922.pdf&quot;&gt;a challenging problem&lt;/a&gt; because we usually only have access to samples of the random variables, but not their underlying distributions. In this work we follow &lt;a href=&quot;https://arxiv.org/abs/2007.12401&quot;&gt;a previous approach&lt;/a&gt; that uses &lt;a href=&quot;https://arxiv.org/abs/1807.03748&quot;&gt;InfoNCE&lt;/a&gt;, a contrastive variational bound on mutual information to optimize the objective. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhP4JXHr2GcWAuGdptvFApp4KqavxmxmDKZ-_OUPBfAOE85mkXcVMNreXx4gZBUp9Hw58xzy0tjKex-m7Ca8xIWpYKXkxg4JFbnljFTpWRMknSi5_Ye8DlcSZiutk7YMVQAMRO7dJb7EmxVbK8Cd1TcSrw_z6oO5kgcEp3x_zLqmpmoBU0E8kaqWlr18w/s1988/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;946&quot; data-original-width=&quot;1988&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhP4JXHr2GcWAuGdptvFApp4KqavxmxmDKZ-_OUPBfAOE85mkXcVMNreXx4gZBUp9Hw58xzy0tjKex-m7Ca8xIWpYKXkxg4JFbnljFTpWRMknSi5_Ye8DlcSZiutk7YMVQAMRO7dJb7EmxVbK8Cd1TcSrw_z6oO5kgcEp3x_zLqmpmoBU0E8kaqWlr18w/s16000/image5.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Left&lt;/b&gt;: We use representation learning to encode PI of the environment. &lt;b&gt;Right&lt;/b&gt;: We train the representation by replaying trajectories from the replay buffer and maximize the predictability between the observation and motion plan in the past and the observation and reward in the future of the trajectory.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Predictive Information with Augmented Random Search&lt;/h2&gt;&lt;p&gt;Next, we combine PI with &lt;a href=&quot;https://arxiv.org/abs/1803.07055&quot;&gt;Augmented Random Search&lt;/a&gt; (ARS), an algorithm that has shown excellent optimization performance for challenging decision-making tasks. At each iteration of ARS, it samples a population of perturbed controller parameters, evaluates their performance in the testing environment, and then computes a gradient that moves the controller towards the ones that performed better.  &lt;/p&gt;&lt;p&gt;We use the learned compact representation from PI to connect PI and ARS, which we call PI-ARS. More specifically, ARS optimizes a controller that takes as input the learned compact representation PI and predicts appropriate robot commands to achieve the task. By optimizing a controller with smaller input space, it allows ARS to find the optimal solution more efficiently. Meanwhile, we use the data collected during ARS optimization to further improve the learned representation, which is then fed into the ARS controller in the next iteration. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZRt3o2sJcpLqpk0o8cDA4qExoJlNdNeW6iZKBPXLCuyTEjrntvzV_HbXJcuqOGyPdDnjP1_AzxDi8HSEaw0vmF1G1OoTjIQmOhWOZabcgyviV9BKxMAYzmnm7elT3ymzFFXgdWytUFAI6y0uDNXJX3qxw8j8XGdRrbCNuwHvJrsmfSmJ2Spp9vgbJ5g/s1100/image1.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;750&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZRt3o2sJcpLqpk0o8cDA4qExoJlNdNeW6iZKBPXLCuyTEjrntvzV_HbXJcuqOGyPdDnjP1_AzxDi8HSEaw0vmF1G1OoTjIQmOhWOZabcgyviV9BKxMAYzmnm7elT3ymzFFXgdWytUFAI6y0uDNXJX3qxw8j8XGdRrbCNuwHvJrsmfSmJ2Spp9vgbJ5g/s16000/image1.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;An overview of the PI-ARS data flow. Our algorithm interleaves between two steps: 1) optimizing the PI objective that updates the policy, which is the weights for the neural network that extracts the learned representation; and 2) sampling new trajectories and updating the controller parameters using ARS.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Visual-Locomotion for Legged Robots&lt;/h2&gt;&lt;p&gt;We evaluate PI-ARS on the problem of visual-locomotion for legged robots. We chose this problem for two reasons: visual-locomotion is a key bottleneck for legged robots to be applied in real-world applications, and the high-dimensional vision-input to the policy and the complex dynamics in legged robots make it an ideal test-case to demonstrate the effectiveness of the PI-ARS algorithm. A demonstration of our task setup in simulation can be seen below. Policies are first trained in simulated environments, and then transferred to hardware. &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuPw15yfYlg6428R8dkbMyEtYZSBAK27PN5HF4ym4bTFJAu4gfsOdmnaCsQZq3E08H85_v1Jo5zJIqM14KzdyEZ2kWWk4BKmvsYOZOzAqcf8mMMrGELgGnyjMGdvQ9y4rIeiUxLW8RfTPoKTDn0ZhQ2446F7ZwcARIBlApK9r0AMKPJ8DnXGCz6Nu0nQ/s480/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuPw15yfYlg6428R8dkbMyEtYZSBAK27PN5HF4ym4bTFJAu4gfsOdmnaCsQZq3E08H85_v1Jo5zJIqM14KzdyEZ2kWWk4BKmvsYOZOzAqcf8mMMrGELgGnyjMGdvQ9y4rIeiUxLW8RfTPoKTDn0ZhQ2446F7ZwcARIBlApK9r0AMKPJ8DnXGCz6Nu0nQ/s16000/image2.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;An illustration of the visual-locomotion task setup. The robot is equipped with two cameras to observe the environment (illustrated by the transparent pyramids). The observations and robot state are sent to the policy to generate a high-level motion plan, such as feet landing location and desired moving speed. The high-level motion plan is then achieved by a low-level Motion Predictive Control (MPC) controller.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Experiment Results&lt;/h2&gt;&lt;p&gt;We first evaluate the PI-ARS algorithm on four challenging simulated tasks: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;em&gt;Uneven stepping stones&lt;/em&gt;: The robot needs to walk over uneven terrain while avoiding gaps.  &lt;/li&gt;&lt;li&gt;&lt;em&gt;Quincuncial piles&lt;/em&gt;: The robot needs to avoid gaps both in front and sideways.  &lt;/li&gt;&lt;li&gt;&lt;em&gt;Moving platforms&lt;/em&gt;: The robot needs to walk over stepping stones that are randomly moving horizontally or vertically. This task illustrates the flexibility of learning a vision-based policy in comparison to explicitly reconstructing the environment.  &lt;/li&gt;&lt;li&gt;&lt;em&gt;Indoor navigation&lt;/em&gt;: The robot needs to navigate to a random location while avoiding obstacles in an indoor environment. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As shown below, PI-ARS is able to significantly outperform ARS in all four tasks in terms of the total task reward it can obtain (by 30-50%). &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj66o8DzwYZV_nCYvp-On0X4nTtmUCQJTdnizSuKvwZdF7XYROIS2VN8E9QunXB4WE7VgPth9RykGhpPIhXYrtdryUyUZHRTqZbdkJIsS9SYAGMfXW4Xb_2pZjG-fqaFVf2P6Jz089mv9qLbAx59DzFNz1XIERlnZZ2u81m3h2obr0skf3IEmkKJLjg_w/s1417/image11.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1417&quot; data-original-width=&quot;900&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj66o8DzwYZV_nCYvp-On0X4nTtmUCQJTdnizSuKvwZdF7XYROIS2VN8E9QunXB4WE7VgPth9RykGhpPIhXYrtdryUyUZHRTqZbdkJIsS9SYAGMfXW4Xb_2pZjG-fqaFVf2P6Jz089mv9qLbAx59DzFNz1XIERlnZZ2u81m3h2obr0skf3IEmkKJLjg_w/s16000/image11.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Left: &lt;/b&gt;Visualization of PI-ARS policy performance in simulation. &lt;b&gt;Right: &lt;/b&gt;Total task reward (i.e., episode return) for PI-ARS (&lt;b&gt;green line&lt;/b&gt;) and ARS (&lt;b&gt;red line&lt;/b&gt;). The PI-ARS algorithm significantly outperforms ARS on four challenging visual-locomotion tasks.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;We further deploy the trained policies to a real &lt;a href=&quot;http://www.unitree.cc/e/action/ShowInfo.php?classid=6&amp;amp;id=1&quot;&gt;Laikago&lt;/a&gt; robot on two tasks: &lt;a href=&quot;https://youtu.be/59vMC3fTsuQ?t=20&quot;&gt;random stepping stone&lt;/a&gt; and &lt;a href=&quot;https://youtu.be/59vMC3fTsuQ?t=43&quot;&gt;indoor navigation&lt;/a&gt;. We demonstrate that our trained policies can successfully handle real-world tasks. Notably, the success rate of the random stepping stone task improved from 40% in &lt;a href=&quot;https://openreview.net/forum?id=NDYbXf-DvwZ&quot;&gt;the prior work&lt;/a&gt; to 100%. &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw71HD7nnemeCE7GWYPQlGx1invJqNhOpy16wwoYjKP0PQ89nxPlx9uIEDtVVajh5_gy9_Rq2k0gQtVyMHN7ivFBjhGHWlqwtFercs_Il9jnwVHQq_EwxyQTmOJVgjEqbij1dyvGxj7Jp18J1RhzpN-yyGcNwvj16KMeUJj4UtKNPO9YiFmRNzwctlFg/s480/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw71HD7nnemeCE7GWYPQlGx1invJqNhOpy16wwoYjKP0PQ89nxPlx9uIEDtVVajh5_gy9_Rq2k0gQtVyMHN7ivFBjhGHWlqwtFercs_Il9jnwVHQq_EwxyQTmOJVgjEqbij1dyvGxj7Jp18J1RhzpN-yyGcNwvj16KMeUJj4UtKNPO9YiFmRNzwctlFg/s16000/image1.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;PI-ARS trained policy enables a real Laikago robot to navigate around obstacles.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;In this work, we present a new learning algorithm, PI-ARS, that combines gradient-based representation learning with gradient-free evolutionary strategy algorithms to leverage the advantages of both. PI-ARS enjoys the effectiveness, simplicity, and parallelizability of gradient-free algorithms, while relieving a key bottleneck of ES algorithms on handling high-dimensional problems by optimizing a low-dimensional representation. We apply PI-ARS to a set of challenging visual-locomotion tasks, among which PI-ARS significantly outperforms the state of the art. Furthermore, we validate the policy learned by PI-ARS on a real quadruped robot. It enables the robot to walk over randomly-placed stepping stones and navigate in an indoor space with obstacles. Our method opens the possibility of incorporating modern large neural network models and large-scale data into the field of evolutionary strategy for robotics control. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;  &lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;We would like to thank our paper co-authors: Ofir Nachum, Tingnan Zhang, Sergio Guadarrama, and Jie Tan. We would also like to thank Ian Fischer and John Canny for valuable feedback.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/553665404692333245/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/pi-ars-accelerating-evolution-learned.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/553665404692333245" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/553665404692333245" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/pi-ars-accelerating-evolution-learned.html" rel="alternate" title="PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9-FBUlQSRlE2rFmeloCUXYYfBngkIWz6RIDURErqOxjFSd785fpbkjetz4lkfbUhxY8rDB88yOy26ml669f2WcP16SkXH8uZfy60jCgMq28Ggm__ombQNfdhvYe-1qsOF1imso96y26PjhT3pSukVmy__RHkVdKOJRbMTTS_k0hEJfWexdcbY4QJ2QA/s72-c/image3.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-2594486859371625986</id><published>2022-10-20T09:58:00.010-07:00</published><updated>2022-10-21T09:42:42.455-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computational Photography"/><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><title type="text">MUSIQ: Assessing Image Aesthetic and Technical Quality with Multi-scale Transformers</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Junjie Ke, Senior Software Engineer, and Feng Yang, Senior Staff Software Engineer, Google Research&lt;/span&gt;&lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCIbYQlkOE0n8G7IFh_Uo4KhEafpAKdpnx9swpUA06IV4kFaqN2bTjd22zaSmhVUDmzpFOAqTH73AQNtXvffeaW3UZwlI54w427v7dsDsI8_8UQEvY198hGgTCu9f0upoDn33AYsq0fqUbwu12rwb9_FVzrSTTHNewaiJyMIpEDzMURKzFdzpZOubazQ/s637/MUSIQ.png&quot; style=&quot;display: none;&quot; /&gt;&lt;p&gt;Understanding the aesthetic and technical quality of images is important for providing a better user visual experience. &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_quality&quot;&gt;Image quality assessment&lt;/a&gt; (IQA) uses models to build a bridge between an image and a user's subjective perception of its quality. In the deep learning era, many IQA approaches, such as &lt;a href=&quot;https://ai.googleblog.com/2022/08/uvq-measuring-youtubes-perceptual-video.html&quot;&gt;NIMA&lt;/a&gt;, have achieved success by leveraging the power of &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;&gt;convolutional neural networks&lt;/a&gt; (CNNs). However, CNN-based IQA models are often constrained by the fixed-size input requirement in batch training, i.e.,&lt;em&gt; &lt;/em&gt;the input images need to be resized or cropped to a fixed size shape. This preprocessing is problematic for IQA because images can have very different aspect ratios and resolutions. Resizing and cropping can impact image composition or introduce distortions, thus changing the quality of the image.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8GOrjNuB2EyMjRI_LdmmVBHWSR1DCD06iyBk-8p870oUgMOYeybTiuHEFFn5POZHEhNxc9oqPiAgbzqJWM9a809dVxhmnuJFmtYfdrR8y6AKBBK3W0WRz7mCIQda12i8QMCOO2NtF0tJUBihIbMZI7hFehuprkRfGI_3l2ToVbSyeKoz8-r69yTrjeA/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;511&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8GOrjNuB2EyMjRI_LdmmVBHWSR1DCD06iyBk-8p870oUgMOYeybTiuHEFFn5POZHEhNxc9oqPiAgbzqJWM9a809dVxhmnuJFmtYfdrR8y6AKBBK3W0WRz7mCIQda12i8QMCOO2NtF0tJUBihIbMZI7hFehuprkRfGI_3l2ToVbSyeKoz8-r69yTrjeA/s16000/image1.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;In CNN-based models, images need to be resized or cropped to a fixed shape for batch training. However, such preprocessing can alter the image aspect ratio and composition, thus impacting image quality. &lt;a href=&quot;https://www.flickr.com/photos/ddebold/3052189192&quot;&gt;Original image&lt;/a&gt; used under &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;&gt;CC BY 2.0 license&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In “&lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Ke_MUSIQ_Multi-Scale_Image_Quality_Transformer_ICCV_2021_paper.pdf&quot;&gt;MUSIQ: Multi-scale Image Quality Transformer&lt;/a&gt;”, published at &lt;a href=&quot;https://iccv2021.thecvf.com/home&quot;&gt;ICCV 2021&lt;/a&gt;, we propose a patch-based multi-scale image quality &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;transformer&lt;/a&gt; (MUSIQ) to bypass the CNN constraints on fixed input size and predict the image quality effectively on native-resolution images. The MUSIQ model supports the processing of full-size image inputs with varying aspect ratios and resolutions and allows multi-scale feature extraction to capture image quality at different granularities. To support positional encoding in the multi-scale representation, we propose a novel hash-based 2D spatial embedding combined with an embedding that captures the image scaling. We apply MUSIQ on four large-scale IQA datasets, demonstrating consistent state-of-the-art results across three technical quality datasets (&lt;a href=&quot;https://baidut.github.io/PaQ-2-PiQ/&quot;&gt;PaQ-2-PiQ&lt;/a&gt;, &lt;a href=&quot;http://database.mmsp-kn.de/koniq-10k-database.html&quot;&gt;KonIQ-10k&lt;/a&gt;, and &lt;a href=&quot;https://github.com/h4nwei/SPAQ&quot;&gt;SPAQ&lt;/a&gt;) and comparable performance to that of state-of-the-art models on the aesthetic quality dataset &lt;a href=&quot;http://refbase.cvc.uab.es/files/MMP2012a.pdf&quot;&gt;AVA&lt;/a&gt;. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCIbYQlkOE0n8G7IFh_Uo4KhEafpAKdpnx9swpUA06IV4kFaqN2bTjd22zaSmhVUDmzpFOAqTH73AQNtXvffeaW3UZwlI54w427v7dsDsI8_8UQEvY198hGgTCu9f0upoDn33AYsq0fqUbwu12rwb9_FVzrSTTHNewaiJyMIpEDzMURKzFdzpZOubazQ/s637/MUSIQ.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;530&quot; data-original-width=&quot;637&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCIbYQlkOE0n8G7IFh_Uo4KhEafpAKdpnx9swpUA06IV4kFaqN2bTjd22zaSmhVUDmzpFOAqTH73AQNtXvffeaW3UZwlI54w427v7dsDsI8_8UQEvY198hGgTCu9f0upoDn33AYsq0fqUbwu12rwb9_FVzrSTTHNewaiJyMIpEDzMURKzFdzpZOubazQ/s16000/MUSIQ.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The patch-based MUSIQ model can process the full-size image and extract multi-scale features, which better aligns with a person’s typical visual response.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In the following figure, we show a sample of images, their MUSIQ score, and their mean opinion score (MOS) from multiple human raters in the brackets. The range of the score is from 0 to 100, with 100 being the highest perceived quality. As we can see from the figure, MUSIQ predicts high scores for images with high aesthetic quality and high technical quality, and it predicts low scores for images that are not aesthetically pleasing (low aesthetic quality) or that contain visible distortions (low technical quality). &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;    &lt;td rowspan=&quot;2&quot; style=&quot;text-align: left;&quot;&gt;High quality&lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiztsZ31x6DTJz03BbHk3jlNbqutJWQIvbSInQYJuAQaMxVynSMlcaKzdRFOvR_6bWp1UujU-Rzx8qffWIArNT1hyTHrk136OkGF1XXPW2HYwNNVwNZrqaxZJDB-ScT9G1hD7xns8BYHxPFC0OORytBsPSAcvs5KH0S6YEkqp0N1TN6d6IR-y6VHTdqg/s1024/image10.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;1024&quot; height=&quot;150&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiztsZ31x6DTJz03BbHk3jlNbqutJWQIvbSInQYJuAQaMxVynSMlcaKzdRFOvR_6bWp1UujU-Rzx8qffWIArNT1hyTHrk136OkGF1XXPW2HYwNNVwNZrqaxZJDB-ScT9G1hD7xns8BYHxPFC0OORytBsPSAcvs5KH0S6YEkqp0N1TN6d6IR-y6VHTdqg/w200-h150/image10.png&quot; width=&quot;200&quot; /&gt;&lt;/a&gt;&lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr0DKaAoO6qTrJo3hXP8UM3D4AB8gQeNI22Q2QphBVGgn-5v84tjhH3ZWTlGtlUoPdlcx54dM93Qi04MuN7eBbj9WlT8Qxy6B2Us4kcn_53FH28MnTtGCzMPhjCVGIgXRL8ZEMeO-7iue7sNEGxBtgx2bI-eKDQAondM8Dfjb1FaybFgUQji4UU9-0vQ/s1024/image9.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;1024&quot; height=&quot;150&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr0DKaAoO6qTrJo3hXP8UM3D4AB8gQeNI22Q2QphBVGgn-5v84tjhH3ZWTlGtlUoPdlcx54dM93Qi04MuN7eBbj9WlT8Qxy6B2Us4kcn_53FH28MnTtGCzMPhjCVGIgXRL8ZEMeO-7iue7sNEGxBtgx2bI-eKDQAondM8Dfjb1FaybFgUQji4UU9-0vQ/w200-h150/image9.png&quot; width=&quot;200&quot; /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;font-size: small; text-align: center;&quot;&gt;76.10 [74.36]&lt;/td&gt;    &lt;td style=&quot;font-size: small; text-align: center;&quot;&gt;69.29 [70.92]&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;    &lt;td rowspan=&quot;2&quot; style=&quot;text-align: left;&quot;&gt;Low aesthetics quality&lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAmXa-tOH5bOTKSqfUq0vPszEMxbCsC3dHhclyFaTwx5hvkBhc6uZzwRLcVDCPmP5KXN0PVt0hvIKrgpAaw1Qd1ujuBFL-gBAmM4BZJQ5fR3K-_ItIiUKz-UgEOAPMBeTEVbEQkKx8Zm8KKQjaGHWIE6LIXSQMM6tfGur89uUU7O8HURJF9QfYWU3h6Q/s1024/image5.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;1024&quot; height=&quot;150&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAmXa-tOH5bOTKSqfUq0vPszEMxbCsC3dHhclyFaTwx5hvkBhc6uZzwRLcVDCPmP5KXN0PVt0hvIKrgpAaw1Qd1ujuBFL-gBAmM4BZJQ5fR3K-_ItIiUKz-UgEOAPMBeTEVbEQkKx8Zm8KKQjaGHWIE6LIXSQMM6tfGur89uUU7O8HURJF9QfYWU3h6Q/w200-h150/image5.png&quot; width=&quot;200&quot; /&gt;&lt;/a&gt;&lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0-h8-32jlpKIrqt_HYtFEUX8PwZgmd1ig1_yUUpIRQOruypWk_AdywrmMPeISTGTxHfSy4WZy5xwQBCT6EweDT5jC4I9_48R2Aqs-x3-JvQM1lSPxmDfvjo6V8MuZjI3tAU4R9onsJQDSk4BoTSOeziNMN5A0N9trpq2PllpP3GiVW2pBQKyCb4kDqw/s1024/image11.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;1024&quot; height=&quot;150&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0-h8-32jlpKIrqt_HYtFEUX8PwZgmd1ig1_yUUpIRQOruypWk_AdywrmMPeISTGTxHfSy4WZy5xwQBCT6EweDT5jC4I9_48R2Aqs-x3-JvQM1lSPxmDfvjo6V8MuZjI3tAU4R9onsJQDSk4BoTSOeziNMN5A0N9trpq2PllpP3GiVW2pBQKyCb4kDqw/w200-h150/image11.png&quot; width=&quot;200&quot; /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;font-size: small; text-align: center;&quot;&gt;55.37 [53.18]&lt;/td&gt;    &lt;td style=&quot;font-size: small; text-align: center;&quot;&gt;32.50 [35.47]&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;    &lt;td rowspan=&quot;2&quot; style=&quot;text-align: left;&quot;&gt;Low technical quality&lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwCvrl2K6kVpYBy7SWwmTFfg7Xv09im0bnS_UYHF05O7IQTlFg5kktUF-LYCfklepbSKncood20CB6UroQhE8hpV_7xm1EKhKElTD5EOIOx8gL1psXQbdTsTe1HE43YBzBfhjMCDBe2NI7eq8fKrXJrw36QwmwoUQueQQp2IIVqWuWGKsBdi8ET57e5g/s1024/image2.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;1024&quot; height=&quot;150&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwCvrl2K6kVpYBy7SWwmTFfg7Xv09im0bnS_UYHF05O7IQTlFg5kktUF-LYCfklepbSKncood20CB6UroQhE8hpV_7xm1EKhKElTD5EOIOx8gL1psXQbdTsTe1HE43YBzBfhjMCDBe2NI7eq8fKrXJrw36QwmwoUQueQQp2IIVqWuWGKsBdi8ET57e5g/w200-h150/image2.png&quot; width=&quot;200&quot; /&gt;&lt;/a&gt;&lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh--6ooAFsWYmPRh9OlTO9mA8GE0yGoy_e-mIAe2MT1XVuFMNABUw3XE6npSoyfe0pN4QXEl4GHxs_RzBx-E8EK5ge3QUBi2ZLRrNjEsevYoIRQSLPmiJkR80GT1glU-DB7xwxrU7zAMfDXnNt7_-6fLNaKI_qfxn1foBdx8ZRMv_PeJDiTZbZ1DYp6Ow/s1024/image12.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;1024&quot; height=&quot;150&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh--6ooAFsWYmPRh9OlTO9mA8GE0yGoy_e-mIAe2MT1XVuFMNABUw3XE6npSoyfe0pN4QXEl4GHxs_RzBx-E8EK5ge3QUBi2ZLRrNjEsevYoIRQSLPmiJkR80GT1glU-DB7xwxrU7zAMfDXnNt7_-6fLNaKI_qfxn1foBdx8ZRMv_PeJDiTZbZ1DYp6Ow/w200-h150/image12.png&quot; width=&quot;200&quot; /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;font-size: small; text-align: center;&quot;&gt;14.93 [14.38]&lt;/td&gt;    &lt;td style=&quot;font-size: small; text-align: center;&quot;&gt;15.24 [11.86]&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Predicted MUSIQ score (and ground truth) on images from the &lt;a href=&quot;http://database.mmsp-kn.de/koniq-10k-database.html&quot;&gt;KonIQ-10k&lt;/a&gt; dataset. &lt;b&gt;Top: &lt;/b&gt;MUSIQ predicts high scores for high quality images. &lt;b&gt;Middle: &lt;/b&gt;MUSIQ predicts low scores for images with low aesthetic quality, such as images with poor composition or lighting. &lt;b&gt;Bottom: &lt;/b&gt;MUSIQ predicts low scores for images with low technical quality, such as images with visible distortion artifacts (e.g., blurry, noisy).&lt;/td&gt;&lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;The Multi-scale Image Quality Transformer&lt;/h2&gt;&lt;p&gt;MUSIQ tackles the challenge of learning IQA on full-size images. Unlike CNN-models that are often constrained to fixed resolution, MUSIQ can handle inputs with arbitrary aspect ratios and resolutions. &lt;/p&gt;&lt;p&gt;To accomplish this, we first make a multi-scale representation of the input image, containing the native resolution image and its resized variants. To preserve the image composition, we maintain its aspect ratio during resizing. After obtaining the pyramid of images, we then partition the images at different scales into fixed-size patches that are fed into the model.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBa-dfw5Y5O_KldCjZw3c6XM2u96fM4Tr5CV21fpE2fzS-tibqIzRKb834UySVwXeoxu2Yn4RfQM40M8YJPpWWJOIiIwdnnH8RoXJB82RIESYNFYEACv2r1YCKyghVtl9WLW4qFrVKDzyvr25oQGhcfKiIC31Csf0izXpCFpQ2f0Ym2W-LRe4-KKs7vA/s1344/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;708&quot; data-original-width=&quot;1344&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBa-dfw5Y5O_KldCjZw3c6XM2u96fM4Tr5CV21fpE2fzS-tibqIzRKb834UySVwXeoxu2Yn4RfQM40M8YJPpWWJOIiIwdnnH8RoXJB82RIESYNFYEACv2r1YCKyghVtl9WLW4qFrVKDzyvr25oQGhcfKiIC31Csf0izXpCFpQ2f0Ym2W-LRe4-KKs7vA/s16000/image3.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Illustration of the multi-scale image representation in MUSIQ.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Since patches are from images of varying resolutions, we need to effectively encode the multi-aspect-ratio multi-scale input into a sequence of tokens, capturing both the pixel, spatial, and scale information. To achieve this, we design three encoding components in MUSIQ, including: 1) a patch encoding module to encode patches extracted from the multi-scale representation; 2) a novel hash-based spatial embedding module to encode the 2D spatial position for each patch; and 3) a learnable scale embedding to encode different scales. In this way, we can effectively encode the multi-scale input as a sequence of tokens, serving as the input to the Transformer encoder. &lt;/p&gt;&lt;p&gt;To predict the final image quality score, we use the standard approach of prepending an additional learnable “&lt;a href=&quot;https://aclanthology.org/N19-1423/&quot;&gt;classification token&lt;/a&gt;” (CLS). The CLS token state at the output of the Transformer encoder serves as the final image representation. We then add a fully connected layer on top to predict the IQS. The figure below provides an overview of the MUSIQ model. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbR9OlJ3OqPoS9WsMVVIu4QQJBsM6ylGupmX0KKpQ0NulLa3IlH1u7hGwDY_EMZemhiC6nSE-5LQJeCRXDleG_Ko5NyAeRzoXqBBElywfhq1QoL8AXHTUTKnuVf_JjgR2phf-BOxEWR98Yx2_dRRbCJ8NIC3Z1mjfDPeYAu-vM7iHT6iSUZZyJvsm0jg/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1158&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbR9OlJ3OqPoS9WsMVVIu4QQJBsM6ylGupmX0KKpQ0NulLa3IlH1u7hGwDY_EMZemhiC6nSE-5LQJeCRXDleG_Ko5NyAeRzoXqBBElywfhq1QoL8AXHTUTKnuVf_JjgR2phf-BOxEWR98Yx2_dRRbCJ8NIC3Z1mjfDPeYAu-vM7iHT6iSUZZyJvsm0jg/s16000/image7.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Overview of MUSIQ. The multi-scale multi-resolution input will be encoded by three components: the scale embedding (SCE), the hash-based 2D spatial embedding (HSE), and the multi-scale patch embedding (MPE).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Since MUSIQ only changes the input encoding, it is compatible with any Transformer variants. To demonstrate the effectiveness of the proposed method, in our experiments we use the classic Transformer with a relatively lightweight setting so that the model size is comparable to &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf&quot;&gt;ResNet-50&lt;/a&gt;. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Benchmark and Evaluation&lt;/h2&gt;&lt;p&gt;To evaluate MUSIQ, we run experiments on multiple large-scale IQA datasets. On each dataset, we report the &lt;a href=&quot;https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&quot;&gt;Spearman’s rank correlation coefficient&lt;/a&gt; (SRCC) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&quot;&gt;Pearson linear correlation coefficient&lt;/a&gt; (PLCC) between our model prediction and the human evaluators’ mean opinion score. SRCC and PLCC are correlation metrics ranging from -1 to 1. Higher PLCC and SRCC means better alignment between model prediction and human evaluation. The graph below shows that MUSIQ outperforms other methods on &lt;a href=&quot;https://baidut.github.io/PaQ-2-PiQ/&quot;&gt;PaQ-2-PiQ&lt;/a&gt;, &lt;a href=&quot;http://database.mmsp-kn.de/koniq-10k-database.html&quot;&gt;KonIQ-10k&lt;/a&gt;, and &lt;a href=&quot;https://github.com/h4nwei/SPAQ&quot;&gt;SPAQ&lt;/a&gt;.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz8OWuZJCzDHkEJo8ezsscPcFNDKcp0cWulefa4S2P0P9UiH4BglcOfak5zdnUckxGTaiF5yI85vC1JIH4ZBLQN4UEp-4CK5RJJIoX2hXM97oQ6F1KH3tpdDa773RB8kT45UePSuiNHX6OxEr9lRLwYU8ROuHsVMHkMDYjQE7kHiXp6hmWG4gL02h2sg/s1192/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;906&quot; data-original-width=&quot;1192&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz8OWuZJCzDHkEJo8ezsscPcFNDKcp0cWulefa4S2P0P9UiH4BglcOfak5zdnUckxGTaiF5yI85vC1JIH4ZBLQN4UEp-4CK5RJJIoX2hXM97oQ6F1KH3tpdDa773RB8kT45UePSuiNHX6OxEr9lRLwYU8ROuHsVMHkMDYjQE7kHiXp6hmWG4gL02h2sg/s16000/image4.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Performance comparison of MUSIQ and previous state-of-the-art (SOTA) methods on four large-scale IQA datasets. On each dataset we compare the Spearman’s rank correlation coefficient (SRCC) and Pearson linear correlation coefficient (PLCC) of model prediction and ground truth.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Notably, the &lt;a href=&quot;https://baidut.github.io/PaQ-2-PiQ/&quot;&gt;PaQ-2-PiQ&lt;/a&gt; test set is entirely composed of large pictures having at least one dimension exceeding 640 pixels. This is very challenging for traditional deep learning approaches, which require resizing. MUSIQ can outperform previous methods by a large margin on the full-size test set, which verifies its robustness and effectiveness. &lt;/p&gt;&lt;p&gt;It is also worth mentioning that previous CNN-based methods often required sampling as many as 20 crops for each image during testing. This kind of multi-crop ensemble is a way to mitigate the fixed shape constraint in the CNN models. But since each crop is only a sub-view of the whole image, the ensemble is still an approximate approach. Moreover, CNN-based methods both add additional inference cost for every crop and, because they sample different crops, they can introduce randomness in the result. In contrast, because MUSIQ takes the full-size image as input, it can directly learn the best aggregation of information across the full image and it only needs to run the inference once. &lt;/p&gt;&lt;p&gt;To further verify that the MUSIQ model captures different information at different scales, we visualize the attention weights on each image at different scales.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_NxfcTrlUG2pHSP4KwPJnfykJuon-5yBH4irHlfDMv0lGYmGLK6VUnbXnUS2phWcnJ4QabsiNt_fV_AlJYJ3bGOPEPNFZjnxpPaxrjATVBj-PC-aIzr1rk5UcV5GsZgw7Mv85PPJdJYJJ3T1t9HqW22s1dPjitfaBLmZ2X124GOA91lP2PE-xWdJ4wg/s637/Attention.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;498&quot; data-original-width=&quot;637&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_NxfcTrlUG2pHSP4KwPJnfykJuon-5yBH4irHlfDMv0lGYmGLK6VUnbXnUS2phWcnJ4QabsiNt_fV_AlJYJ3bGOPEPNFZjnxpPaxrjATVBj-PC-aIzr1rk5UcV5GsZgw7Mv85PPJdJYJJ3T1t9HqW22s1dPjitfaBLmZ2X124GOA91lP2PE-xWdJ4wg/s16000/Attention.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Attention visualization from the output tokens to the multi-scale representation, including the original resolution image and two proportionally resized images. Brighter areas indicate higher attention, which means that those areas are more important for the model output. Images for illustration are taken from the &lt;a href=&quot;http://refbase.cvc.uab.es/files/MMP2012a.pdf&quot;&gt;AVA&lt;/a&gt; dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We observe that MUSIQ tends to focus on more detailed areas in the full, high-resolution images and on more global areas on the resized ones. For example, for the flower photo above, the model’s attention on the original image is focusing on the pedal details, and the attention shifts to the buds at lower resolutions. This shows that the model learns to capture image quality at different granularities. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We propose a multi-scale image quality transformer (MUSIQ), which can handle full-size image input with varying resolutions and aspect ratios. By transforming the input image to a multi-scale representation with both global and local views, the model can capture the image quality at different granularities. Although MUSIQ is designed for IQA, it can be applied to other scenarios where task labels are sensitive to image resolution and aspect ratio. The MUSIQ model and checkpoints are available at our &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/musiq&quot;&gt;GitHub repository&lt;/a&gt;.&lt;br /&gt;&lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;This work is made possible through a collaboration spanning several teams across Google. We’d like to acknowledge contributions from Qifei Wang, Yilin Wang and Peyman Milanfar.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/2594486859371625986/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/musiq-assessing-image-aesthetic-and.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2594486859371625986" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2594486859371625986" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/musiq-assessing-image-aesthetic-and.html" rel="alternate" title="MUSIQ: Assessing Image Aesthetic and Technical Quality with Multi-scale Transformers" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCIbYQlkOE0n8G7IFh_Uo4KhEafpAKdpnx9swpUA06IV4kFaqN2bTjd22zaSmhVUDmzpFOAqTH73AQNtXvffeaW3UZwlI54w427v7dsDsI8_8UQEvY198hGgTCu9f0upoDn33AYsq0fqUbwu12rwb9_FVzrSTTHNewaiJyMIpEDzMURKzFdzpZOubazQ/s72-c/MUSIQ.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7866944510509103804</id><published>2022-10-19T10:57:00.006-07:00</published><updated>2022-11-03T08:48:04.103-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><title type="text">Do Modern ImageNet Classifiers Accurately Predict Perceptual Similarity?</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Manoj Kumar, Research Engineer, and Ekin Dogus Cubuk, Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;The task of determining the similarity between images is an open problem in computer vision and is crucial for evaluating the realism of machine-generated images. Though there are a number of straightforward methods of estimating image similarity (e.g., low-level metrics that measure pixel differences, such as &lt;a href=&quot;https://ieeexplore.ieee.org/document/5705575&quot;&gt;FSIM&lt;/a&gt; and &lt;a href=&quot;https://ieeexplore.ieee.org/document/1284395&quot;&gt;SSIM&lt;/a&gt;), in many cases, the measured similarity differences do not match the differences perceived by a person. However, &lt;a href=&quot;https://arxiv.org/abs/1801.03924&quot;&gt;more recent work&lt;/a&gt; has demonstrated that intermediate representations of neural network classifiers, such as &lt;a href=&quot;https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&quot;&gt;AlexNet&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;&gt;VGG&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;SqueezeNet&lt;/a&gt; trained on &lt;a href=&quot;https://image-net.org/index.php&quot;&gt;ImageNet&lt;/a&gt;, exhibit perceptual similarity as an emergent property. That is, &lt;a href=&quot;https://en.wikipedia.org/wiki/Euclidean_distance&quot;&gt;Euclidean distances&lt;/a&gt; between encoded representations of images by ImageNet-trained models correlate much better with a person’s judgment of differences between images than estimating perceptual similarity directly from image pixels. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguCNOD5qe7A72gZv0sl7QIgLv4HL06sCxEzVQLELH8Affzu_UzyEFJPj2nGU5PdEFO6h6XErHLFZqpFgcrJQZO0sif3zH70avGxMsop1IBQvfFKTBKfdJ-6OBYdIODCS9xopyjaIRRmcNR58EygavFwwxrGxqlOLidEkIvZxvHCIUdBsnczvy_EC7C/s1379/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;471&quot; data-original-width=&quot;1379&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguCNOD5qe7A72gZv0sl7QIgLv4HL06sCxEzVQLELH8Affzu_UzyEFJPj2nGU5PdEFO6h6XErHLFZqpFgcrJQZO0sif3zH70avGxMsop1IBQvfFKTBKfdJ-6OBYdIODCS9xopyjaIRRmcNR58EygavFwwxrGxqlOLidEkIvZxvHCIUdBsnczvy_EC7C/s16000/image5.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Two sets of sample images from the &lt;a href=&quot;https://github.com/richzhang/PerceptualSimilarity&quot;&gt;BAPPS dataset&lt;/a&gt;. Trained networks agree more with human judgements as compared to low-level metrics (&lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;&gt;PSNR&lt;/a&gt;, SSIM, FSIM). Image source: &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html&quot;&gt;Zhang et al. (2018)&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In “&lt;a href=&quot;https://openreview.net/forum?id=qrGKGZZvH0&quot;&gt;Do better ImageNet classifiers assess perceptual similarity better?&lt;/a&gt;” published in &lt;em&gt;&lt;a href=&quot;https://www.jmlr.org/tmlr/&quot;&gt;Transactions on Machine Learning Research&lt;/a&gt;&lt;/em&gt;, we contribute an extensive experimental study on the relationship between the accuracy of ImageNet classifiers and their emergent ability to capture perceptual similarity. To evaluate this emergent ability, we follow &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html&quot;&gt;previous work&lt;/a&gt; in measuring the perceptual scores (PS), which is roughly the correlation between human preferences to that of a model for image similarity on the &lt;a href=&quot;https://github.com/richzhang/PerceptualSimilarity&quot;&gt;BAPPS dataset&lt;/a&gt;. While prior work studied the first generation of ImageNet classifiers, such as AlexNet, SqueezeNet and VGG, we significantly increase the scope of the analysis incorporating modern classifiers, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Residual_neural_network&quot;&gt;ResNets&lt;/a&gt; and &lt;a href=&quot;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&quot;&gt;Vision Transformers&lt;/a&gt; (ViTs), across a wide range of hyper-parameters. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Relationship Between Accuracy and Perceptual Similarity&lt;/h2&gt;&lt;p&gt;It is well established that features learned via training on ImageNet transfer well to a number of &lt;a href=&quot;https://arxiv.org/abs/1910.04867&quot;&gt;downstream&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1705.07750&quot;&gt;tasks&lt;/a&gt;, making ImageNet pre-training a standard recipe. Further, better accuracy on ImageNet usually implies better performance on a diverse set of downstream tasks, such as &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html&quot;&gt;robustness to common corruptions&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1902.10811&quot;&gt;out-of-distribution&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.04649&quot;&gt;generalization&lt;/a&gt; and transfer learning on &lt;a href=&quot;https://arxiv.org/abs/1805.08974&quot;&gt;smaller classification datasets&lt;/a&gt;. Contrary to prevailing evidence that suggests models with high validation accuracies on ImageNet are likely to transfer better to other tasks, surprisingly, we find that representations from underfit ImageNet models with modest validation accuracies achieve the best perceptual scores. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ4bKam5PwhK_KhnWYVYdt4vPCrYTLF1_39bAR5Ofpn74gYD9dq3kqDQ8g_fYHioVBh2_w26_nKu0fEQN5WK5zI8hZO_Y--RZxGaSUHHTn2nzhvWeUedJubE2vd2CRRh59maGf8alD5jyRTy2p5OXuz-T-dsj6BKogLd7v4zcg-CqSMCRhtIV4l9FQ7A/s800/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;506&quot; data-original-width=&quot;800&quot; height=&quot;253&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ4bKam5PwhK_KhnWYVYdt4vPCrYTLF1_39bAR5Ofpn74gYD9dq3kqDQ8g_fYHioVBh2_w26_nKu0fEQN5WK5zI8hZO_Y--RZxGaSUHHTn2nzhvWeUedJubE2vd2CRRh59maGf8alD5jyRTy2p5OXuz-T-dsj6BKogLd7v4zcg-CqSMCRhtIV4l9FQ7A/w400-h253/image6.gif&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Plot of perceptual scores (PS) on the 64 × 64 &lt;a href=&quot;https://github.com/richzhang/PerceptualSimilarity&quot;&gt;BAPPS dataset&lt;/a&gt; (y-axis) against the &lt;a href=&quot;https://image-net.org/index.php&quot;&gt;ImageNet&lt;/a&gt; 64 × 64 validation accuracies (x-axis). Each blue dot represents an ImageNet classifier. Better ImageNet classifiers achieve better PS up to a certain point (dark blue), beyond which improving the accuracy lowers the PS. The best PS are attained by classifiers with moderate accuracy (20.0–40.0).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKFHZ-d3xni61fU1_S2ZjAg66jPP_y3tYeZCuOpdsV7seOiMYhclYtG2fo0XmYKjwwjt720MJd3_uHmJ5LbgO-msPp5k9UWW5_Xlts_6qx-Botu2ZNWOkDERxYTpPGzed_xWSMWr8wRfEIA9ulQMwoG7nDwiMurPHodNVZ_S2tds-5MXe2FiHlyacq/s530/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;344&quot; data-original-width=&quot;530&quot; height=&quot;260&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKFHZ-d3xni61fU1_S2ZjAg66jPP_y3tYeZCuOpdsV7seOiMYhclYtG2fo0XmYKjwwjt720MJd3_uHmJ5LbgO-msPp5k9UWW5_Xlts_6qx-Botu2ZNWOkDERxYTpPGzed_xWSMWr8wRfEIA9ulQMwoG7nDwiMurPHodNVZ_S2tds-5MXe2FiHlyacq/w400-h260/image6.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Plot of perceptual scores (PS) on the 64 × 64 &lt;a href=&quot;https://github.com/richzhang/PerceptualSimilarity&quot;&gt;BAPPS Dataset&lt;/a&gt; (y-axis) against the &lt;a href=&quot;https://image-net.org/index.php&quot;&gt;ImageNet&lt;/a&gt; 64 × 64 validation accuracies (x-axis). Each blue dot represents an ImageNet classifier. Better ImageNet classifiers achieve better PS up to a certain point (dark blue), beyond which improving the accuracy lowers the PS. The best PS are attained by classifiers with moderate accuracy (20.0–40.0).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;--&gt;  &lt;p&gt;We study the variation of perceptual scores as a function of neural network hyperparameters: width, depth, number of training steps, &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2&quot;&gt;weight decay&lt;/a&gt;,&lt;a href=&quot;https://ieeexplore.ieee.org/document/7780677&quot;&gt; label smoothing&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Dilution_(neural_networks)&quot;&gt;dropout&lt;/a&gt;. For each hyperparameter, there exists an optimal accuracy up to which improving accuracy improves PS. This optimum is fairly low and is attained quite early in the hyperparameter sweep. Beyond this point, improved classifier accuracy corresponds to worse PS. &lt;/p&gt;&lt;p&gt;As illustration, we present the variation of PS with respect to two hyperparameters: training steps in ResNets and width in ViTs. The PS of ResNet-50 and ResNet-200 peak very early at the first few epochs of training. After the peak, PS of better classifiers decrease more drastically. ResNets are trained with a learning rate schedule that causes a stepwise increase in accuracy as a function of training steps. Interestingly, after the peak, they also exhibit a step-wise decrease in PS that matches this step-wise accuracy increase.&lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgg-BBJ6hr9O1Ixc77DP1usrpbUrYgst2OM88mU79csMXwfNv06H4dam1LHjZIZjNrSLOXPhG-_IEHWwoGadhW_kylZviGcZtRvNSm1VtcpLjhm3QCTKs1KnnFx_bYHWvt9yfnlO5DPK5lDjYf9t4QEHDY-i7t1StkcO7PEh-ushZpP56PBrK3L6Yx8/s423/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;276&quot; data-original-width=&quot;423&quot; height=&quot;261&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgg-BBJ6hr9O1Ixc77DP1usrpbUrYgst2OM88mU79csMXwfNv06H4dam1LHjZIZjNrSLOXPhG-_IEHWwoGadhW_kylZviGcZtRvNSm1VtcpLjhm3QCTKs1KnnFx_bYHWvt9yfnlO5DPK5lDjYf9t4QEHDY-i7t1StkcO7PEh-ushZpP56PBrK3L6Yx8/w400-h261/image8.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8vyrykb9SDnaz1HH2mkFokJ9tzUnzBQYk8_SJ__HfAAzooORF5KYXXfOglAC2K3wk7zERnYJ4xnzkhj6v43kK49edBHK2IWP8uY-SZHLDLvFYbvI1v21pNO9mANR1RKnBDP_t7_5hbY2RNruKHoPLusUqCPNMkDRyesC-BhsS06lIsxQB4j0vfCJn/s423/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;276&quot; data-original-width=&quot;423&quot; height=&quot;261&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8vyrykb9SDnaz1HH2mkFokJ9tzUnzBQYk8_SJ__HfAAzooORF5KYXXfOglAC2K3wk7zERnYJ4xnzkhj6v43kK49edBHK2IWP8uY-SZHLDLvFYbvI1v21pNO9mANR1RKnBDP_t7_5hbY2RNruKHoPLusUqCPNMkDRyesC-BhsS06lIsxQB4j0vfCJn/w400-h261/image3.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Early-stopped ResNets attain the best PS across different depths of 6, 50 and 200.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;ViTs consist of a stack of transformer blocks applied to the input image. The width of a ViT model is the number of output neurons of a single transformer block. Increasing its width is an effective way to improve its accuracy. Here, we vary the width of two ViT variants, B/8 and L/4 (i.e., Base and Large ViT models with patch sizes 4 and 8 respectively), and evaluate both the accuracy and PS. Similar to our observations with early-stopped ResNets, narrower ViTs with lower accuracies perform better than the default widths. Surprisingly, the optimal width of ViT-B/8 and ViT-L/4 are 6 and 12% of their default widths.&amp;nbsp;For a more comprehensive list of experiments involving other hyperparameters such as width, depth, number of training steps, weight decay, label smoothing and dropout across both ResNets and ViTs, check out &lt;a href=&quot;https://openreview.net/pdf?id=qrGKGZZvH0&quot;&gt;our paper&lt;/a&gt;.&lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDl21gStyRmEcR-61uVBmHVyWVdd9dQxWyglkbBnz1EzTisb1h9kKxesv_0dw5uZpd6tw8s4YJ-eswj5OIEPO1DfvHbTOrfERnnwo4OwE3kmQewrZJHpgc57Bicye5eQCJYT9NQMkhP8SnCxk6xrJhM2DCrSeAz0nGfiPgHJUAgFnR52V_RShRq-yq/s423/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;276&quot; data-original-width=&quot;423&quot; height=&quot;261&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDl21gStyRmEcR-61uVBmHVyWVdd9dQxWyglkbBnz1EzTisb1h9kKxesv_0dw5uZpd6tw8s4YJ-eswj5OIEPO1DfvHbTOrfERnnwo4OwE3kmQewrZJHpgc57Bicye5eQCJYT9NQMkhP8SnCxk6xrJhM2DCrSeAz0nGfiPgHJUAgFnR52V_RShRq-yq/w400-h261/image7.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFhvSTKUw16K74BqmHea-iiWfUioxac-bM2zYcDAw-wkcNdYxvacibnZFkeva3hSvelhVCPd02YNYNyc4PT22Jo4M9Ek-xS0bJUQx2sggz3sJTySePP_HvkOPG_rHOyL4rQGMGplQMoKK15mq0KMBqsiAUHy7l5cIczHGfHhZumHwGbEfu62rcFwGK/s423/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;276&quot; data-original-width=&quot;423&quot; height=&quot;261&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFhvSTKUw16K74BqmHea-iiWfUioxac-bM2zYcDAw-wkcNdYxvacibnZFkeva3hSvelhVCPd02YNYNyc4PT22Jo4M9Ek-xS0bJUQx2sggz3sJTySePP_HvkOPG_rHOyL4rQGMGplQMoKK15mq0KMBqsiAUHy7l5cIczHGfHhZumHwGbEfu62rcFwGK/w400-h261/image1.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Narrow ViTs attain the best PS.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Scaling Down Models Improves Perceptual Scores&lt;/h2&gt;&lt;p&gt;Our results prescribe a simple strategy to improve an architecture’s PS: scale down the model to reduce its accuracy until it attains the optimal perceptual score. The table below summarizes the improvements in PS obtained by scaling down each model across every hyperparameter. Except for ViT-L/4, early stopping yields the highest improvement in PS, regardless of architecture. In addition, early stopping is the most efficient strategy as there is no need for an expensive grid search. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: 11%; margin-right: 11%;&quot;&gt;      &lt;colgroup&gt;     &lt;col style=&quot;width: 15%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 9%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 9%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 9%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 9%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 9%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 9%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 9%;&quot;&gt;&lt;/col&gt;  &lt;/colgroup&gt;  &lt;tbody&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;b&gt;&lt;em&gt;Model&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Default&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Width&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Depth&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Weight&lt;br /&gt;Decay&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Central&lt;br /&gt;Crop&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Train&lt;br /&gt;Steps&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Best&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;ResNet-6&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;69.1    &lt;/td&gt;   &lt;td&gt;+0.4    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;+0.3    &lt;/td&gt;   &lt;td&gt;0.0    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;+0.5&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;69.6    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;ResNet-50&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;68.2    &lt;/td&gt;   &lt;td&gt;+0.4    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;+0.7    &lt;/td&gt;   &lt;td&gt;+0.7    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;+1.5&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;69.7    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;ResNet-200&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;67.6    &lt;/td&gt;   &lt;td&gt;+0.2    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;+1.3    &lt;/td&gt;   &lt;td&gt;+1.2    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;+1.9&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;69.5    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;ViT B/8&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;67.6    &lt;/td&gt;   &lt;td&gt;+1.1    &lt;/td&gt;   &lt;td&gt;+1.0    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;+1.3&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;+0.9    &lt;/td&gt;   &lt;td&gt;+1.1    &lt;/td&gt;   &lt;td&gt;68.9    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;ViT L/4&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;67.9    &lt;/td&gt;   &lt;td&gt;+0.4    &lt;/td&gt;   &lt;td&gt;+0.4    &lt;/td&gt;   &lt;td&gt;-0.1    &lt;/td&gt;   &lt;td&gt;-1.1    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;+0.5&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;68.4    &lt;/td&gt;  &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Perceptual Score improves by scaling down ImageNet models. Each value denotes the improvement obtained by scaling down a model across a given hyperparameter over the model with default hyperparameters.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Global Perceptual Functions&lt;/h2&gt;&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/abs/1801.03924&quot;&gt;prior work&lt;/a&gt;, the perceptual similarity function was computed using Euclidean distances across the spatial dimensions of the image. This assumes a direct correspondence between pixels, which may not hold for warped, translated or rotated images. Instead, we adopt two perceptual functions that rely on global representations of images, namely the style-loss function from the &lt;a href=&quot;https://arxiv.org/abs/1508.06576&quot;&gt;Neural Style Transfer&lt;/a&gt; work that captures stylistic similarity between two images, and a normalized &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers&quot;&gt;mean pool&lt;/a&gt; distance function. The style-loss function compares the inter-channel cross-correlation matrix between two images while the mean pool function compares the spatially averaged global representations. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjHYIY3m7EyJOKW8znpmUVVTy7G1Bg-XSiOvkeDoXjhRcukUgWpJ5d3t0JOgXaV6vptIk30Qt1g9Yo4sEiBqHh56rIhlZWZ44pqfROujg4DDI_4f7gZBJwaywrOCxv2CXJkbpCRh7g5XU7QjeSZW8umduUPlHI7DA1ptPZbWBL0P7Coqr9XQnXHxjz/s423/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;277&quot; data-original-width=&quot;423&quot; height=&quot;263&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjHYIY3m7EyJOKW8znpmUVVTy7G1Bg-XSiOvkeDoXjhRcukUgWpJ5d3t0JOgXaV6vptIk30Qt1g9Yo4sEiBqHh56rIhlZWZ44pqfROujg4DDI_4f7gZBJwaywrOCxv2CXJkbpCRh7g5XU7QjeSZW8umduUPlHI7DA1ptPZbWBL0P7Coqr9XQnXHxjz/w400-h263/image2.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjleEhvKhCBu_6LCUxbbNN4KPxzD-abx_tAVwT1q8KLhLGDixECks5SvRDZyOs8kcm_QxkeaKX7Jz00P4ucN9S3XuHGAXqAud3UCAzN_XApUWEa23WvA89gD3E8Yqm1U-Gb-CwNlmwieClTG3eLKB91rDzITV9Xfj-q5x9G5eKNAZxoi5bm7h6YLZ9h/s423/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;277&quot; data-original-width=&quot;423&quot; height=&quot;263&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjleEhvKhCBu_6LCUxbbNN4KPxzD-abx_tAVwT1q8KLhLGDixECks5SvRDZyOs8kcm_QxkeaKX7Jz00P4ucN9S3XuHGAXqAud3UCAzN_XApUWEa23WvA89gD3E8Yqm1U-Gb-CwNlmwieClTG3eLKB91rDzITV9Xfj-q5x9G5eKNAZxoi5bm7h6YLZ9h/w400-h263/image4.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Global perceptual functions consistently improve PS across both networks trained with default hyperparameters (&lt;b&gt;top&lt;/b&gt;) and ResNet-200 as a function of train epochs (&lt;b&gt;bottom&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We probe a number of hypotheses to explain the relationship between accuracy and PS and come away with a few additional insights. For example, the accuracy of models without commonly used skip-connections also inversely correlate with PS, and layers close to the input on average have lower PS as compared to layers close to the output. For further exploration involving distortion sensitivity, ImageNet class granularity, and spatial frequency sensitivity, check out &lt;a href=&quot;https://openreview.net/pdf?id=qrGKGZZvH0&quot;&gt;our paper&lt;/a&gt;. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;In this paper, we explore the question of whether improving classification accuracy yields better perceptual metrics. We study the relationship between accuracy and PS on ResNets and ViTs across many different hyperparameters and observe that PS exhibits an inverse-U relationship with accuracy, where accuracy correlates with PS up to a certain point, and then exhibits an inverse-correlation. Finally, in our paper, we discuss in detail a number of explanations for the  observed relationship between accuracy and PS, involving skip connections, global similarity functions, distortion sensitivity, layerwise perceptual scores, spatial frequency sensitivity and ImageNet class granularity. While the exact explanation for the observed tradeoff between ImageNet accuracy and perceptual similarity is a mystery, we are excited that our paper opens the door for further research in this area. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;This is joint work with Neil Houlsby and Nal Kalchbrenner. We would additionally like to thank Basil Mustafa, Kevin Swersky, Simon Kornblith, Johannes Balle, Mike Mozer, Mohammad Norouzi and Jascha Sohl-Dickstein for useful discussions.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/7866944510509103804/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/do-modern-imagenet-classifiers.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7866944510509103804" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7866944510509103804" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/do-modern-imagenet-classifiers.html" rel="alternate" title="Do Modern ImageNet Classifiers Accurately Predict Perceptual Similarity?" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguCNOD5qe7A72gZv0sl7QIgLv4HL06sCxEzVQLELH8Affzu_UzyEFJPj2nGU5PdEFO6h6XErHLFZqpFgcrJQZO0sif3zH70avGxMsop1IBQvfFKTBKfdJ-6OBYdIODCS9xopyjaIRRmcNR58EygavFwwxrGxqlOLidEkIvZxvHCIUdBsnczvy_EC7C/s72-c/image5.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7982911050269775769</id><published>2022-10-18T10:50:00.024-07:00</published><updated>2022-10-21T10:07:54.413-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Robotics"/><title type="text">Table Tennis: A Research Platform for Agile Robotics</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Avi Singh, Research Scientist, and Laura Graesser, Research Engineer, Robotics at Google&lt;/span&gt;&lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh563V_fA2AfwFfnKgijiPz38oX40zeziDIAGd6chEHZiNJRLpo1N3IXjqYRhF9acNGgkwrrBvbDMKIeTxov5QJHtTx3FO36jdOVlvMpRWdjrASBWSrGLNWP8gIPoS_qEk4z5fdJiNhLKwYWiMQDsjiFwvE5iABgFNyoREW37VzHjJ102qWxpNP7aWBvg/s775/i-S2R.gif&quot; style=&quot;display: none;&quot; /&gt;&lt;p&gt;Robot learning has been applied to a wide range of challenging real world tasks, including &lt;a href=&quot;https://arxiv.org/abs/1910.07113&quot;&gt;dexterous manipulation&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2004.00784&quot;&gt;legged locomotion&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/1806.10293&quot;&gt;grasping&lt;/a&gt;. It is less common to see robot learning applied to dynamic, high-acceleration tasks requiring tight-loop human-robot interactions, such as table tennis. There are two complementary properties of the table tennis task that make it interesting for robotic learning research. First, the task requires both speed and precision, which puts significant demands on a learning algorithm. At the same time, the problem is highly-structured (with a fixed, predictable environment) and naturally multi-agent (the robot can play with humans or another robot), making it a desirable testbed to investigate questions about human-robot interaction and &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;&gt;reinforcement learning&lt;/a&gt;. These properties have led to several research groups developing table tennis research platforms [&lt;a href=&quot;https://www.ias.informatik.tu-darmstadt.de/Research/LearningToPlayPing-pong&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/kognitive-systeme/projects/table-tennis-robot/&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://www.omron.com/global/en/technology/omrontechnics/vol51/016.html&quot;&gt;3&lt;/a&gt;, &lt;a href=&quot;https://core-robotics.gatech.edu/&quot;&gt;4&lt;/a&gt;].  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;The &lt;a href=&quot;https://research.google/teams/robotics/&quot;&gt;Robotics&lt;/a&gt; team at Google has built such a platform to study problems that arise from robotic learning in a multi-player, dynamic and interactive setting. In the rest of this post we introduce two projects, &lt;a href=&quot;https://sites.google.com/view/is2r&quot;&gt;Iterative-Sim2Real&lt;/a&gt; (to be presented at &lt;a href=&quot;https://corl2022.org/&quot;&gt;CoRL 2022&lt;/a&gt;) and &lt;a href=&quot;https://sites.google.com/view/goals-eye&quot;&gt;GoalsEye&lt;/a&gt; (&lt;a href=&quot;https://iros2022.org/&quot;&gt;IROS 2022&lt;/a&gt;), which illustrate the problems we have been investigating so far. Iterative-Sim2Real enables a robot to hold rallies of over 300 hits with a human player, while GoalsEye enables learning goal-conditioned policies that match the precision of amateur humans. &lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;80%&quot;&gt; &lt;source src=&quot;https://github.com/lauragraesser/videos/blob/main/i-S2R_highlights.mp4?raw=true&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;&lt;/video&gt; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;80%&quot;&gt; &lt;source src=&quot;https://github.com/lauragraesser/videos/blob/main/goalseye-highlights-1x-4x-1x_compressed.mp4?raw=true&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;&lt;/video&gt;&lt;/div&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Iterative-Sim2Real policies playing cooperatively with humans (&lt;b&gt;top&lt;/b&gt;) and a GoalsEye policy returning balls to different locations (&lt;b&gt;bottom&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Iterative-Sim2Real: Leveraging a Simulator to Play Cooperatively with Humans&lt;/h2&gt;&lt;p&gt;In this project, the goal for the robot is &lt;em&gt;cooperative&lt;/em&gt; in nature: to carry out a rally with a human for as long as possible. Since it would be tedious and time-consuming to train directly against a human player in the real world, we adopt a simulation-based (i.e., &lt;a href=&quot;https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html&quot;&gt;sim-to-real&lt;/a&gt;) approach. However, because it is difficult to simulate human behavior accurately, applying sim-to-real learning to tasks that require tight, close-loop interaction with a human participant is difficult. &lt;/p&gt;&lt;p&gt;In &lt;a href=&quot;https://sites.google.com/view/is2r&quot;&gt;Iterative-Sim2Real&lt;/a&gt;, (i.e., i-S2R), we present a method for learning human behavior models for human-robot interaction tasks, and instantiate it on our robotic table tennis platform. We have built a system that can achieve rallies of up to 340 hits with an amateur human player (shown below). &lt;/p&gt;   &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/Fh7VK0WPvU4?rel=0&amp;amp;&quot; width=&quot;640&quot; youtube-src-id=&quot;Fh7VK0WPvU4&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A 340-hit rally lasting over 4 minutes.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Learning Human Behavior Models: a Chicken and Egg Problem&lt;/h2&gt;&lt;p&gt;The central problem in learning accurate human behavior models for robotics is the following: if we do not have a good-enough robot policy to begin with, then we cannot collect high-quality data on how a person might interact with the robot. But without a human behavior model, we cannot obtain robot policies in the first place. An alternative would be to train a robot policy directly in the real world, but this is often slow, cost-prohibitive, and poses safety-related challenges, which are further exacerbated when people are involved. i-S2R, visualized below, is a solution to this chicken and egg problem. It uses a simple model of human behavior as an approximate starting point and alternates between training in simulation and deploying in the real world. In each iteration, both the human behavior model and the policy are refined. &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihM2ooQnHGqn_FavYojW0TKDGhT6fAVimaT8kYONHI-WZuNolM0Ve1zx9_kQQxdSSoW7_17aGyf8ylmELcFm1wZQ0hm0wa2W-WCVNkAJoyQeeoWsMH11Ferqgc5Ei34qq2QAPuAbsvLNs93CwqgdhhCcgxmkjPfIs-mYvMHdlDePfWMle-sMB6vPPoKQ/s1360/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;785&quot; data-original-width=&quot;1360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihM2ooQnHGqn_FavYojW0TKDGhT6fAVimaT8kYONHI-WZuNolM0Ve1zx9_kQQxdSSoW7_17aGyf8ylmELcFm1wZQ0hm0wa2W-WCVNkAJoyQeeoWsMH11Ferqgc5Ei34qq2QAPuAbsvLNs93CwqgdhhCcgxmkjPfIs-mYvMHdlDePfWMle-sMB6vPPoKQ/s16000/image4.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;i-S2R Methodology.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;h2&gt;Results&lt;/h2&gt;&lt;p&gt;To evaluate i-S2R, we repeated the training process five times with five different human opponents and compared it with a baseline approach of ordinary sim-to-real plus fine-tuning (&lt;a href=&quot;https://arxiv.org/abs/2110.05457&quot;&gt;S2R+FT&lt;/a&gt;). When aggregated across all players, the i-S2R rally length is higher than S2R+FT by about 9% (below on the left). The histogram of rally lengths for i-S2R and S2R+FT (below on the right) shows that a large fraction of the rallies for S2R+FT are shorter (i.e., less than 5), while i-S2R achieves longer rallies more frequently.  &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS_HGiUUqbp1nTJlfbMKmQWVRUA7T-pCNlZsprSNEvarV-MI1nATDjZ566oLzlApAGGa6mVsq2qlu4IN0KMsJBYaU4q6LPz8aap6sZqCO-fzkB1_eqa-aMrrx3yQuWyUtuCcFKeWZc0G8YfxQ8WbbFin3j_qpXg0Zxdce-BsNRBcoUAAqmzoedLMukNQ/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;935&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS_HGiUUqbp1nTJlfbMKmQWVRUA7T-pCNlZsprSNEvarV-MI1nATDjZ566oLzlApAGGa6mVsq2qlu4IN0KMsJBYaU4q6LPz8aap6sZqCO-fzkB1_eqa-aMrrx3yQuWyUtuCcFKeWZc0G8YfxQ8WbbFin3j_qpXg0Zxdce-BsNRBcoUAAqmzoedLMukNQ/s16000/image1.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Summary of i-S2R results. &lt;/b&gt;Boxplot details: The white circle is the mean, the horizontal line is the median, box bounds are the 25th and 75th percentiles.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;       &lt;p&gt;We also break down the results based on player type: beginner (40% players), intermediate (40% of players) and advanced (20% players). We see that i-S2R significantly outperforms S2R+FT for both beginner and intermediate players (80% of players). &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6i_dHv7JuJXFKJkEjmAgj0P90YdjWLXBe-Ah5OL5UrhlaV-4Uql2-snTz8VKfQenGZvQO-uAuIkOUvSYe9u4A6pYKkUCHX3ddhbqe67yMgNOvy9K1-gbFFIHU8I0f_xMTtkJc9sHa8X9rO-_OFBAHuKB7LVoXX2nfQnaRiR8-6fvePCV8aeVCDSzwlw/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;645&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6i_dHv7JuJXFKJkEjmAgj0P90YdjWLXBe-Ah5OL5UrhlaV-4Uql2-snTz8VKfQenGZvQO-uAuIkOUvSYe9u4A6pYKkUCHX3ddhbqe67yMgNOvy9K1-gbFFIHU8I0f_xMTtkJc9sHa8X9rO-_OFBAHuKB7LVoXX2nfQnaRiR8-6fvePCV8aeVCDSzwlw/s16000/image2.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;i-S2R Results by player type.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;More details on i-S2R can be found on our &lt;a href=&quot;https://arxiv.org/abs/2207.06572&quot;&gt;preprint&lt;/a&gt;, &lt;a href=&quot;https://sites.google.com/view/is2r&quot;&gt;website&lt;/a&gt;, and also in the following summary video. &lt;/p&gt;  &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/vtVUFXV9qR4?rel=0&amp;amp;&quot; width=&quot;640&quot; youtube-src-id=&quot;vtVUFXV9qR4&quot;&gt;&lt;/iframe&gt;&lt;/div&gt; &lt;div style=&quot;line-height:120%;&quot;&gt;  &lt;br&gt;  &lt;br&gt;&lt;/div&gt; &lt;h2&gt;GoalsEye: Learning to Return Balls Precisely on a Physical Robot&lt;/h2&gt;&lt;p&gt;While we focused on sim-to-real learning in i-S2R, it is sometimes desirable to learn using only real-world data — closing the sim-to-real gap in this case is unnecessary. &lt;a href=&quot;https://en.wikipedia.org/wiki/Imitative_learning&quot;&gt;Imitation learning&lt;/a&gt; (IL) provides a simple and stable approach to learning in the real world, but it requires access to demonstrations and cannot exceed the performance of the teacher. Collecting expert human demonstrations of precise goal-targeting in high speed settings is challenging and sometimes impossible (due to limited precision in human movements). While reinforcement learning (RL) is well-suited to such high-speed, high-precision tasks, it faces a difficult exploration problem (especially at the start), and can be very sample inefficient. In &lt;a href=&quot;https://sites.google.com/view/goals-eye&quot;&gt;GoalsEye&lt;/a&gt;, we demonstrate an approach that combines recent behavior cloning techniques [&lt;a href=&quot;https://arxiv.org/abs/1903.01973&quot;&gt;5&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.06088&quot;&gt;6&lt;/a&gt;] to learn a precise goal-targeting policy, starting from a small, weakly-structured, non-targeting dataset. &lt;/p&gt;&lt;p&gt;Here we consider a different table tennis task with an emphasis on precision. We want the robot to return the ball to an arbitrary goal location on the table, e.g. “hit the back left corner&quot; or ''land the ball just over the net on the right side&quot; (see left video below). Further, we wanted to find a method that can be applied &lt;em&gt;directly&lt;/em&gt; on our real world table tennis environment with no simulation involved. We found that the synthesis of two existing imitation learning techniques, &lt;a href=&quot;https://arxiv.org/abs/1903.01973&quot;&gt;Learning from Play&lt;/a&gt; (LFP) and &lt;a href=&quot;https://arxiv.org/abs/1912.06088&quot;&gt;Goal-Conditioned Supervised Learning&lt;/a&gt; (GCSL), scales to this setting. It is safe and &lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_complexity&quot;&gt;sample efficient&lt;/a&gt; enough to train a policy on a physical robot which is as accurate as amateur humans at the task of returning balls to specific goals on the table. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;     &lt;td style=&quot;text-align: center;&quot;&gt;    &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;&gt; &lt;source src=&quot;https://github.com/lauragraesser/videos/blob/main/goals-eye-narrow-incoming-ball-robot-5-goals-compressed.mp4?raw=true&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;&lt;/video&gt;  &lt;/td&gt; &lt;td style=&quot;text-align: center;&quot;&gt; &amp;nbsp;   &lt;/td&gt; &lt;td style=&quot;text-align: center;&quot;&gt;    &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;&gt; &lt;source src=&quot;https://github.com/lauragraesser/videos/blob/main/goals-eye-narrow-incoming-ball-human-5-goals-compressed-unblurred.mp4?raw=true&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;&lt;/video&gt;  &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;GoalsEye policy aiming at a 20cm diameter goal (&lt;b&gt;left&lt;/b&gt;). Human player aiming at the same goal (&lt;b&gt;right&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;     &lt;p&gt;The essential ingredients of success are:  &lt;/p&gt;&lt;ol&gt; &lt;li&gt;&lt;em&gt;A minimal, but non-goal-directed “bootstrap” dataset&lt;strong&gt; &lt;/strong&gt;&lt;/em&gt;of the robot hitting the ball to overcome an initial difficult exploration problem.   &lt;/li&gt;&lt;li&gt;&lt;em&gt;Hindsight relabeled goal conditioned behavioral cloning&lt;/em&gt; (GCBC) to train a goal-directed policy to reach any goal in the dataset.   &lt;/li&gt;&lt;li&gt;&lt;em&gt;Iterative self-supervised goal reaching.&lt;/em&gt; The agent improves continuously by setting random goals and attempting to reach them using the current policy. All attempts are relabeled and added into a continuously expanding training set. This &lt;em&gt;self-practice&lt;/em&gt;, in which the robot expands the training data by setting and attempting to reach goals, is repeated iteratively. &lt;/li&gt;&lt;/ol&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfFWLsqP8TmC8sJpRao5WeWAjDB9iJrOSA3XSkvY5K8jzdCN4raHg7LkHziyvD0YJum9Jk8fVF8uDrUGdPPAsLLksX2n9drZPlylksvST40WDnwhGiONNPSBW6f6z8qE3v4-G_i-VlcjJso-kIE9EW1zvUsIgSnDjtJGbKeD3QCKtEDnJmDg-hMZ2Zxg/s1600/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;468&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfFWLsqP8TmC8sJpRao5WeWAjDB9iJrOSA3XSkvY5K8jzdCN4raHg7LkHziyvD0YJum9Jk8fVF8uDrUGdPPAsLLksX2n9drZPlylksvST40WDnwhGiONNPSBW6f6z8qE3v4-G_i-VlcjJso-kIE9EW1zvUsIgSnDjtJGbKeD3QCKtEDnJmDg-hMZ2Zxg/s16000/image5.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;GoalsEye methodology.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;h2&gt;Demonstrations and Self-Improvement Through Practice Are Key&lt;/h2&gt;&lt;p&gt;The synthesis of techniques is crucial. The policy’s objective is to return a &lt;em&gt;variety &lt;/em&gt;of incoming balls to &lt;em&gt;any &lt;/em&gt;location on the opponent’s side of the table. A policy trained on the initial 2,480 demonstrations only accurately reaches within 30 cm of the goal 9% of the time. However, after a policy has self-practiced for ~13,500 attempts, goal-reaching accuracy rises to 43% (below on the right). This improvement is clearly visible as shown in the videos below. Yet if a policy only self-practices, training fails completely in this setting. Interestingly, the number of demonstrations improves the efficiency of subsequent self-practice, albeit with diminishing returns. This indicates that demonstration data and self-practice could be substituted depending on the relative time and cost to gather demonstration data compared with self-practice. &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHQc683I9kCBQui1CUv4FI_NEDTFoGFR4OnKWHiVChfjvh9HE8ohQU1nhzq2_wKXhMJXu7Sb1WzP6Qh7tWgM9UA2F6_QZFdGVVGyj8eGANnM-jgbqTVRYcl9pXLCaNgirMhHEHaV5drnfzziNRY_3fsSPW2a_jEmfLvVpnB6ak4zxHtoTWMqnOYOKZmA/s1441/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;541&quot; data-original-width=&quot;1441&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHQc683I9kCBQui1CUv4FI_NEDTFoGFR4OnKWHiVChfjvh9HE8ohQU1nhzq2_wKXhMJXu7Sb1WzP6Qh7tWgM9UA2F6_QZFdGVVGyj8eGANnM-jgbqTVRYcl9pXLCaNgirMhHEHaV5drnfzziNRY_3fsSPW2a_jEmfLvVpnB6ak4zxHtoTWMqnOYOKZmA/s16000/image7.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Self-practice substantially improves accuracy. &lt;b&gt;Left&lt;/b&gt;: simulated training. &lt;b&gt;Right&lt;/b&gt;: real robot training. The demonstration datasets contain ~2,500 episodes, both in simulation and the real world.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;     &lt;td style=&quot;text-align: center;&quot;&gt;    &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;&gt; &lt;source src=&quot;https://github.com/lauragraesser/videos/blob/main/goals-eye-ckpt_5_goal_E_no_audio_4x_compressed.mp4?raw=true&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;&lt;/video&gt;  &lt;/td&gt; &lt;td style=&quot;text-align: center;&quot;&gt; &amp;nbsp;   &lt;/td&gt; &lt;td style=&quot;text-align: center;&quot;&gt;    &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;&gt; &lt;source src=&quot;https://github.com/lauragraesser/videos/blob/main/goals-eye-ckpt-300-goal-E-no-audio-4x_compressed.mp4?raw=true&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;&lt;/video&gt;  &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Visualizing the benefits of self-practice. &lt;b&gt;Left&lt;/b&gt;: policy trained on initial 2,480 demonstrations. &lt;b&gt;Right&lt;/b&gt;: policy after an additional 13,500 self-practice attempts.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;More details on GoalsEye can be found in the &lt;a href=&quot;https://arxiv.org/abs/2210.03662&quot;&gt;preprint&lt;/a&gt; and on our &lt;a href=&quot;https://sites.google.com/view/goals-eye&quot;&gt;website&lt;/a&gt;. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt; &lt;h2&gt;Conclusion and Future Work&lt;/h2&gt;&lt;p&gt;We have presented two complementary projects using our robotic table tennis research platform. i-S2R learns RL policies that are able to interact with humans, while GoalsEye demonstrates that learning from real-world unstructured data combined with self-supervised practice is effective for learning goal-conditioned policies in a precise, dynamic setting. &lt;/p&gt;&lt;p&gt;One interesting research direction to pursue on the table tennis platform would be to build a robot “coach” that could adapt its play style according to the skill level of the human participant to keep things challenging and exciting.  &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt; &lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;We thank our co-authors, Saminda Abeyruwan, Alex Bewley, Krzysztof Choromanski, David B. D’Ambrosio, Tianli Ding, Deepali Jain, Corey Lynch, Pannag R. Sanketi, Pierre Sermanet and Anish Shankar. We are also grateful for the support of many members of the Robotics Team who are listed in the acknowledgement sections of the papers.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/7982911050269775769/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/table-tennis-research-platform-for.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7982911050269775769" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7982911050269775769" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/table-tennis-research-platform-for.html" rel="alternate" title="Table Tennis: A Research Platform for Agile Robotics" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh563V_fA2AfwFfnKgijiPz38oX40zeziDIAGd6chEHZiNJRLpo1N3IXjqYRhF9acNGgkwrrBvbDMKIeTxov5QJHtTx3FO36jdOVlvMpRWdjrASBWSrGLNWP8gIPoS_qEk4z5fdJiNhLKwYWiMQDsjiFwvE5iABgFNyoREW37VzHjJ102qWxpNP7aWBvg/s72-c/i-S2R.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-1370943694206193273</id><published>2022-10-14T11:47:00.009-07:00</published><updated>2022-10-21T10:24:04.673-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="NLP"/><category scheme="http://www.blogger.com/atom/ns#" term="Publications"/><category scheme="http://www.blogger.com/atom/ns#" term="Research"/><title type="text">UL2 20B: An Open Source Unified Language Learner</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Yi Tay and Mostafa Dehghani, Research Scientists, Google Research, Brain Team&lt;/span&gt;&lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoRWMTOf1JUl345eb5BqKEPTRRxPvzPdzvspKtqlwNHqo4BVq98MJYkvEVPZAPdYmLaFMLQKAolOdzKD3uzbYTdYM8S9Z-y5BXgy6kotdukG8w9VCkrZt3Vb0H-BEDp8XC5bGIsA_OEQPWWll1vNRZbSBwJWowTCTf9cnW-7fDOXT8MmyH5s8KzieCQg/s1300/image3.gif&quot; style=&quot;display: none;&quot; /&gt;&lt;p&gt;Building models that understand and generate natural language well is one the grand goals of machine learning (ML) research and has a direct impact on building smart systems for everyday applications. Improving the quality of language models is a key target for researchers to make progress toward such a goal.   &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;Most common paradigms to build and train language models use either autoregressive decoder-only architectures (e.g., &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;&gt;PaLM&lt;/a&gt; or &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;GPT-3&lt;/a&gt;), where the model is trained to predict the next word for a given prefix phrase, or span corruption-based encoder-decoder architectures (e.g., &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;T5&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2202.08906&quot;&gt;ST-MoE&lt;/a&gt;), where the training objective is to recover the subset of words masked out of the input. On the one hand, T5-like models perform well on &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;&gt;supervised fine-tuning&lt;/a&gt; tasks, but struggle with few-shot in-context learning. On the other hand, autoregressive language models are great for &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;open-ended generation&lt;/a&gt; (e.g., dialog generation with&lt;a href=&quot;https://arxiv.org/abs/2201.08239&quot;&gt; LaMDA&lt;/a&gt;) and prompt-based learning (e.g., in-context learning with PaLM), but may perform suboptimally on fine-tuning tasks. Thus, there remains an opportunity to create an effective unified framework for pre-training models. &lt;/p&gt;&lt;p&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2205.05131&quot;&gt;Unifying Language Learning Paradigms&lt;/a&gt;”, we present a novel language pre-training paradigm called Unified Language Learner (UL2) that improves the performance of language models universally across datasets and setups. UL2 frames different objective functions for training language models as &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;denoising&lt;/a&gt; tasks, where the model has to recover missing sub-sequences of a given input. During pre-training it uses a novel &lt;em&gt;mixture-of-denoisers&lt;/em&gt; that samples from a varied set of such objectives, each with different configurations. We demonstrate that models trained using the UL2 framework perform well in a variety of language domains, including prompt-based few-shot learning and models fine-tuned for down-stream tasks. Additionally, we show that UL2 excels in &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_generation&quot;&gt;generation&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural-language_understanding&quot;&gt;language understanding&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Information_retrieval&quot;&gt;retrieval&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Semantic_analysis_(machine_learning)&quot;&gt;long-text understanding&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;&gt;question answering&lt;/a&gt; tasks. Finally, we are excited to publicly release the &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/ul2&quot;&gt;checkpoints&lt;/a&gt; for our best performing UL2 20 billion parameter model. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt; &lt;h2&gt;Background: Language Modeling Objectives and Architectures&lt;/h2&gt;&lt;p&gt;Common objective functions for training language models can mostly be framed as learning data transformations that map inputs to targets. The model is conditioned on different forms of input to predict target tokens. To this end, different objectives utilize different properties of the inputs. &lt;/p&gt;&lt;p&gt;The standard &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;Causal Language&lt;/a&gt; modeling objective (CausalLM) is trained to predict full sequence lengths and so, only recognizes tokens in the target output. The &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;prefix language modeling&lt;/a&gt; objective (PrefixLM) modifies this process by randomly sampling a contiguous span of &lt;em&gt;k&lt;/em&gt; tokens from the given tokenized text to form the input of the model, referred to as the “prefix”. The &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;span corruption&lt;/a&gt; objective masks contiguous spans from the inputs and trains the model to predict these masked spans.  &lt;/p&gt;&lt;p&gt;In the table below, we list the common objectives on which state-of-the-art language models are trained along with different characteristics of the input, i.e., how it is presented to the model. Moreover, we characterize the example efficiency of each objective in terms of the ability of the model for exploiting supervision signals from a single input, e.g., how much of the input tokens contribute to the calculation of the loss. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot;&gt;      &lt;colgroup&gt;     &lt;col style=&quot;width: 18%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 18%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 18%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 18%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 18%;&quot;&gt;&lt;/col&gt;  &lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;  &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;&lt;b&gt;Objective&lt;br /&gt;Function&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;  &lt;td&gt;&lt;em&gt;&lt;b&gt;Inputs&lt;br /&gt;(Bi-directional)&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;  &lt;td&gt;&lt;em&gt;&lt;b&gt;Targets&lt;br /&gt;(Causal)&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;&lt;b&gt;Input&lt;br /&gt;Properties&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;  &lt;td&gt;&lt;em&gt;&lt;b&gt;Example&lt;br /&gt;Efficiency&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;CausalLM&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;none    &lt;/td&gt;   &lt;td&gt;text    &lt;/td&gt;   &lt;td&gt;N/A    &lt;/td&gt;   &lt;td&gt;full seq_len    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;     &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;PrefixLM&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;text&lt;br&gt;(up to position &lt;em&gt;k&lt;/em&gt;)    &lt;/td&gt;   &lt;td&gt;text&lt;br&gt;(after position &lt;em&gt;k&lt;/em&gt;)    &lt;/td&gt;   &lt;td&gt;contiguous    &lt;/td&gt;   &lt;td&gt;seq_len - &lt;em&gt;k&lt;/em&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;Span corruption&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;masked text    &lt;/td&gt;   &lt;td&gt;masked_tokens    &lt;/td&gt;   &lt;td&gt;&lt;span style=&quot;font-size: small;&quot;&gt;non-contiguous, may be bi-directional&lt;/span&gt;&lt;/td&gt;   &lt;td&gt;&lt;span style=&quot;font-size: small;&quot;&gt;typically lower than others&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Common objectives used in today’s language models. Throughout, “text” indicates tokenized text.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;UL2 leverages the strengths of each of these objective functions through a framework that generalizes over each of them, which enables the ability to reason and unify common pre-training objectives. Based on this framework, the main task for training a language model is to learn the transformation of a sequence of input tokens to a sequence of target tokens. Then all the objective functions introduced above can be simply reduced to different ways of generating input and target tokens. For instance, the PrefixLM objective can be viewed as a transformation that moves a segment of &lt;em&gt;k&lt;/em&gt; contiguous tokens from the inputs to the targets. Meanwhile, the span corruption objective is a data transformation that corrupts spans (a subsequence of tokens in the input), replacing them with mask tokens that are shifted to the targets. &lt;/p&gt;&lt;p&gt;It is worth noting that one can decouple the model architecture and the objective function with which it’s trained. Thus, it is possible to train different architectures, such as the common single stack decoder-only and two-stack encoder-decoder models, with any of these objectives. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt; &lt;h2&gt;Mixture of Denoisers&lt;/h2&gt;&lt;p&gt;The UL2 framework can be used to train a model on a mixture of pre-training objectives and supply it with capabilities and inductive bias benefits from different pre-training tasks. Training on the mixture helps the model leverage the strengths of different tasks and mitigates the weaknesses of others. For instance, the mixture-of-denoisers objective can strongly improve the prompt-based learning capability of the model as opposed to a span corruption-only T5 model.  &lt;/p&gt;&lt;p&gt;UL2 is trained using a mixture of three denoising tasks: (1) &lt;em&gt;R-denoising&lt;/em&gt; (or regular span corruption), which emulates the standard T5 span corruption objective; (2) &lt;em&gt;X-denoising&lt;/em&gt; (or extreme span corruption); and (3) &lt;em&gt;S-denoising&lt;/em&gt; (or sequential PrefixLM). During pre-training, we sample from the available denoising tasks based on user-specified ratios (i.e., different combinations of the R, X, and S-denoisers) and prepare the input and target appropriately. Then, a paradigm token is appended to the input (one of &lt;code&gt;[R]&lt;/code&gt;, &lt;code&gt;[X]&lt;/code&gt;, or &lt;code&gt;[S]&lt;/code&gt;) indicating the denoising task at hand. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoRWMTOf1JUl345eb5BqKEPTRRxPvzPdzvspKtqlwNHqo4BVq98MJYkvEVPZAPdYmLaFMLQKAolOdzKD3uzbYTdYM8S9Z-y5BXgy6kotdukG8w9VCkrZt3Vb0H-BEDp8XC5bGIsA_OEQPWWll1vNRZbSBwJWowTCTf9cnW-7fDOXT8MmyH5s8KzieCQg/s1300/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1300&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoRWMTOf1JUl345eb5BqKEPTRRxPvzPdzvspKtqlwNHqo4BVq98MJYkvEVPZAPdYmLaFMLQKAolOdzKD3uzbYTdYM8S9Z-y5BXgy6kotdukG8w9VCkrZt3Vb0H-BEDp8XC5bGIsA_OEQPWWll1vNRZbSBwJWowTCTf9cnW-7fDOXT8MmyH5s8KzieCQg/s16000/image3.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;An overview of the denoising objectives used in UL2’s mixture-of-denoisers.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Improving Trade-Offs Across Learning Paradigms&lt;/h2&gt;&lt;p&gt;Many existing commonly used language learning paradigms typically excel at one type of task or application, such as fine-tuning performance or prompt-based in-context learning. In the plot below, we show baseline objective functions on different tasks compared to UL2: CausalLM (&lt;em&gt;referred to as GPT-like&lt;/em&gt;), PrefixLM, Span Corrupt (&lt;em&gt;also referred to as T5 in the plot&lt;/em&gt;), and a baseline objective function proposed by &lt;a href=&quot;https://arxiv.org/abs/1905.03197&quot;&gt;UniLM&lt;/a&gt;. We use these objectives for training decoder only architectures (green) and encoder-decoder architectures (blue) and evaluate different combinations of objective functions and architectures on two main sets of tasks:  &lt;/p&gt;&lt;ol&gt;&lt;li&gt;Fine-tuning, by measuring performance on &lt;a href=&quot;https://super.gluebenchmark.com/&quot;&gt;SuperGLUE&lt;/a&gt; (y-axis of the plot below) &lt;/li&gt;  &lt;li&gt;In-context learning, by measuring performance of the model on a suite of 1-shot &lt;a href=&quot;https://gem-benchmark.com/&quot;&gt;GEM tasks&lt;/a&gt; (e.g., &lt;a href=&quot;https://arxiv.org/abs/1808.08745&quot;&gt;XSUM&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1909.05855&quot;&gt;SGD or Schema guided dialog&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2004.14373&quot;&gt;TOTTO&lt;/a&gt;) (x-axis of the plot below).  &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;For most of the existing language learning paradigms, there is a trade-off between the quality of the model on these two sets of tasks. We show that UL2 bridges this trade-off across in-context learning and fine-tuning.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiozftwuxITX87OmCkAwkBouHRkjmpZHlfHCZYxRdp6_E5rLigiia3l1JlxvSnhih67iQ_CI1lQmtfffvuXNLGhuO5rFsrifmT1rk5wfLTCKcYK-6ngoendoOUzqUP1SENoQs9WvB-nsu7QDgha57NZXVMU6OpxOrbu9Mh4qKzsE3t6a0BGhlyMYhSLkw/s1004/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;868&quot; data-original-width=&quot;1004&quot; height=&quot;346&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiozftwuxITX87OmCkAwkBouHRkjmpZHlfHCZYxRdp6_E5rLigiia3l1JlxvSnhih67iQ_CI1lQmtfffvuXNLGhuO5rFsrifmT1rk5wfLTCKcYK-6ngoendoOUzqUP1SENoQs9WvB-nsu7QDgha57NZXVMU6OpxOrbu9Mh4qKzsE3t6a0BGhlyMYhSLkw/w400-h346/image1.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;In both decoder-only and encoder-decoder setups, UL2 strikes a significantly improved balance in performance between fine-tuned discriminative tasks and prompt-based 1-shot open-ended text generation compared to previous methods. (All models are comparable in terms of computational costs, i.e., FLOPs (EncDec models are 300M and Dec models are 150M parameters).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;UL2 for Few-Shot Prompting and Chain-of-Thought Reasoning&lt;/h2&gt;&lt;p&gt;We scale up UL2 and train a 20 billion parameter encoder-decoder model on the public &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;C4 corpus&lt;/a&gt; and demonstrate some impressive capabilities of the UL2 20B model.  &lt;/p&gt;&lt;p&gt;UL2 is a powerful in-context learner that excels at both few-shot and &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;&gt;chain-of-thought&lt;/a&gt; (CoT) prompting. In the table below, we compare UL2 with other state-of-the-art models (e.g, &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;T5 XXL&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;&gt;PaLM&lt;/a&gt;) for few-shot prompting on the XSUM summarization dataset. Our results show that UL2 20B outperforms PaLM and T5, both of which are in the same ballpark of compute cost. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: 17%; margin-right: 17%;&quot;&gt;  &lt;colgroup&gt;     &lt;col style=&quot;width: 18%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 16%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 16%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 16%;&quot;&gt;&lt;/col&gt;  &lt;/colgroup&gt;  &lt;tbody&gt;&lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;&lt;b&gt;ROUGE-1&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;&lt;b&gt;ROUGE-2&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;&lt;b&gt;ROUGE-L&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;LaMDA 137B&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;   &lt;td&gt;5.4    &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;PaLM 62B&lt;/em&gt;     &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;   &lt;td&gt;11.2    &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;PaLM 540B&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;   &lt;td&gt;&lt;strong&gt;12.2&lt;/strong&gt;   &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;PaLM 8B&lt;/em&gt;     &lt;/td&gt;   &lt;td&gt;–     &lt;/td&gt;   &lt;td&gt;4.5    &lt;/td&gt;   &lt;td&gt;–     &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;T5 XXL 11B&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.6    &lt;/td&gt;   &lt;td&gt;0.1    &lt;/td&gt;   &lt;td&gt;0.6     &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;T5 XXL 11B + LM&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;13.3    &lt;/td&gt;   &lt;td&gt; 2.3    &lt;/td&gt;   &lt;td&gt; 10.7     &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;UL2 20B&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;strong&gt;25.5&lt;/strong&gt;   &lt;/td&gt;   &lt;td&gt;&lt;strong&gt;8.6&lt;/strong&gt;   &lt;/td&gt;   &lt;td&gt;&lt;strong&gt;19.8&lt;/strong&gt;   &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Comparison of UL2 with &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;T5 XXL&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;&gt;PaLM&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2201.08239&quot;&gt;LamDA 137B&lt;/a&gt; on 1-shot summarization (&lt;a href=&quot;https://arxiv.org/abs/1808.08745&quot;&gt;XSUM&lt;/a&gt;) in terms of &lt;a href=&quot;https://arxiv.org/abs/1808.08745&quot;&gt;ROUGE-1/2/L&lt;/a&gt; (higher is better), which captures the quality by comparing the generated summaries with the gold summaries as reference.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Most CoT prompting results have been obtained using much larger language models, such as GPT-3 175B, PaLM 540B, or LaMDA 137B. We show that reasoning via CoT prompting can be achieved with UL2 20B, which is both publicly available and several times smaller than prior models that leverage chain-of-thought prompting. This enables an open avenue for researchers to conduct research on CoT prompting and reasoning at an accessible scale. In the table below, we show that for UL2, CoT prompting outperforms standard prompting on math word problems with a range of difficulties (&lt;a href=&quot;http://go/arxiv/2110.14168&quot;&gt;GSM8K&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2103.07191&quot;&gt;SVAMP&lt;/a&gt;, &lt;a href=&quot;https://aclanthology.org/2020.acl-main.92/&quot;&gt;ASDiv&lt;/a&gt;, &lt;a href=&quot;https://aclanthology.org/P17-1015&quot;&gt;AQuA&lt;/a&gt;, and &lt;a href=&quot;https://aclanthology.org/N16-1136/&quot;&gt;MAWPS&lt;/a&gt;). We also show that &lt;a href=&quot;https://arxiv.org/abs/2203.11171&quot;&gt;self-consistency&lt;/a&gt; further improves performance. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikv4KU6NqBBwsQqHUvtvfBOFK9Tkly0AZzl0p-JQTtpVgWhqwtHguYYYB-jZdvB0zdsVZRKZkEStnNKHPqDE-U7wnJWXseLGaSmq48fwEN-eoN_1lmx5lFvTYBij9eVYNm0y62Hy1UXrLBs-lqN13dEXhBTI1Pg8oJWvGx03tHeQVUGKJ6YUjWAEgQMg/s1760/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;874&quot; data-original-width=&quot;1760&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikv4KU6NqBBwsQqHUvtvfBOFK9Tkly0AZzl0p-JQTtpVgWhqwtHguYYYB-jZdvB0zdsVZRKZkEStnNKHPqDE-U7wnJWXseLGaSmq48fwEN-eoN_1lmx5lFvTYBij9eVYNm0y62Hy1UXrLBs-lqN13dEXhBTI1Pg8oJWvGx03tHeQVUGKJ6YUjWAEgQMg/s16000/image2.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Chain-of-thought (CoT) prompting and &lt;a href=&quot;https://arxiv.org/abs/2203.11171&quot;&gt;self-consistency&lt;/a&gt; (SC) results on five arithmetic reasoning benchmarks.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Conclusion and Future Directions&lt;/h2&gt;&lt;p&gt;UL2 demonstrates superior performance on a plethora of fine-tuning and few-shot tasks. We &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/ul2&quot;&gt;publicly release&lt;/a&gt; checkpoints of our best performing UL2 model with 20 billion parameters, which we hope will inspire faster progress in developing better language models in the machine learning community as a whole.  &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt; &lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;It was an honor and privilege to work on this with Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby and Donald Metzler. We further acknowledge Alexey Gritsenko, Andrew M. Dai, Jacob Devlin, Jai Gupta, William Fedus, Orhan Firat, Sebastian Gerhmann, Nan Du, Dave Uthus, Siamak Shakeri, Slav Petrov and Quoc Le for support and discussions. We thank the Jax and T5X team for building such wonderful infrastructure that made this research possible. &lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/1370943694206193273/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1370943694206193273" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1370943694206193273" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html" rel="alternate" title="UL2 20B: An Open Source Unified Language Learner" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoRWMTOf1JUl345eb5BqKEPTRRxPvzPdzvspKtqlwNHqo4BVq98MJYkvEVPZAPdYmLaFMLQKAolOdzKD3uzbYTdYM8S9Z-y5BXgy6kotdukG8w9VCkrZt3Vb0H-BEDp8XC5bGIsA_OEQPWWll1vNRZbSBwJWowTCTf9cnW-7fDOXT8MmyH5s8KzieCQg/s72-c/image3.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-4616939676419842373</id><published>2022-10-13T09:53:00.014-07:00</published><updated>2022-10-21T10:44:33.146-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="datasets"/><category scheme="http://www.blogger.com/atom/ns#" term="EMNLP"/><category scheme="http://www.blogger.com/atom/ns#" term="Multimodal Learning"/><title type="text">Crossmodal-3600 — Multilingual Reference Captions for Geographically Diverse Images</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Ashish Thapliyal, Software Engineer, and Jordi Pont-Tuset, Research Scientist, Google Research&lt;/span&gt;&lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhA3qznWJUsCY3Jh7Z5CPA5U3dJIBWD28I3OOEXklBBY4TseAh5iap2pmDdYi-s4PbBZZu7lgMut_pW9WpbEr9FLyRK0WBnzejaerkRjJfNAc4tKhOvbnxhTY70akSiIrEir7NUxXhgbFyA1DopoQdqY0feRtoiTRkuyh6Goiqs2m73Yz9bFeBvkhekSg/s600/xm3600_animation_long.gif&quot; style=&quot;display: none;&quot; /&gt;&lt;p&gt;&lt;a href=&quot;https://ai.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html&quot;&gt;Image captioning&lt;/a&gt; is the machine learning task of automatically generating a fluent natural language description for a given image. This task is important for &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/more-accessible-web-images-arrive-10-new-languages/&quot;&gt;improving accessibility&lt;/a&gt; for visually impaired users and is a core task in multimodal research encompassing both vision and language modeling.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt; &lt;p&gt;However, datasets for image captioning are primarily available in English. Beyond that, there are only a few datasets covering a limited number of languages that represent just a small fraction of the world’s population. Further, these datasets feature images that severely under-represent the richness and diversity of cultures from across the globe. These aspects have hindered research on image captioning for a wide variety of languages, and directly hamper the deployment of accessibility solutions for a large potential audience around the world.  &lt;/p&gt;&lt;p&gt;Today we present and make publicly available the &lt;a href=&quot;https://google.github.io/crossmodal-3600/&quot;&gt;Crossmodal 3600&lt;/a&gt; (XM3600) image captioning evaluation dataset as a robust benchmark for multilingual image captioning that enables researchers to reliably compare research contributions in this emerging field. XM3600 provides 261,375 human-generated reference captions in 36 languages for a geographically diverse set of 3600 images. We show that the captions are of high quality and the style is consistent across languages. &lt;/p&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;  &lt;video align=&quot;center&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; syle=&quot;margin-left:20%&quot; width=&quot;80%&quot;&gt;&lt;source src=&quot;https://google.github.io/crossmodal-3600/web-data/xm3600_animation.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;&lt;/video&gt;&lt;/div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The Crossmodal 3600 dataset includes reference captions in 36 languages for each of a geographically diverse set of 3600 images. All images used with permission under the &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;&gt;CC-BY 2.0 license&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Overview of the Crossmodal 3600 Dataset&lt;/h2&gt;&lt;p&gt;Creating large training and evaluation datasets in multiple languages is a resource-intensive endeavor. &lt;a href=&quot;https://aclanthology.org/2020.acl-main.16/&quot;&gt;Recent work&lt;/a&gt; has shown that it is feasible to build multilingual image captioning models trained on machine-translated data with English captions as the starting point. However, some of the most reliable automatic metrics for image captioning are much less effective when applied to evaluation sets with translated image captions, resulting in poorer agreement with human evaluations compared to the English case. As such, trustworthy model evaluation at present can only be based on extensive human evaluation. Unfortunately, such evaluations usually cannot be replicated across different research efforts, and therefore do not offer a fast and reliable mechanism to automatically evaluate multiple model parameters and configurations (e.g., model &lt;a href=&quot;https://en.wikipedia.org/wiki/Hill_climbing&quot;&gt;hill climbing&lt;/a&gt;) or to compare multiple lines of research. &lt;/p&gt;&lt;p&gt;XM3600 provides 261,375 human-generated reference captions in 36 languages for a geographically diverse set of 3600 images from the &lt;a href=&quot;https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html&quot;&gt;Open Images&lt;/a&gt; dataset. We measure the quality of generated captions by comparing them to the manually provided captions using the &lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;&gt;CIDEr&lt;/a&gt; metric, which ranges from 0 (unrelated to the reference captions) to 10 (perfectly matching the reference captions). When comparing pairs of models, we observed strong correlations between the differences in the CIDEr scores of the model outputs, and side-by-side human evaluations comparing the model outputs. , making XM3600 is a reliable tool for high-quality automatic comparisons between image captioning models on a wide variety of languages beyond English. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Language Selection&lt;/h2&gt;&lt;p&gt;We chose 30 languages beyond English, roughly based on their percentage of web content. In addition, we chose an additional five languages that include under-resourced languages that have many native speakers or major native languages from continents that would not be covered otherwise. Finally, we also included English as a baseline, thus resulting in a total of 36 languages, as listed in the table below. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: 9%; margin-right: 9%;&quot;&gt;    &lt;colgroup&gt;     &lt;col style=&quot;width: 12%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 2%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 12%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 2%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 12%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 2%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 12%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 2%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 12%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 2%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 12%;&quot;&gt;&lt;/col&gt;  &lt;/colgroup&gt;  &lt;tbody&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: left;&quot;&gt;Arabic     &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Bengali*    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Chinese    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Croatian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;&lt;span style=&quot;font-size: small;&quot;&gt;Cusco&lt;br /&gt;Quechua*&lt;/span&gt;   &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Czech    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Danish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Dutch    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;English    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Filipino    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Finnish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;French    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;German    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Greek    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Hebrew    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Hindi    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Hungarian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Indonesian    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Italian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Japanese    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Korean    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Maori*    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Norwegian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Persian    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Polish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Portuguese    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Romanian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Russian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Spanish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Swahili*    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Swedish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Telugu*    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Thai    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Turkish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Ukrainian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;Vietnamese &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;List of languages used in XM3600. &amp;nbsp; *Low-resource languages with many native speakers, or major native languages from continents that would not be covered otherwise.&lt;/td&gt;&lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Image Selection&lt;/h2&gt;&lt;p&gt;The images were selected from among those in the &lt;a href=&quot;https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html&quot;&gt;Open Images&lt;/a&gt; dataset that have location metadata. Since there are many regions where more than one language is spoken, and some areas are not well covered by these images, we designed an algorithm to maximize the correspondence between selected images and the regions where the targeted languages are spoken. The algorithm starts with the selection of images with geo-data corresponding to the languages for which we have the smallest pool (e.g., Persian) and processes them in increasing order of their candidate image pool size. If there aren't enough images in an area where a language is spoken, then we gradually expand the geographic selection radius to: (i) a country where the language is spoken; (ii) a continent where the language is spoken; and, as last resort, (iii) from anywhere in the world. This strategy succeeded in providing our target number of 100 images from an appropriate region for most of the 36 languages, except for Persian (where 14 continent-level images are used) and Hindi (where all 100 images are at the global level, because the in-region images were assigned to Bengali and Telugu). &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;English&lt;br&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUXdopufo1pMrTexTsSfYyhLtAZxopwhRzfHUr3VSxJLw8xQJEEo7Sm_xOwLxM8nCCl5s_C_k1qIsvo2WRQh8Hmg1qtaqdPbBGh2UUxMU82HtQzO1ZJWxRLtlvdRn3w-_auygp_QCL9IBBtq8wf_mBARP9Ytoj6yaHQzKYVLFAoABLNBiE-TKeG-uGyQ/s640/1-1-car.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;640&quot; height=&quot;225&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUXdopufo1pMrTexTsSfYyhLtAZxopwhRzfHUr3VSxJLw8xQJEEo7Sm_xOwLxM8nCCl5s_C_k1qIsvo2WRQh8Hmg1qtaqdPbBGh2UUxMU82HtQzO1ZJWxRLtlvdRn3w-_auygp_QCL9IBBtq8wf_mBARP9Ytoj6yaHQzKYVLFAoABLNBiE-TKeG-uGyQ/s320/1-1-car.jpg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style=&quot;font-size: small;&quot;&gt;&lt;em&gt;&lt;a href=&quot;https://www.flickr.com/photos/lodekka/5072748008&quot;&gt;Photo&lt;/a&gt; by &lt;a href=&quot;https://www.flickr.com/people/lodekka/&quot;&gt;Chris Sampson&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;Swahili&lt;br&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJpLGjrS_EZAPP9qOKLccHt3k6mAmHPyLxQ-Ekrf3JDmqqVRlTqM_dmYOtxRWmBhyrCe28RmNCYkKcf2NYUDCLpY7GPLO5Ci5qjhonWrgCrOrgGrXWgwX3QmDcmfLeV_I6IpYduolhCGqJH28LwtSryoaZbdyib3YEqxqTg5ZEitR8sfcHJJyT1lkdsg/s640/1-2-giraffe.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;640&quot; height=&quot;225&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJpLGjrS_EZAPP9qOKLccHt3k6mAmHPyLxQ-Ekrf3JDmqqVRlTqM_dmYOtxRWmBhyrCe28RmNCYkKcf2NYUDCLpY7GPLO5Ci5qjhonWrgCrOrgGrXWgwX3QmDcmfLeV_I6IpYduolhCGqJH28LwtSryoaZbdyib3YEqxqTg5ZEitR8sfcHJJyT1lkdsg/s320/1-2-giraffe.jpg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style=&quot;font-size: small;&quot;&gt;&lt;em&gt;&lt;a href=&quot;https://www.flickr.com/photos/henrikpalm/7555980588&quot;&gt;Photo&lt;/a&gt; by &lt;a href=&quot;https://www.flickr.com/people/henrikpalm/&quot;&gt;Henrik Palm&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;Telugu&lt;br&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhddQGm3mPIfOcY890mh4C9tQ-Xo0cEumcT0O7aqimhcZbepCNvNrV5oDMB2eIIRVf-8ivNsBJt2kFZuTqE_udgyKfKbp_fAtuYQPa-72OxCvWOfFdG5tIV1pdV-4ednxTfeRUPwYZSPEMPwCo0UR049ELcc1beaCkDOmN1ySxlnk2fyK7euV7uJmKwAw/s640/1-3-statue.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;640&quot; height=&quot;225&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhddQGm3mPIfOcY890mh4C9tQ-Xo0cEumcT0O7aqimhcZbepCNvNrV5oDMB2eIIRVf-8ivNsBJt2kFZuTqE_udgyKfKbp_fAtuYQPa-72OxCvWOfFdG5tIV1pdV-4ednxTfeRUPwYZSPEMPwCo0UR049ELcc1beaCkDOmN1ySxlnk2fyK7euV7uJmKwAw/s320/1-3-statue.jpg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style=&quot;font-size: small;&quot;&gt;&lt;em&gt;&lt;a href=&quot;https://www.flickr.com/photos/14675798@N06/6901350577&quot;&gt;Photo&lt;/a&gt; by &lt;a href=&quot;https://www.flickr.com/people/14675798@N06/&quot;&gt;rojypala&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt; &lt;td&gt;&lt;br /&gt;&lt;/td&gt; &lt;td&gt;&lt;br /&gt;&lt;/td&gt; &lt;td&gt;&lt;br /&gt;&lt;/td&gt; &lt;td&gt;&lt;br /&gt;&lt;/td&gt; &lt;td&gt;&lt;br /&gt;&lt;/td&gt; &lt;/tr&gt;    &lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;Cusco Quechua&lt;br /&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTIJpYoJsVkjbASNxVmKnHNmliR7nNgzwHDrc7fRfdpfufCaaaThJyrmyH1JO7rblvKzZaTEhube_88MONPKMyDJKtR76JytM3NA1WKUzDrtGsehZFC0rhoXpab4eYCUTuWnmCIkZq7D6G-PE60qPjIp6ATUGTK5chOo0mSB7854JbU18nUqF1WeZ1aw/s640/2-1-tree.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;640&quot; height=&quot;225&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTIJpYoJsVkjbASNxVmKnHNmliR7nNgzwHDrc7fRfdpfufCaaaThJyrmyH1JO7rblvKzZaTEhube_88MONPKMyDJKtR76JytM3NA1WKUzDrtGsehZFC0rhoXpab4eYCUTuWnmCIkZq7D6G-PE60qPjIp6ATUGTK5chOo0mSB7854JbU18nUqF1WeZ1aw/s320/2-1-tree.jpg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style=&quot;font-size: small;&quot;&gt;&lt;em&gt;&lt;a href=&quot;https://www.flickr.com/photos/mckaysavage/8296819497&quot;&gt;Photo&lt;/a&gt; by &lt;a href=&quot;https://www.flickr.com/people/mckaysavage/&quot;&gt;McKay Savage&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;Filipino&lt;br /&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbgawSApxqJ3sQsKu44_hq1pvwD-EwvI1BopfQuYzByO2fACkuu5VcBJZMKyAyqbZ88JQX8SBpU0HR1yGCyxKAmOkpHs7WSHgZqNuC9W5aRb_sl-4ZKfH5dC93z7jK_Ah7BoKMieVaT4p26ejONReBy8F5ftIUOUzRcpuBFgx-u5T1CWdVxj7aD2x2qw/s640/2-2-boats.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;640&quot; height=&quot;225&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbgawSApxqJ3sQsKu44_hq1pvwD-EwvI1BopfQuYzByO2fACkuu5VcBJZMKyAyqbZ88JQX8SBpU0HR1yGCyxKAmOkpHs7WSHgZqNuC9W5aRb_sl-4ZKfH5dC93z7jK_Ah7BoKMieVaT4p26ejONReBy8F5ftIUOUzRcpuBFgx-u5T1CWdVxj7aD2x2qw/s320/2-2-boats.jpg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style=&quot;font-size: small;&quot;&gt;&lt;em&gt;&lt;a href=&quot;https://www.flickr.com/photos/schoeters/2683433042&quot;&gt;Photo&lt;/a&gt; by &lt;a href=&quot;https://www.flickr.com/people/schoeters/&quot;&gt;Simon Schoeters&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style=&quot;text-align: center;&quot;&gt;Chinese&lt;br /&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU2PoU1t2XWou_P-DbBgczROZd8rM3sh0eAjqFxb-wvKrQbfqOiiBqMXNhrsh0UcIUK8Pcp1U8sgbQ6eeqNGVnVP-f3BVGlR88ODnCXzXI1ZZ2FdW9A0i2EeRqBRs6phGw_WomOqyHVZXqgP5YpO75qL0Nb8pCce_a0t88CkDp-iZMUS_I1MuU0wolLA/s640/2-3-carving.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;640&quot; height=&quot;225&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU2PoU1t2XWou_P-DbBgczROZd8rM3sh0eAjqFxb-wvKrQbfqOiiBqMXNhrsh0UcIUK8Pcp1U8sgbQ6eeqNGVnVP-f3BVGlR88ODnCXzXI1ZZ2FdW9A0i2EeRqBRs6phGw_WomOqyHVZXqgP5YpO75qL0Nb8pCce_a0t88CkDp-iZMUS_I1MuU0wolLA/s320/2-3-carving.jpg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style=&quot;font-size: small;&quot;&gt;&lt;em&gt;&lt;a href=&quot;https://www.flickr.com/photos/rapidtravelchai/8639346056&quot;&gt;Photo&lt;/a&gt; by &lt;a href=&quot;https://www.flickr.com/people/rapidtravelchai/&quot;&gt;Stefan Krasowski&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;br&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Sample images showcasing the geographical diversity of the annotated images. Images used under &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;&gt;CC BY 2.0 license&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;h2&gt;Caption Generation&lt;/h2&gt;&lt;p&gt;In total, all 3600 images (100 images per language) are annotated in all 36 languages, each with an average of two annotations per language, yielding a total of 261,375 captions. &lt;/p&gt;&lt;p&gt;Annotators work in batches of 15 images. The first screen shows all 15 images with their captions in English as generated by a captioning model trained to output a consistent style of the form &quot;&amp;lt;main salient objects&amp;gt; doing &amp;lt;activities&amp;gt; in the &amp;lt;environment&amp;gt;&quot;, often with object attributes, such as a &quot;smiling&quot; person, &quot;red&quot; car, etc. The annotators are asked to rate the caption quality given guidelines for a 4-point scale from &quot;excellent&quot; to &quot;bad&quot;, plus an option for &quot;not_enough_information&quot;. This step forces the annotators to carefully assess caption quality and it primes them to internalize the style of the captions. The following screens show the images again but individually and without the English captions, and the annotators are asked to produce descriptive captions in the target language for each image.  &lt;/p&gt;&lt;p&gt;The image batch size of 15 was chosen so that the annotators would internalize the style without remembering the exact captions. Thus, we expect the raters to generate captions based on the image content only and lacking translation artifacts. For example in the example shown below, the Spanish caption mentions “number 42” and the Thai caption mentions “convertibles”, none of which are mentioned in the English captions. The annotators were also provided with a protocol to use when creating the captions, thus achieving style consistency across languages. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;  &lt;colgroup&gt;     &lt;col style=&quot;width: 30%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 2%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 10%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 2%;&quot;&gt;&lt;/col&gt;     &lt;col style=&quot;width: 58%;&quot;&gt;&lt;/col&gt;  &lt;/colgroup&gt;  &lt;tbody&gt;  &lt;tr&gt;&lt;td rowspan=&quot;8&quot; style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBYq6ilHcp-miykaD9OAZ4ICJi0qJFJX273i6gN2iuLkdu04HLsS_hDwvi8TUF06PC5Lo6ZzmLNCg9v-KUZGj7OOe_qxwFYNGaZgoTwUtXe0BywmrtNiArB_vV8b7YLL0AcP4PylDdD_5KcJSFDpHNm5an3Q3Hu03nEKTVSMLIioWJTRbXlGpvUvywtw/s641/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;479&quot; data-original-width=&quot;641&quot; height=&quot;239&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBYq6ilHcp-miykaD9OAZ4ICJi0qJFJX273i6gN2iuLkdu04HLsS_hDwvi8TUF06PC5Lo6ZzmLNCg9v-KUZGj7OOe_qxwFYNGaZgoTwUtXe0BywmrtNiArB_vV8b7YLL0AcP4PylDdD_5KcJSFDpHNm5an3Q3Hu03nEKTVSMLIioWJTRbXlGpvUvywtw/s320/image3.jpg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style=&quot;font-size: small;&quot;&gt;&lt;em&gt;&lt;a href=&quot;https://www.flickr.com/photos/briansolis/5129089526&quot;&gt;Photo&lt;/a&gt; by &lt;a href=&quot;https://www.flickr.com/people/briansolis/&quot;&gt;Brian Solis&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;    &lt;td rowspan=&quot;8&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td rowspan=&quot;2&quot; style=&quot;text-align: left;&quot;&gt;&lt;span style=&quot;font-size: small;&quot;&gt;English&lt;/span&gt;&lt;/td&gt;    &lt;td rowspan=&quot;2&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;• &lt;span style=&quot;font-size: small;&quot;&gt;A vintage sports car in a showroom with many other vintage sports cars&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: left;&quot;&gt;• &lt;span style=&quot;font-size: small;&quot;&gt;The branded classic cars in a row at display&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;    &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td rowspan=&quot;2&quot; style=&quot;text-align: left;&quot;&gt;&lt;span style=&quot;font-size: small;&quot;&gt;Spanish&lt;/span&gt;&lt;/td&gt;    &lt;td rowspan=&quot;2&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;• &lt;span style=&quot;font-size: small;&quot;&gt;Automóvil clásico deportivo en exhibición de automóviles de galería — &lt;em&gt;(Classic sports car in gallery car show)&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td style=&quot;text-align: left;&quot;&gt;• &lt;span style=&quot;font-size: small;&quot;&gt;Coche pequeño de carreras color plateado con el número 42 en una exhibición de coches — &lt;em&gt;(Small silver racing car with the number 42 at a car show)&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;    &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td rowspan=&quot;2&quot; style=&quot;text-align: left;&quot;&gt;&lt;span style=&quot;font-size: small;&quot;&gt;Thai&lt;/span&gt;&lt;/td&gt;    &lt;td rowspan=&quot;2&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;• &lt;span style=&quot;font-size: small;&quot;&gt;รถเปิดประทุนหลายสีจอดเรียงกันในที่จัดแสดง — &lt;em&gt;(Multicolored convertibles line up in the exhibit)&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style=&quot;text-align: left;&quot;&gt;• &lt;span style=&quot;font-size: small;&quot;&gt;รถแข่งวินเทจจอดเรียงกันหลายคันในงานจัดแสดง — &lt;em&gt;(Several vintage racing cars line up at the show.)&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;br&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Sample captions in three different languages (out of 36 — see full list of captions in Appendix A of the &lt;a href=&quot;https://arxiv.org/abs/2205.12522&quot;&gt;Crossmodal-3600 paper&lt;/a&gt;), showcasing the creation of annotations that are consistent in style across languages, while being free of direct-translation artifacts (e.g., the Spanish “number 42” or the Thai “convertibles” would not be possible when directly translating from the English versions). Image used under &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;&gt;CC BY 2.0 license&lt;/a&gt;.&lt;/td&gt;  &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Caption Quality and Statistics&lt;/h2&gt;&lt;p&gt;We ran two to five pilot studies per language to troubleshoot the caption generation process and to ensure high quality captions. We then manually evaluated a random subset of captions. First we randomly selected a sample of 600 images. Then, to measure the quality of captions in a particular language, for each image, we selected for evaluation one of the manually generated captions. We found that: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;For 25 out of 36 languages, the percentage of captions rated as “Good” or “Excellent” is above 90%, and the rest are all above 70%.&lt;/li&gt;  &lt;li&gt;For 26 out of 36 languages, the percentage of captions rated as “Bad” is below 2%, and the rest are all below 5%.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For languages that use spaces to separate words, the number of words per caption can be as low as 5 or 6 for some &lt;a href=&quot;https://en.wikipedia.org/wiki/Agglutinative_language&quot;&gt;agglutinative languages&lt;/a&gt; like Cusco Quechua and Czech, and as high as 18 for an &lt;a href=&quot;https://en.wikipedia.org/wiki/Analytic_language&quot;&gt;analytic language&lt;/a&gt; like Vietnamese. The number of characters per caption also varies drastically — from mid-20s for Korean to mid-90s for Indonesian — depending on the alphabet and the script of the language. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Empirical Evaluation and Results&lt;/h2&gt;&lt;p&gt;We empirically measured the ability of the XM3600 annotations to rank image captioning model variations by training four variations of a multilingual image captioning model and comparing the CIDEr differences of the models’ outputs over the XM3600 dataset for 30+ languages, to side-by-side human evaluations. We observed strong correlations between the CIDEr differences and the human evaluations. These results support the use of the XM3600 references as a means to achieve high-quality automatic comparisons between image captioning models on a wide variety of languages beyond English. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Recent Uses&lt;/h2&gt;&lt;p&gt;Recently &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;&gt;PaLI&lt;/a&gt; used XM3600 to evaluate model performance beyond English for image captioning, image-to-text retrieval and text-to-image retrieval. The key takeaways they found when evaluating on XM3600 were that multilingual captioning greatly benefits from scaling the PaLI models, especially for low-resource languages. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;We would like to acknowledge the coauthors of this work: Xi Chen and Radu Soricut.&lt;/em&gt;   &lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/4616939676419842373/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/crossmodal-3600-multilingual-reference.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4616939676419842373" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4616939676419842373" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/crossmodal-3600-multilingual-reference.html" rel="alternate" title="Crossmodal-3600 — Multilingual Reference Captions for Geographically Diverse Images" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhA3qznWJUsCY3Jh7Z5CPA5U3dJIBWD28I3OOEXklBBY4TseAh5iap2pmDdYi-s4PbBZZu7lgMut_pW9WpbEr9FLyRK0WBnzejaerkRjJfNAc4tKhOvbnxhTY70akSiIrEir7NUxXhgbFyA1DopoQdqY0feRtoiTRkuyh6Goiqs2m73Yz9bFeBvkhekSg/s72-c/xm3600_animation_long.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-8280632452472362334</id><published>2022-10-06T13:05:00.002-07:00</published><updated>2022-10-22T19:48:00.054-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Acoustic Modeling"/><category scheme="http://www.blogger.com/atom/ns#" term="Audio"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><title type="text">AudioLM: a Language Modeling Approach to Audio Generation</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Zalán Borsos, Research Software Engineer, and Neil Zeghidour, Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;Generating realistic audio requires modeling information represented at different scales. For example, just as music builds complex musical phrases from individual notes, speech combines temporally local structures, such as phonemes or syllables, into words and sentences. Creating well-structured and coherent audio sequences at all these scales is a challenge that has been addressed by coupling audio with transcriptions that can guide the generative process, be it &lt;a href=&quot;https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html&quot;&gt;text transcripts for speech synthesis&lt;/a&gt; or &lt;a href=&quot;https://magenta.tensorflow.org/music-transformer&quot;&gt;MIDI representations for piano&lt;/a&gt;. However, this approach breaks when trying to model untranscribed aspects of audio, such as speaker characteristics necessary to &lt;a href=&quot;https://ai.googleblog.com/2019/07/parrotron-new-research-into-improving.html&quot;&gt;help people with speech impairments recover their voice&lt;/a&gt;, or stylistic components of a piano performance. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2209.03143&quot;&gt;AudioLM: a Language Modeling Approach to Audio Generation&lt;/a&gt;”, we propose a new framework for audio generation that learns to generate realistic speech and piano music by listening to audio only. Audio generated by AudioLM demonstrates long-term consistency (e.g., syntax in speech, melody in music) and high fidelity, outperforming previous systems and pushing the frontiers of audio generation with applications in speech synthesis or computer-assisted music. Following our &lt;a href=&quot;http://ai.google/principles&quot;&gt;AI Principles&lt;/a&gt;, we've also developed a model to identify synthetic audio generated by AudioLM. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;From Text to Audio Language Models&lt;/h2&gt;&lt;p&gt;In recent years, language models trained on very large text corpora have demonstrated their exceptional generative abilities, from &lt;a href=&quot;https://blog.google/technology/ai/lamda/&quot;&gt;open-ended dialogue&lt;/a&gt; to &lt;a href=&quot;https://ai.googleblog.com/2022/05/24-new-languages-google-translate.html&quot;&gt;machine translation&lt;/a&gt; or even &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;&gt;common-sense reasoning&lt;/a&gt;. They have further shown their capacity to model other signals than texts, such &lt;a href=&quot;https://parti.research.google/&quot;&gt;as natural images&lt;/a&gt;. The key intuition behind AudioLM is to leverage such advances in language modeling to generate audio without being trained on annotated data.  &lt;/p&gt;&lt;p&gt;However, some challenges need to be addressed when moving from text language models to audio language models. First, one must cope with the fact that the data rate for audio is significantly higher, thus leading to much longer sequences — while a written sentence can be represented by a few dozen characters, its audio &lt;a href=&quot;https://en.wikipedia.org/wiki/Waveform&quot;&gt;waveform&lt;/a&gt; typically contains hundreds of thousands of values. Second, there is a one-to-many relationship between text and audio. This means that the same sentence can be rendered by different speakers with different speaking styles, emotional content and recording conditions.  &lt;/p&gt;&lt;p&gt;To overcome both challenges, AudioLM leverages two kinds of audio tokens. First, &lt;em&gt;semantic tokens&lt;/em&gt; are extracted from &lt;a href=&quot;https://arxiv.org/abs/2108.06209&quot;&gt;w2v-BERT&lt;/a&gt;, a self-supervised audio model. These tokens capture both local dependencies (e.g., phonetics in speech, local melody in piano music) and global long-term structure (e.g., language syntax and semantic content in speech, harmony and rhythm in piano music), while heavily downsampling the audio signal to allow for modeling long sequences.   &lt;/p&gt;&lt;p&gt;However, audio reconstructed from these tokens demonstrates poor fidelity. To overcome this limitation, in addition to semantic tokens, we rely on &lt;em&gt;acoustic tokens&lt;/em&gt; produced by &lt;a href=&quot;https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html&quot;&gt;a SoundStream neural codec&lt;/a&gt;, which capture the details of the audio waveform (such as speaker characteristics or recording conditions) and allow for high-quality synthesis. Training a system to generate both semantic and acoustic tokens leads simultaneously to high audio quality and long-term consistency.  &lt;/p&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiX0MyO_IXk730mCbbJX7LXxBRIxJk2K41Y4leuEk4WQRjz0kgIp9CGHFwLePaKt3qEcCK8fvhAxjJ7J_sXH05q7xnsMbjZZFDDLPIlVyaKr3yYo77oT2KBqe9gw4MFuUZnUfxFprP67ExPzr2RNxduB0SruUGjJXghihHSoxvMtlG3YNtHHesZOJzY/s960/image2.png&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;330&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiX0MyO_IXk730mCbbJX7LXxBRIxJk2K41Y4leuEk4WQRjz0kgIp9CGHFwLePaKt3qEcCK8fvhAxjJ7J_sXH05q7xnsMbjZZFDDLPIlVyaKr3yYo77oT2KBqe9gw4MFuUZnUfxFprP67ExPzr2RNxduB0SruUGjJXghihHSoxvMtlG3YNtHHesZOJzY/s16000/image2.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;h2&gt;Training an Audio-Only Language Model&lt;/h2&gt;&lt;p&gt;AudioLM is a pure audio model that is trained without any text or symbolic representation of music. AudioLM models an audio sequence hierarchically, from semantic tokens up to fine acoustic tokens, by chaining several &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;Transformer&lt;/a&gt; models, one for each stage. Each stage is trained for the next token prediction based on past tokens, as one would train a text language model. The first stage performs this task on semantic tokens to model the high-level structure of the audio sequence. &lt;/p&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyLXFSPJXdgbOkxanFQ4TChCGNF6dRkntaSTtsoFChIR4Tp97RJesNvMhwVlgHLcI7j_D6Vkur2u5GtTVOVrswAsoNK93VtOEU0xBFqgS6SPrnYCl5O8cA_9qNCRADnty2_seXrB5fYsYoLjRyKAFdHlRfwJvHl-iDTW7V07EQJUQeBAen3jfktwLn/s958/image6.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;184&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyLXFSPJXdgbOkxanFQ4TChCGNF6dRkntaSTtsoFChIR4Tp97RJesNvMhwVlgHLcI7j_D6Vkur2u5GtTVOVrswAsoNK93VtOEU0xBFqgS6SPrnYCl5O8cA_9qNCRADnty2_seXrB5fYsYoLjRyKAFdHlRfwJvHl-iDTW7V07EQJUQeBAen3jfktwLn/s16000/image6.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;  In the second stage, we concatenate the entire semantic token sequence, along with the past coarse acoustic tokens, and feed both as conditioning to the coarse acoustic model, which then predicts the future tokens. This step models acoustic properties such as speaker characteristics in speech or timbre in music. &lt;/p&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwPPMHQ8s-D3GVIeGIxaRmwVqcb4sh7eZHs7El7gwP7pvoXk3Cv3qSjBhETkMzJzt7_3eBV8t4FkdAExVCVV-i7RrWo2HZJoai3w4Nj1R7mf4tatvPa8jOOO-iqZWmMYyg-o9we-MLi81QgCp4pG5AGDI-ifHgrw1Oc1-6kb2ZMflnEsqjdnonLOuM/s960/image1.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwPPMHQ8s-D3GVIeGIxaRmwVqcb4sh7eZHs7El7gwP7pvoXk3Cv3qSjBhETkMzJzt7_3eBV8t4FkdAExVCVV-i7RrWo2HZJoai3w4Nj1R7mf4tatvPa8jOOO-iqZWmMYyg-o9we-MLi81QgCp4pG5AGDI-ifHgrw1Oc1-6kb2ZMflnEsqjdnonLOuM/s16000/image1.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;  In the third stage, we process the coarse acoustic tokens with the fine acoustic model, which adds even more detail to the final audio. Finally, we feed acoustic tokens to the SoundStream decoder to reconstruct a waveform. &lt;/p&gt;&lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdFRqUq5R0CwA7eaE0qAXStL2AAZw3LNOxus3NYoz_JkbWSXz1ydyb839s0Z5qPk_wTGgAYL4B_XZBQRXaSsYtb9RYZZpf8kB8UjhwbGMZBhqswxse110R2OaVc4szGTvcSqZLm4hSCQ3howGlmEMoJxhvonK3MWkp49RquIhciqCJ349fCv6KxvUl/s960/image4.png&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdFRqUq5R0CwA7eaE0qAXStL2AAZw3LNOxus3NYoz_JkbWSXz1ydyb839s0Z5qPk_wTGgAYL4B_XZBQRXaSsYtb9RYZZpf8kB8UjhwbGMZBhqswxse110R2OaVc4szGTvcSqZLm4hSCQ3howGlmEMoJxhvonK3MWkp49RquIhciqCJ349fCv6KxvUl/s16000/image4.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;  After training, one can condition AudioLM on a few seconds of audio, which enables it to generate consistent continuation. In order to showcase the general applicability of the AudioLM framework, we consider two tasks from different audio domains:  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Speech continuation&lt;/em&gt;, where the model is expected to retain the speaker characteristics, &lt;a href=&quot;https://en.wikipedia.org/wiki/Prosody_(linguistics)&quot;&gt;prosody&lt;/a&gt; and recording conditions of the prompt while producing new content that is syntactically correct and semantically consistent. &lt;/li&gt;&lt;li&gt;&lt;em&gt;Piano continuation&lt;/em&gt;, where the model is expected to generate piano music that is coherent with the prompt in terms of melody, harmony and rhythm. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In the video below, you can listen to examples where the model is asked to continue either speech or music and generate new content that was not seen during training. As you listen, note that everything you hear after the gray vertical line was generated by AudioLM and that the model has never seen any text or musical transcription, but rather just learned from raw audio. We release more samples on &lt;a href=&quot;https://google-research.github.io/seanet/audiolm/examples/&quot;&gt;this webpage&lt;/a&gt;. &lt;/p&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/_xkZwJ0H9IU&quot; width=&quot;640&quot; youtube-src-id=&quot;_xkZwJ0H9IU&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;p&gt;To validate our results, we asked human raters to listen to short audio clips and decide whether it is an original recording of human speech or a synthetic continuation generated by AudioLM. Based on the ratings collected, we observed a 51.2% success rate, which is not statistically significantly different from the 50% success rate achieved when assigning labels at random. This means that speech generated by AudioLM is hard to distinguish from real speech for the average listener. &lt;/p&gt;&lt;p&gt;Our work on AudioLM is for research purposes and we have no plans to release it more broadly at this time. In alignment with our &lt;a href=&quot;http://ai.google/principles&quot;&gt;AI Principles&lt;/a&gt;, we sought to understand and mitigate the possibility that people could misinterpret the short speech samples synthesized by AudioLM as real speech. For this purpose, we trained a classifier that can detect synthetic speech generated by AudioLM with very high accuracy (98.6%). This shows that despite being (almost) indistinguishable to some listeners, continuations generated by AudioLM are very easy to detect with a simple audio classifier. This is a crucial first step to help protect against the potential misuse of AudioLM, with future efforts potentially exploring technologies such as audio “watermarking”. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We introduce AudioLM, a language modeling approach to audio generation that provides both long-term coherence and high audio quality. Experiments on speech generation show not only that AudioLM can generate syntactically and semantically coherent speech without any text, but also that continuations produced by the model are almost indistinguishable from real speech by humans. Moreover, AudioLM goes well beyond speech and can model arbitrary audio signals such as piano music. This encourages the future extensions to other types of audio (e.g., multilingual speech, polyphonic music, and audio events) as well as integrating AudioLM into an encoder-decoder framework for conditioned tasks such as text-to-speech or speech-to-speech translation. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgments&lt;/h2&gt;&lt;p&gt;&lt;em&gt;The work described here was authored by Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi and Neil Zeghidour. We are grateful for all discussions and feedback on this work that we received from our colleagues at Google. &lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/8280632452472362334/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8280632452472362334" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8280632452472362334" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html" rel="alternate" title="AudioLM: a Language Modeling Approach to Audio Generation" type="text/html"><author><name>Andrew Helton</name><uri>http://www.blogger.com/profile/12510940840342590054</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiX0MyO_IXk730mCbbJX7LXxBRIxJk2K41Y4leuEk4WQRjz0kgIp9CGHFwLePaKt3qEcCK8fvhAxjJ7J_sXH05q7xnsMbjZZFDDLPIlVyaKr3yYo77oT2KBqe9gw4MFuUZnUfxFprP67ExPzr2RNxduB0SruUGjJXghihHSoxvMtlG3YNtHHesZOJzY/s72-c/image2.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-4364275298594755861</id><published>2022-10-04T11:43:00.005-07:00</published><updated>2022-10-22T19:51:19.210-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computational Photography"/><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="video"/><title type="text">Large Motion Frame Interpolation</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Fitsum Reda and Janne Kontkanen, Google Research&lt;/span&gt; &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Motion_interpolation&quot;&gt;Frame interpolation&lt;/a&gt; is the process of synthesizing in-between images from a given set of images. The technique is often used for &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Frame_rate%23Frame_rate_up-conversion&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1664322285856563&amp;amp;usg=AOvVaw1YyWoW7xD9EqEtOSAjIKSD&quot;&gt;temporal up-sampling&lt;/a&gt; to increase the refresh rate of videos or to create slow motion effects. Nowadays, with digital cameras and smartphones, we often take several photos within a few seconds to capture the best picture. Interpolating between these “near-duplicate” photos can lead to engaging videos that reveal scene motion, often delivering an even more pleasing sense of the moment than the original photos.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;  &lt;p&gt;Frame interpolation between consecutive video frames, which often have small motion, has been studied extensively. Unlike videos, however, the temporal spacing between near-duplicate photos can be several seconds, with commensurately large in-between motion, which is a major failing point of existing frame interpolation methods. Recent methods attempt to handle large motion by training on datasets with &lt;a href=&quot;https://arxiv.org/abs/2103.16206&quot;&gt;extreme motion&lt;/a&gt;, albeit with limited effectiveness on &lt;a href=&quot;https://arxiv.org/abs/2108.06815&quot;&gt;smaller motions&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;In “&lt;a href=&quot;https://arxiv.org/pdf/2202.04901.pdf&quot;&gt;FILM: Frame Interpolation for Large Motion&lt;/a&gt;”, published at &lt;a href=&quot;https://eccv2022.ecva.net/&quot;&gt;ECCV 2022&lt;/a&gt;, we present a method to create high quality slow-motion videos from near-duplicate photos. FILM is a new neural network architecture that achieves state-of-the-art results in large motion, while also handling smaller motions well. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRIrNGJ_ULWX3iM63cdI0H6CJx9200rUy_yxYJbT15kNRlAkgiZ4o_8NfWyfZ2GnTusotAQXxMjVkhh0zvOW92meJkwRniD2Fh6DyvZukZDOo_ZujCcsos37LOeE9rpZlyh2VNFYVMa5WapuSpCxdTJ9UWVBLZvRdlup7ACXPlq_zhR6tyE8KoUpsg0g/s1920/image3.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRIrNGJ_ULWX3iM63cdI0H6CJx9200rUy_yxYJbT15kNRlAkgiZ4o_8NfWyfZ2GnTusotAQXxMjVkhh0zvOW92meJkwRniD2Fh6DyvZukZDOo_ZujCcsos37LOeE9rpZlyh2VNFYVMa5WapuSpCxdTJ9UWVBLZvRdlup7ACXPlq_zhR6tyE8KoUpsg0g/s16000/image3.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;FILM interpolating between two near-duplicate photos to create a slow motion video.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;FILM Model Overview&lt;/h2&gt;&lt;p&gt;The FILM model takes two images as input and outputs a middle image. At inference time, we recursively invoke the model to output in-between images. FILM has three components: (1) A feature extractor that summarizes each input image with deep multi-scale (&lt;a href=&quot;https://arxiv.org/abs/1612.03144&quot;&gt;pyramid&lt;/a&gt;) features; (2) a bi-directional motion estimator that computes pixel-wise motion (i.e., flows) at each pyramid level; and (3) a fusion module that outputs the final interpolated image. We train FILM on regular video frame triplets, with the middle frame serving as the ground-truth for supervision. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiblix_Ee8FHA04AR4SV3sL5MhDduduPcIDc22CjitRMyqzZqrZh7_sAt32LsNSkjljYp1gr-tjKsZsOq7oMOkjSJbm8kqUYp750Jjiboa2dnRVT4QKR1Pi6fNc4Km-C6K-atgssRffENi7OfNAO7S4-YZsKdkpMZTSCJM5ztk5ACQDBuhorhwD-f3_Ng/s530/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;530&quot; data-original-width=&quot;319&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiblix_Ee8FHA04AR4SV3sL5MhDduduPcIDc22CjitRMyqzZqrZh7_sAt32LsNSkjljYp1gr-tjKsZsOq7oMOkjSJbm8kqUYp750Jjiboa2dnRVT4QKR1Pi6fNc4Km-C6K-atgssRffENi7OfNAO7S4-YZsKdkpMZTSCJM5ztk5ACQDBuhorhwD-f3_Ng/s16000/image4.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A standard&lt;a href=&quot;https://arxiv.org/abs/1612.03144&quot;&gt; feature pyramid&lt;/a&gt; extraction on two input images. Features are processed at each level by a series of convolutions, which are then downsampled to half the spatial resolution and passed as input to the deeper level.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Scale-Agnostic Feature Extraction &lt;/h2&gt;&lt;p&gt;Large motion is typically handled with hierarchical motion estimation using multi-resolution feature pyramids (shown above). However, this method struggles with small and fast-moving objects because they can disappear at the deepest pyramid levels. In addition, there are far fewer available pixels to derive supervision at the deepest level. &lt;/p&gt; &lt;p&gt;To overcome these limitations, we adopt a &lt;a href=&quot;https://augmentedperception.github.io/pixelfusion/&quot;&gt;feature extractor&lt;/a&gt; that shares weights across scales to create a “scale-agnostic” feature pyramid. This feature extractor (1) allows the use of a shared motion estimator across pyramid levels (next section) by equating large motion at shallow levels with small motion at deeper levels, and (2) creates a compact network with fewer weights.  &lt;/p&gt; &lt;p&gt;Specifically, given two input images, we first create an image pyramid by successively downsampling each image. Next, we use a shared &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28#Bib1&quot;&gt;U-Net&lt;/a&gt; convolutional encoder to extract a smaller feature pyramid from each image pyramid level (columns in the figure below). As the third and final step, we construct a scale-agnostic feature pyramid by horizontally concatenating features from different convolution layers that have the same spatial dimensions. Note that from the third level onwards, the feature stack is constructed with the same set of shared convolution weights (shown in the same color). This ensures that all features are similar, which allows us to continue to share weights in the subsequent motion estimator. The figure below depicts this process using four pyramid levels, but in practice, we use seven. &lt;/p&gt; &lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Bi-directional Flow Estimation &lt;/h2&gt;&lt;p&gt;After feature extraction, FILM performs pyramid-based residual flow estimation to compute the flows from the yet-to-be-predicted middle image to the two inputs. The flow estimation is done once for each input, starting from the deepest level, using a stack of convolutions. We estimate the flow at a given level by adding a residual correction to the upsampled estimate from the next deeper level. This approach takes the following as its input: (1) the features from the first input at that level, and (2) the features of the second input after it is warped with the upsampled estimate. The same convolution weights are shared across all levels, except for the two finest levels. &lt;/p&gt; &lt;p&gt;Shared weights allow the interpretation of small motions at deeper levels to be the same as large motions at shallow levels, boosting the number of pixels available for large motion supervision. Additionally, shared weights not only enable the training of powerful models that may reach a higher &lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;&gt;peak signal-to-noise ratio&lt;/a&gt; (PSNR), but are also needed to enable models to fit into GPU memory for practical applications.&lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;2&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;  &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcbs3LhMChr0VhdiLdluPnPIopUdLH6_Oq-iRNjcirN6flBWD-hmOjvEPduIym1aW5fO_gPOlkP_LwXxPqYWpVq2Yg0RxJ0rHY4BpjdDN7RLJg6emzHvaZBVFkeGtDs8nn9poGdmX6Om-4kP5E1KR2k3J-N_ERqGK0LRFugWnEbIamBGeatWPLdq1cww/s768/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;579&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcbs3LhMChr0VhdiLdluPnPIopUdLH6_Oq-iRNjcirN6flBWD-hmOjvEPduIym1aW5fO_gPOlkP_LwXxPqYWpVq2Yg0RxJ0rHY4BpjdDN7RLJg6emzHvaZBVFkeGtDs8nn9poGdmX6Om-4kP5E1KR2k3J-N_ERqGK0LRFugWnEbIamBGeatWPLdq1cww/s16000/image1.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZjPZaKydxStrnc2i8NAsN7UUl2qYFSe2GcI5-ZBxt1RCe-4xoxI3gB8KjmoRh9y2YXg_kDL4OEwXAQpKpRXBhQ50fSuhIO6JCj1GkvOCkw6Bu3y__bBJFCH28V6u9cK27mAEsEXQcwrBWKZ_ZnNrNU0Vex5ZlQOCoUEwrWPRR-R76rdAQmaqUCAIXLA/s768/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;571&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZjPZaKydxStrnc2i8NAsN7UUl2qYFSe2GcI5-ZBxt1RCe-4xoxI3gB8KjmoRh9y2YXg_kDL4OEwXAQpKpRXBhQ50fSuhIO6JCj1GkvOCkw6Bu3y__bBJFCH28V6u9cK27mAEsEXQcwrBWKZ_ZnNrNU0Vex5ZlQOCoUEwrWPRR-R76rdAQmaqUCAIXLA/s16000/image4.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The impact of weight sharing on image quality. &lt;b&gt;Left&lt;/b&gt;: no sharing, &lt;b&gt;Right&lt;/b&gt;: sharing. For this ablation we used a smaller version of our model (called FILM-med in the &lt;a href=&quot;https://arxiv.org/pdf/2202.04901.pdf&quot;&gt;paper&lt;/a&gt;) because the full model without weight sharing would diverge as the regularization benefit of weight sharing was lost.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Fusion and Frame Generation&lt;/h2&gt;&lt;p&gt;Once the bi-directional flows are estimated, we warp the two feature pyramids into alignment. We obtain a concatenated feature pyramid by stacking, at each pyramid level, the two aligned feature maps, the bi-directional flows and the input images. Finally, a &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28#Bib1&quot;&gt;U-Net&lt;/a&gt; decoder synthesizes the interpolated output image from the aligned and stacked feature pyramid.  &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0SmdFNEGohaKfz2VoKw15mX92A_62xLRmPB7DAMWZqRGFdV9EYkTQpGZo6gIgTHstm2Bxd_suOyFZtAf6o6MF3esTx6jJ_gWf4-1E7EoHnDPx7RkgaozqKBSBja8uKgHkbXUVV4uZ3FdPFjwicqqI7rhGNGPtDomn1o4lYdiMZe8HtFnV9cjF42FjMA/s1600/image9.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0SmdFNEGohaKfz2VoKw15mX92A_62xLRmPB7DAMWZqRGFdV9EYkTQpGZo6gIgTHstm2Bxd_suOyFZtAf6o6MF3esTx6jJ_gWf4-1E7EoHnDPx7RkgaozqKBSBja8uKgHkbXUVV4uZ3FdPFjwicqqI7rhGNGPtDomn1o4lYdiMZe8HtFnV9cjF42FjMA/s16000/image9.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;FILM Architecture. &lt;b&gt;FEATURE EXTRACTION&lt;/b&gt;: we extract scale-agnostic features. The features with matching colors are extracted using shared weights. &lt;b&gt;FLOW ESTIMATION&lt;/b&gt;: we compute bi-directional flows using shared weights across the deeper pyramid levels and warp the features into alignment. &lt;b&gt;FUSION&lt;/b&gt;: A U-Net decoder outputs the final interpolated frame.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Loss Functions &lt;/h2&gt;&lt;p&gt;During training, we supervise FILM by combining three losses. First, we use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Least_absolute_deviations&quot;&gt;absolute L1&lt;/a&gt; difference between the predicted and ground-truth frames to capture the motion between input images. However, this produces blurry images when used alone. Second, we use &lt;a href=&quot;https://arxiv.org/abs/1603.08155&quot;&gt;perceptual loss&lt;/a&gt; to improve image fidelity. This minimizes the L1 difference between the &lt;a href=&quot;https://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt; pre-trained &lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;&gt;VGG-19&lt;/a&gt; features extracted from the predicted and ground truth frames. Third, we use &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf&quot;&gt;Style loss&lt;/a&gt; to minimize the L2 difference between the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gram_matrix&quot;&gt;Gram matrix&lt;/a&gt; of the ImageNet pre-trained VGG-19 features. The Style loss enables the network to produce sharp images and realistic &lt;a href=&quot;https://en.wikipedia.org/wiki/Inpainting&quot;&gt;inpaintings&lt;/a&gt; of large pre-occluded regions. Finally, the losses are combined with weights empirically selected such that each loss contributes equally to the total loss. &lt;/p&gt;  &lt;p&gt;Shown below, the combined loss greatly improves sharpness and image fidelity when compared to training FILM with L1 loss and VGG losses. The combined loss maintains the sharpness of the tree leaves.  &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmVUdzOeQQv4NG9pIYl0zzzg0b39-R1XT_1-r7EXNwBx4SaAZuFqkJ9MdS-HTwSVDaUEIkZwqhoSR7qA5jhHk2gX076TkrRKBt9G4MJqKpHuh4o6ChvuKJep_ew7uPDEZw_Y0DlVApCDWaZHsLubJfd8PI7Hp2aiBTWgekdGd3vjNwzWAk1i9mXGIp1Q/s1629/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;640&quot; data-original-width=&quot;1629&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmVUdzOeQQv4NG9pIYl0zzzg0b39-R1XT_1-r7EXNwBx4SaAZuFqkJ9MdS-HTwSVDaUEIkZwqhoSR7qA5jhHk2gX076TkrRKBt9G4MJqKpHuh4o6ChvuKJep_ew7uPDEZw_Y0DlVApCDWaZHsLubJfd8PI7Hp2aiBTWgekdGd3vjNwzWAk1i9mXGIp1Q/s16000/image8.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;FILM’s combined loss functions. L1 loss (&lt;b&gt;left&lt;/b&gt;), L1 plus VGG loss (&lt;b&gt;middle&lt;/b&gt;), and Style loss (&lt;b&gt;right&lt;/b&gt;), showing significant sharpness improvements (green box).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Image and Video Results&lt;/h2&gt;&lt;p&gt;We evaluate FILM on an internal near-duplicate photos dataset that exhibits large scene motion. Additionally, we compare FILM to recent frame interpolation methods: &lt;a href=&quot;https://arxiv.org/abs/2003.05534&quot;&gt;SoftSplat&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2108.06815&quot;&gt;ABME&lt;/a&gt;. FILM performs favorably when interpolating across large motion. Even in the presence of motion as large as 100 pixels, FILM generates sharp images consistent with the inputs.&lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnJ4WEQIhreXe_RRfbGtfyETixhFV36yKFC9LU0RnixWuYVhiBELkVkxThn04uLzpUFBljXR3arlME9wX_wSXdIa1s8aEX3LVjrQG7xL4ao9UJVQYDZS7xzPjucXX_gS627x2oNo6s4XX3xAKEjLQBcxiUJx0TNMQM0Gf48QDLITpstfo3GdDbukbpIg/s1897/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;909&quot; data-original-width=&quot;1897&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnJ4WEQIhreXe_RRfbGtfyETixhFV36yKFC9LU0RnixWuYVhiBELkVkxThn04uLzpUFBljXR3arlME9wX_wSXdIa1s8aEX3LVjrQG7xL4ao9UJVQYDZS7xzPjucXX_gS627x2oNo6s4XX3xAKEjLQBcxiUJx0TNMQM0Gf48QDLITpstfo3GdDbukbpIg/s16000/image6.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Frame interpolation with SoftSplat (&lt;b&gt;left&lt;/b&gt;), ABME (&lt;b&gt;middle&lt;/b&gt;) and FILM (&lt;b&gt;right&lt;/b&gt;) showing favorable image quality and temporal consistency. &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwFw0mLZbtSiumwE8BQfqmGmElkNFqdeVFsFdeZIO6jG12pHvnfompGPb2J6XZmCgrpbYN76Q4b7VXmK3Jk8csJ_23JdvI75QjGYjM5XRcKA0-7JyGQFF2zUsBaztihQYAJ7gJJCkRrZVi2lXey8KfoTakpqbk9UeC3YteI8uzP5wSZDCInnmRHv10cg/s1896/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;909&quot; data-original-width=&quot;1896&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwFw0mLZbtSiumwE8BQfqmGmElkNFqdeVFsFdeZIO6jG12pHvnfompGPb2J6XZmCgrpbYN76Q4b7VXmK3Jk8csJ_23JdvI75QjGYjM5XRcKA0-7JyGQFF2zUsBaztihQYAJ7gJJCkRrZVi2lXey8KfoTakpqbk9UeC3YteI8uzP5wSZDCInnmRHv10cg/s16000/image3.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh30uP1TzIx05mFEbwmhDa0LVzvLWGI5Y0PD4mUjLd5o4XbjryGniowokFDuFBBgqe2vYPsokkEajRrq_0PNbQt21nKkoitX0YwqWp2gzFR4VbDSK-A1jWhoObZmw9CbZY6WY8ymj4P5f78kcewYIPgSE4IVOBFSmzOJ-glnLsMzC9ZzhPOW7RECIsT_A/s1174/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;756&quot; data-original-width=&quot;1174&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh30uP1TzIx05mFEbwmhDa0LVzvLWGI5Y0PD4mUjLd5o4XbjryGniowokFDuFBBgqe2vYPsokkEajRrq_0PNbQt21nKkoitX0YwqWp2gzFR4VbDSK-A1jWhoObZmw9CbZY6WY8ymj4P5f78kcewYIPgSE4IVOBFSmzOJ-glnLsMzC9ZzhPOW7RECIsT_A/s16000/image5.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Large motion interpolation. &lt;b&gt;Top&lt;/b&gt;: 64x slow motion video. &lt;b&gt;Bottom&lt;/b&gt; (left to right): The two input images blended, SoftSplat interpolation, ABME interpolation, and FILM interpolation. FILM captures the dog’s face while maintaining the background details.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We introduce FILM, a large motion frame interpolation neural network. At its core, FILM adopts a scale-agnostic feature pyramid that shares weights across scales, which allows us to build a “scale-agnostic” bi-directional motion estimator that learns from frames with normal motion and generalizes well to frames with large motion. To handle wide disocclusions caused by large scene motion, we supervise FILM by matching the Gram matrix of ImageNet pre-trained VGG-19 features, which results in realistic inpainting and crisp images. FILM performs favorably on large motion, while also handling small and medium motions well, and generates temporally smooth high quality videos. &lt;/p&gt; &lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Try It Out Yourself&lt;/h2&gt;&lt;p&gt;You can try out FILM on your photos using the &lt;a href=&quot;https://film-net.github.io&quot;&gt;source code&lt;/a&gt;, which is now publicly available. &lt;/p&gt; &lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;i&gt;We would like to thank Eric Tabellion, Deqing Sun, Caroline Pantofaru, Brian Curless for their contributions. We thank Marc Comino Trinidad for his contributions on the scale-agnostic feature extractor, Orly Liba and Charles Herrmann for feedback on the text, Jamie Aspinall for the imagery in the paper, Dominik Kaeser, Yael Pritch, Michael Nechyba, William T. Freeman, David Salesin, Catherine Wah, and Ira Kemelmacher-Shlizerman for support. Thanks to Tom Small for creating the animated diagram in this post.&lt;/i&gt;  &lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/4364275298594755861/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/large-motion-frame-interpolation.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4364275298594755861" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4364275298594755861" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/10/large-motion-frame-interpolation.html" rel="alternate" title="Large Motion Frame Interpolation" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRIrNGJ_ULWX3iM63cdI0H6CJx9200rUy_yxYJbT15kNRlAkgiZ4o_8NfWyfZ2GnTusotAQXxMjVkhh0zvOW92meJkwRniD2Fh6DyvZukZDOo_ZujCcsos37LOeE9rpZlyh2VNFYVMa5WapuSpCxdTJ9UWVBLZvRdlup7ACXPlq_zhR6tyE8KoUpsg0g/s72-c/image3.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-5097682856425549750</id><published>2022-09-27T10:32:00.002-07:00</published><updated>2022-10-22T19:53:27.922-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="resource optimization"/><title type="text">Quantization for Fast and Environmentally Sustainable Reinforcement Learning</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Srivatsan Krishnan, Student Researcher, and Aleksandra Faust, Senior Staff Research Scientist, Google Research, Brain Team &lt;/span&gt;  &lt;p&gt;Deep &lt;a href=&quot;https://en.wikipedia.org/wiki/Q-learning&quot;&gt;reinforcement learning&lt;/a&gt; (RL) continues to make great strides in solving real-world sequential decision-making problems such as &lt;a href=&quot;https://ai.googleblog.com/2022/02/the-balloon-learning-environment.html&quot;&gt;balloon navigation&lt;/a&gt;, &lt;a href=&quot;https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control&quot;&gt;nuclear physics&lt;/a&gt;, &lt;a href=&quot;https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html&quot;&gt;robotics&lt;/a&gt;, and &lt;a href=&quot;https://ai.googleblog.com/2019/06/introducing-google-research-football.html&quot;&gt;games&lt;/a&gt;. Despite its promise, one of its limiting factors is long training times. While the current approach to &lt;a href=&quot;https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html&quot;&gt;speed up RL training&lt;/a&gt; on complex and difficult tasks leverages &lt;a href=&quot;https://github.com/deepmind/acme&quot;&gt;distributed training&lt;/a&gt; scaling up to hundreds or even thousands of computing nodes, it still requires the use of significant hardware resources which makes RL training expensive, while increasing its environmental impact. However, recent work [&lt;a href=&quot;https://blog.google/technology/ai/minimizing-carbon-footprint/&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://proceedings.mlsys.org/paper/2022/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf&quot;&gt;2&lt;/a&gt;] indicates that performance optimizations on existing hardware can reduce the &lt;a href=&quot;https://en.wikipedia.org/wiki/Carbon_footprint&quot;&gt;carbon footprint&lt;/a&gt; (i.e., total &lt;a href=&quot;https://en.wikipedia.org/wiki/Greenhouse_gas_emissions&quot;&gt;greenhouse gas&lt;/a&gt; emissions) of training and inference.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;  &lt;p&gt;RL can also benefit from similar system optimization techniques that can reduce training time, improve hardware utilization and reduce carbon dioxide (CO&lt;sub&gt;2&lt;/sub&gt;) emissions. One such technique is quantization, a process that converts full-precision floating point (&lt;a href=&quot;https://en.wikipedia.org/wiki/Single-precision_floating-point_format&quot;&gt;FP32&lt;/a&gt;) numbers to lower precision (&lt;a href=&quot;https://www.gnu.org/software/libc/manual/html_node/Integers.html&quot;&gt;int8&lt;/a&gt;) numbers and then performs computation using the lower precision numbers. Quantization can save memory storage cost and bandwidth for faster and more energy-efficient computation. Quantization has been successfully applied to supervised learning to &lt;a href=&quot;https://blog.tensorflow.org/2021/06/how-tensorflow-helps-edge-impulse-make-ml-accessible.html&quot;&gt;enable edge deployments&lt;/a&gt; of machine learning (ML) models and achieve &lt;a href=&quot;https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/&quot;&gt;faster training&lt;/a&gt;. However, there remains an opportunity to apply quantization to RL training.  &lt;/p&gt;  &lt;p&gt;To that end, we present “&lt;a href=&quot;https://openreview.net/pdf?id=xwWsiFmUEs&quot;&gt;QuaRL: Quantization for Fast and Environmentally Sustainable&lt;/a&gt;&lt;a href=&quot;https://openreview.net/pdf?id=xwWsiFmUEs&quot;&gt;Reinforcement Learning&lt;/a&gt;”, published in the &lt;i&gt;&lt;a href=&quot;https://jmlr.org/tmlr/&quot;&gt;Transactions of Machine Learning Research&lt;/a&gt;&lt;/i&gt; journal, which introduces&lt;i&gt; &lt;/i&gt;a new paradigm called &lt;i&gt;ActorQ &lt;/i&gt;that applies quantization to speed up RL training by 1.5-5.4x while maintaining performance. Additionally, we demonstrate that compared to training in full-precision, the carbon footprint is also significantly reduced by a factor of 1.9-3.8x. &lt;/p&gt;  &lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Applying Quantization to RL Training&lt;/h2&gt;&lt;p&gt;In traditional RL training, a &lt;i&gt;learner &lt;/i&gt;policy is applied to an &lt;i&gt;actor&lt;/i&gt;, which uses the policy to explore the environment and collect data samples. The samples collected by the &lt;i&gt;actor&lt;/i&gt; are then used by the &lt;i&gt;learner&lt;/i&gt; to continuously refine the initial policy. Periodically, the policy trained on the learner side is used to update the &lt;i&gt;actor's&lt;/i&gt; policy. To apply quantization to RL training, we develop the ActorQ paradigm. ActorQ performs the same sequence described above, with one key difference being that the policy update from learner to actors is quantized, and the actor explores the environment using the int8 quantized policy to collect samples. &lt;/p&gt; &lt;p&gt;Applying quantization to RL training in this fashion has two key benefits. First, it reduces the &lt;a href=&quot;https://en.wikipedia.org/wiki/Memory_footprint&quot;&gt;memory footprint&lt;/a&gt; of the policy. For the same peak bandwidth, less data is transferred between learners and actors, which reduces the communication cost for policy updates from learners to actors. Second, the actors perform inference on the quantized policy to generate actions for a given environment state. The quantized inference process is much faster when compared to performing inference in full precision.  &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF_4o81i0gPf7bU-_E8Fqhy_Ih-AdGI6s_CNhcfVv6wirCsGJlMdwX7DQamF3VYTUbNNJn1E2omMJ3tziscCnTxrxMslApGiPfrg67-3H7WpKCcxXJg5ihqaSrKlOLClmgTIxy8yDKMIFZIQzZL1tNamHlGGvtXZW6zWDAHHlN8kRCZAjYGcuFuDaOYw/s1600/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;322&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF_4o81i0gPf7bU-_E8Fqhy_Ih-AdGI6s_CNhcfVv6wirCsGJlMdwX7DQamF3VYTUbNNJn1E2omMJ3tziscCnTxrxMslApGiPfrg67-3H7WpKCcxXJg5ihqaSrKlOLClmgTIxy8yDKMIFZIQzZL1tNamHlGGvtXZW6zWDAHHlN8kRCZAjYGcuFuDaOYw/s16000/image4.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;An overview of traditional RL training (&lt;b&gt;left&lt;/b&gt;) and ActorQ RL training (&lt;b&gt;right&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;In ActorQ, we use the &lt;a href=&quot;https://github.com/deepmind/acme&quot;&gt;ACME&lt;/a&gt; distributed RL framework. The quantizer block performs uniform quantization that converts the FP32 policy to int8. The actor performs inference using optimized int8 computations. Though we use uniform quantization when designing the quantizer block, we believe that other &lt;a href=&quot;https://arxiv.org/abs/1806.08342&quot;&gt;quantization techniques&lt;/a&gt; can replace uniform quantization and produce similar results. The samples collected by the actors are used by the learner to train a neural network policy. Periodically the learned policy is quantized by the quantizer block and broadcasted to the actors. &lt;/p&gt; &lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Quantization Improves RL Training Time and Performance&lt;/h2&gt;&lt;p&gt;We evaluate ActorQ in a range of environments, including the &lt;a href=&quot;https://www.deepmind.com/open-source/deepmind-control-suite&quot;&gt;Deepmind Control Suite&lt;/a&gt; and the &lt;a href=&quot;https://openai.com/blog/openai-gym-beta/&quot;&gt;OpenAI Gym&lt;/a&gt;. We demonstrate the speed-up and improved performance of &lt;a href=&quot;https://arxiv.org/abs/1804.08617&quot;&gt;D4PG&lt;/a&gt; and &lt;a href=&quot;https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf&quot;&gt;DQN&lt;/a&gt;. We chose D4PG as it was the best learning algorithm in &lt;a href=&quot;https://arxiv.org/abs/2006.00979&quot;&gt;ACME&lt;/a&gt; for Deepmind Control Suite tasks, and DQN is a widely used and standard RL algorithm.  &lt;/p&gt; &lt;p&gt;We observe a significant speedup (between 1.5x and 5.41x) in training RL policies. More importantly, performance is maintained even when actors perform int8 quantized inference. The figures below demonstrate this for the D4PG and DQN agents for Deepmind Control Suite and OpenAI Gym tasks. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_9JtTGkLC_0r_leaxnELxfZ8ZxbKDwaMi-wunwlbllT4SihT7gyDrGnk2rXC3eO5N9UshGSAUqyYge5DfU_at-hxyN8h1nnaNhMu4BArs_KxXarLdHmskQ5QqZHxjWVvxoTKJXzFQJIORP78gdXcAXgI_zod1Ef9RK683LQcWTNVn6DGj3rI-2ty-0g/s957/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;957&quot; data-original-width=&quot;566&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_9JtTGkLC_0r_leaxnELxfZ8ZxbKDwaMi-wunwlbllT4SihT7gyDrGnk2rXC3eO5N9UshGSAUqyYge5DfU_at-hxyN8h1nnaNhMu4BArs_KxXarLdHmskQ5QqZHxjWVvxoTKJXzFQJIORP78gdXcAXgI_zod1Ef9RK683LQcWTNVn6DGj3rI-2ty-0g/s16000/image2.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A comparison of RL training using the FP32 policy (q=32) and the quantized int8 policy (q=8) for D4PG agents on various Deepmind Control Suite tasks. Quantization achieves speed-ups of 1.5x to 3.06x.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyF643UhZYXbEb90Ue5asz0D5SL_OKcn_qY-fYvTsmUh7k5Yf5Jfua_LSMmEEHu_nTRU1TITAlH7Dwxmgmz9HnSmBlItTd6o24aFXv9V-i2m9940IdCpPl2uAcBcAme4GuBJ5O1AD88LK4np_KRwWCmraqPnxxT7ryhu3qsPnMgK8QkomODbUTnGA7CA/s1244/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;1244&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyF643UhZYXbEb90Ue5asz0D5SL_OKcn_qY-fYvTsmUh7k5Yf5Jfua_LSMmEEHu_nTRU1TITAlH7Dwxmgmz9HnSmBlItTd6o24aFXv9V-i2m9940IdCpPl2uAcBcAme4GuBJ5O1AD88LK4np_KRwWCmraqPnxxT7ryhu3qsPnMgK8QkomODbUTnGA7CA/s16000/image5.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A comparison of RL training using the FP32 policy (q=32) and the quantized int8 policy (q=8) for DQN agents in the OpenAI Gym environment. Quantization achieves a speed-up of 2.2x to 5.41x.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Quantization Reduces Carbon Emission&lt;/h2&gt;&lt;p&gt;Applying quantization in RL using ActorQ improves training time without affecting performance. The direct consequence of using the hardware more efficiently is a smaller carbon footprint. We measure the carbon footprint improvement by taking the ratio of carbon emission when using the FP32 policy during training over the carbon emission when using the int8 policy during training. &lt;/p&gt; &lt;p&gt;In order to measure the carbon emission for the RL training experiment, we use the &lt;a href=&quot;https://github.com/Breakend/experiment-impact-tracker&quot;&gt;experiment-impact-tracker&lt;/a&gt; proposed in &lt;a href=&quot;https://jmlr.org/papers/volume21/20-312/20-312.pdf&quot;&gt;prior work&lt;/a&gt;. We instrument the ActorQ system with carbon monitor APIs to measure the energy and carbon emissions for each training experiment. &lt;/p&gt; &lt;p&gt;Compared to the carbon emission when running in full precision (FP32), we observe that the quantization of policies reduces the carbon emissions anywhere from 1.9x to 3.76x, depending on the task. As RL systems are scaled to run on thousands of distributed hardware cores and accelerators, we believe that the absolute carbon reduction (measured in kilograms of CO&lt;sub&gt;2&lt;/sub&gt;) can be quite significant. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGC7inzPO6k_D-ZacfPL32YNfnSXt4AheMAFeRuGJisa8uTKac1kwOhepphdeXqx7KnrMFqV1ldHkxvmg2hzEBIGbhgdceEaNnxwWUwCXtHZ61TfjwQi0Ujdwqi-GcRNxZCYLFUXE2_oNr0ZdHG1mMyHv1xB0l5Nj0y31ebzv5QhNvYadQlGg1eZnetw/s1999/image8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;939&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGC7inzPO6k_D-ZacfPL32YNfnSXt4AheMAFeRuGJisa8uTKac1kwOhepphdeXqx7KnrMFqV1ldHkxvmg2hzEBIGbhgdceEaNnxwWUwCXtHZ61TfjwQi0Ujdwqi-GcRNxZCYLFUXE2_oNr0ZdHG1mMyHv1xB0l5Nj0y31ebzv5QhNvYadQlGg1eZnetw/s16000/image8.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Carbon emission comparison between training using a FP32 policy and an int8 policy. The X-axis scale is normalized to the carbon emissions of the FP32 policy. Shown by the red bars greater than 1, ActorQ reduces carbon emissions.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Conclusion and Future Directions&lt;/h2&gt;&lt;p&gt;We introduce ActorQ, a novel paradigm that applies quantization to RL training and achieves speed-up improvements of 1.5-5.4x while maintaining performance. Additionally, we demonstrate that ActorQ can reduce RL training’s carbon footprint by a factor of 1.9-3.8x compared to training in full-precision without quantization. &lt;/p&gt; &lt;p&gt;ActorQ demonstrates that quantization can be effectively applied to many aspects of RL, from obtaining high-quality and efficient quantized policies to reducing training times and carbon emissions. As RL continues to make great strides in solving real-world problems, we believe that making RL training sustainable will be critical for adoption. As we scale RL training to thousands of cores and GPUs, even a 50% improvement (as we have experimentally demonstrated) will generate significant savings in absolute dollar cost, energy, and carbon emissions. Our work is the first step toward applying quantization to RL training to achieve efficient and environmentally sustainable training.  &lt;/p&gt; &lt;p&gt;While our design of the quantizer in ActorQ relied on simple uniform quantization, we believe that other forms of quantization, compression and sparsity can be applied (e.g., distillation, sparsification, etc.). We hope that future work will consider applying more aggressive quantization and compression methods, which may yield additional benefits to the performance and accuracy tradeoff obtained by the trained RL policies. &lt;/p&gt; &lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgments&lt;/h2&gt;&lt;p&gt;&lt;i&gt;We would like to thank our co-authors Max Lam, Sharad Chitlangia, Zishen Wan, and Vijay Janapa Reddi (Harvard University), and Gabriel Barth-Maron (DeepMind), for their contribution to this work. We also thank the Google Cloud team for providing research credits to seed this work.&lt;/i&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/5097682856425549750/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/quantization-for-fast-and.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5097682856425549750" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5097682856425549750" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/quantization-for-fast-and.html" rel="alternate" title="Quantization for Fast and Environmentally Sustainable Reinforcement Learning" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF_4o81i0gPf7bU-_E8Fqhy_Ih-AdGI6s_CNhcfVv6wirCsGJlMdwX7DQamF3VYTUbNNJn1E2omMJ3tziscCnTxrxMslApGiPfrg67-3H7WpKCcxXJg5ihqaSrKlOLClmgTIxy8yDKMIFZIQzZL1tNamHlGGvtXZW6zWDAHHlN8kRCZAjYGcuFuDaOYw/s72-c/image4.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-4863623362813999190</id><published>2022-09-22T14:33:00.009-07:00</published><updated>2022-10-22T20:01:27.187-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Cloud Computing"/><category scheme="http://www.blogger.com/atom/ns#" term="datasets"/><category scheme="http://www.blogger.com/atom/ns#" term="open source"/><title type="text">TensorStore for High-Performance, Scalable Array Storage</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Jeremy Maitin-Shepard and Laramie Leavitt, Software Engineers, Connectomics at Google&lt;/span&gt;&lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX9DAdPcFpMfSPteAKTplWpi2h3okSemN9o1J3B6K45LHBjkp2OiSbNimITgluVBLLyyVwXIhe0tvhwDIG01lWXXPCW-Lh0HqOg6qRBE96ON4ndUBmb4kdDmQ0cMsk2dZcsHkTNU2quF7UMttmX8-GbfMLGmXfKePb1sGIa0qSODsf6oV4dXPZhoo-/s800/image1.gif&quot; style=&quot;display: none;&quot; /&gt;&lt;p&gt;Many exciting contemporary applications of computer science and machine learning (ML) manipulate multidimensional datasets that span a single large coordinate system, for example, &lt;a href=&quot;https://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;&gt;weather modeling&lt;/a&gt; from atmospheric measurements over a spatial grid or &lt;a href=&quot;https://ai.googleblog.com/2021/09/detecting-abnormal-chest-x-rays-using.html&quot;&gt;medical imaging&lt;/a&gt; predictions from multi-channel image intensity values in a 2d or 3d scan. In these settings, even a single dataset may require terabytes or petabytes of data storage. Such datasets are also challenging to work with as users may read and write data at irregular intervals and varying scales, and are often interested in performing analyses using numerous machines working in parallel.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;Today we are introducing &lt;a href=&quot;https://github.com/google/tensorstore&quot;&gt;TensorStore&lt;/a&gt;, an open-source C++ and Python software library designed for storage and manipulation of &lt;em&gt;n-&lt;/em&gt;dimensional data that: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;Provides a uniform API for reading and writing multiple array formats, including &lt;a href=&quot;https://zarr.readthedocs.io/en/stable/&quot;&gt;zarr&lt;/a&gt; and &lt;a href=&quot;https://github.com/saalfeldlab/n5&quot;&gt;N5&lt;/a&gt;.  &lt;/li&gt;  &lt;li&gt;Natively supports &lt;a href=&quot;https://google.github.io/tensorstore/kvstore/index.html&quot;&gt;multiple storage systems&lt;/a&gt;, including &lt;a href=&quot;https://google.github.io/tensorstore/kvstore/gcs/index.html&quot;&gt;Google Cloud Storage&lt;/a&gt;, &lt;a href=&quot;https://google.github.io/tensorstore/kvstore/file/index.html&quot;&gt;local and network filesystems&lt;/a&gt;, &lt;a href=&quot;https://google.github.io/tensorstore/kvstore/http/index.html&quot;&gt;HTTP servers&lt;/a&gt;, and &lt;a href=&quot;https://google.github.io/tensorstore/kvstore/memory/index.html&quot;&gt;in-memory storage&lt;/a&gt;. &lt;/li&gt;  &lt;li&gt;Supports read/writeback caching and transactions, with strong &lt;a href=&quot;https://en.wikipedia.org/wiki/ACID&quot;&gt;atomicity, isolation, consistency, and durability&lt;/a&gt; (ACID) guarantees. &lt;/li&gt;  &lt;li&gt;Supports safe, efficient access from multiple processes and machines via optimistic concurrency. &lt;/li&gt;  &lt;li&gt;Offers an asynchronous API to enable high-throughput access even to high-latency remote storage. &lt;/li&gt;  &lt;li&gt;Provides advanced, fully composable &lt;a href=&quot;https://google.github.io/tensorstore/python/indexing.html&quot;&gt;indexing&lt;/a&gt; operations and virtual views. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;TensorStore has already been used to solve key engineering challenges in scientific computing (e.g., management and processing of large datasets in neuroscience, such as &lt;a href=&quot;https://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html&quot;&gt;peta-scale 3d electron microscopy data&lt;/a&gt; and “4d” &lt;a href=&quot;https://www.youtube.com/watch?v=Nxa19uWC_oA&quot;&gt;videos of neuronal activity&lt;/a&gt;). TensorStore has also been used in the creation of large-scale machine learning models such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;&gt;PaLM&lt;/a&gt; by addressing the problem of managing model parameters (checkpoints) during distributed training.  &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Familiar API for Data Access and Manipulation&lt;/h2&gt;&lt;p&gt;TensorStore provides a simple Python API for loading and manipulating large array data. In the following example, we create a TensorStore object that represents a 56 trillion voxel &lt;a href=&quot;https://ai.googleblog.com/2020/01/releasing-drosophila-hemibrain.html&quot;&gt;3d image of a fly brain&lt;/a&gt; and access a small 100x100 patch of the data as a &lt;a href=&quot;https://numpy.org/&quot;&gt;NumPy&lt;/a&gt; array: &lt;/p&gt;&lt;span style=&quot;font-size: small;&quot;&gt;  &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorstore as ts&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np&lt;br /&gt;&lt;br /&gt;# Create a TensorStore object to work with fly brain data.&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; dataset = ts.open({&lt;br /&gt;...     'driver':&lt;br /&gt;...         'neuroglancer_precomputed',&lt;br /&gt;...     'kvstore':&lt;br /&gt;...         'gs://neuroglancer-janelia-flyem-hemibrain/' + &lt;br /&gt;...         'v1.1/segmentation/',&lt;br /&gt;... }).result()&lt;br /&gt;&lt;br /&gt;# Create a 3-d view (remove singleton 'channel' dimension):&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; dataset_3d = dataset[ts.d['channel'][0]]&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; dataset_3d.domain&lt;br /&gt;{ &quot;x&quot;: [0, 34432), &quot;y&quot;: [0, 39552), &quot;z&quot;: [0, 41408) }&lt;br /&gt;&lt;br /&gt;# Convert a 100x100x1 slice of the data to a numpy ndarray&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; slice = np.array(dataset_3d[15000:15100, 15000:15100, 20000])&lt;/pre&gt;&lt;/span&gt; &lt;p&gt;Crucially, no actual data is accessed or stored in memory until the specific 100x100 slice is requested; hence arbitrarily large underlying datasets can be loaded and manipulated without having to store the entire dataset in memory, using indexing and manipulation syntax largely identical to standard NumPy operations. TensorStore also provides extensive support for &lt;a href=&quot;https://google.github.io/tensorstore/python/indexing.html&quot;&gt;advanced indexing features&lt;/a&gt;, including transforms, &lt;a href=&quot;https://google.github.io/tensorstore/index_space.html#alignment-and-broadcasting&quot;&gt;alignment, broadcasting&lt;/a&gt;, and virtual views (&lt;a href=&quot;https://google.github.io/tensorstore/python/api/tensorstore.cast.html#tensorstore.cast&quot;&gt;data type conversion&lt;/a&gt;, &lt;a href=&quot;https://google.github.io/tensorstore/python/api/tensorstore.downsample-store.html&quot;&gt;downsampling&lt;/a&gt;, &lt;a href=&quot;https://google.github.io/tensorstore/python/api/tensorstore.virtual_chunked.html&quot;&gt;lazily on-the-fly generated arrays&lt;/a&gt;).  &lt;/p&gt;&lt;p&gt;The following example demonstrates how TensorStore can be used to create a zarr array, and how its asynchronous API enables higher throughput: &lt;/p&gt;&lt;span style=&quot;font-size: small;&quot;&gt;&lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorstore as ts&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np&lt;br /&gt;&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; # Create a zarr array on the local filesystem&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; dataset = ts.open({&lt;br /&gt;...     'driver': 'zarr',&lt;br /&gt;...     'kvstore': 'file:///tmp/my_dataset/',&lt;br /&gt;... },&lt;br /&gt;... dtype=ts.uint32,&lt;br /&gt;... chunk_layout=ts.ChunkLayout(chunk_shape=[256, 256, 1]),&lt;br /&gt;... create=True,&lt;br /&gt;... shape=[5000, 6000, 7000]).result()&lt;br /&gt;&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; # Create two numpy arrays with example data to write.&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; a = np.arange(100*200*300, dtype=np.uint32).reshape((100, 200, 300))&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; b = np.arange(200*300*400, dtype=np.uint32).reshape((200, 300, 400))&lt;br /&gt;&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; # Initiate two asynchronous writes, to be performed concurrently.&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; future_a = dataset[1000:1100, 2000:2200, 3000:3300].write(a)&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; future_b = dataset[3000:3200, 4000:4300, 5000:5400].write(b)&lt;br /&gt;&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; # Wait for the asynchronous writes to complete&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; future_a.result()&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; future_b.result()&lt;/pre&gt;&lt;/span&gt; &lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Safe and Performant Scaling &lt;/h2&gt;&lt;p&gt;Processing and analyzing large numerical datasets requires significant computational resources. This is typically achieved through parallelization across numerous CPU or accelerator cores spread across many machines. Therefore a fundamental goal of TensorStore has been to enable parallel processing of individual datasets that is both safe (i.e., avoids corruption or inconsistencies arising from parallel access patterns) and high performance (i.e., reading and writing to TensorStore is not a bottleneck during computation). In fact, in a test within Google’s datacenters, we found nearly linear scaling of read and write performance as the number of CPUs was increased: &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8b07lDLRkn-9O0urUMRv4nU96Yd-m3chrVmIAgw5tlceIU3dRKtpvK0IZAUS-hiqxtT-U1oe-AZBLdri1V-PJmTMw8EoVMlSO0BqzQRmmJN0oadFab5OyP4CieM1m9S1nNFOCWGNZ7obq5Jx9N5AXhf6zjChzLFnM7YECO3mJ6YEqQpc0o8JagSVX/s1999/image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1066&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8b07lDLRkn-9O0urUMRv4nU96Yd-m3chrVmIAgw5tlceIU3dRKtpvK0IZAUS-hiqxtT-U1oe-AZBLdri1V-PJmTMw8EoVMlSO0BqzQRmmJN0oadFab5OyP4CieM1m9S1nNFOCWGNZ7obq5Jx9N5AXhf6zjChzLFnM7YECO3mJ6YEqQpc0o8JagSVX/s16000/image2.jpg&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Read and write performance for a TensorStore dataset in &lt;a href=&quot;https://google.github.io/tensorstore/driver/zarr/index.html&quot;&gt;zarr format&lt;/a&gt; residing on Google Cloud Storage (GCS) accessed concurrently using a variable number of single-core compute tasks in Google data centers. Both read and write performance scales nearly linearly with the number of compute tasks.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Performance is achieved by implementing core operations in C++, extensive use of multithreading for operations such as encoding/decoding and network I/O, and partitioning large datasets into much smaller units through chunking to enable efficiently reading and writing subsets of the entire dataset. TensorStore also provides configurable in-memory caching (which reduces slower storage system interactions for frequently accessed data) and an asynchronous API that enables a read or write operation to continue in the background while a program completes other work.  &lt;/p&gt;&lt;p&gt;Safety of parallel operations when many machines are accessing the same dataset is achieved through the use of &lt;a href=&quot;https://en.wikipedia.org/wiki/Optimistic_concurrency_control&quot;&gt;optimistic concurrency&lt;/a&gt;, which maintains compatibility with diverse underlying storage layers (including Cloud storage platforms, such as &lt;a href=&quot;https://cloud.google.com/storage/?gclid=CjwKCAjwlqOXBhBqEiwA-hhitER-4ub5UNrYCHC4rdBFwwNEl7PDYPvu9maM9G1d1wM1eY5QrzJ4NhoCme4QAvD_BwE&amp;amp;gclsrc=aw.ds&quot;&gt;GCS&lt;/a&gt;, as well as local filesystems) without significantly impacting performance. TensorStore also provides strong ACID guarantees for all individual operations executing within a single runtime.  &lt;/p&gt;&lt;p&gt;To make distributed computing with TensorStore compatible with many existing data processing workflows, we have also integrated TensorStore with parallel computing libraries such as &lt;a href=&quot;https://beam.apache.org/&quot;&gt;Apache Beam&lt;/a&gt; (&lt;a href=&quot;https://github.com/google/tensorstore/tree/master/tensorstore/examples/python/beam&quot;&gt;example code&lt;/a&gt;) and &lt;a href=&quot;https://www.dask.org/&quot;&gt;Dask&lt;/a&gt; (&lt;a href=&quot;https://github.com/google-research/connectomics&quot;&gt;example code&lt;/a&gt;).  &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Use Case: Language Models &lt;/h2&gt;&lt;p&gt;An exciting recent development in ML is the emergence of more advanced language models such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;&gt;PaLM&lt;/a&gt;. These neural networks contain hundreds of billions of parameters and exhibit &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;&gt;some surprising capabilities&lt;/a&gt; in natural language understanding and generation. These models also push the limits of computational infrastructure; in particular, training a language model such as PaLM requires thousands of TPUs working in parallel.  &lt;/p&gt;&lt;p&gt;One challenge that arises during this training process is efficiently reading and writing the model parameters. Training is distributed across many separate machines, but parameters must be regularly saved to a single object (“checkpoint”) on a permanent storage system without slowing down the overall training process. Individual training jobs must also be able to read just the specific set of parameters they are concerned with in order to avoid the overhead that would be required to load the entire set of model parameters (which could be hundreds of gigabytes).  &lt;/p&gt;&lt;p&gt;TensorStore has already been used to address these challenges. It has been applied to manage checkpoints associated with large-scale (“&lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/googles-scalable-supercomputers-for-machine-learning-cloud-tpu-pods-are-now-publicly-available-in-beta&quot;&gt;multipod&lt;/a&gt;”) models trained with &lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt;&amp;nbsp;(&lt;a href=&quot;https://github.com/google/jax/blob/640e15fe070a887197143b76a19a3dced816c8df/jax/experimental/gda_serialization/serialization.py&quot;&gt;code example&lt;/a&gt;) and has been integrated with frameworks such as &lt;a href=&quot;https://arxiv.org/pdf/2203.17189.pdf&quot;&gt;T5X&lt;/a&gt;&amp;nbsp;(&lt;a href=&quot;https://github.com/google-research/t5x/blob/d34fff62ddc71c86b680300f8dca198f9db7b246/t5x/checkpoints.py&quot;&gt;code example&lt;/a&gt;) and &lt;a href=&quot;https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/&quot;&gt;Pathways&lt;/a&gt;. Model parallelism is used to partition the full set of parameters, which can occupy more than a terabyte of memory, over hundreds of TPUs. Checkpoints are stored in zarr format using TensorStore, with a chunk structure chosen to allow the partition for each TPU to be read and written independently in parallel. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX9DAdPcFpMfSPteAKTplWpi2h3okSemN9o1J3B6K45LHBjkp2OiSbNimITgluVBLLyyVwXIhe0tvhwDIG01lWXXPCW-Lh0HqOg6qRBE96ON4ndUBmb4kdDmQ0cMsk2dZcsHkTNU2quF7UMttmX8-GbfMLGmXfKePb1sGIa0qSODsf6oV4dXPZhoo-/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;746&quot; data-original-width=&quot;800&quot; height=&quot;373&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX9DAdPcFpMfSPteAKTplWpi2h3okSemN9o1J3B6K45LHBjkp2OiSbNimITgluVBLLyyVwXIhe0tvhwDIG01lWXXPCW-Lh0HqOg6qRBE96ON4ndUBmb4kdDmQ0cMsk2dZcsHkTNU2quF7UMttmX8-GbfMLGmXfKePb1sGIa0qSODsf6oV4dXPZhoo-/w400-h373/image1.gif&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;When saving a checkpoint, each model parameter is written using TensorStore in zarr format using a chunk grid that further subdivides the grid used to partition the parameter over TPUs. The host machines write in parallel the zarr chunks for each of the partitions assigned to TPUs attached to that host. Using TensorStore's asynchronous API, training proceeds even while the data is still being written to persistent storage. When resuming from a checkpoint, each host reads only the chunks that make up the partitions assigned to that host.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Use Case: 3D Brain Mapping&lt;/h2&gt;&lt;p&gt;The field of synapse-resolution &lt;a href=&quot;https://en.wikipedia.org/wiki/Connectomics&quot;&gt;connectomics&lt;/a&gt; aims to map the wiring of &lt;a href=&quot;https://ai.googleblog.com/2020/01/releasing-drosophila-hemibrain.html&quot;&gt;animal&lt;/a&gt; and &lt;a href=&quot;http://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html&quot;&gt;human&lt;/a&gt; brains at the detailed level of individual synaptic connections. This requires imaging the brain at extremely high resolution (nanometers) over fields of view of up to millimeters or more, which yields datasets that can span petabytes in size. In the future these datasets may extend to exabytes as scientists contemplate mapping &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0092867420310011&quot;&gt;entire mouse&lt;/a&gt; or primate brains. However, even current datasets pose significant challenges related to storage, manipulation, and processing; in particular, even a single brain sample may require millions of gigabytes with a coordinate system (pixel space) of hundreds of thousands pixels in each dimension.  &lt;/p&gt;&lt;p&gt;We have used TensorStore to solve computational challenges associated with large-scale connectomic datasets. Specifically, TensorStore has managed some of the largest and most widely accessed connectomic datasets, with Google Cloud Storage as the underlying object storage system. For example, it has been applied to the &lt;a href=&quot;https://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html&quot;&gt;human cortex “h01” dataset&lt;/a&gt;, which is a 3d nanometer-resolution image of human brain tissue. The raw imaging data is 1.4 petabytes (roughly 500,000 * 350,000 * 5,000 pixels large, and is further associated with additional content such as 3d segmentations and annotations that reside in the same coordinate system. The raw data is subdivided into individual chunks 128x128x16 pixels large and stored in the “&lt;a href=&quot;https://google.github.io/tensorstore/driver/neuroglancer_precomputed/index.html&quot;&gt;Neuroglancer precomputed&lt;/a&gt;” format, which is optimized for &lt;a href=&quot;https://github.com/google/neuroglancer&quot;&gt;web-based interactive viewing&lt;/a&gt; and can be easily &lt;a href=&quot;https://colab.research.google.com/gist/jbms/1ec1192c34ec816c2c517a3b51a8ed6c/h01_data_access.ipynb&quot;&gt;manipulated from TensorStore&lt;/a&gt;.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhM2p80P4faZAfq1_1m0GtjZP1-pFR1idRhN_z8XKijvrnvCYmmY_8bpF3qzloedAy444kxa8VqgAGSHLaWehjgLmLUn_-ZEZlAz3W_0rvIqk86DJhdoxg8Hp3eHcVqhYfrvpcpT7glU88g0GvbTqA5ho2okVK2emPi3OQuhvd65Vg14KKdq6O7AdHz/s640/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;360&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhM2p80P4faZAfq1_1m0GtjZP1-pFR1idRhN_z8XKijvrnvCYmmY_8bpF3qzloedAy444kxa8VqgAGSHLaWehjgLmLUn_-ZEZlAz3W_0rvIqk86DJhdoxg8Hp3eHcVqhYfrvpcpT7glU88g0GvbTqA5ho2okVK2emPi3OQuhvd65Vg14KKdq6O7AdHz/s16000/image3.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;A fly brain &lt;a href=&quot;https://ai.googleblog.com/2020/01/releasing-drosophila-hemibrain.html&quot;&gt;reconstruction&lt;/a&gt; for which the underlying data can be easily &lt;a href=&quot;https://colab.research.google.com/gist/jbms/10b7a91c8b2f8ecbf30a869a1c50defb/flyem-hemibrain-data-access.ipynb&quot;&gt;accessed and manipulated using TensorStore&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Getting Started&lt;/h2&gt;&lt;p&gt;To get started using the TensorStore Python API, you can install the &lt;a href=&quot;https://pypi.org/project/tensorstore/&quot;&gt;tensorstore PyPI package&lt;/a&gt; using: &lt;/p&gt;&lt;code style=&quot;margin-left: 40px; margin-right: 40px;&quot;&gt;pip install tensorstore &lt;/code&gt;&lt;p&gt;Refer to the &lt;a href=&quot;https://google.github.io/tensorstore/python/tutorial.html&quot;&gt;tutorials&lt;/a&gt; and &lt;a href=&quot;https://google.github.io/tensorstore/python/api/index.html&quot;&gt;API documentation&lt;/a&gt; for usage details. For other installation options and for using the C++ API, refer to &lt;a href=&quot;https://google.github.io/tensorstore/installation.html&quot;&gt;installation instructions&lt;/a&gt;. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&quot;https://research.google/people/105450/&quot;&gt;Tim Blakely&lt;/a&gt;, &lt;a href=&quot;https://research.google/people/VirenJain/&quot;&gt;Viren Jain&lt;/a&gt;, &lt;a href=&quot;https://github.com/yashk2810&quot;&gt;Yash Katariya&lt;/a&gt;, &lt;a href=&quot;https://www.jan-matthis.de/&quot;&gt;Jan-Matthis Luckmann&lt;/a&gt;, &lt;a href=&quot;https://research.google/people/MichalJanuszewski/&quot;&gt;Michał Januszewski&lt;/a&gt;, &lt;a href=&quot;https://research.google/people/PeterLi/&quot;&gt;Peter Li&lt;/a&gt;, &lt;a href=&quot;https://research.google/people/104881/&quot;&gt;Adam Roberts&lt;/a&gt;, &lt;a href=&quot;https://research.google/people/107327/&quot;&gt;Brain Williams&lt;/a&gt;, and &lt;a href=&quot;https://research.google/people/HectorYee/&quot;&gt;Hector Yee&lt;/a&gt; from Google Research, and &lt;a href=&quot;https://www.janelia.org/people/davis-bennett&quot;&gt;Davis Bennet&lt;/a&gt;, &lt;a href=&quot;https://www.janelia.org/people/stuart-berg&quot;&gt;Stuart Berg&lt;/a&gt;, &lt;a href=&quot;https://www.yikes.com/~eric/&quot;&gt;Eric Perlman&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=Pc1TkM0AAAAJ&amp;amp;hl=en&quot;&gt;Stephen Plaza&lt;/a&gt;, and &lt;a href=&quot;https://research.monash.edu/en/persons/juan-nunez-iglesias&quot;&gt;Juan Nunez-Iglesias&lt;/a&gt; from the broader scientific community for valuable feedback on the design, early testing and debugging.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/4863623362813999190/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4863623362813999190" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4863623362813999190" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html" rel="alternate" title="TensorStore for High-Performance, Scalable Array Storage" type="text/html"><author><name>Andrew Helton</name><uri>http://www.blogger.com/profile/12510940840342590054</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX9DAdPcFpMfSPteAKTplWpi2h3okSemN9o1J3B6K45LHBjkp2OiSbNimITgluVBLLyyVwXIhe0tvhwDIG01lWXXPCW-Lh0HqOg6qRBE96ON4ndUBmb4kdDmQ0cMsk2dZcsHkTNU2quF7UMttmX8-GbfMLGmXfKePb1sGIa0qSODsf6oV4dXPZhoo-/s72-c/image1.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-6099606360064831644</id><published>2022-09-21T12:27:00.003-07:00</published><updated>2022-10-22T20:05:08.679-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="CVPR"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><title type="text">View Synthesis with Transformers</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Carlos Esteves and Ameesh Makadia, Research Scientists, Google Research&lt;/span&gt; &lt;p&gt;A long-standing problem in the intersection of computer vision and computer graphics, &lt;a href=&quot;https://en.wikipedia.org/wiki/View_synthesis&quot;&gt;view synthesis&lt;/a&gt; is the task of creating new views of a scene from multiple pictures of that scene. This has received increased attention [&lt;a href=&quot;https://dellaert.github.io/NeRF/&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://dellaert.github.io/NeRF21/&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://dellaert.github.io/NeRF22/&quot;&gt;3&lt;/a&gt;] since &lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;&gt;the introduction of neural radiance fields&lt;/a&gt; (NeRF). The problem is challenging because to accurately synthesize new views of a scene, a model needs to capture many types of information — its detailed 3D structure, materials, and illumination —  from a small set of reference images.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;In this post, we present recently published deep learning models for view synthesis. In “&lt;a href=&quot;https://light-field-neural-rendering.github.io/&quot;&gt;Light Field Neural Rendering&lt;/a&gt;” (LFNR), presented at &lt;a href=&quot;https://cvpr2022.thecvf.com/&quot;&gt;CVPR 2022&lt;/a&gt;, we address the challenge of accurately reproducing view-dependent effects by using &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;transformers&lt;/a&gt; that learn to combine reference pixel colors. Then in “&lt;a href=&quot;https://mohammedsuhail.net/gen_patch_neural_rendering/&quot;&gt;Generalizable Patch-Based Neural Rendering&lt;/a&gt;” (GPNR), to be presented at &lt;a href=&quot;https://eccv2022.ecva.net/&quot;&gt;ECCV 2022&lt;/a&gt;, we address the challenge of generalizing to unseen scenes by using a sequence of transformers with canonicalized positional encoding that can be trained on a set of scenes to synthesize views of new scenes. These models have some unique features. They perform image-based rendering, combining colors and features from the reference images to render novel views. They are purely transformer-based, operating on sets of image patches, and they leverage a &lt;a href=&quot;https://en.wikipedia.org/wiki/Light_field#The_4D_light_field&quot;&gt;4D light field&lt;/a&gt; representation for positional encoding, which helps to model view-dependent effects. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkJzGICK0DKkQBjC-O0ts_iqk-W3qo90RcUbJl9xOA4NIpzqRHiDx1C8C3KV0ylyBR3cPwgvoW1I9s3y2OW1VdGVArKtWDaIDaxmLFSRoOo66YcMVA9Df7Tj9_zaR2fZplryCBphBKgTtDRegu6POYDiyMy_waqgL3_37KTFeMGjCD7R-U7jT0boYong/s960/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;444&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkJzGICK0DKkQBjC-O0ts_iqk-W3qo90RcUbJl9xOA4NIpzqRHiDx1C8C3KV0ylyBR3cPwgvoW1I9s3y2OW1VdGVArKtWDaIDaxmLFSRoOo66YcMVA9Df7Tj9_zaR2fZplryCBphBKgTtDRegu6POYDiyMy_waqgL3_37KTFeMGjCD7R-U7jT0boYong/s16000/image6.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6wR1MX9K5D3v-kOvSAzBwV4x7Ex3q_nqTIXgrrspTcgez4gGqdeR5MeQYZn2h6FX8WPet_zxok41c4qEtTnFvZzOpufJcO8PZARqeRpBWxoTpTuOmgTPtiefzceefdvu52VxRC9AxUtlxbQEMnxUa_IS4raPaXH8uyB3APcTiiWKH2cEinApucd2Y4A/s504/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;288&quot; data-original-width=&quot;504&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6wR1MX9K5D3v-kOvSAzBwV4x7Ex3q_nqTIXgrrspTcgez4gGqdeR5MeQYZn2h6FX8WPet_zxok41c4qEtTnFvZzOpufJcO8PZARqeRpBWxoTpTuOmgTPtiefzceefdvu52VxRC9AxUtlxbQEMnxUa_IS4raPaXH8uyB3APcTiiWKH2cEinApucd2Y4A/s16000/image5.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;We train deep learning models that are able to produce new views of a scene given a few images of it. These models are particularly effective when handling view-dependent effects like the refractions and translucency on the test tubes. This animation is compressed; see the original-quality renderings &lt;a href=&quot;https://light-field-neural-rendering.github.io/static/videos/shiny_lab.mp4&quot;&gt;here&lt;/a&gt;. Source: Lab scene from the &lt;a href=&quot;https://nex-mpi.github.io/&quot;&gt;NeX/Shiny&lt;/a&gt; dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Overview&lt;/h2&gt;&lt;p&gt;The input to the models consists of a set of reference images and their camera parameters (focal length, position, and orientation in space), along with the coordinates of the target &lt;a href=&quot;https://en.wikipedia.org/wiki/Line_(geometry)#Ray&quot;&gt;ray&lt;/a&gt; whose color we want to determine. To produce a new image, we start from the camera parameters of the input images, obtain the coordinates of the target rays (each corresponding to a pixel), and query the model for each.  &lt;/p&gt;&lt;p&gt;Instead of processing each reference image completely, we look only at the regions that are likely to influence the target pixel. These regions are determined via &lt;a href=&quot;https://en.wikipedia.org/wiki/Epipolar_geometry&quot;&gt;epipolar geometry&lt;/a&gt;, which maps each target pixel to a line on each reference frame. For robustness, we take small regions around a number of points on the epipolar line, resulting in the set of patches that will actually be processed by the model. The transformers then act on this set of patches to obtain the color of the target pixel.  &lt;/p&gt;&lt;p&gt;Transformers are especially useful in this setting since their self-attention mechanism naturally takes sets as inputs, and the attention weights themselves can be used to combine reference view colors and features to predict the output pixel colors. These transformers follow the architecture introduced in &lt;a href=&quot;https://github.com/google-research/vision_transformer&quot;&gt;ViT&lt;/a&gt;. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjGFUDAhjY-yJytdfcPFABdtwKgq_BwD7rSib6T_3Y6IY8jOm6S7ZLOdr_9gX8W7oj6bFKBvL8H7h9QVCJ1Kdw75UPv2Ekubh-TpiGhqOCf7e4fXuC0sQscp8KNn6hCuHNFd7e1ih-OpyXp2mf338pS-h2Qw5zqRIo4ZJYvgrVJ4PLC-umaEVDgIUZVg/s1050/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;417&quot; data-original-width=&quot;1050&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjGFUDAhjY-yJytdfcPFABdtwKgq_BwD7rSib6T_3Y6IY8jOm6S7ZLOdr_9gX8W7oj6bFKBvL8H7h9QVCJ1Kdw75UPv2Ekubh-TpiGhqOCf7e4fXuC0sQscp8KNn6hCuHNFd7e1ih-OpyXp2mf338pS-h2Qw5zqRIo4ZJYvgrVJ4PLC-umaEVDgIUZVg/s16000/image4.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;To predict the color of one pixel, the models take a set of patches extracted around the epipolar line of each reference view. Image source: &lt;a href=&quot;https://bmild.github.io/llff/index.html&quot;&gt;LLFF&lt;/a&gt; dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Light Field Neural Rendering&lt;/h2&gt;&lt;p&gt;In &lt;a href=&quot;https://light-field-neural-rendering.github.io/&quot;&gt;Light Field Neural Rendering&lt;/a&gt; (LFNR), we use a sequence of two transformers to map the set of patches to the target pixel color. The first transformer aggregates information along each epipolar line, and the second along each reference image. We can interpret the first transformer as finding potential correspondences of the target pixel on each reference frame, and the second as reasoning about occlusion and view-dependent effects, which are common challenges of image-based rendering. &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6stcn8aiJ2QXdr4PLJo68S11D5ycKYVf9CJ9yOtjoZbAcl5crn_7SmKNOl3f_zsa04pXF3Q8VXyGaS6vVk8ZmHn1-gWjOaLlJ8MlS6wnZm58sSee-uyCTzN6t2gjG0roaoWBbPIdDC8tuSGBD9zMKNVpKq04-iRSVfslAz2HD7RXrqttKM8Chf28zSw/s1999/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;584&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6stcn8aiJ2QXdr4PLJo68S11D5ycKYVf9CJ9yOtjoZbAcl5crn_7SmKNOl3f_zsa04pXF3Q8VXyGaS6vVk8ZmHn1-gWjOaLlJ8MlS6wnZm58sSee-uyCTzN6t2gjG0roaoWBbPIdDC8tuSGBD9zMKNVpKq04-iRSVfslAz2HD7RXrqttKM8Chf28zSw/s16000/image8.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;LFNR uses a sequence of two transformers to map a set of patches extracted along epipolar lines to the target pixel color.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;LFNR improved the state-of-the-art on the most popular view synthesis benchmarks (Blender and Real Forward-Facing scenes from &lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;&gt;NeRF&lt;/a&gt; and Shiny from &lt;a href=&quot;https://nex-mpi.github.io/&quot;&gt;NeX&lt;/a&gt;) with margins as large as 5dB &lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;&gt;peak signal-to-noise ratio&lt;/a&gt; (PSNR). This corresponds to a reduction of the pixel-wise error by a factor of 1.8x. We show qualitative results on challenging scenes from the &lt;a href=&quot;https://nex-mpi.github.io/&quot;&gt;Shiny&lt;/a&gt; dataset below: &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJ4SRa5sepHJhmbySIqPmalNVkIBwrGTxaoIPVFiOXt_muv4Q8ZDjd8fnnTaEw0iqAu2pb4HzPKFElNytafFAdXlCdFtaDq2WK3fbXrYbKK7YxAGKZcVx6AwUTsJgFxWlC95SQ-v6FowCxYsI-pK34zq-xqq8xgokz1K1jAKl4JNeGvp4sBG2PVOazfw/s504/image9.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;288&quot; data-original-width=&quot;504&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJ4SRa5sepHJhmbySIqPmalNVkIBwrGTxaoIPVFiOXt_muv4Q8ZDjd8fnnTaEw0iqAu2pb4HzPKFElNytafFAdXlCdFtaDq2WK3fbXrYbKK7YxAGKZcVx6AwUTsJgFxWlC95SQ-v6FowCxYsI-pK34zq-xqq8xgokz1K1jAKl4JNeGvp4sBG2PVOazfw/s16000/image9.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;LFNR reproduces challenging view-dependent effects like the rainbow and reflections on the CD, reflections, refractions and translucency on the bottles. This animation is compressed; see the original quality renderings &lt;a href=&quot;https://light-field-neural-rendering.github.io/static/videos/shiny_cd.mp4&quot;&gt;here&lt;/a&gt;. Source: CD scene from the &lt;a href=&quot;https://nex-mpi.github.io/&quot;&gt;NeX/Shiny&lt;/a&gt; dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheYJQqMchzPTvb4mtom8VmoYtE91IB7Q47qygzupMAHvhp5lDyUEsyKlTx5567vlT32M4EV2JiZ9ERt_A-InkESptwHeteVcapgSjoPC8dTKpGtURFvk51UxjfiI_q5fdBGVNPBB1Kl2i333JU-nBLjJhsuij9q7c0saaXN_DGTbkbYL_3xbypGTQ3tA/s1402/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;422&quot; data-original-width=&quot;1402&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheYJQqMchzPTvb4mtom8VmoYtE91IB7Q47qygzupMAHvhp5lDyUEsyKlTx5567vlT32M4EV2JiZ9ERt_A-InkESptwHeteVcapgSjoPC8dTKpGtURFvk51UxjfiI_q5fdBGVNPBB1Kl2i333JU-nBLjJhsuij9q7c0saaXN_DGTbkbYL_3xbypGTQ3tA/s16000/image1.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Prior methods such as &lt;a href=&quot;https://nex-mpi.github.io/&quot;&gt;NeX&lt;/a&gt; and &lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;&gt;NeRF&lt;/a&gt; fail to reproduce view-dependent effects like the translucency and refractions in the test tubes on the Lab scene from the &lt;a href=&quot;https://nex-mpi.github.io/&quot;&gt;NeX/Shiny&lt;/a&gt; dataset. See also our video of this scene at the top of the post and the original quality outputs &lt;a href=&quot;https://light-field-neural-rendering.github.io/static/videos/shiny_lab.mp4&quot;&gt;here&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Generalizing to New Scenes&lt;/h2&gt;&lt;p&gt;One limitation of LFNR is that the first transformer collapses the information along each epipolar line independently for each reference image. This means that it decides which information to preserve based only on the output ray coordinates and patches from each reference image, which works well when training on a single scene (as most neural rendering methods do), but it does not generalize across scenes. Generalizable methods are important because they can be applied to new scenes without needing to retrain. &lt;/p&gt;&lt;p&gt;We overcome this limitation of LFNR in &lt;a href=&quot;https://mohammedsuhail.net/gen_patch_neural_rendering/&quot;&gt;Generalizable Patch-Based Neural Rendering&lt;/a&gt; (GPNR). We add a transformer that runs before the other two and exchanges information between points at the same depth over all reference images. For example, this first transformer looks at the columns of the patches from the park bench shown above and can use cues like the flower that appears at corresponding depths in two views, which indicates a potential match. Another key idea of this work is to canonicalize the positional encoding based on the target ray, because to generalize across scenes, it is necessary to represent quantities in relative and not absolute frames of reference. The animation below shows an overview of the model.  &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgl_5a5VpgxxOOSrFjTOv4IIFMgSTCnzxda-l9XVzac4sXBWdIUrmi921QTlj0ZQggGxNW7kpYbFulLie1Hy7umjVmaxQoi9YtZnIPL_XnFqvQF9OU_wT7WyYyJLcNCbYmTu1TEZe7D5dzAOb_WFpz2KIoENs_Wv_loRm6LK68mHm-TNO76JsrXJeydlQ/s960/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgl_5a5VpgxxOOSrFjTOv4IIFMgSTCnzxda-l9XVzac4sXBWdIUrmi921QTlj0ZQggGxNW7kpYbFulLie1Hy7umjVmaxQoi9YtZnIPL_XnFqvQF9OU_wT7WyYyJLcNCbYmTu1TEZe7D5dzAOb_WFpz2KIoENs_Wv_loRm6LK68mHm-TNO76JsrXJeydlQ/s16000/image3.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;GPNR consists of a sequence of three transformers that map a set of patches extracted along epipolar lines to a pixel color. Image patches are mapped via the linear projection layer to initial features (shown as blue and green boxes). Then those features are successively refined and aggregated by the model, resulting in the final feature/color represented by the gray rectangle. Park bench image source: &lt;a href=&quot;https://bmild.github.io/llff/index.html&quot;&gt;LLFF&lt;/a&gt; dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;To evaluate the generalization performance, we train GPNR on a set of scenes and test it on new scenes. GPNR improved the state-of-the-art on several benchmarks (following &lt;a href=&quot;https://ibrnet.github.io/&quot;&gt;IBRNet&lt;/a&gt; and &lt;a href=&quot;https://apchenstu.github.io/mvsnerf/&quot;&gt;MVSNeRF&lt;/a&gt; protocols) by 0.5–1.0 dB on average. On the &lt;a href=&quot;https://ibrnet.github.io/&quot;&gt;IBRNet&lt;/a&gt; benchmark, GPNR outperforms the baselines while using only 11% of the training scenes. The results below show new views of unseen scenes rendered with no fine-tuning.  &lt;/p&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9puTw4NAnf8DUJcCCv2LSvAQu6kmIMsOK-bkCSKtDV0WeqKpwoJmvXolP1ibzHQ8u_VqVVikaOz1S2Z2PBO5SpLowKSRdDvxf3_QN1JD1pTHLXnGcepNeflOvsofeLMGWoF3iIMfupMcDvplslsRU1G-JrHGZHLSgbRTbDwq5ZGZpRGLx27jFqB1z8Q/s576/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width=&quot;576&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9puTw4NAnf8DUJcCCv2LSvAQu6kmIMsOK-bkCSKtDV0WeqKpwoJmvXolP1ibzHQ8u_VqVVikaOz1S2Z2PBO5SpLowKSRdDvxf3_QN1JD1pTHLXnGcepNeflOvsofeLMGWoF3iIMfupMcDvplslsRU1G-JrHGZHLSgbRTbDwq5ZGZpRGLx27jFqB1z8Q/s16000/image2.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;GPNR-generated views of held-out scenes, without any fine tuning. This animation is compressed; see the original quality renderings &lt;a href=&quot;https://mohammedsuhail.net/gen_patch_neural_rendering/img/combined_results.mp4&quot;&gt;here&lt;/a&gt;. Source: &lt;a href=&quot;https://ibrnet.github.io/&quot;&gt;IBRNet&lt;/a&gt; collected dataset. &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKN1tnJDaafkhry535hVj4XZr4QVgcwebkox-YLImsrYSBcharQlzjgpp81SGwTTprQR6t4W2rJa4AvuRixFk5MYQavaXGmI0xBgQjKOOfJ1bIon1xEfzyjrt8FA5NL14pGU5o5M61XNvBHxIBoiL2jBdh9t1MMnHqcYTU1DocsH6HACg0TOnuktUp2Q/s1772/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;363&quot; data-original-width=&quot;1772&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKN1tnJDaafkhry535hVj4XZr4QVgcwebkox-YLImsrYSBcharQlzjgpp81SGwTTprQR6t4W2rJa4AvuRixFk5MYQavaXGmI0xBgQjKOOfJ1bIon1xEfzyjrt8FA5NL14pGU5o5M61XNvBHxIBoiL2jBdh9t1MMnHqcYTU1DocsH6HACg0TOnuktUp2Q/s16000/image7.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Details of GPNR-generated views on held-out scenes from &lt;a href=&quot;https://nex-mpi.github.io/&quot;&gt;NeX/Shiny&lt;/a&gt; (&lt;b&gt;left&lt;/b&gt;) and &lt;a href=&quot;https://bmild.github.io/llff/index.html&quot;&gt;LLFF&lt;/a&gt; (&lt;b&gt;right&lt;/b&gt;), without any fine tuning. GPNR reproduces more accurately the details on the leaf and the refractions through the lens when compared against &lt;a href=&quot;https://ibrnet.github.io/&quot;&gt;IBRNet&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Future Work&lt;/h2&gt;&lt;p&gt;One limitation of most neural rendering methods, including ours, is that they require camera poses for each input image. Poses are not easy to obtain and typically come from offline optimization methods that can be slow, limiting possible applications, such as those on mobile devices. Research on jointly learning view synthesis and input poses is a promising future direction. Another limitation of our models is that they are computationally expensive to train. There is an active line of research on faster transformers which might help improve our models’ efficiency. For the papers, more results, and open-source code, you can check out the projects pages for &quot;&lt;a href=&quot;https://light-field-neural-rendering.github.io/&quot;&gt;Light Field Neural Rendering&lt;/a&gt;&quot; and &quot;&lt;a href=&quot;https://mohammedsuhail.net/gen_patch_neural_rendering/&quot;&gt;Generalizable Patch-Based Neural Rendering&lt;/a&gt;&quot;. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Potential Misuse&lt;/h2&gt;&lt;p&gt;In our research, we aim to accurately reproduce an existing scene using images from that scene, so there is little room to generate fake or non-existing scenes. Our models assume static scenes, so synthesizing moving objects, such as people, will not work. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgments&lt;/h2&gt;&lt;p&gt;&lt;em&gt;All the hard work was done by our amazing intern – &lt;a href=&quot;https://mohammedsuhail.net/&quot;&gt;Mohammed Suhail&lt;/a&gt; – a PhD student at UBC, in collaboration with &lt;a href=&quot;https://machc.github.io/&quot;&gt;Carlos Esteves&lt;/a&gt; and &lt;a href=&quot;http://www.ameeshmakadia.com/&quot;&gt;Ameesh Makadia&lt;/a&gt; from Google Research, and &lt;a href=&quot;https://www.cs.ubc.ca/~lsigal/&quot;&gt;Leonid Sigal&lt;/a&gt; from UBC. We are thankful to Corinna Cortes for supporting and encouraging this project.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Our work is inspired by &lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;&gt;NeRF&lt;/a&gt;, which sparked the recent interest in view synthesis, and &lt;a href=&quot;https://ibrnet.github.io/&quot;&gt;IBRNet&lt;/a&gt;, which first considered generalization to new scenes. Our light ray positional encoding is inspired by the seminal paper &lt;a href=&quot;https://graphics.stanford.edu/papers/light/&quot;&gt;Light Field Rendering&lt;/a&gt; and our use of transformers follow &lt;a href=&quot;https://github.com/google-research/vision_transformer&quot;&gt;ViT&lt;/a&gt;.  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Video results are from scenes from &lt;a href=&quot;https://bmild.github.io/llff/index.html&quot;&gt;LLFF&lt;/a&gt;, &lt;a href=&quot;https://nex-mpi.github.io/&quot;&gt;Shiny&lt;/a&gt;, and &lt;a href=&quot;https://ibrnet.github.io/&quot;&gt;IBRNet&lt;/a&gt; collected datasets.&lt;/em&gt;&lt;/p&gt; </content><link href="http://ai.googleblog.com/feeds/6099606360064831644/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/view-synthesis-with-transformers.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6099606360064831644" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6099606360064831644" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/view-synthesis-with-transformers.html" rel="alternate" title="View Synthesis with Transformers" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkJzGICK0DKkQBjC-O0ts_iqk-W3qo90RcUbJl9xOA4NIpzqRHiDx1C8C3KV0ylyBR3cPwgvoW1I9s3y2OW1VdGVArKtWDaIDaxmLFSRoOo66YcMVA9Df7Tj9_zaR2fZplryCBphBKgTtDRegu6POYDiyMy_waqgL3_37KTFeMGjCD7R-U7jT0boYong/s72-c/image6.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-6639900871004963241</id><published>2022-09-20T10:00:00.010-07:00</published><updated>2022-10-31T14:01:27.282-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Google Brain"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><title type="text">FindIt: Generalized Object Localization with Natural Language Queries</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Weicheng Kuo and Anelia Angelova, Research Scientists, Google Research, Brain Team&lt;/span&gt; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRaPsdl4AdqQprPVoN9xRVQpw4oyc4e9wXl3QDbM6hu9u3K1o1A0nwVNxA-JnCIg2YyMc1NVYPLCkjqRQUHInynLpgdo5E13kdffkV9XyatQpnZl7xR_5CXSW-uSX7r4H934VAgGHpuAFDOmQf-rcUdnDVMquinbeP6emaXqg7cu9aiGHbXLXQ5wLOoA/s1600/image4.gif&quot; style=&quot;display: none;&quot; /&gt; &lt;p&gt;Natural language enables flexible descriptive queries about images. The interaction between text queries and images grounds linguistic meaning in the visual world, facilitating a better understanding of object relationships, human intentions towards objects, and interactions with the environment. The research community has studied object-level visual grounding through a range of tasks, including &lt;a href=&quot;https://paperswithcode.com/task/referring-expression-comprehension&quot;&gt;referring expression comprehension&lt;/a&gt;, &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_Towards_General_Purpose_Vision_Systems_An_End-to-End_Task-Agnostic_Vision-Language_Architecture_CVPR_2022_paper.pdf&quot;&gt;text-based localization&lt;/a&gt;, and more broadly &lt;a href=&quot;https://paperswithcode.com/task/object-detection&quot;&gt;object detection&lt;/a&gt;, each of which require different skills in a model. For example, object detection seeks to find all objects from a predefined set of classes, which requires accurate localization and classification, while referring expression comprehension localizes an object from a referring text and often requires complex reasoning on prominent objects. At the intersection of the two is text-based localization, in which a simple category-based text query prompts the model to detect the objects of interest. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;Due to their dissimilar task properties, referring expression comprehension, detection, and text-based localization are mostly studied through separate benchmarks with most models only dedicated to one task. As a result, existing models have not adequately synthesized information from the three tasks to achieve a more holistic visual and linguistic understanding. Referring expression comprehension models, for instance, are trained to predict one object per image, and often struggle to localize multiple objects, reject negative queries, or detect novel categories. In addition, detection models are unable to process text inputs, and text-based localization models often struggle to process complex queries that refer to one object instance, such as “Left half sandwich.” Lastly, none of the models can generalize sufficiently well beyond their training data and categories. &lt;/p&gt;&lt;p&gt;To address these limitations, we are presenting “&lt;a href=&quot;https://arxiv.org/abs/2203.17273&quot;&gt;FindIt: Generalized Localization with Natural Language Queries&lt;/a&gt;” at &lt;a href=&quot;https://eccv2022.ecva.net/&quot;&gt;ECCV 2022&lt;/a&gt;. Here we propose a unified, general-purpose and multitask visual grounding model, called FindIt, that can flexibly answer different types of grounding and detection queries. Key to this architecture is a multi-level cross-modality fusion module that can perform complex reasoning for referring expression comprehension and simultaneously recognize small and challenging objects for text-based localization and detection. In addition, we discover that a standard object detector and detection losses are sufficient and surprisingly effective for all three tasks without the need for task-specific design and losses common in existing works. FindIt is simple, efficient, and outperforms alternative state-of-the-art models on the referring expression comprehension and text-based localization benchmarks, while being competitive on the detection benchmark. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjPIIB2rIV0acedyIsoK8cqvf_ZNc6ZY55KXI_L6Cv8LuDHY3OZTJ3d4MQlMq4WCGu73Cp41NOM8J07yCMEHTrUViWdCqdntnhKw86c34datYAJZTaNrbxaof2xw5GAdWhXX2bX6xd5avmM7nH1acHnsCp8zxRJe11yzwuR5VViFiAUrmwwPqFV-W8TyQ/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1085&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjPIIB2rIV0acedyIsoK8cqvf_ZNc6ZY55KXI_L6Cv8LuDHY3OZTJ3d4MQlMq4WCGu73Cp41NOM8J07yCMEHTrUViWdCqdntnhKw86c34datYAJZTaNrbxaof2xw5GAdWhXX2bX6xd5avmM7nH1acHnsCp8zxRJe11yzwuR5VViFiAUrmwwPqFV-W8TyQ/s16000/image5.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;FindIt is a unified model for referring expression comprehension (col. 1), text-based localization (col. 2), and the object detection task (col. 3). FindIt can respond accurately when tested on object types/classes not known during training, e.g. “Find the desk” (col. 4). Compared to existing baselines (&lt;a href=&quot;https://arxiv.org/abs/1801.08186&quot;&gt;MattNet&lt;/a&gt; and &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_Towards_General_Purpose_Vision_Systems_An_End-to-End_Task-Agnostic_Vision-Language_Architecture_CVPR_2022_paper.pdf&quot;&gt;GPV&lt;/a&gt;), FindIt can perform these tasks well and in a single model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Multi-level Image-Text Fusion&lt;/h2&gt;&lt;p&gt;Different localization tasks are created with different semantic understanding objectives. For example, because the referring expression task primarily references prominent objects in the image rather than small, occluded or faraway objects, low resolution images generally suffice. In contrast, the detection task aims to detect objects with various sizes and occlusion levels in higher resolution images. Apart from these benchmarks, the general visual grounding problem is inherently multiscale, as natural queries can refer to objects of any size. This motivates the need for a multi-level image-text fusion model for efficient processing of higher resolution images over different localization tasks.  &lt;/p&gt;&lt;p&gt;The premise of FindIt is to fuse the higher level semantic features using more expressive &lt;a href=&quot;https://arxiv.org/abs/1706.03762v5&quot;&gt;transformer&lt;/a&gt; layers, which can capture all-pair interactions between image and text. For the lower-level and higher-resolution features, we use a cheaper &lt;a href=&quot;https://en.wikipedia.org/wiki/Dot_product&quot;&gt;dot-product&lt;/a&gt; fusion to save computation and memory cost. We attach a detector head (e.g., &lt;a href=&quot;https://arxiv.org/abs/1506.01497&quot;&gt;Faster R-CNN&lt;/a&gt;) on top of the fused feature maps to predict the boxes and their classes. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRaPsdl4AdqQprPVoN9xRVQpw4oyc4e9wXl3QDbM6hu9u3K1o1A0nwVNxA-JnCIg2YyMc1NVYPLCkjqRQUHInynLpgdo5E13kdffkV9XyatQpnZl7xR_5CXSW-uSX7r4H934VAgGHpuAFDOmQf-rcUdnDVMquinbeP6emaXqg7cu9aiGHbXLXQ5wLOoA/s1600/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;575&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRaPsdl4AdqQprPVoN9xRVQpw4oyc4e9wXl3QDbM6hu9u3K1o1A0nwVNxA-JnCIg2YyMc1NVYPLCkjqRQUHInynLpgdo5E13kdffkV9XyatQpnZl7xR_5CXSW-uSX7r4H934VAgGHpuAFDOmQf-rcUdnDVMquinbeP6emaXqg7cu9aiGHbXLXQ5wLOoA/s16000/image4.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;FindIt accepts an image and a query text as inputs, and processes them separately in image/text backbones before applying the multi-level fusion. We feed the fused features to Faster R-CNN to predict the boxes referred to by the text. The feature fusion uses more expressive &lt;a href=&quot;https://arxiv.org/abs/1706.03762v5&quot;&gt;transformers&lt;/a&gt; at higher levels and cheaper dot-product at the lower levels.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;   &lt;h2&gt;Multitask Learning&lt;/h2&gt;&lt;p&gt;Apart from the multi-level fusion described above, we adapt the text-based localization and detection tasks to take the same inputs as the referring expression comprehension task. For the text-based localization task, we generate a set of queries over the categories present in the image. For any present category, the text query takes the form “Find the [&lt;code&gt;object&lt;/code&gt;],” where [&lt;code&gt;object&lt;/code&gt;] is the category name. The objects corresponding to that category are labeled as foreground and the other objects as background. Instead of using the aforementioned prompt, we use a static prompt for the detection task, such as “Find all the objects.”. We found that the specific choice of prompts is not important for text-based localization and detection tasks.  &lt;/p&gt;&lt;p&gt;After adaptation, all tasks in consideration share the same inputs and outputs — an image input, a text query, and a set of output bounding boxes and classes. We then combine the datasets and train on the mixture. Finally, we use the standard object detection losses for all tasks, which we found to be surprisingly simple and effective. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Evaluation&lt;/h2&gt;&lt;p&gt;We apply FindIt to the popular &lt;a href=&quot;https://paperswithcode.com/dataset/refcoco&quot;&gt;RefCOCO&lt;/a&gt; benchmark for referring expression comprehension tasks. When only the &lt;a href=&quot;https://cocodataset.org/#home&quot;&gt;COCO&lt;/a&gt; and RefCOCO dataset is available, FindIt outperforms the state-of-the-art-model on all tasks. In the settings where external datasets are allowed, FindIt sets a new state of the art by using COCO and all RefCOCO splits together (no other datasets). On the challenging &lt;a href=&quot;https://github.com/lichengunc/refer&quot;&gt;Google and UMD splits&lt;/a&gt;, FindIt outperforms the state of the art by a 10% margin, which, taken together, demonstrate the benefits of multitask learning. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUY4nOj2imEyW4rV-mBEfdA4v5W_MIJ_fDr9pxjXaOolm_YcYqsoqKj9GsHepuXyZxfuxNQk6re_SCfX6qmVg4RvNUsxsb3cRvFaJWLWttP06RYfifP_-wPV20_Y2GiNAsKW4jeGX6VGQWTkK_t6d-zhFoHsJ6h6GxGkXU6IakldIAtD6Wu1UNG29kdg/s1324/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;886&quot; data-original-width=&quot;1324&quot; height=&quot;428&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUY4nOj2imEyW4rV-mBEfdA4v5W_MIJ_fDr9pxjXaOolm_YcYqsoqKj9GsHepuXyZxfuxNQk6re_SCfX6qmVg4RvNUsxsb3cRvFaJWLWttP06RYfifP_-wPV20_Y2GiNAsKW4jeGX6VGQWTkK_t6d-zhFoHsJ6h6GxGkXU6IakldIAtD6Wu1UNG29kdg/w640-h428/image3.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Comparison with the state of the art on the popular referring expression benchmark. FindIt is superior on both the &lt;a href=&quot;https://cocodataset.org/#home&quot;&gt;COCO&lt;/a&gt; and unconstrained settings (additional training data allowed).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;On the text-based localization benchmark, FindIt achieves 79.7%, higher than the &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_Towards_General_Purpose_Vision_Systems_An_End-to-End_Task-Agnostic_Vision-Language_Architecture_CVPR_2022_paper.pdf&quot;&gt;GPV&lt;/a&gt; (73.0%), and &lt;a href=&quot;https://arxiv.org/abs/1506.01497&quot;&gt;Faster R-CNN&lt;/a&gt; baselines (75.2%). Please refer to the &lt;a href=&quot;https://arxiv.org/abs/2203.17273&quot;&gt;paper&lt;/a&gt; for more quantitative evaluation. &lt;/p&gt;&lt;p&gt;We further observe that FindIt generalizes better to novel categories and super-categories in the text-based localization task compared to competitive single-task baselines on the popular &lt;a href=&quot;https://cocodataset.org/#home&quot;&gt;COCO&lt;/a&gt; and &lt;a href=&quot;https://www.objects365.org/overview.html&quot;&gt;Objects365&lt;/a&gt; datasets, shown in the figure below. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr1Z0xl-mLE30eKpa69jWj6_BYASXXd6O0p_kpfwDhTAndm4mnER3QS37UkWG1mQTdyfo215knHHLVtw89EKrPYBVPNCVVhruF-n7MSNHvLPJv4y_GDnxz0XTnLJdGjIiSh0ymMcogMRpx1M0Nj4jBCRpYmPgej2USGOfGjB6dcqY6LQYTRFFw3TByTA/s1375/Screenshot%202022-09-20%209.40.47%20AM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;411&quot; data-original-width=&quot;1375&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr1Z0xl-mLE30eKpa69jWj6_BYASXXd6O0p_kpfwDhTAndm4mnER3QS37UkWG1mQTdyfo215knHHLVtw89EKrPYBVPNCVVhruF-n7MSNHvLPJv4y_GDnxz0XTnLJdGjIiSh0ymMcogMRpx1M0Nj4jBCRpYmPgej2USGOfGjB6dcqY6LQYTRFFw3TByTA/s16000/Screenshot%202022-09-20%209.40.47%20AM.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;FindIt on novel and super categories. &lt;b&gt;Left:&lt;/b&gt; FindIt outperforms the single-task baselines especially on the novel categories. &lt;b&gt;Right:&lt;/b&gt; FindIt outperforms the single-task baselines on the unseen super categories. “Rec-Single” is the Referring expression comprehension single task model and “Loc-Single” is the text-based localization single task model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Efficiency&lt;/h2&gt;&lt;p&gt;We also benchmark the inference times on the referring expression comprehension task (see Table below). FindIt is efficient and comparable with existing one-stage approaches while achieving higher accuracy. For fair comparison, all running times are measured on one &lt;a href=&quot;https://www.nvidia.com/en-us/geforce/10-series/&quot;&gt;GTX 1080Ti&lt;/a&gt; GPU. &lt;/p&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;  &lt;td style=&quot;text-align: left;&quot;&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Image Size&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Backbone&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Runtime (ms)&lt;/b&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1801.08186&quot;&gt;MattNet&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;1000&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;R101&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;378&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_A_Fast_and_Accurate_One-Stage_Approach_to_Visual_Grounding_ICCV_2019_paper.pdf&quot;&gt;FAOA&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;256&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.02767&quot;&gt;DarkNet53&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;39&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.08813&quot;&gt;MCN&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;416&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.02767&quot;&gt;DarkNet53&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;56&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.08541&quot;&gt;TransVG&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;640&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;R50&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;62&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;FindIt (Ours)&lt;/em&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;640&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;R50&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;107&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;FindIt (Ours)&lt;/em&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;384&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;R50&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style=&quot;text-align: center;&quot;&gt;57&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div style=&quot;line-height:120%;&quot;&gt;    &lt;br&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We present Findit, which unifies referring expression comprehension, text-based localization, and object detection tasks. We propose multi-scale cross-attention to unify the diverse localization requirements of these tasks. Without any task-specific design, FindIt surpasses the state of the art on referring expression and text-based localization, shows competitive performance on detection, and generalizes better to out-of-distribution data and novel classes. All of these are accomplished in a single, unified, and efficient model. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;This work is conducted by Weicheng Kuo, Fred Bertsch, Wei Li, AJ Piergiovanni, Mohammad Saffar, and Anelia Angelova. We would like to thank Ashish Vaswani, Prajit Ramachandran, Niki Parmar, David Luan, Tsung-Yi Lin, and other colleagues at Google Research for their advice and helpful discussions. We would like to thank Tom Small for preparing the animation.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/6639900871004963241/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/findit-generalized-object-localization.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6639900871004963241" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6639900871004963241" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/findit-generalized-object-localization.html" rel="alternate" title="FindIt: Generalized Object Localization with Natural Language Queries" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRaPsdl4AdqQprPVoN9xRVQpw4oyc4e9wXl3QDbM6hu9u3K1o1A0nwVNxA-JnCIg2YyMc1NVYPLCkjqRQUHInynLpgdo5E13kdffkV9XyatQpnZl7xR_5CXSW-uSX7r4H934VAgGHpuAFDOmQf-rcUdnDVMquinbeP6emaXqg7cu9aiGHbXLXQ5wLOoA/s72-c/image4.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-5836135189460527312</id><published>2022-09-17T19:00:00.004-07:00</published><updated>2022-10-31T14:05:36.843-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="conference"/><category scheme="http://www.blogger.com/atom/ns#" term="Interspeech"/><category scheme="http://www.blogger.com/atom/ns#" term="Speech"/><title type="text">Google at Interspeech 2022</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Cat Armato, Program Manager, Google&lt;/span&gt; &lt;p&gt;This week, the 23rd Annual &lt;a href=&quot;https://www.interspeech2022.org/&quot;&gt;Conference of the International Speech Communication Association&lt;/a&gt; (INTERSPEECH 2022) is being held in Incheon, South Korea, representing one of the world’s most extensive conferences on research and technology of spoken language understanding and processing. Over 2,000 experts in speech-related research fields gather to take part in oral presentations and poster sessions and to collaborate with streamed events across the globe. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;We are excited to be a Diamond Sponsor of INTERSPEECH 2022, where we will be showcasing nearly 50 research publications and supporting a number of workshops, special sessions and tutorials. We welcome in-person attendees to drop by the Google booth to meet our researchers and participate in Q&amp;amp;As and demonstrations of some of our latest speech technologies, which help to improve accessibility and provide convenience in communication for billions of users. In addition, online attendees are encouraged to visit our virtual booth in &lt;a href=&quot;https://spotvirtual.com/invite/lifeskilz-69dZxjOOvd&quot;&gt;GatherTown&lt;/a&gt; where you can get up-to-date information on research and opportunities at Google. You can also learn more about the Google research being presented at INTERSPEECH 2022 below (Google affiliations in &lt;b&gt;bold&lt;/b&gt;). &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Organizing Committee&lt;/h2&gt;&lt;p&gt;Industry Liaisons include: &lt;b&gt;&lt;i&gt;Bhuvana Ramabahdran&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Area Chairs include: &lt;i&gt;&lt;b&gt;John Hershey&lt;/b&gt;, &lt;b&gt;Heiga Zen&lt;/b&gt;, &lt;b&gt;Shrikanth Narayanan&lt;/b&gt;, &lt;b&gt;Bastiaan Kleijn&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;ISCA Fellows&lt;/h2&gt;&lt;p&gt;Include: &lt;i&gt;&lt;b&gt;Tara Sainath&lt;/b&gt;, &lt;b&gt;Heiga Zen&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Publications&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.06322.pdf&quot;&gt;Production Federated Keyword Spotting via Distillation, Filtering, and Joint Federated-Centralized Training&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Andrew Hard&lt;/b&gt;, &lt;b&gt;Kurt Partridge&lt;/b&gt;, &lt;b&gt;Neng Chen&lt;/b&gt;, &lt;b&gt;Sean Augenstein&lt;/b&gt;, &lt;b&gt;Aishanee Shah&lt;/b&gt;, &lt;b&gt;Hyun Jin Park&lt;/b&gt;, &lt;b&gt;Alex Park&lt;/b&gt;, &lt;b&gt;Sara Ng&lt;/b&gt;, &lt;b&gt;Jessica Nguyen&lt;/b&gt;, &lt;b&gt;Ignacio Lopez Moreno&lt;/b&gt;, &lt;b&gt;Rajiv Mathews&lt;/b&gt;, &lt;b&gt;Françoise Beaufays&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.13339.pdf&quot;&gt;Leveraging Unsupervised and Weakly-Supervised Data to Improve Direct Speech-to-Speech Translation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Ye Jia&lt;/b&gt;, &lt;b&gt;Yifan Ding&lt;/b&gt;, &lt;b&gt;Ankur Bapna&lt;/b&gt;, &lt;b&gt;Colin Cherry&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Alexis Conneau&lt;/b&gt;, &lt;b&gt;Nobu Morioka&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.05008.pdf&quot;&gt;Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;W. Ronny Huang&lt;/b&gt;, &lt;b&gt;Cal Peyser&lt;/b&gt;, &lt;b&gt;Tara N. Sainath&lt;/b&gt;, &lt;b&gt;Ruoming Pang&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Shankar Kumar&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.00706.pdf&quot;&gt;UserLibri: A Dataset for ASR Personalization Using Only Text&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Theresa Breiner&lt;/b&gt;, &lt;b&gt;Swaroop Ramaswamy&lt;/b&gt;, &lt;b&gt;Ehsan Variani&lt;/b&gt;, &lt;b&gt;Shefali Garg&lt;/b&gt;, &lt;b&gt;Rajiv Mathews&lt;/b&gt;, &lt;b&gt;Khe Chai Sim&lt;/b&gt;, &lt;b&gt;Kilol Gupta&lt;/b&gt;, &lt;b&gt;Mingqing Chen&lt;/b&gt;, &lt;b&gt;Lara McConnaughey&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2111.00764.pdf&quot;&gt;SNRi Target Training for Joint Speech Enhancement and Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Yuma Koizumi&lt;/b&gt;, &lt;b&gt;Shigeki Karita&lt;/b&gt;, &lt;b&gt;Arun Narayanan&lt;/b&gt;, &lt;b&gt;Sankaran Panchapagesan&lt;/b&gt;, &lt;b&gt;Michiel Bacchiani&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.13321.pdf&quot;&gt;Turn-Taking Prediction for Natural Conversational Speech&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Shuo-Yiin Chang&lt;/b&gt;, &lt;b&gt;Bo Li&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, &lt;b&gt;Chao Zhang&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Qiao Liang&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.13322.pdf&quot;&gt;Streaming Intended Query Detection Using E2E Modeling for Continued Conversation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Shuo-Yiin Chang&lt;/b&gt;, &lt;b&gt;Guru Prakash&lt;/b&gt;, &lt;b&gt;Zelin Wu&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, &lt;b&gt;Bo Li&lt;/b&gt;, &lt;b&gt;Qiao Liang&lt;/b&gt;, &lt;b&gt;Adam Stambler&lt;/b&gt;, &lt;b&gt;Shyam Upadhyay&lt;/b&gt;, &lt;b&gt;Manaal Faruqui&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.16104.pdf&quot;&gt;Improving Distortion Robustness of Self-Supervised Speech Processing Tasks with Domain Adaptation&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Kuan Po Huang, Yu-Kuan Fu,&lt;b&gt; Yu Zhang&lt;/b&gt;, Hung-yi Lee &lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2111.09296.pdf&quot;&gt;XLS-R: Self-Supervised Cross-Lingual Speech Representation Learning at Scale&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, &lt;b&gt;Alexis Conneau&lt;/b&gt;, Michael Auli &lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.08345.pdf&quot;&gt;Extracting Targeted Training Data from ASR Models, and How to Mitigate It&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Ehsan Amid&lt;/b&gt;, &lt;b&gt;Om Thakkar&lt;/b&gt;, &lt;b&gt;Arun Narayanan&lt;/b&gt;, &lt;b&gt;Rajiv Mathews&lt;/b&gt;, &lt;b&gt;Françoise Beaufays&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2204.09606&quot;&gt;Detecting Unintended Memorization in Language-Model-Fused ASR&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;W. Ronny Huang&lt;/b&gt;, &lt;b&gt;Steve Chien&lt;/b&gt;, &lt;b&gt;Om Thakkar&lt;/b&gt;, &lt;b&gt;Rajiv Mathews&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2206.07684.pdf&quot;&gt;AVATAR: Unconstrained Audiovisual Speech Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Valentin Gabeur&lt;/b&gt;, &lt;b&gt;Paul Hongsuck Seo&lt;/b&gt;, &lt;b&gt;Arsha Nagrani&lt;/b&gt;, &lt;b&gt;Chen Sun&lt;/b&gt;, Karteek Alahari, &lt;b&gt;Cordelia Schmid&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.00652.pdf&quot;&gt;End-to-End Multi-talker Audio-Visual ASR Using an Active Speaker Attention Module&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Richard Rose&lt;/b&gt;, &lt;b&gt;Olivier Siohan&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2201.10439.pdf&quot;&gt;Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition for Single and Multi-person Video&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Dmitriy Serdyuk&lt;/b&gt;, &lt;b&gt;Otavio Braga&lt;/b&gt;, &lt;b&gt;Olivier Siohan&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.01981.pdf&quot;&gt;Unsupervised Data Selection via Discrete Speech Representation for ASR&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Zhiyun Lu&lt;/b&gt;, &lt;b&gt;Yongqiang Wang&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Wei Han&lt;/b&gt;, &lt;b&gt;Zhehuai Chen&lt;/b&gt;, &lt;b&gt;Parisa Haghani&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2209.06987&quot;&gt;Non-parallel Voice Conversion for ASR Augmentation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Gary Wang&lt;/b&gt;, &lt;b&gt;Andrew Rosenberg&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;, &lt;b&gt;Fadi Biadsy&lt;/b&gt;, &lt;b&gt;Jesse Emond&lt;/b&gt;, &lt;b&gt;Yinghui Huang&lt;/b&gt;, &lt;b&gt;Pedro J. Moreno&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.02262.pdf&quot;&gt;Ultra-Low-Bitrate Speech Coding with Pre-trained Transformers&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Ali Siahkoohi, &lt;b&gt;Michael Chinen&lt;/b&gt;, &lt;b&gt;Tom Denton&lt;/b&gt;, &lt;b&gt;W. Bastiaan Kleijn&lt;/b&gt;, &lt;b&gt;Jan Skoglund&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://drive.google.com/file/d/1EOLTniiA30yenTHyM3dgXcL2whthz1hc/view?usp=sharing&amp;amp;resourcekey=0-xmtb_udWK0467lOOMA42IA&quot;&gt;Streaming End-to-End Multilingual Speech Recognition with Joint Language Identification&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Chao Zhang&lt;/b&gt;, &lt;b&gt;Bo Li&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Sepand Mavandadi&lt;/b&gt;, &lt;b&gt;Shuo-Yiin Chang&lt;/b&gt;, &lt;b&gt;Parisa Haghani&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2206.14716.pdf&quot;&gt;Improving Deliberation by Text-Only and Semi-supervised Training&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Ke Hu, Tara N. Sainath&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;Rohit Prabhavalkar&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Sepand Mavandadi&lt;/b&gt;, &lt;b&gt;Weiran Wang&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.10749.pdf&quot;&gt;E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;W. Ronny Huang&lt;/b&gt;, &lt;b&gt;Shuo-yiin Chang&lt;/b&gt;, &lt;b&gt;David Rybach&lt;/b&gt;, &lt;b&gt;Rohit Prabhavalkar&lt;/b&gt;, &lt;b&gt;Tara N. Sainath&lt;/b&gt;, &lt;b&gt;Cyril Allauzen&lt;/b&gt;, &lt;b&gt;Cal Peyser&lt;/b&gt;, &lt;b&gt;Zhiyun Lu&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.15652.pdf&quot;&gt;CycleGAN-Based Unpaired Speech Dereverberation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Alexis Conneau&lt;/b&gt;, &lt;b&gt;Ankur Bapna&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Min Ma&lt;/b&gt;, Patrick von Platen, Anton Lozhkov, &lt;b&gt;Colin Cherry&lt;/b&gt;, &lt;b&gt;Ye Jia&lt;/b&gt;, &lt;b&gt;Clara Rivera&lt;/b&gt;, &lt;b&gt;Mihir Kale&lt;/b&gt;, &lt;b&gt;Daan van Esch&lt;/b&gt;, &lt;b&gt;Vera Axelrod&lt;/b&gt;, &lt;b&gt;Simran Khanuja&lt;/b&gt;, &lt;b&gt;Jonathan Clark&lt;/b&gt;, &lt;b&gt;Orhan Firat&lt;/b&gt;, Michael Auli, &lt;b&gt;Sebastian Ruder&lt;/b&gt;, &lt;b&gt;Jason Riesa, Melvin Johnson&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aps.arxiv.org/pdf/2203.00236.pdf&quot;&gt;TRILLsson: Distilled Universal Paralinguistic Speech Representations&lt;/a&gt;&lt;b&gt; &lt;/b&gt;(see &lt;a href=&quot;https://ai.googleblog.com/2022/03/trillsson-small-universal-speech.html&quot;&gt;blog post&lt;/a&gt;) &lt;br /&gt;  &lt;i&gt;&lt;b&gt;Joel Shor&lt;/b&gt;, &lt;b&gt;Subhashini Venugopalan&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.15519.pdf&quot;&gt;Learning Neural Audio Features Without Supervision&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Sarthak Yadav, &lt;b&gt;Neil Zeghidour&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2202.07273.pdf&quot;&gt;SpeechPainter: Text-Conditioned Speech Inpainting&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Zalan Borsos&lt;/b&gt;, &lt;b&gt;Matthew Sharifi&lt;/b&gt;, &lt;b&gt;Marco Tagliasacchi&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.16749.pdf&quot;&gt;SpecGrad: Diffusion Probabilistic Model-Based Neural Vocoder with Adaptive Noise Spectral Shaping&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Yuma Koizumi&lt;/b&gt;, &lt;b&gt;Heiga Zen&lt;/b&gt;, Kohei Yatabe, &lt;b&gt;Nanxin Chen&lt;/b&gt;, &lt;b&gt;Michiel Bacchiani&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.00562.pdf&quot;&gt;Distance-Based Sound Separation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Katharine Patterson&lt;/b&gt;, &lt;b&gt;Kevin Wilson&lt;/b&gt;, &lt;b&gt;Scott Wisdom&lt;/b&gt;, &lt;b&gt;John R. Hershey&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2209.06096.pdf&quot;&gt;Analysis of Self-Attention Head Diversity for Conformer-Based Automatic Speech Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Kartik Audhkhasi&lt;/b&gt;, &lt;b&gt;Yinghui Huang&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;, &lt;b&gt;Pedro J. Moreno&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.07553.pdf&quot;&gt;Improving Rare Word Recognition with LM-Aware MWER Training&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Wang Weiran&lt;/b&gt;, &lt;b&gt;Tongzhou Chen&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, &lt;b&gt;Ehsan Variani&lt;/b&gt;, &lt;b&gt;Rohit Prabhavalkar&lt;/b&gt;, &lt;b&gt;W. Ronny Huang&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;, &lt;b&gt;Neeraj Gaur&lt;/b&gt;, &lt;b&gt;Sepand Mavandadi&lt;/b&gt;, &lt;b&gt;Cal Peyser&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;David Rybach&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.03409.pdf&quot;&gt;MAESTRO: Matched Speech Text Representations Through Modality Matching&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Zhehuai Chen&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Andrew Rosenberg&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;, &lt;b&gt;Pedro J. Moreno&lt;/b&gt;, &lt;b&gt;Ankur Bapna&lt;/b&gt;, &lt;b&gt;Heiga Zen&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.12668.pdf&quot;&gt;Pseudo Label is Better Than Human Label&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Dongseong Hwang&lt;/b&gt;, &lt;b&gt;Khe Chai Sim&lt;/b&gt;, &lt;b&gt;Zhouyuan Huo&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;On the Optimal Interpolation Weights for Hybrid Autoregressive Transducer Model &lt;br /&gt;  &lt;i&gt;&lt;b&gt;Ehsan Variani&lt;/b&gt;, &lt;b&gt;Michael Riley&lt;/b&gt;, &lt;b&gt;David Rybach&lt;/b&gt;, &lt;b&gt;Cyril Allauzen&lt;/b&gt;, &lt;b&gt;Tongzhou Chen&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.07556.pdf&quot;&gt;Streaming Align-Refine for Non-autoregressive Deliberation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Wang Weiran&lt;/b&gt;, &lt;b&gt;Ke Hu&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2209.06359.pdf&quot;&gt;Federated Pruning: Improving Neural Network Efficiency with Federated Learning&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Rongmei Lin&lt;a href=&quot;#1&quot; name=&quot;top1&quot;&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;Yonghui Xiao&lt;/b&gt;, &lt;b&gt;Tien-Ju Yang&lt;/b&gt;, &lt;b&gt;Ding Zhao&lt;/b&gt;, Li Xiong, &lt;b&gt;Giovanni Motta&lt;/b&gt;, &lt;b&gt;Fran&lt;/b&gt;&lt;/i&gt;&lt;i&gt;&lt;b&gt;ç&lt;/b&gt;&lt;/i&gt;&lt;i&gt;&lt;b&gt;oise Beaufays&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.06164.pdf&quot;&gt;A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Shaojin Ding&lt;/b&gt;, &lt;b&gt;Weiran Wang&lt;/b&gt;, &lt;b&gt;Ding Zhao&lt;/b&gt;, &lt;b&gt;Tara N Sainath&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;Robert David&lt;/b&gt;, &lt;b&gt;Rami Botros&lt;/b&gt;, &lt;b&gt;Xin Wang&lt;/b&gt;, &lt;b&gt;Rina Panigrahy&lt;/b&gt;, &lt;b&gt;Qiao Liang&lt;/b&gt;, &lt;b&gt;Dongseong Hwang&lt;/b&gt;, &lt;b&gt;Ian McGraw&lt;/b&gt;, &lt;b&gt;Rohit Prabhavalkar&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.15952.pdf&quot;&gt;4-Bit Conformer with Native Quantization Aware Training for Speech Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Shaojin Ding&lt;/b&gt;, &lt;b&gt;Phoenix Meadowlark&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;Lukasz Lew&lt;/b&gt;, &lt;b&gt;Shivani Agrawal&lt;/b&gt;, &lt;b&gt;Oleg Rybakov&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2207.07935.pdf&quot;&gt;Visually-Aware Acoustic Event Detection Using Heterogeneous Graphs&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Amir Shirian, &lt;b&gt;Krishna Somandepalli&lt;/b&gt;, Victor Sanchez, Tanaya Guha &lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2205.03481.pdf&quot;&gt;A Conformer-Based Waveform-Domain Neural Acoustic Echo Canceller Optimized for ASR Accuracy&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Sankaran Panchapagesan&lt;/b&gt;, &lt;b&gt;Arun Narayanan&lt;/b&gt;, &lt;b&gt;Turaj Zakizadeh Shabestary&lt;/b&gt;, &lt;b&gt;Shuai Shao&lt;/b&gt;, &lt;b&gt;Nathan Howard&lt;/b&gt;, &lt;b&gt;Alex Park&lt;/b&gt;, &lt;b&gt;James Walker&lt;/b&gt;, &lt;b&gt;Alexander Gruenstein&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://research.google/pubs/pub51530/&quot;&gt;Reducing Domain Mismatch in Self-Supervised Speech Pre-training&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Murali Karthick Baskar, &lt;b&gt;Andrew Rosenberg&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Nicolás Serrano&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://drive.google.com/file/d/1vCrHH2lfWgVzyxH0R6i9H-wPtb0GYTmx/view?usp=sharing&quot;&gt;On-the-Fly ASR Corrections with Audio Exemplars&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Golan Pundak&lt;/b&gt;, &lt;b&gt;Tsendsuren Munkhdalai&lt;/b&gt;, &lt;b&gt;Khe Chai Sim&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.13916.pdf&quot;&gt;A Language Agnostic Multilingual Streaming On-Device ASR System&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Bo Li&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, Ruoming Pang*, &lt;b&gt;Shuo-Yiin Chang&lt;/b&gt;, &lt;b&gt;Qiumin Xu&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Vince Chen&lt;/b&gt;, &lt;b&gt;Qiao Liang&lt;/b&gt;, &lt;b&gt;Heguang Liu&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;Parisa Haghani&lt;/b&gt;, &lt;b&gt;Sameer Bidichandani&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.10752.pdf&quot;&gt;XTREME-S: Evaluating Cross-Lingual Speech Representations&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Alexis Conneau&lt;/b&gt;, &lt;b&gt;Ankur Bapna&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Min Ma&lt;/b&gt;, Patrick von Platen, Anton Lozhkov, &lt;b&gt;Colin Cherry&lt;/b&gt;, &lt;b&gt;Ye Jia&lt;/b&gt;, &lt;b&gt;Clara Rivera&lt;/b&gt;, &lt;b&gt;Mihir Kale&lt;/b&gt;, &lt;b&gt;Daan van Esch&lt;/b&gt;, &lt;b&gt;Vera Axelrod&lt;/b&gt;, &lt;b&gt;Simran Khanuja&lt;/b&gt;, &lt;b&gt;Jonathan Clark&lt;/b&gt;, &lt;b&gt;Orhan Firat&lt;/b&gt;, Michael Auli, &lt;b&gt;Sebastian Ruder&lt;/b&gt;, &lt;b&gt;Jason Riesa&lt;/b&gt;, &lt;b&gt;Melvin Johnson&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.13191.pdf&quot;&gt;Towards Disentangled Speech Representations&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Cal Peyser&lt;/b&gt;, &lt;b&gt;Ronny Huang&lt;/b&gt;, &lt;b&gt;Andrew Rosenberg&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, Michael Picheny, Kyunghyun Cho &lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.03793.pdf&quot;&gt;Personal VAD 2.0: Optimizing Personal Voice Activity Detection for On-Device Speech Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Shaojin Ding&lt;/b&gt;, &lt;b&gt;Rajeev Rikhye&lt;/b&gt;, &lt;b&gt;Qiao Liang&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;Quan Wang&lt;/b&gt;, &lt;b&gt;Arun Narayanan&lt;/b&gt;, &lt;b&gt;Tom O'Malley&lt;/b&gt;, &lt;b&gt;Ian McGraw&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2209.06410.pdf&quot;&gt;A Universally-Deployable ASR Frontend for Joint Acoustic Echo Cancellation, Speech Enhancement, and Voice Separation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Tom O’Malley&lt;/b&gt;, &lt;b&gt;Arun Narayanan&lt;/b&gt;, &lt;b&gt;Quan Wang&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2208.13183&quot;&gt;Training Text-To-Speech Systems From Synthetic Data: A Practical Approach For Accent Transfer Tasks&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Lev Finkelstein&lt;/b&gt;, &lt;b&gt;Heiga Zen&lt;/b&gt;, Norman Casagrande, &lt;b&gt;Chun-an Chan&lt;/b&gt;, &lt;b&gt;Ye Jia&lt;/b&gt;, &lt;b&gt;Tom Kenter&lt;/b&gt;, &lt;b&gt;Alex Petelin&lt;/b&gt;, Jonathan Shen*,&lt;b&gt; Vincent Wan&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Yonghui Wu&lt;/b&gt;, &lt;b&gt;Robert Clark&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.12559.pdf&quot;&gt;A Scalable Model Specialization Framework for Training and Inference Using Submodels and Its Application to Speech Model Personalization&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Fadi Biadsy&lt;/b&gt;, &lt;b&gt;Youzheng Chen&lt;/b&gt;, &lt;b&gt;Xia Zhang&lt;/b&gt;, &lt;b&gt;Oleg Rybakov&lt;/b&gt;, &lt;b&gt;Andrew Rosenberg&lt;/b&gt;, &lt;b&gt;Pedro Moreno&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.05738.pdf&quot;&gt;Text-Driven Separation of Arbitrary Sounds&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Kevin Kilgour&lt;/b&gt;, &lt;b&gt;Beat Gfeller&lt;/b&gt;, &lt;b&gt;Qingqing Huang&lt;/b&gt;, &lt;b&gt;Aren Jansen&lt;/b&gt;, &lt;b&gt;Scott Wisdom&lt;/b&gt;, &lt;b&gt;Marco Tagliasacchi&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Workshops, Tutorials &amp;amp; Special  Sessions&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc&quot;&gt;The VoxCeleb Speaker Recognition Challenge 2022 (VoxSRC-22)&lt;/a&gt;&lt;br /&gt;  Organizers include: &lt;b&gt;&lt;i&gt;Arsha Nagrani&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Self-Supervised Representation Learning for Speech Processing &lt;br /&gt;  Organizers include: &lt;b&gt;&lt;i&gt;Tara Sainath&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Learning from Weak Labels &lt;br /&gt;  Organizers include: &lt;b&gt;&lt;i&gt;Ankit Shah&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.03543.pdf&quot;&gt;RNN Transducers for Named Entity Recognition with Constraints on Alignment for Understanding Medical Conversations&lt;/a&gt;&lt;br /&gt;  Authors: &lt;i&gt;&lt;b&gt;Hagen Soltau&lt;/b&gt;, &lt;b&gt;Izhak Shafran&lt;/b&gt;, &lt;b&gt;Mingqiu Wang&lt;/b&gt;, &lt;b&gt;Laurent El Shafey&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://claritychallenge.org/clarity2021-workshop/papers/Clarity_2021_paper_yang.pdf&quot;&gt;Listening with Googlears: Low-Latency Neural Multiframe Beamforming and Equalization for Hearing Aids&lt;/a&gt;&lt;br /&gt;  Authors: &lt;i&gt;&lt;b&gt;Samuel Yang&lt;/b&gt;, &lt;b&gt;Scott Wisdom&lt;/b&gt;, &lt;b&gt;Chet Gnegy&lt;/b&gt;, &lt;b&gt;Richard F. Lyon&lt;/b&gt;, &lt;b&gt;Sagar Savla&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2209.06358.pdf&quot;&gt;Using Rater and System Metadata to Explain Variance in the VoiceMOS Challenge 2022 Dataset&lt;/a&gt;&lt;br /&gt;  Authors: &lt;i&gt;&lt;b&gt;Michael Chinen&lt;/b&gt;, &lt;b&gt;Jan Skoglund&lt;/b&gt;, &lt;b&gt;Chandan K. A. Reddy&lt;/b&gt;, Alessandro Ragano, Andrew Hines &lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2110.00155.pdf&quot;&gt;Incremental Layer-Wise Self-Supervised Learning for Efficient Unsupervised Speech Domain Adaptation On Device&lt;/a&gt;&lt;br /&gt;  Authors: &lt;i&gt;&lt;b&gt;Zhouyuan Huo&lt;/b&gt;, &lt;b&gt;Dongseong Hwang&lt;/b&gt;, &lt;b&gt;Khe Chai Sim&lt;/b&gt;, &lt;b&gt;Shefali Garg&lt;/b&gt;, &lt;b&gt;Ananya Misra&lt;/b&gt;, &lt;b&gt;Nikhil Siddhartha&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Fran&lt;/b&gt;&lt;/i&gt;&lt;i&gt;&lt;b&gt;ç&lt;/b&gt;&lt;/i&gt;&lt;i&gt;&lt;b&gt;oise Beaufays&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Trustworthy Speech Processing &lt;br /&gt;  Organizers include: &lt;b&gt;&lt;i&gt;Shrikanth Narayanan&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;!--Footnotes themselves at the bottom.--&gt; &lt;hr width=&quot;80%&quot; /&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;&gt;&lt;br /&gt;  &lt;a name=&quot;1&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt;Work done while at Google.&lt;a href=&quot;#top1&quot;&gt; &amp;nbsp;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;  </content><link href="http://ai.googleblog.com/feeds/5836135189460527312/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/google-at-interspeech-2022.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5836135189460527312" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5836135189460527312" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/google-at-interspeech-2022.html" rel="alternate" title="Google at Interspeech 2022" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-2426489791409654175</id><published>2022-09-16T13:35:00.002-07:00</published><updated>2022-10-31T14:13:35.967-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="ads"/><category scheme="http://www.blogger.com/atom/ns#" term="Algorithms"/><category scheme="http://www.blogger.com/atom/ns#" term="market algorithms"/><title type="text">Robust Online Allocation with Dual Mirror Descent</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Santiago Balseiro, Staff Research Scientist, Google Research, and Associate Professor at Columbia University, and Vahab Mirrokni, Distinguished Scientist, Google Research&lt;/span&gt; &lt;p&gt;The emergence of digital technologies has transformed decision making across commercial sectors such as airlines, online retailing, and internet advertising. Today, real-time decisions need to be repeatedly made in highly uncertain and rapidly changing environments. Moreover, organizations usually have limited resources, which need to be efficiently allocated across decisions. Such problems are referred to as &lt;em&gt;online allocation problems with resource constraints&lt;/em&gt;, and applications abound. Some examples include: &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;ul&gt; &lt;li&gt;&lt;b&gt;Bidding with Budget Constraints: &lt;/b&gt;Advertisers increasingly purchase ad slots using auction-based marketplaces such as search engines and ad exchanges. A typical advertiser can participate in a large number of auctions in a given month. Because the supply in these marketplaces is uncertain, advertisers set budgets to control their total spend. Therefore, advertisers need to determine how to optimally place bids while limiting total spend and maximizing conversions.  &lt;/li&gt;&lt;li&gt;&lt;b&gt;Dynamic Ad Allocation: &lt;/b&gt;Publishers can monetize their websites by signing deals with advertisers guaranteeing a number of impressions or by auctioning off slots in the open market. To make this choice, publishers need to trade off, in real-time, the short-term revenue from selling slots in the open market and the long-term benefits of delivering good quality spots to reservation ads.  &lt;/li&gt;&lt;li&gt;&lt;b&gt;Airline Revenue Management:&lt;/b&gt; Planes have a limited number of seats that need to be filled up as much as possible before a flight’s departure. But demand for flights changes over time and airlines would like to sell airline tickets to the customers who are willing to pay the most. Thus, airlines have increasingly adopted sophisticated automated systems to manage the pricing and availability of airline tickets.   &lt;/li&gt;&lt;li&gt;&lt;b&gt;Personalized Retailing with Limited Inventories:&lt;/b&gt; Online retailers can use real-time data to personalize their offerings to customers who visit their store. Because product inventory is limited and cannot be easily replenished, retailers need to dynamically decide which products to offer and at what price to maximize their revenue while satisfying their inventory constraints. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The common feature of these problems is the presence of resource constraints (budgets, contractual obligations, seats, or inventory, respectively in the examples above) and the need to make dynamic decisions in environments &lt;a href=&quot;https://en.wikipedia.org/wiki/Knightian_uncertainty&quot;&gt;with uncertainty&lt;/a&gt;. Resource constraints are challenging because they link decisions across time — e.g., in the bidding problem, bidding too high early can leave advertisers with no budget, and thus missed opportunities later. Conversely, bidding too conservatively can result in a low number of conversions or clicks. &lt;/p&gt;  &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCOTNurjAHYOaGD6AgsHuh7Q2HDcJMVY7TkzCCZ1zebUJbkWDEG1glO-4C5UAxS3B_QvOBcaHgjY74DCu9Gt7woFDMLhi0oYChKj9bntmdyI9lDAl_HlBYG5n8wm1IjTrgUc9f3iAD_CEiYP9ezYFacEf8dXQlEiR5VxqxDu1ySlnH3BpL-dDD9dmVg/s1273/image10.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;664&quot; data-original-width=&quot;1273&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCOTNurjAHYOaGD6AgsHuh7Q2HDcJMVY7TkzCCZ1zebUJbkWDEG1glO-4C5UAxS3B_QvOBcaHgjY74DCu9Gt7woFDMLhi0oYChKj9bntmdyI9lDAl_HlBYG5n8wm1IjTrgUc9f3iAD_CEiYP9ezYFacEf8dXQlEiR5VxqxDu1ySlnH3BpL-dDD9dmVg/s16000/image10.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Two central resource allocation problems faced by advertisers and publishers in internet advertising markets.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;In this post, we discuss state-of-the-art algorithms that can help maximize goals in dynamic, resource-constrained environments. In particular, we have recently developed a new class of algorithms for online allocation problems, called &lt;em&gt;dual mirror descent&lt;/em&gt;, that are simple, robust, and flexible. Our papers have appeared in &lt;a href=&quot;https://arxiv.org/abs/2011.10124&quot;&gt;Operations Research&lt;/a&gt;, &lt;a href=&quot;https://proceedings.mlr.press/v119/balseiro20a.html&quot;&gt;ICML’20&lt;/a&gt;, and &lt;a href=&quot;https://proceedings.mlr.press/v139/balseiro21a.html&quot;&gt;ICML’21&lt;/a&gt;, and we have &lt;a href=&quot;https://arxiv.org/abs/2202.06152&quot;&gt;ongoing work&lt;/a&gt; to continue progress in this space. Compared to existing approaches, dual mirror descent is faster as it does not require solving auxiliary optimization problems, is more flexible because it can handle many applications across different sectors with minimal modifications, and is more robust as it enjoys remarkable performance under different environments. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Online Allocation Problems&lt;/h2&gt;&lt;p&gt;In an online allocation problem, a decision maker has a limited amount of total resources (&lt;span style=&quot;font-family: Times New Roman;&quot;&gt;&lt;em&gt;B&lt;/em&gt;&lt;/span&gt;) and receives a certain number of requests over time (&lt;span style=&quot;font-family: Times New Roman;&quot;&gt;&lt;em&gt;T&lt;/em&gt;&lt;/span&gt;). At any point in time (&lt;span style=&quot;font-family: Times New Roman;&quot;&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt;), the decision maker receives a reward function (&lt;span style=&quot;font-family: Times New Roman;&quot;&gt;&lt;em&gt;f&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/span&gt;)  and resource consumption function (&lt;span style=&quot;font-family: Times New Roman;&quot;&gt;&lt;em&gt;b&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/span&gt;), and takes an action (&lt;span style=&quot;font-family: Times New Roman;&quot;&gt;&lt;em&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/span&gt;). The reward and resource consumption functions change over time and the objective is to maximize the total reward within the resource constraints. If all the requests were &lt;em&gt;known in advance&lt;/em&gt;, then an &lt;em&gt;optimal&lt;/em&gt; allocation could be obtained by solving an &lt;em&gt;offline&lt;/em&gt; optimization problem for how to maximize the reward function over time within the resource constraints&lt;sup id=&quot;fnref1&quot;&gt;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;1&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;The optimal &lt;em&gt;offline&lt;/em&gt; allocation cannot be implemented in practice because it requires knowing future requests. However, this is still useful for framing the goal of &lt;em&gt;online&lt;/em&gt; allocation problems: to design an algorithm whose performance is as close to optimal as possible &lt;em&gt;without&lt;/em&gt; knowing future requests. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Achieving the Best of Many Worlds with Dual Mirror Descent&lt;/h2&gt;&lt;p&gt;A simple, yet powerful idea to handle resource constraints is introducing “prices” for the resources, which enables accounting for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Opportunity_cost&quot;&gt;opportunity cost&lt;/a&gt; of consuming resources when making decisions. For example, selling a seat on a plane today means it can’t be sold tomorrow. These prices are useful as an internal accounting system of the algorithm. They serve the purpose of coordinating decisions at different moments in time and allow decomposing&lt;a href=&quot;https://en.wikipedia.org/wiki/Lagrangian_relaxation&quot;&gt; a complex problem&lt;/a&gt; with resource constraints into simpler subproblems: one per time period with no resource constraints. For example, in a bidding problem, the prices capture an advertiser’s opportunity cost of consuming one unit of budget and allow the advertiser to handle each auction as an independent bidding problem. &lt;/p&gt;&lt;p&gt;This reframes the online allocation problem as a problem of pricing resources to enable optimal decision making. The key innovation of our algorithm is using machine learning to predict optimal prices in an online fashion: we choose prices dynamically using &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Mirror_descent&quot;&gt;mirror descent&lt;/a&gt;&lt;/em&gt;, a popular optimization algorithm for training machine learning predictive models. Because prices for resources are referred to as &quot;&lt;a href=&quot;https://en.wikipedia.org/wiki/Duality_(optimization)&quot;&gt;dual variables&lt;/a&gt;&quot; in the field of optimization, we call the resulting algorithm &lt;a href=&quot;https://arxiv.org/abs/2011.10124&quot;&gt;dual mirror descent&lt;/a&gt;.   &lt;/p&gt;&lt;p&gt;The algorithm works sequentially by assuming uniform resource consumption over time is optimal and updating the dual variables after each action. It starts at a moment in time (&lt;span style=&quot;font-family: Times New Roman;&quot;&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt;) by taking an action (&lt;span style=&quot;font-family: Times New Roman;&quot;&gt;&lt;em&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/span&gt;) that maximizes the reward minus the opportunity cost of consuming resources (shown in the top gray box below). The action (e.g., how much to bid or which ad to show) is implemented if there are enough resources available. Then, the algorithm computes the error in the resource consumption (&lt;span style=&quot;font-family: Times New Roman;&quot;&gt;&lt;em&gt;g&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/span&gt;), which is the difference between uniform consumption over time and the actual resource consumption (below in the third gray box). A new dual variable for the next time period is computed using mirror descent based on the error, which then informs the next action. Mirror descent seeks to make the error as close as possible to zero, improving the accuracy of its estimate of the dual variable, so that resources are consumed uniformly over time. While the assumption of uniform resource consumption may be surprising, it helps avoid missing good opportunities and often aligns with commercial goals so is effective. Mirror descent also allows a variety of update rules; more details are in the paper. &lt;/p&gt;    &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxmJQO-O-g_9ueC0lFFhTzjHEfy_tj68ams9V4INm_8bQrRSvzKOyKtVcE33LbvVwS4TEy0u_h3AoXGTh4k7_u6KXf-Z6gLJYSDw-FZQdfbzBnrSP9OIK4klcN3e9EA7jYoFW0Jrv3YhOXn7MyxldWm2KVu34E4pVqy4ci3fdKn1dm8UpYYfdwFEsfg/s1273/image20.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1139&quot; data-original-width=&quot;1273&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxmJQO-O-g_9ueC0lFFhTzjHEfy_tj68ams9V4INm_8bQrRSvzKOyKtVcE33LbvVwS4TEy0u_h3AoXGTh4k7_u6KXf-Z6gLJYSDw-FZQdfbzBnrSP9OIK4klcN3e9EA7jYoFW0Jrv3YhOXn7MyxldWm2KVu34E4pVqy4ci3fdKn1dm8UpYYfdwFEsfg/s16000/image20.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;An overview of the dual mirror descent algorithm.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;         &lt;p&gt;By design, dual mirror descent has a self-correcting feature that prevents depleting resources too early or waiting too long to consume resources and missing good opportunities. When a request consumes more or less resources than the target, the corresponding dual variable is increased or decreased. When resources are then priced higher or lower, future actions are chosen to consume resources more conservatively or aggressively.  &lt;/p&gt;&lt;p&gt;This algorithm is easy to implement, fast, and enjoys remarkable performance under different environments. These are some salient features of our algorithm: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;Existing methods require periodically &lt;a href=&quot;https://pubsonline.informs.org/doi/abs/10.1287/opre.2014.1289&quot;&gt;solving large auxiliary optimization problems&lt;/a&gt; using past data. In contrast, this algorithm does not need to solve any auxiliary optimization problem and has a very simple rule to update the dual variables, which, in many cases, can be run in &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_complexity&quot;&gt;linear time complexity&lt;/a&gt;. Thus, it is appealing for many real-time applications that require fast decisions.  &lt;/li&gt;&lt;li&gt;There are minimal requirements on the structure of the problem. Such flexibility allows dual mirror descent to handle many applications across different sectors with minimal modifications. Moreover, our algorithms are flexible since they accommodate different objectives, constraints, or regularizers. By &lt;a href=&quot;https://proceedings.mlr.press/v139/balseiro21a.html&quot;&gt;incorporating regularizers&lt;/a&gt;, decision makers can include important objectives beyond economic efficiency, such as fairness.  &lt;/li&gt;&lt;li&gt;Existing algorithms for online allocation problems are tailored for either adversarial or stochastic input data. Algorithms for &lt;a href=&quot;https://ieeexplore.ieee.org/document/1530720&quot;&gt;adversarial inputs&lt;/a&gt; are robust as they make almost no assumptions on the structure of the data but, in turn, obtain performance guarantees that are too pessimistic in practice. On the other hand, algorithms for &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/1566374.1566384&quot;&gt;stochastic inputs&lt;/a&gt; enjoy better performance guarantees by exploiting statistical patterns in the data but can perform poorly when the model is &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_model_specification&quot;&gt;misspecified&lt;/a&gt;. Dual mirror descent, however, attains performance close to optimal in both stochastic and adversarial input models while being oblivious to the structure of the input model. Compared to existing work on simultaneous approximation algorithms, our method &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/1.9781611973099.134&quot;&gt;is more general&lt;/a&gt;, applies to a wide range of problems, and requires &lt;a href=&quot;https://dl.acm.org/doi/10.1145/2764468.2764536&quot;&gt;no forecasts&lt;/a&gt;.  Below is a comparison of our algorithm to other state-of-the-art methods. Results are based on synthetic data for an &lt;a href=&quot;https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2014.2017&quot;&gt;ad allocation problem&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;      &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFbs6yYYocSijnXw-NrM1isG8h-PxhAyS7o8L_vJY3gG43YDoldz2Q6nvNkd4h0EuOku9t17V4nVUm-HKI7eRyAfaciWzMhzBFcdrsauJRQcDb2pf3cwE7AAcVQuY0QDjgwSR5lqmXGob5OpssT1d4VZ89s5va8p2SarAYcJtDrm8SlQ9jRg0QYqEqRQ/s1200/image12.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFbs6yYYocSijnXw-NrM1isG8h-PxhAyS7o8L_vJY3gG43YDoldz2Q6nvNkd4h0EuOku9t17V4nVUm-HKI7eRyAfaciWzMhzBFcdrsauJRQcDb2pf3cwE7AAcVQuY0QDjgwSR5lqmXGob5OpssT1d4VZ89s5va8p2SarAYcJtDrm8SlQ9jRg0QYqEqRQ/s16000/image12.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Performance of dual mirror descent, &lt;a href=&quot;https://dl.acm.org/doi/10.1145/1566374.1566384&quot;&gt;a training based method,&lt;/a&gt; and &lt;a href=&quot;https://ieeexplore.ieee.org/document/1530720&quot;&gt;an adversarial method&lt;/a&gt; relative to the optimal offline solution. Lower values indicate performance closer to the optimal offline allocation. Results are generated using synthetic experiments based on &lt;a href=&quot;https://pubsonline.informs.org/doi/10.1287/mnsc.2014.2017&quot;&gt;public data&lt;/a&gt; for an ad allocation problem.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;In this post we introduced dual mirror descent, an algorithm for online allocation problems that is simple, robust, and flexible. It is particularly notable that after a long line of work in online allocation algorithms, dual mirror descent provides a way to analyze a wider range of algorithms with superior robustness priorities compared to previous techniques.  Dual mirror descent has a wide range of applications across several commercial sectors and has been used over time at Google to help advertisers capture more value through better algorithmic decision making. We are also exploring &lt;a href=&quot;https://arxiv.org/abs/2202.06152&quot;&gt;further work&lt;/a&gt; related to mirror descent and its connections to &lt;a href=&quot;https://en.wikipedia.org/wiki/PID_controller&quot;&gt;PI controllers&lt;/a&gt;. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;We would like to thank our co-authors Haihao Lu and Balu Sivan, and Kshipra Bhawalkar for their exceptional support and contributions. We would also like to thank our collaborators in the ad quality team and market algorithm research.&lt;/em&gt;&lt;/p&gt;    &lt;!--Footnotes--&gt;&lt;hr width=&quot;80%&quot; /&gt;&lt;p&gt;&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;&gt;&lt;sup&gt;&lt;a name=&quot;fn1&quot;&gt;&lt;b&gt;1&lt;/b&gt;&lt;/a&gt;&lt;/sup&gt;Formalized in the equation below:&amp;nbsp; &lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;&gt;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;br&gt;  &lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwlilkKvmPKrZhHmes0ianEnwF-jUTHBRB25lPXRbZLR0VWTEPVPmgfY416f64hox-v0aAHrsPl5HAGcA0CbkIZprG-NHCWTFErz6YwGI54WEZO7Af523tm32J0fIp_N3t9UzMziOIDPaEtyrsFrHtfnYrYUo2dJjOh23RkdypGfK9Ax49SCBgWg238Q/s2220/Screen%20Shot%202022-09-16%20at%201.02.10%20PM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;349&quot; data-original-width=&quot;2220&quot; height=&quot;50&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwlilkKvmPKrZhHmes0ianEnwF-jUTHBRB25lPXRbZLR0VWTEPVPmgfY416f64hox-v0aAHrsPl5HAGcA0CbkIZprG-NHCWTFErz6YwGI54WEZO7Af523tm32J0fIp_N3t9UzMziOIDPaEtyrsFrHtfnYrYUo2dJjOh23RkdypGfK9Ax49SCBgWg238Q/w320-h50/Screen%20Shot%202022-09-16%20at%201.02.10%20PM.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;  &lt;/span&gt;&lt;/p&gt; &lt;!--&lt;table align=&quot;left&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwlilkKvmPKrZhHmes0ianEnwF-jUTHBRB25lPXRbZLR0VWTEPVPmgfY416f64hox-v0aAHrsPl5HAGcA0CbkIZprG-NHCWTFErz6YwGI54WEZO7Af523tm32J0fIp_N3t9UzMziOIDPaEtyrsFrHtfnYrYUo2dJjOh23RkdypGfK9Ax49SCBgWg238Q/s2220/Screen%20Shot%202022-09-16%20at%201.02.10%20PM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;349&quot; data-original-width=&quot;2220&quot; height=&quot;50&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwlilkKvmPKrZhHmes0ianEnwF-jUTHBRB25lPXRbZLR0VWTEPVPmgfY416f64hox-v0aAHrsPl5HAGcA0CbkIZprG-NHCWTFErz6YwGI54WEZO7Af523tm32J0fIp_N3t9UzMziOIDPaEtyrsFrHtfnYrYUo2dJjOh23RkdypGfK9Ax49SCBgWg238Q/w320-h50/Screen%20Shot%202022-09-16%20at%201.02.10%20PM.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;/span&gt;&lt;/p&gt;--&gt;</content><link href="http://ai.googleblog.com/feeds/2426489791409654175/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/robust-online-allocation-with-dual.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2426489791409654175" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2426489791409654175" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/robust-online-allocation-with-dual.html" rel="alternate" title="Robust Online Allocation with Dual Mirror Descent" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCOTNurjAHYOaGD6AgsHuh7Q2HDcJMVY7TkzCCZ1zebUJbkWDEG1glO-4C5UAxS3B_QvOBcaHgjY74DCu9Gt7woFDMLhi0oYChKj9bntmdyI9lDAl_HlBYG5n8wm1IjTrgUc9f3iAD_CEiYP9ezYFacEf8dXQlEiR5VxqxDu1ySlnH3BpL-dDD9dmVg/s72-c/image10.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-1528557374592447782</id><published>2022-09-15T12:16:00.005-07:00</published><updated>2022-10-31T14:17:34.201-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Multimodal Learning"/><title type="text">PaLI: Scaling Language-Image Learning in 100+ Languages</title><content type="html">&lt;span class=&quot;byline-author&quot;&gt;Posted by Xi Chen and Xiao Wang, Software Engineers, Google Research&lt;/span&gt; &lt;p&gt;Advanced language models (e.g., &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;GPT&lt;/a&gt;, &lt;a href=&quot;https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html&quot;&gt;GLaM&lt;/a&gt;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;&gt;PaLM&lt;/a&gt; and &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;&gt;T5&lt;/a&gt;) have demonstrated diverse capabilities and achieved impressive results across tasks and languages by scaling up their number of parameters. Vision-language (VL) models can benefit from similar scaling to address many tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_generation#Image_Captioning&quot;&gt;image captioning&lt;/a&gt;, visual &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;&gt;question answering&lt;/a&gt; (VQA), &lt;a href=&quot;https://en.wikipedia.org/wiki/Outline_of_object_recognition&quot;&gt;object recognition&lt;/a&gt;, and in-context &lt;a href=&quot;https://en.wikipedia.org/wiki/Optical_character_recognition&quot;&gt;optical-character-recognition&lt;/a&gt; (OCR). Increasing the success rates for these practical tasks is important for everyday interactions and applications. Furthermore, for a truly universal system, vision-language models should be able to operate in many languages, not just one. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;&gt;PaLI: A Jointly-Scaled Multilingual Language-Image Model&lt;/a&gt;”, we introduce a unified language-image model trained to perform many tasks and in over 100 languages. These tasks span vision, language, and multimodal image and language applications, such as &lt;a href=&quot;https://arxiv.org/abs/1505.00468&quot;&gt;visual question answering&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_generation#Image_Captioning&quot;&gt;image captioning&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Object_detection&quot;&gt;object detection&lt;/a&gt;, &lt;a href=&quot;https://paperswithcode.com/task/image-classification&quot;&gt;image classification&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Optical_character_recognition&quot;&gt;OCR&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning&quot;&gt;text reasoning&lt;/a&gt;, and others. Furthermore, we use a collection of public images that includes automatically collected annotations in 109 languages, which we call the WebLI dataset. The PaLI model pre-trained on WebLI achieves state-of-the-art performance on challenging image and language benchmarks, such as &lt;a href=&quot;https://arxiv.org/abs/1504.00325&quot;&gt;COCO-Captions&lt;/a&gt;,&amp;nbsp;&lt;a href=&quot;https://textvqa.org/textcaps/&quot;&gt;TextCaps&lt;/a&gt;, &lt;a href=&quot;https://visualqa.org/&quot;&gt;VQAv2&lt;/a&gt;, &lt;a href=&quot;https://okvqa.allenai.org/&quot;&gt;OK-VQA&lt;/a&gt;, &lt;a href=&quot;https://textvqa.org/&quot;&gt;TextVQA&lt;/a&gt; and others. It also outperforms prior models’ multilingual visual captioning and visual question answering benchmarks. &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Overview&lt;/h2&gt;&lt;p&gt;One goal of this project is to examine how language and vision models interact at scale and specifically the scalability of language-image models. We explore both per-modality scaling and the resulting cross-modal interactions of scaling. We train our largest model to 17 billion (17B) parameters, where the visual component is scaled up to 4B parameters and the language model to 13B.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The PaLI model architecture is simple, reusable and scalable. It consists of a &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;Transformer&lt;/a&gt; encoder that processes the input text, and an auto-regressive Transformer decoder that generates the output text. To process images, the input to the Transformer encoder also includes &quot;visual words&quot; that represent an image processed by a &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;Vision Transformer&lt;/a&gt; (ViT). A key component of the PaLI model is reuse, in which we seed the model with weights from previously-trained uni-modal vision and language models, such as &lt;a href=&quot;https://arxiv.org/abs/2010.11934&quot;&gt;mT5-XXL&lt;/a&gt; and large &lt;a href=&quot;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&quot;&gt;ViTs&lt;/a&gt;. This reuse not only enables the transfer of capabilities from uni-modal training, but also saves computational cost.  &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Pn9OEBEctQryii0Xw5Uj5Dqq6GUBAu5YcVqcv9zObFj3-VcYYy4w58MEKFHPorcr0kDUoMaldwUdwDr4eEL-0y-79hSf-6TUuS59dgEsvCarxv8U4gI3VvsxJ7We73J794BalwvaK7ao8o3FXDwlynySsmBRzCcwPhUbRptT36QUm17ahjAydIzbnw/s1600/LILM%20%20PaLI%2006.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;454&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Pn9OEBEctQryii0Xw5Uj5Dqq6GUBAu5YcVqcv9zObFj3-VcYYy4w58MEKFHPorcr0kDUoMaldwUdwDr4eEL-0y-79hSf-6TUuS59dgEsvCarxv8U4gI3VvsxJ7We73J794BalwvaK7ao8o3FXDwlynySsmBRzCcwPhUbRptT36QUm17ahjAydIzbnw/s16000/LILM%20%20PaLI%2006.gif&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The PaLI model addresses a wide range of tasks in the language-image, language-only and image-only domain using the same API (e.g., visual-question answering, image captioning, scene-text understanding, etc.). The model is trained to support over 100 languages and tuned to perform multilingually for multiple language-image tasks.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;   &lt;h2&gt;Dataset: Language-Image Understanding in 100+ Languages&lt;/h2&gt;&lt;p&gt;Scaling studies for deep learning show that larger models require larger datasets to train effectively. To unlock the potential of language-image pretraining, we construct WebLI, a multilingual language-image dataset built from images and text available on the public web. &lt;/p&gt;&lt;p&gt;WebLI scales up the text language from English-only datasets to 109 languages, which enables us to perform downstream tasks in many languages. The data collection process is similar to that employed by other datasets, e.g. &lt;a href=&quot;https://arxiv.org/abs/2102.05918&quot;&gt;ALIGN&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2111.07991&quot;&gt;LiT&lt;/a&gt;, and enabled us to scale the WebLI dataset to 10 billion images and 12 billion alt-texts.  &lt;/p&gt;&lt;p&gt;In addition to annotation with web text, we apply the &lt;a href=&quot;https://cloud.google.com/vision&quot;&gt;Cloud Vision API&lt;/a&gt; to perform OCR on the images, leading to 29 billion image-OCR pairs. We perform near-deduplication of the images against the train, validation and test splits of 68 common vision and vision-language datasets, to avoid leaking data from downstream evaluation tasks, as is standard in the literature. To further improve the data quality, we score image and alt-text pairs based on their cross-modal similarity, and tune the threshold to keep only 10% of the images, for a total of 1 billion images used for training PaLI. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigSmeBge-LPNYi7dcLEgt3DKUPHeq4uPWd4WUCozQzAmhs3-pEVuZav3vxz8qH0Kid8IL5DqTC5n_FO3LWpIVT_vlSFp6mqOh-0HMw8RrSs6FNbEBitPN7_ih-2I7HS85BNJdf2mxLliZokGa75EyJB2uG-2wIsx4GCjB5zUfkBYFZ-D8G8TrRrwSicg/s1999/image2.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;691&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigSmeBge-LPNYi7dcLEgt3DKUPHeq4uPWd4WUCozQzAmhs3-pEVuZav3vxz8qH0Kid8IL5DqTC5n_FO3LWpIVT_vlSFp6mqOh-0HMw8RrSs6FNbEBitPN7_ih-2I7HS85BNJdf2mxLliZokGa75EyJB2uG-2wIsx4GCjB5zUfkBYFZ-D8G8TrRrwSicg/s16000/image2.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Sampled images from WebLI associated with multilingual alt-text and OCR. The second image is by jopradier (&lt;a href=&quot;https://www.geneanet.org/cartes-postales/view/6573969#0&quot;&gt;original&lt;/a&gt;), used under the&lt;a href=&quot;https://www.creativecommons.org/licenses/by-nc-sa/2.0/fr/deed.en&quot;&gt; CC BY-NC-SA 2.0 license&lt;/a&gt;. Remaining images are also used with permission.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgx8WYm-cmOCCHfoTOd2NK51zUXhW8qN9SwaXFBM477IQl6t4hAJ5BT0Oa089szfVlbMOw2_shK5zqdX8HAxP5ALKpn9LGzi4Fu7HNI28TgBhEbJcu9YhYMADzS4vqjjkP-WE2G5gw4_iz_4v-pdZe92Lwk0i17JGnosqG3Jezw5XKrafZPwpaFwerd0A/s1999/image3.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;626&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgx8WYm-cmOCCHfoTOd2NK51zUXhW8qN9SwaXFBM477IQl6t4hAJ5BT0Oa089szfVlbMOw2_shK5zqdX8HAxP5ALKpn9LGzi4Fu7HNI28TgBhEbJcu9YhYMADzS4vqjjkP-WE2G5gw4_iz_4v-pdZe92Lwk0i17JGnosqG3Jezw5XKrafZPwpaFwerd0A/s16000/image3.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Statistics of recognized languages from alt-text and OCR in WebLI.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbKpPjtzNzu7HuitGdREtnSheTGYlSPYOhgvVCiCIrc9enygtEewRkz3DeOg4wiiQQ0TE3lPJCVhc4zLPtpRrs2RfL7fh2EEEALOJWV-PVjyTRtF0atTuuoMGo3J1EUjjvSYTnPV3xELnWq0gHJz7evyaL5O9lR1Fc1Y6-wcCrjtF2_HM4h2wKDf8Wcg/s954/image5.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;686&quot; data-original-width=&quot;954&quot; height=&quot;288&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbKpPjtzNzu7HuitGdREtnSheTGYlSPYOhgvVCiCIrc9enygtEewRkz3DeOg4wiiQQ0TE3lPJCVhc4zLPtpRrs2RfL7fh2EEEALOJWV-PVjyTRtF0atTuuoMGo3J1EUjjvSYTnPV3xELnWq0gHJz7evyaL5O9lR1Fc1Y6-wcCrjtF2_HM4h2wKDf8Wcg/w400-h288/image5.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Image-text pair counts of WebLI and other large-scale vision-language datasets, &lt;a href=&quot;https://openai.com/blog/clip/&quot;&gt;CLIP&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2102.05918&quot;&gt;ALIGN&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2111.07991&quot;&gt;LiT&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Training Large Language-Image Models&lt;/h2&gt;&lt;p&gt;Vision-language tasks require different capabilities and sometimes have diverging goals. Some tasks inherently require localization of objects to solve the task accurately, whereas some other tasks might need a more global view. Similarly, different tasks might require either long or compact answers. To address all of these objectives, we leverage the richness of the WebLI pre-training data and introduce a mixture of pre-training tasks, which prepare the model for a variety of downstream applications. To accomplish the goal of solving a wide variety of tasks, we enable knowledge-sharing between multiple image and language tasks by casting all tasks into a single generalized API (input: image + text; output: text), which is also shared with the pretraining setup. The objectives used for pre-training are cast into the same API as a weighted mixture aimed at both maintaining the ability of the reused model components and training the model to perform new tasks (e.g., split-captioning for image description, OCR prediction for scene-text comprehension, VQG and VQA prediction). &lt;/p&gt;&lt;p&gt;The model is trained in &lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; with &lt;a href=&quot;https://github.com/google/flax&quot;&gt;Flax&lt;/a&gt; using the open-sourced &lt;a href=&quot;https://github.com/google-research/t5x&quot;&gt;T5X&lt;/a&gt; and &lt;a href=&quot;https://github.com/google/flaxformer&quot;&gt;Flaxformer&lt;/a&gt; framework. For the visual component, we introduce and train a large &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;ViT&lt;/a&gt; architecture, named ViT-e, with 4B parameters using the open-sourced &lt;a href=&quot;https://github.com/google-research/big_vision&quot;&gt;BigVision&lt;/a&gt; framework. ViT-e follows the same recipe as the &lt;a href=&quot;https://arxiv.org/abs/2106.04560&quot;&gt;ViT-G&lt;/a&gt; architecture (which has 2B parameters). For the language component, we concatenate the dense token embeddings with the patch embeddings produced by the visual component, together as the input to the multimodal encoder-decoder, which is initialized from mT5-XXL. During the training of PaLI, the weights of this visual component are frozen, and only the weights of the multimodal encoder-decoder are updated.&lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Results&lt;/h2&gt;&lt;p&gt;We compare PaLI on common vision-language benchmarks that are varied and challenging. The PaLI model achieves state-of-the-art results on these tasks, even outperforming very large models in the literature. For example, it outperforms the &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;&gt;Flamingo&lt;/a&gt; model, which is several times larger (80B parameters), on several VQA and image-captioning tasks, and it also sustains performance on challenging language-only and vision-only tasks, which were not the main training objective. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf8R7SdVR3WqhSPWvKyWsVh9mVgHhxtwUaFjB4WuqVocEZtmqd7pOfAcucfzPBO---KdLKRNbEJ-j-Mp1sG-QzQTJS0pduBf_AfIX9wQeGqnStzsPKQz-caYkCC8GN9dQbmKHZwZx_dXdtqSnyBWAqdDvJFLx9Oe-JhqeYPk_U-T8_dvCppdtfZeOJdw/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;840&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf8R7SdVR3WqhSPWvKyWsVh9mVgHhxtwUaFjB4WuqVocEZtmqd7pOfAcucfzPBO---KdLKRNbEJ-j-Mp1sG-QzQTJS0pduBf_AfIX9wQeGqnStzsPKQz-caYkCC8GN9dQbmKHZwZx_dXdtqSnyBWAqdDvJFLx9Oe-JhqeYPk_U-T8_dvCppdtfZeOJdw/s16000/image7.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;PaLI (17B parameters) outperforms the state-of-the-art approaches (including &lt;a href=&quot;https://arxiv.org/abs/2108.10904&quot;&gt;SimVLM&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2205.01917&quot;&gt;CoCa&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2205.14100.pdf&quot;&gt;GIT2&lt;/a&gt;, &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;&gt;Flamingo&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2208.10442&quot;&gt;BEiT3&lt;/a&gt;) on multiple vision-and-language tasks. In this plot we show the absolute score differences compared with the previous best model to highlight the relative improvements of PaLI. Comparison is on the official test splits when available. &lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;&gt;CIDEr&lt;/a&gt; score is used for evaluation of the image captioning tasks, whereas VQA tasks are evaluated by VQA Accuracy.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Model Scaling Results&lt;/h2&gt;&lt;p&gt;We examine how the image and language model components interact with each other with regards to model scaling and where the model yields the most gains. We conclude that scaling both components jointly results in the best performance, and specifically, scaling the visual component, which requires relatively few parameters, is most essential. Scaling is also critical for better performance across multilingual tasks. &lt;/p&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wQNoCEBG5hbvM8OYjFwlcTdG51euqx4pkhBMOU93J16ZAhWf7n5knnCL97H5-x8U-JqPKIvtrA6bT0ESuVGyCsZfcjLtJ3qhWa7oZQm1JUiCOTLNfv1zq6aqLCowh4RfvnoqSRyi1htt0sqSb0A-PYMav-BJ1cOEWffi4LUd8Fecjd2eLJJR9kFx/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1007&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wQNoCEBG5hbvM8OYjFwlcTdG51euqx4pkhBMOU93J16ZAhWf7n5knnCL97H5-x8U-JqPKIvtrA6bT0ESuVGyCsZfcjLtJ3qhWa7oZQm1JUiCOTLNfv1zq6aqLCowh4RfvnoqSRyi1htt0sqSb0A-PYMav-BJ1cOEWffi4LUd8Fecjd2eLJJR9kFx/s16000/image1.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Scaling both the language and the visual components of the PaLI model contribute to improved performance. The plot shows the score differences compared to the PaLI-3B model: &lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;&gt;CIDEr&lt;/a&gt; score is used for evaluation of the image captioning tasks, whereas VQA tasks are evaluated by VQA Accuracy.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVOj5hGOaPKkUpgzeaUJMEBOA91dvGz9vGgbmP3QerXumL1qvQEVfL3cgAp4XhltbEL9DCTTmhdJyZNKb31XehcQn7bpTWxx5PPWQ_Mgb39eLXxUFuHFtm32drQdMbomNV708oe_rBDzd4tXFVqY39VtP3g6cTEZjaRSXNpgGC89w4k1L0ikBdBrBh/s1574/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;732&quot; data-original-width=&quot;1574&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVOj5hGOaPKkUpgzeaUJMEBOA91dvGz9vGgbmP3QerXumL1qvQEVfL3cgAp4XhltbEL9DCTTmhdJyZNKb31XehcQn7bpTWxx5PPWQ_Mgb39eLXxUFuHFtm32drQdMbomNV708oe_rBDzd4tXFVqY39VtP3g6cTEZjaRSXNpgGC89w4k1L0ikBdBrBh/s16000/image4.png&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Multilingual captioning greatly benefits from scaling the PaLI models. We evaluate PaLI on a 35-language benchmark Crossmodal-3600. Here we present the average score over all 35 languages and the individual score for seven diverse languages.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Model Introspection: Model Fairness, Biases, and Other Potential Issues&lt;/h2&gt;&lt;p&gt;To avoid creating or reinforcing unfair bias within large language and image models, important first steps are to (1) be transparent about the data that were used and how the model used those data, and (2) test for model fairness and conduct responsible data analyses. To address (1), our &lt;a href=&quot;http://arxiv.org/abs/2209.06794&quot;&gt;paper&lt;/a&gt; includes a &lt;a href=&quot;https://arxiv.org/abs/2204.01075&quot;&gt;data card&lt;/a&gt; and &lt;a href=&quot;https://modelcards.withgoogle.com/about&quot;&gt;model card&lt;/a&gt;. To address (2), the paper includes results of demographic analyses of the dataset. We consider this a first step and know that it will be important to continue to measure and mitigate potential biases as we apply our model to new tasks, in alignment with our &lt;a href=&quot;http://ai.google/principles&quot;&gt;AI Principles&lt;/a&gt;.  &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We presented PaLI, a scalable multi-modal and multilingual model designed for solving a variety of vision-language tasks. We demonstrate improved performance across visual-, language- and vision-language tasks. Our work illustrates the importance of scale in both the visual and language parts of the model and the interplay between the two. We see that accomplishing vision and language tasks, especially in multiple languages, actually requires large scale models and data, and will potentially benefit from further scaling. We hope this work inspires further research in multi-modal and multilingual models.  &lt;/p&gt;&lt;div style=&quot;line-height:40%;&quot;&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;We thank all the authors who conducted this research Soravit (Beer) Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut. We also thank Claire Cui, Slav Petrov, Tania Bedrax-Weiss, Joelle Barral, Tom Duerig, Paul Natsev, Fernando Pereira, Jeff Dean, Jeremiah Harmsen, Zoubin Ghahramani, Erica Moreira, Victor Gomes, Sarah Laszlo, Kathy Meier-Hellstern, Susanna Ricco, Rich Lee, Austin Tarango, Emily Denton, Bo Pang, Wei Li, Jihyung Kil, Tomer Levinboim, Julien Amelot, Zhenhai Zhu, Xiangning Chen, Liang Chen, Filip Pavetic, Daniel Keysers, Matthias Minderer, Josip Djolonga, Ibrahim Alabdulmohsin, Mostafa Dehghani, Yi Tay, Elizabeth Adkison, James Cockerille, Eric Ni, Anna Davies, and Maysam Moussalem for their suggestions, improvements and support. We thank Tom Small for providing visualizations for the blogpost. &lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/1528557374592447782/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html#comment-form" rel="replies" title="0 Comments" type="text/html"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1528557374592447782" rel="edit" type="application/atom+xml"><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1528557374592447782" rel="self" type="application/atom+xml"><link href="http://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html" rel="alternate" title="PaLI: Scaling Language-Image Learning in 100+ Languages" type="text/html"><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Pn9OEBEctQryii0Xw5Uj5Dqq6GUBAu5YcVqcv9zObFj3-VcYYy4w58MEKFHPorcr0kDUoMaldwUdwDr4eEL-0y-79hSf-6TUuS59dgEsvCarxv8U4gI3VvsxJ7We73J794BalwvaK7ao8o3FXDwlynySsmBRzCcwPhUbRptT36QUm17ahjAydIzbnw/s72-c/LILM%20%20PaLI%2006.gif" width="72"/><thr:total>0</thr:total></entry></feed>